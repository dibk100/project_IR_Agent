{"task": "time-series classification", "instruction": "As a project manager overseeing a cutting-edge machine learning team, your objective is to develop a high-performance time series classification model for the real-world Heartbeat dataset. The dataset, divided into train, validation, and test sets with intricate dynamics (INPUT_SEQ_LEN=405 and INPUT_DIM=61), requires expertise in extracting temporal dependencies. The challenge lies in achieving superior accuracy while handling complex patterns and classifying sequences into binary labels ({0, 1}). The team must not only design the model but also optimize its performance across various time lags, ensuring generalization across the unseen test set. Develop, evaluate, and document the entire process for future reference.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"real-world Heartbeat dataset\",\n        \"description\": \"Develop a high-performance time series classification model for the Heartbeat dataset. The dataset is divided into train, validation, and test sets with INPUT_SEQ_LEN=405 and INPUT_DIM=61. The task involves extracting temporal dependencies, handling complex patterns, and achieving binary classification (0, 1) while ensuring generalization across test set for various time lags.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"time lags optimization\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Heartbeat\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"binary labels (0, 1)\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 405,\n                \"INPUT_DIM\": 61\n            },\n            \"description\": \"A real-world Heartbeat dataset with train, validation, and test sets, emphasizing intricate dynamics and temporal dependencies.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"temporal dependency handling\": \"expert-level\",\n                \"complex pattern recognition\": \"expert-level\"\n            },\n            \"description\": \"A high-performance time series classification model designed for the Heartbeat dataset, capable of extracting dependencies and handling complex patterns.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"temporal feature extraction\", \"lag-based optimization\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Document the entire development, evaluation, and optimization process.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction was parsed into a JSON format based on the schema. The user's intent is to build a model with high expertise, and the problem statement covers the details of the task, datasets, and requirements. However, specific performance metrics and complexity optimization levels are not provided, which might be obtained from the user or derived from benchmarking similar models.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning project manager, your objective is to revamp the image classification system for the Galaxy Zoo dataset. Mandate the team to employ a novel, fine-tuned Vision Transformer (ViT-XT) architecture, integrating multi-scale feature extraction and adversarial training. Emphasize the system's robustness against noisy data, while maintaining at least 95% accuracy on validation set, and its adaptability to classify rare celestial phenomena for enhanced astronomical research.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Revamp the image classification system for the Galaxy Zoo dataset, using a novel fine-tuned Vision Transformer (ViT-XT) architecture, with multi-scale feature extraction and adversarial training. The system should demonstrate robustness against noisy data, while achieving at least 95% accuracy on the validation set, and the ability to classify rare celestial phenomena for enhanced astronomical research.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_format\": \"galaxy images\",\n                \"preprocessing_steps\": \"multi-scale feature extraction\"\n            },\n            \"description\": \"The Galaxy Zoo dataset for image classification, focusing on astronomical images.\",\n            \"preprocessing\": [\"multi-scale feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ViT-XT\",\n            \"family\": \"Vision Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novel_features\": \"multi-scale feature extraction\",\n                \"training_method\": \"adversarial\"\n            },\n            \"description\": \"A fine-tuned Vision Transformer (ViT-XT) designed for the Galaxy Zoo dataset, targeting robustness and accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Importance of robustness against noisy data for reliable astronomical research\",\n        \"Goal of classifying rare celestial phenomena for scientific advancements\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adversarial\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"image classification for celestial objects\"],\n        \"demonstration\": \"Ability to classify rare astronomical objects\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure was accurately mapped to the user's instruction, indicating a high-level machine learning project manager expertise. Performance targets, like accuracy and adversarial training, were included as well as the objective to classify rare celestial objects. The dataset requirements, model specifications, and service details were inferred based on the instruction. However, the deployment endpoint is not specified as it was not mentioned in the user's instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "Utilize the OpenSubtitles corpus for developing a real-time, multimodal translation system. The BERT-based model must specialize in English-Mandarin conversion, preserving context and handling not only slang but also regional dialects and informal speech with at least 95% precision in preserving cultural nuances and idiomatic expressions in conversations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"real-time, multimodal conversation\",\n        \"description\": \"Develop a real-time, multimodal translation system using the OpenSubtitles corpus, specifically focusing on English-Mandarin conversion. The BERT-based model should preserve context, handle slang, regional dialects, and informal speech, while maintaining at least 95% precision in capturing cultural nuances and idiomatic expressions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95,\n                \"unit\": \"%\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": \"English-Mandarin\",\n                \"corpus_type\": \"real-time conversations\"\n            },\n            \"description\": \"A large-scale multilingual corpus for training a real-time translation system, with a focus on English-Mandarin dialogues and preserving cultural context.\",\n            \"preprocessing\": [\n                \"BERT pre-processing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-based\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_type\": \"Transformer\",\n                \"language_pair\": \"English-Mandarin\",\n                \"contextual\": true,\n                \"dialect_handling\": true\n            },\n            \"description\": \"A BERT-based model tailored for real-time English-Mandarin translation with emphasis on context preservation and handling slang, regional dialects, and informal speech.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"context\": \"Cultural nuances and idiomatic expressions\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time deployment\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"on-device or cloud-based\"\n        ],\n        \"demonstration\": \"Real-time conversation translation with emphasis on cultural accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction was parsed accurately, capturing the focus on real-time translation, BERT-based model, target languages, and performance metrics. The high expertise level suggests a more advanced understanding of AI and ML concepts, and the response includes reasonable assumptions on deployment and inference engines. However, the specific deployment endpoint is not provided, leaving it unclear where the model will be deployed.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "For a highly specialized project focused on classifying a diverse collection of endangered and folklore cat species in ancient illustrations, please design a hybrid deep learning model that not only differentiates between domestic felines, wildcats, and lesser-known breeds, but also accurately identifies cryptic species with unique physical features and cultural significance. The model should account for variations in artistic styles, antique filters, and low-resolution images, and be accompanied by a comprehensive annotation system for manual validation and a user-friendly interface for non-experts to explore and learn about these captivating creatures.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"art history, endangered species, folklore\",\n        \"description\": \"Design a hybrid deep learning model to classify endangered and folklore cat species in ancient illustrations, differentiating between domestic felines, wildcats, lesser-known breeds, and identifying cryptic species with unique physical features. The model must handle variations in artistic styles, antique filters, and low-resolution images.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"annotation system efficiency\",\n                \"value\": null,\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ancient Cat Illustrations\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"endangered species category\", \"folklore classification\"],\n            \"specification\": {\n                \"image_style_diversity\": true,\n                \"antique_filter_variations\": true,\n                \"low_resolution\": true\n            },\n            \"description\": \"A collection of diverse endangered and folklore cat species from ancient illustrations, including variations in artistic styles and quality.\",\n            \"preprocessing\": [\"artistic style normalization\", \"filter mitigation\"],\n            \"augmentation\": [\"low-resolution image enhancement\"],\n            \"visualization\": [\"model interpretability for art experts\", \"annotation comparison\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Deep Learning Model (e.g., CatsNet-Hybrid)\",\n            \"family\": \"convolutional neural networks (CNN) and recurrent neural networks (RNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"dual-modal (image and metadata)\",\n                \"resilience_to_artistic_variations\": true\n            },\n            \"description\": \"A deep learning model designed to classify endangered and folklore cats, accounting for artistic variations and low-quality images.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cryptic species recognition and the importance of cultural significance in annotations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"transfer learning for low-resource species\"],\n        \"target_device\": \"user-friendly, potentially cloud or web-based\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"deterministic, GPU-optimized\"],\n        \"demonstration\": \"interactive image exploration and educational content for non-experts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction specifies a highly specialized project with requirements for high accuracy and handling unique challenges. The user's high expertise level suggests they are well-informed but may need help with specific details such as performance targets, deployment platform, and model architecture. The provided JSON response captures the essence of the project while acknowledging potential areas for clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager overseeing a specialized machine learning team, you are tasked with developing an advanced time-series forecasting model for the ETTm2 dataset, a complex real-world time series benchmark. The data has been meticulously divided into train, validation, and test sets, each with unique INPUT_SEQ_LEN (96) and INPUT_DIM (7) characteristics. The model must exhibit prowess in capturing long-term patterns while predicting PRED_SEQ_LEN (96) steps ahead with a focus on both mean squared error (MSE) and mean absolute error (MAE) metrics. In addition to standard performance, request a model that exhibits interpretability and adaptability, ensuring it can handle irregularities and seasonality while maintaining state-of-the-art efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-world time series data\",\n        \"description\": \"Develop an advanced time-series forecasting model for the ETTm2 dataset, which is a complex benchmark with train, validation, and test sets. The model must handle sequences of length 96 (INPUT_SEQ_LEN), input dimensions 7 (INPUT_DIM), and predict 96 steps ahead (PRED_SEQ_LEN). Focus on performance metrics like mean squared error (MSE) and mean absolute error (MAE), with a requirement for interpretability, adaptability, and efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"comprehensible\"\n            },\n            {\n                \"name\": \"adaptability\",\n                \"value\": null,\n                \"unit\": \"environmental changes\"\n            },\n            {\n                \"name\": \"efficiency\",\n                \"value\": null,\n                \"unit\": \"state-of-the-art\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 96\n            },\n            \"description\": \"The ETTm2 dataset, a complex real-world time series benchmark divided into train, validation, and test sets with given characteristics.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"long-term pattern capture\": \"true\",\n                \"interpretable\": null,\n                \"seasonality handling\": \"true\",\n                \"efficiency\": \"state-of-the-art\"\n            },\n            \"description\": \"An advanced time-series forecasting model targeting ETTm2 dataset's requirements for performance, interpretability, adaptability, and efficiency.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"explanatory visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was created based on the provided instruction, mapping the given task to a 'build' intent. A 'high' expertise level is assumed due to the complexity of the project. Performance and complexity metrics are outlined, but specific values are not provided as they would require additional information. The model's interpretability, adaptability, and efficiency are explicitly sought after. The 'service' section is kept general as some details, such as deployment, are not specified in the instruction.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "The upcoming project involves enhancing a transformer-based machine translation system using the extensive Common Crawl dataset, focusing on English-to-Japanese translations. However, in addition to general fluency, the model must excel in deciphering and reproducing colloquial expressions with a minimum accuracy of 98%, considering cultural nuances and regional variations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"The project aims to enhance a transformer-based machine translation system, focusing on English-to-Japanese translations. It requires high performance in capturing colloquial expressions, maintaining cultural nuances, and achieving a minimum accuracy of 98%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl corpus\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"total_examples\": \"extensive\",\n                    \"language\": \"English-Japanese\"\n                },\n                \"domain\": \"general and colloquial language\"\n            },\n            \"description\": \"A large and diverse text corpus from Common Crawl for English-to-Japanese translation with emphasis on colloquial expressions.\",\n            \"preprocessing\": [\"data cleaning, tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based system (enhanced)\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"focus_on\": [\n                    \"colloquial expressions\",\n                    \"cultural nuances\"\n                ],\n                \"minimum_accuracy\": 0.98\n            },\n            \"description\": \"An advanced transformer model for English-to-Japanese machine translation, emphasizing colloquial expression translation with a target accuracy of 98%.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Understanding the importance of cultural nuances and regional variations in colloquial expression translation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"targeted colloquial expression handling\"],\n        \"target_device\": \"cloud or high-performance computing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated for faster translation\"],\n        \"demonstration\": \"Translation examples showcasing colloquial expressions and high accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear task to enhance a transformer model for specific language translation. High expertise suggests a solid understanding of the problem. Performance metrics and dataset specifics were included. Some assumptions were made about the feature engineering and target device for service needs.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for a cutting-edge AI research team, your objective is to develop a tailored text classifier for the \"Polaris AI Generation Attribution (PAGA)\" dataset. This exclusive collection contains diverse, manually annotated texts distinguishing human-written passages (label 0) from state-of-the-art AI-generated content (label 1). Mandate peak precision and recall, utilizing advanced algorithms and evaluating on a novel, adversarially crafted subset of the dataset.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"The project objective is to develop a tailored text classifier for the Polaris AI Generation Attribution (PAGA) dataset, which differentiates human-written passages from AI-generated content. Peak precision and recall are mandatory, and the model should be evaluated on an adversarially crafted subset of the dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 1.0\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 1.0\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Polaris AI Generation Attribution\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"human-written\", \"AI-generated\"],\n            \"specification\": {\n                \"annotation_types\": [\"human-written\", \"AI-generated\"]\n            },\n            \"description\": \"A dataset for text classification containing human-written and AI-generated passages, with adversarial annotations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"algorithm\": \"advanced\"\n            },\n            \"description\": \"A state-of-the-art text classifier specifically designed for distinguishing human-written from AI-generated content on the Polaris AI Generation Attribution dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"on adversarial subset of the dataset\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a clear 'build' intent, targeting a text classification project for an AI research team with high-level expertise. Performance requirements for precision and recall are explicit, and the model should be evaluated on a specific adversarial subset. The 'service' section assumes that device deployment and inference engines will be decided based on team preferences or requirements. However, it lacks explicit details for demonstration.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for the machine learning development team, your goal is to enhance the Weather time-series forecasting project by introducing a real-world constraint: the model must incorporate seasonality and trend analysis while maintaining interpretability. Use interpretable deep learning techniques like Explainable Artificial Neural Networks (X-ANNs) with a 96-step input and 96-step prediction, and optimize for both MSE and MAE, ensuring a minimum 10% improvement over baseline models on the validation set.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"weather forecasting\",\n        \"description\": \"Enhance the Weather time-series forecasting project by adding seasonality and trend analysis. Use interpretable deep learning techniques like Explainable Artificial Neural Networks (X-ANNs) with 96-step input and 96-step prediction. Optimize for both mean squared error (MSE) and mean absolute error (MAE), targeting a minimum 10% improvement over baseline models on the validation set.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": 1.0\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Weather Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": \"varies (weather data dimensions)\",\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": \"varies (weather forecast dimensions)\"\n            },\n            \"description\": \"Weather data including historical data for incorporating seasonality and trend analysis.\",\n            \"preprocessing\": [\"seasonality and trend analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Explainable Artificial Neural Network (X-ANN)\",\n            \"family\": \"interpretable deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"input_size\": \"96 (Input_SEQ_LEN)\",\n                \"output_size\": \"96 (PRED_SEQ_LEN)\",\n                \"interpretability\": \"ensured by X-ANN technique\"\n            },\n            \"description\": \"A model that incorporates seasonality and trend analysis in weather forecasting while maintaining interpretability.\"\n        }\n    ],\n    \"knowledge\": [\"Seasonality and trend analysis techniques, X-ANN model interpretability methods\"],\n    \"service\": {\n        \"feature_engineering\": [\"seasonality and trend analysis\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"visualization of interpretable model outputs and performance improvement\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the requirements of the Weather forecasting project, including the use of X-ANNs, input and prediction lengths, and the need for interpretablility with a focus on performance improvement. The high expertise level in the user indicates the level of involvement and understanding. However, details about the target variables, data dimensions, and specific deployment details are not provided as they were not specified in the input.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For a boutique grocery store, mandate a state-of-the-art YOLOv5.1 model, fine-tuned exclusively on the \"BoutiqueFoods Shelf Insights Dataset\", which necessitates differentiating among 150 unique gourmet product categories. This model should exhibit a minimum precision of 95% and recall of 93%, with inventory real-time synchronization integrated into the system, ensuring inventory updates for each artisanal product take no longer than 1.5 seconds per high-resolution shelf image.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"boutique grocery\",\n        \"description\": \"The user requires a state-of-the-art YOLOv5.1 model, specifically fine-tuned for the 'BoutiqueFoods Shelf Insights Dataset' with a focus on differentiating among 150 unique gourmet product categories.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.93\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 1.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoutiqueFoods Shelf Insights Dataset\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A dataset tailored for boutique grocery, featuring high-resolution images that need to be processed for 150 unique gourmet product categories.\",\n            \"preprocessing\": [\n                \"fine-tuning\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5.1\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"number_of_categories\": 150\n            },\n            \"description\": \"A YOLOv5.1 model fine-tuned for gourmet product recognition with a focus on precision and recall.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"inventory_real_time_synchronization\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"inventory updates for each high-resolution shelf image\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed as a 'build' request, targeting a high-expertise user who requires a specific model fine-tuning and performance metrics. The inventory synchronization and real-time processing times are clear performance requirements. The user expects a detailed model description and fine-tuning, as indicated by the requirement. However, the target device for deployment is missing and might need clarification. The confidence score is high given the specifics provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "As a project manager for the machine learning team, can you explore the feasibility of developing a multi-layered, explainable model that not only predicts connections in the Citeseer dataset but also accounts for temporal dynamics and handles missing data, while maintaining high accuracy and interpretability?", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"citation networks\",\n        \"description\": \"Explore the feasibility of developing a multi-layered, explainable model for link prediction in the Citeseer dataset. The model should consider temporal dynamics and handle missing data, while maintaining high accuracy and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Citeseer\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"edges\"],\n            \"specification\": {\n                \"temporal\": true,\n                \"missing_data_handling\": true\n            },\n            \"description\": \"A citation network dataset with temporal dynamics and missing data challenges.\",\n            \"preprocessing\": [\"handling missing values\", \"temporal feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model interpretability\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": {\n        \"items\": [\n            {\n                \"name\": \"\",\n                \"family\": \"\",\n                \"type\": \"neural networks\",\n                \"specification\": {\n                    \"explainability\": true\n                },\n                \"description\": \"A multi-layer model designed for link prediction, considering temporal dynamics and interpretability.\"\n            }\n        ]\n    },\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal feature engineering\",\n            \"handling interpretability techniques\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretable models\"],\n        \"demonstration\": \"explanatory model predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user is seeking advice for a consult-build scenario with a high level of expertise. They describe the requirements for a model that incorporates various aspects, such as multi-layered, explainable, temporal, missing data handling, and high accuracy. The specific metrics are not provided, but expected to be high. The lack of endpoint and deployment details indicates the need for guidance. Overall, the request is clear and likely from someone with domain knowledge who wants a strategic approach.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the elite AI research team, you've been tasked with enhancing the machine learning text classification system. The WebMD Review dataset, containing a massive, unstructured corpus of patient feedback, now demands not just binary sentiment analysis (0 for negative, 1 for positive) but also nuanced grading with a 5-point scale. The goal is to achieve state-of-the-art accuracy while maintaining efficiency. Conduct multi-label classification, incorporate domain-specific context, and include interpretability in your model's design, ensuring rigorous benchmarking against existing literature.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"healthcare, specifically patient feedback\",\n        \"description\": \"Enhance a machine learning text classification system for a WebMD Review dataset with 5-point scale sentiment grading, focusing on multi-label classification, domain-specific context, and interpretability. Aim for state-of-the-art accuracy while maintaining efficiency and rigorous benchmarking.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"multi-label classification metrics\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WebMD Review\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"unstructured_corpus\": true,\n                \"sentiment_scale\": 5,\n                \"multi-label\": true\n            },\n            \"description\": \"A massive unstructured dataset of patient feedback from WebMD, requiring a comprehensive sentiment analysis with a 5-point scale for nuanced grading.\",\n            \"preprocessing\": [\"domain-specific tokenization\", \"lemmatization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true,\n                \"domain_specific\": true\n            },\n            \"description\": \"A state-of-the-art machine learning model for text classification with multi-label capability, incorporating domain context and interpretability features.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Current literature on multi-label text classification with 5-point sentiment grading\",\n        \"Research on interpretability techniques for NLP models\",\n        \"Domain-specific NLP models for healthcare\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": null,\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"rigorous benchmarking against existing literature\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction specifies a high-level enhancement project with a focus on accuracy, efficiency, and interpretability. The user's high expertise indicates they require a sophisticated solution. Some metrics (e.g., accuracy) are left unspecified for the model performance, which needs to be filled based on research and experimentation.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "time-series classification", "instruction": "For the UWave Gesture Library dataset, a time series classification challenge with low-power hardware constraints is presented. The train, valid, and test splits consist of feature sequences (INPUT_SEQ_LEN=315, INPUT_DIM=3) that must be processed efficiently. Design a model utilizing state-of-the-art techniques for energy-efficient prediction, targeting labels 0 to 7, with an emphasis on optimizing for accuracy while maintaining real-time performance on resource-constrained devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"low-power hardware constraints\",\n        \"description\": \"This task involves a time series classification challenge using the UWave Gesture Library dataset. The feature sequences have a fixed length (INPUT_SEQ_LEN=315, INPUT_DIM=3), and the model must be designed efficiently for energy-efficient prediction on resource-constrained devices while optimizing for accuracy and real-time performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"operations per Joule\"\n            },\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UWave Gesture Library\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"labels: 0 to 7\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 315,\n                \"INPUT_DIM\": 3\n            },\n            \"description\": \"A time series dataset with train, valid, and test splits, designed for energy-efficient classification on low-power devices.\",\n            \"preprocessing\": [\"Low-pass filtering\", \"Normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"State-of-the-art for energy-efficient classification\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"energy-efficient techniques\": true,\n                \"optimized for accuracy\": true,\n                \"real-time performance\": true\n            },\n            \"description\": \"A model designed to handle the UWave Gesture Library's classification challenge with low-power hardware constraints.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider deploying the model using TensorFlow Lite or a lightweight deep learning framework for resource efficiency.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature selection based on hardware limitations\",\n            \"Model pruning for reduced computations\"\n        ],\n        \"target_device\": \"low-power hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"TensorFlow Lite\"\n        ],\n        \"demonstration\": \"Real-time demonstration on a constrained device\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction for time-series classification on UWave Gesture Library dataset. However, specific performance metrics for accuracy and hardware-specific details are not provided and need to be confirmed.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "For the \"Holliday HighSchool Friendship Dynamics Competition,\" design a state-of-the-art Spatio-Temporal Graph Neural Network (STGNN) with adaptive edge weighting that accurately categorizes students based on their evolving cliques and significant interactions. The model should handle daily data updates and produce visual heatmaps showcasing shifts in social hierarchies over three school semesters.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"Holliday HighSchool Friendship Dynamics\",\n        \"description\": \"Design a state-of-the-art Spatio-Temporal Graph Neural Network (STGNN) for the Holliday HighSchool Friendship Dynamics Competition. The model should classify students based on evolving cliques and significant interactions, with adaptive edge weighting. It must handle daily data updates and provide visual heatmaps for changes in social hierarchies over three school semesters.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Holliday HighSchool Friendship Dynamics\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [\"student classification\"],\n            \"specification\": {\n                \"temporal_frequency\": \"daily\",\n                \"number_of_semesters\": 3,\n                \"node_features\": [\"clique information\", \"interaction strength\"],\n                \"edge_features\": [\"adapted weighting\"]\n            },\n            \"description\": \"Dataset for the Holliday HighSchool competition, capturing student interactions in a spatio-temporal graph.\",\n            \"preprocessing\": [\"data cleaning, node and edge feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"heatmap generation for semester-wise social hierarchy shifts\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art STGNN\",\n            \"family\": \"Spatio-Temporal Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"adaptive edge weighting\"\n            },\n            \"description\": \"An STGNN designed to handle the Holliday HighSchool dataset and its requirements for evolving social dynamics.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"node embedding extraction\", \"temporal filtering\"],\n        \"target_device\": \"cloud for daily updates\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"visualization of dynamic social hierarchies\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction is parsed into a JSON response, indicating a 'build' intent with a high level of expertise. The problem area is identified as graph machine learning, and a specific task of node classification is defined. The user requests a state-of-the-art Spatio-Temporal GNN with adaptive edge weighting. They expect daily data updates and visual heatmaps. The model and service components have been detailed accordingly, while acknowledging the need for specific performance metric values and some assumptions for service details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Develop a highly specialized urban green space segmentation project using the Vaihingen dataset, focusing on advanced multi-scale SegNet architecture with fine-grained distinctions of not only vegetation types but also rare flora and fauna. Ensure model robustness against extreme weather, time-of-day variations, and seasonal changes, while integrating interpretability features for urban planners to facilitate data-driven decisions on ecosystem conservation and urban renewal strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning and ecology\",\n        \"description\": \"Develop a highly specialized urban green space segmentation project using the Vaihingen dataset. The project should employ an advanced multi-scale SegNet architecture, aiming for fine-grained distinctions of vegetation types, rare flora, and fauna. The model must be robust to extreme weather, time-of-day variations, and seasonal changes. Interpretability features are required for urban planners to aid in ecosystem conservation and urban renewal decision-making.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vegetation types\", \"rare flora\", \"fauna\"],\n            \"specification\": {\n                \"image_depth\": null,\n                \"image_size\": null,\n                \"classes_count\": null\n            },\n            \"description\": \"A high-resolution urban dataset for green space segmentation, focusing on Vaihingen area with diverse flora and fauna.\",\n            \"preprocessing\": [\"extreme weather normalization\", \"time-of-day normalization\", \"seasonal normalization\"],\n            \"augmentation\": [\"synthetic flora and fauna examples\"],\n            \"visualization\": [\"model interpretability plots\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"advanced SegNet\",\n            \"family\": \"multi-scale\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"SegNet\",\n                \"model_depth\": null,\n                \"specifics\": \"fine-grained distinctions and interpretability\"\n            },\n            \"description\": \"A highly specialized SegNet model for urban green space segmentation, resistant to extreme weather, time-of-day variations, and seasonal changes.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interpretability techniques for urban planners\",\n        \"Importance of ecosystem conservation in urban renewal\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonal adjustment\",\n            \"weather normalization\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"urban_planning_dashboard\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"Interactive dashboard with visual interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction specifies a complex urban green space segmentation project, which is parsed with the high expertise level. It outlines the requirements for the SegNet architecture, interpretability, and robustness. However, performance metrics are not given, and they require to be inferred based on the user's expectations for accuracy. The lack of specific metric values, deployment endpoint, and fine-tuning details maintain some ambiguity.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, you need the ML team to design an innovative image classifier for haute couture garments using the unique Paris Fashion-MNIST dataset. Mandate the implementation of advanced Dynamic Routing Capsule Networks, targeting a stringent benchmark of 95% accuracy while maintaining exceptional resistance to distortion effects like shearing. Request a comprehensive analysis that not only showcases the superiority over conventional Convolutional Neural Networks but also investigates computational efficiency and memory footprint for real-world scalability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"haute couture garments\",\n        \"description\": \"Design an innovative image classifier for haute couture garments using the Paris Fashion-MNIST dataset. Specify the use of Dynamic Routing Capsule Networks and aim for a 95% accuracy while addressing distortion effects like shearing. Comprehensive analysis is required to compare with Convolutional Neural Networks and assess real-world scalability in terms of computational efficiency and memory footprint.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resistance to distortion\",\n                \"unit\": \"shearing\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"unit\": \"% compared to similar models\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"unit\": \"MB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Paris Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"garment class\"],\n            \"specification\": null,\n            \"description\": \"A unique dataset for haute couture garment classification, focusing on the Paris Fashion-MNIST style.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dynamic Routing Capsule Networks\",\n            \"family\": \"Capsule Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"resistance_to_distortion\": \"shearing\",\n                \"comparison_to_CNNs\": {\n                    \"efficiency\": \"\",\n                    \"memory\": \"\"\n                }\n            },\n            \"description\": \"Advanced implementation for haute couture image classification using Dynamic Routing Capsule Networks.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Include an analysis comparing superiority to Convolutional Neural Networks.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Comparative evaluation of performance withCNNs and scalability analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON was parsed based on the given requirements. The high expertise indicates the user is familiar with the technical aspects. The project manager's request for benchmarking, distortion resistance, and efficiency analysis was translated into relevant metrics. However, since details on computational efficiency and memory footprint were not provided, they are marked as null.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for a cutting-edge AI research team, your objective is to develop a highly specialized transformer-based neural machine translation model using the arXiv Q&A dataset and Tatoeba corpus fusion. The focus will be on translating obscure and niche scientific terminologies from Hungarian to cutting-edge English, with a requirement for a never-before-seen BLEU score of 50, accompanied by a human evaluation ensuring semantic accuracy and fluency in the translated content. Additionally, incorporate a novel context-awareness mechanism to handle multi-disciplinary jargon, and document a thorough methodology for replication and improvement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"AI research, scientific terminology\",\n        \"description\": \"Develop a highly specialized transformer-based neural machine translation model focusing on translating Hungarian to cutting-edge English scientific terminologies, with a target BLEU score of 50. Include a context-aware mechanism for multi-disciplinary jargon and ensure semantic accuracy and fluency through human evaluation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 50\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv Q&A dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source_domain\": \"science\",\n                \"language_pair\": \"Hungarian to English\",\n                \"obscure_niche_terms\": true\n            },\n            \"description\": \"A dataset for training the model with obscure scientific terminologies.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Tatoeba corpus fusion\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": \"Hungarian to English\",\n                \"contextual_corpus\": true\n            },\n            \"description\": \"A corpus to enhance context-awareness by fusing with Tatoeba dataset.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based Neural Machine Translation Model\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novel_features\": \"context-awareness mechanism for multi-disciplinary jargon\",\n                \"target_performance\": \"50 BLEU score\"\n            },\n            \"description\": \"A specialized model designed for translating obscure scientific terms with context-awareness and target BLEU score of 50.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Human evaluation is crucial for ensuring semantic accuracy and fluency.\",\n        \"Documenting methodology for replication and improvement will be essential.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": {\n            \"methodology\": \"Presentation of model design, evaluation process, and replication instructions\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the project's focus, dataset requirements, and high-level model specifications, including the target BLEU score and the need for human evaluation. The user's high expertise level is reflected, but the details on target device and deployment are inferred. The confidence score is high, given the clarity of the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for the machine learning development team, you've been tasked with tackling a complex time-series forecasting challenge. The Electricity dataset, a real-world, high-dimensional dataset (INPUT_SEQ_LEN=96, INPUT_DIM=321) is divided into train, validation, and test sets. Your objective is to design and implement an advanced forecasting model that can handle sequential patterns and adapt to different seasonalities. The model should not only predict the subsequent 96-step sequence (PRED_SEQ_LEN=96, PRED_DIM=321) but also incorporate multiple temporal features and external indicators. Your team is expected to optimize for both mean squared error (MSE) and mean absolute error (MAE) metrics, ensuring robustness and generalization across all splits, while maintaining model interpretability and computational efficiency. Develop a comprehensive strategy and execute a state-of-the-art model for this intricate forecasting problem.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-world, high-dimensional data\",\n        \"description\": \"Design and implement an advanced forecasting model for a complex time-series challenge using the Electricity dataset. The model must handle sequential patterns, seasonalities, multi-temporal features, and external indicators. Targets include predicting a 96-step sequence (PRED_SEQ_LEN=96, PRED_DIM=321), optimize for MSE and MAE, maintain interpretability, and computational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 321\n            },\n            \"description\": \"A real-world, high-dimensional dataset for time-series forecasting, divided into train, validation, and test sets.\",\n            \"preprocessing\": [\n                \"extract temporal features\",\n                \"extract external indicators\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"visualize sequence patterns and seasonalities\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced time-series forecasting model (e.g., deepAR, Transformer, LSTM variants with attention)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"medium\",\n                \"computational_efficiency\": \"optimized\"\n            },\n            \"description\": \"State-of-the-art time-series forecasting model incorporating temporal patterns, seasonality, external indicators, and designed for interpretability and efficiency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature scaling\",\n            \"normalization\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"TensorFlow Serving\",\n            \"Apache Kafka for real-time stream processing\"\n        ],\n        \"demonstration\": {\n            \"example_predictions\": \"for train, validation, and test sets\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction was parsed into a high-level JSON structure, considering the user's role as a project manager and the complex nature of the task. Expertise was assumed high as a requirement for tackling this complex challenge. The dataset and model specifications were deduced from the details given, such as high-dimensional data, multiple temporal features, and advanced modeling. The 'service' section includes expected feature engineering and deployment components. However, the specific MSE and MAE values are left unspecified, assuming they would need to be calculated from the model performance.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "The project manager needs a deep learning model that not only classifies cat and dog images with high accuracy, but also detects breed-specific characteristics and distinguishes between closely related species, ensuring a minimum precision of 95% on a diverse and unseen dataset of 10,000 images.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal recognition\",\n        \"description\": \"The project requires a deep learning model for accurate classification of cat and dog images, with a focus on breed-specific detection and distinguishing between closely related species. The model must achieve a minimum precision of 95% on a diverse and unseen dataset of 10,000 images.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"image labels (cat, dog, breed-specific characteristics)\"],\n            \"specification\": {\n                \"size\": 10000,\n                \"unseen\": true,\n                \"diversity\": \"high\"\n            },\n            \"description\": \"A large and diverse set of 10,000 cat and dog images, unseen in training, for evaluation purposes.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Learning Model for Cat and Dog Classification\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"convolutional neural network (CNN) with breed-detection and fine-grained classification layers\"\n            },\n            \"description\": \"A model designed for cat and dog image classification with a focus on breed-specific detection and high precision (95% or higher).\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Model performance demonstration on unseen image samples from the dataset\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been structured based on the given instruction. The user's intent to build a model and high expertise indicate a detailed understanding. The requirements are clearly specified for image classification, breed detection, and precision. The dataset size, unseen status, and diversity are noted. However, the model name and family are assumed as there were none provided. The deployment-related details are not specified in the input.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager, you seek to develop a real-time, high-performance car model recognition system for parking lot surveillance. Compare COCO and Pascal VOC datasets, considering factors like image diversity, annotation quality, and suitability for low-resolution input. Additionally, investigate multiple object detection architectures (e.g., YOLOv5, SSD, or RetinaNet) that excel in speed and accuracy under constrained hardware. Present a detailed analysis and recommendation for the optimal model selection.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\", \n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"parking lot surveillance\",\n        \"description\": \"The project manager aims to develop a real-time, high-performance car model recognition system for use in parking lot surveillance. The comparison should focus on the COCO and Pascal VOC datasets regarding image diversity, annotation quality, and their suitability for low-resolution input.\",\n        \"performance_metrics\": [\n            {\"name\": \"image diversity\", \"value\": null},\n            {\"name\": \"annotation quality\", \"value\": null},\n            {\"name\": \"suitability for low-resolution input\", \"value\": null}\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image diversity\": null,\n                \"annotation quality\": null,\n                \"suitability for low-resolution\": null\n            },\n            \"description\": \"COCO dataset for car model recognition, which might have a diverse set of images and varying annotation quality.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\" \n        },\n        {\n            \"name\": \"Pascal VOC\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image diversity\": null,\n                \"annotation quality\": null,\n                \"suitability for low-resolution\": null\n            },\n            \"description\": \"Pascal VOC dataset for car model recognition, known for its annotation quality and potential suitability for low-resolution input.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed\": null,\n                \"accuracy\": null,\n                \"resource constraints compatibility\": null\n            },\n            \"description\": \"YOLOv5, a fast and accurate object detection architecture.\"\n        },\n        {\n            \"name\": \"SSD\",\n            \"family\": \"Single Shot MultiBox Detector\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed\": null,\n                \"accuracy\": null,\n                \"resource constraints compatibility\": null\n            },\n            \"description\": \"SSD, a model known for its speed and accuracy in object detection.\"\n        },\n        {\n            \"name\": \"RetinaNet\",\n            \"family\": \"Focal Loss for Dense Object Detection\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed\": null,\n                \"accuracy\": null,\n                \"resource constraints compatibility\": null\n            },\n            \"description\": \"RetinaNet with a focus on improving object detection performance under resource constraints.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider evaluating models' performance with different metrics, such as inference speed and mAP (mean average precision) for accuracy, taking hardware constraints into account.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time, resource constrained\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"requirements\": \"comparison of model performance with varying input resolutions and hardware configurations\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction requires consulting on dataset selection and model architecture, with high emphasis on performance and hardware constraints. The 'performance_metrics' are open-ended for analysis and 'service' includes a requirement for specific hardware adaptation. The user's high expertise level indicates they may require guidance on the fine details.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "question answering", "instruction": "For a project to develop an advanced child-oriented educational question-answering system, we need you to design and implement a custom BERT model variant, specifically tailored for the KidsQA dataset. This model should prioritize simplicity, clarity, and pedagogical efficacy. The system must include a visually appealing, intuitive interface with interactive features and age-appropriate design, aiming to foster ongoing learning engagement among young users.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"child-oriented educational\",\n        \"description\": \"Design and implement a custom BERT model variant for the KidsQA dataset. The model should prioritize simplicity, clarity, and pedagogical efficacy. The project requires a visually appealing, interactive interface suitable for young users.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"pedagogical effectiveness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"age适宜性\": \"child-oriented\",\n                \"customization\": \"BERT model variant\"\n            },\n            \"description\": \"A dataset for an advanced child-oriented educational question-answering system, focused on promoting learning engagement in young users.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"custom BERT for KidsQA\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"simplification\": true,\n                \"clarity\": true,\n                \"pedagogical_techniques\": null\n            },\n            \"description\": \"A custom BERT model designed for the KidsQA dataset, optimized for simplicity, clarity, and pedagogical effectiveness.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"age-appropriate interface design\",\n            \"interactivity\"\n        ],\n        \"target_device\": null,\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"visually appealing, engaging for young users\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the details of the given instruction, with the user's intent to build a model, high expertise level, and the requirements for a NLP question-answering system. It specifies the custom BERT model for the KidsQA dataset and emphasizes the focus on simplicity, clarity, and pedagogical effectiveness. The dataset section mentions that the model should be suitable for a child-oriented environment. However, performance metrics and specific deployment details are not provided.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "In light of recent advancements, can you explore the feasibility and potential improvements of deploying transformers, specifically designed for multi-modal learning, to accurately discern and differentiate between genuine and manipulated videos within the diverse 'Celeb-DF' dataset? Please provide a detailed analysis comparing their performance to a baseline random guessing strategy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"video analysis\",\n        \"description\": \"Explore the feasibility and potential improvements of deploying transformers, specifically designed for multi-modal learning, on the 'Celeb-DF' dataset for detecting genuine and manipulated videos. Compare the performance with a baseline random guessing strategy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\n                \"image\",\n                \"audio\"\n            ],\n            \"target_variables\": [\"forged\", \"genuine\"],\n            \"specification\": {\n                \"total_samples\": null,\n                \"modalities_diversity\": \"diverse\"\n            },\n            \"description\": \"A dataset for multi-modal forgery detection in videos containing genuine and manipulated content.\",\n            \"preprocessing\": [\n                \"multimodal feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"performance comparison\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer for multi-modal forgery detection\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specifically designed for multi-modal learning\"\n            },\n            \"description\": \"An advanced transformer model for discerning genuine from manipulated videos in the Celeb-DF dataset.\"\n        },\n        {\n            \"name\": \"Random guessing baseline\",\n            \"description\": \"A naive baseline strategy which randomly predicts 'forged' or 'genuine' with equal probability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": null,\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"CPU\", \"GPU\"],\n        \"demonstration\": \"performance comparison visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user expresses a consult-build intent, suggesting they seek advice on developing a model for the given task. The provided dataset and model specifics are clear, along with the comparison to a random guessing strategy. However, there are missing details like dataset size, performance metrics, and target device, which might need to be gathered from the user or inferred based on context.\",\n        \"score\": 0.7\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, devise a multi-tiered medical Q&A model for precise diagnosis support, leveraging the extensive BioASQ dataset. Emphasize fine-tuning a state-of-the-art BioBERT architecture, ensuring explainable results with citations from credible medical sources and focusing on incremental performance improvement with each iteration.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medical\",\n        \"description\": \"Develop a multi-tiered medical Q&A model for precise diagnosis support, using the BioASQ dataset. The model should be based on a fine-tuned BioBERT architecture and provide explainable results with citations from credible sources. Performance improvement should be incremental across iterations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explainability\",\n                \"value\": {\n                    \"citations\": {\n                        \"type\": \"credible medical sources\"\n                    }\n                }\n            },\n            {\n                \"name\": \"incremental improvement\",\n                \"value\": true\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"complexity\": \"extensive\",\n                \"format\": \"for medical question answering\"\n            },\n            \"description\": \"A large-scale dataset for medical question answering tasks.\",\n            \"preprocessing\": [\"BioBERT-compatible preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BioBERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine-tuning\": true,\n                \"explainability\": {\n                    \"citations\": \"credible medical sources\"\n                }\n            },\n            \"description\": \"Fine-tuned BioBERT architecture for the medical Q&A model\"\n        }\n    ],\n    \"knowledge\": [\n        \"Explainable AI is key for medical domain\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Incremental learning strategies\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"Interactive system with feedback loop for continuous improvement\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is clear to build a model, and given their expertise level, they likely expect high-level guidance and optimization. The problem description includes details about the dataset, architecture, and performance goals. However, specific performance metrics are not provided, so the model performance values are set to null. The 'service' section assumes a cloud deployment and GPU-optimized inference. The knowledge piece highlights the importance of explainability.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For our specialized maritime security initiative, we require an advanced Mask R-CNN architecture, trained on a proprietary Oceanic Vessel Classification (OVC) dataset capturing diverse ship subclasses, dimensions, and headings. The model should achieve a precision above 95% and exhibit robustness against varying weather patterns and maritime traffic. Although latency is not a top priority, the algorithm must optimize for latency under 200 milliseconds on high-performance cloud GPUs, ensuring seamless operation without compromising resource allocation for other cloud services.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"maritime security\",\n        \"description\": \"For a specialized maritime security initiative, an advanced Mask R-CNN architecture is needed, trained on a proprietary Oceanic Vessel Classification (OVC) dataset. The model should be capable of identifying diverse ship subclasses, dimensions, and headings, while achieving a precision greater than 95%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 200,\n                \"unit\": \"milliseconds\",\n                \"priority\": \"optimization\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Oceanic Vessel Classification (OVC)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"ship subclasses\", \"dimensions\", \"headings\"],\n            \"specification\": {\n                \"diversity\": \"varied weather patterns and maritime traffic\",\n                \"proprietary\": true\n            },\n            \"description\": \"A custom dataset for maritime vessel classification, capturing diverse ship subclasses and scenarios.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"weather simulators\", \"varied traffic scenarios\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Mask R-CNN\",\n            \"family\": \"Advanced object detection architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": {\n                    \"constraint\": \"above 95%\"\n                },\n                \"latency\": {\n                    \"on_GPUs\": \"optimization for < 200ms on high-performance cloud GPUs\"\n                }\n            },\n            \"description\": \"An advanced Mask R-CNN model specifically designed for maritime vessel classification task\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance cloud GPUs\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for cloud environment\"],\n        \"demonstration\": \"seamless operation without impacting resource allocation for other cloud services\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects a clear intent to build an advanced Mask R-CNN model with high performance requirements on precision, latency, and robustness. It captures the complex use case and the need for high expertise level in interpreting the task. The service requirements reflect a focus on cloud deployment and latency optimization.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "For the upcoming Amazon Co-purchase Graph analysis, we need a state-of-the-art Spectral ChebNet model tailored for large-scale node classification. It should demonstrate exceptional scalability, achieve a minimum of 95% accuracy, and incorporate real-time feature adaptability for optimal performance in dynamic environments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"Amazon Co-purchase Graph analysis\",\n        \"description\": \"The user requires a state-of-the-art Spectral ChebNet model for large-scale node classification on the Amazon Co-purchase Graph. The model should be highly scalable, achieve at least 95% accuracy, and incorporate real-time feature adaptability for dynamic environments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Amazon Co-purchase Graph\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"scalability\": \"large-scale\"\n            },\n            \"description\": \"The dataset for Amazon Co-purchase Graph analysis, emphasizing the need for a scalable model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Spectral ChebNet\",\n            \"family\": \"ChebNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time feature adaptability\": true\n            },\n            \"description\": \"A state-of-the-art Spectral ChebNet model designed for large-scale node classification on the Amazon Co-purchase Graph, targeting optimal performance in dynamic environments.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction details a clear task with specific requirements. The user seeks to build a Spectral ChebNet model with high accuracy and real-time adaptability for the Amazon Co-purchase Graph, suggesting a high level of expertise. However, specific details on dataset size, feature engineering, deployment, and demonstration are assumed based on the given context.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project mandate demands a specialized machine translation system for e-commerce products, honed specifically on the Alibaba Taobao dataset. Utilizing the cutting-edge Baidu-influenced U-Net++ architecture, the model must excel in deciphering nuanced retail and haute couture terminology. Target a stringent BLEU benchmark of 40 and ensure compatibility with SaaS platforms like Shopify and Magento, enabling instant and context-aware translation of dynamic product narratives for uninterrupted international customer experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"e-commerce (retail and haute couture)\",\n        \"description\": \"The project requires a specialized machine translation system for Alibaba Taobao e-commerce data. A Baidu-influenced U-Net++ architecture should be employed, targeting nuanced retail and haute couture terminology.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU\",\n                \"value\": 40\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Alibaba Taobao\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"training_data\": {\n                    \"source_language\": \"unknown\",\n                    \"target_language\": \"unknown\"\n                },\n                \"contextualization\": {\n                    \"ecommerce_elements\": \"true\"\n                }\n            },\n            \"description\": \"The dataset for fine-tuning the machine translation model on Alibaba Taobao e-commerce data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Baidu-inspired U-Net++\",\n            \"family\": \"U-Net++\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"U-Net++ specific to Baidu influences\",\n                \"translation_domain_specialization\": \"e-commerce and haute couture\"\n            },\n            \"description\": \"A specialized U-Net++ machine translation model for retail and haute couture product narratives.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model must be adept at deciphering nuanced language specific to the retail and haute couture industries.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"compatibility with Shopify and Magento\",\n        \"inference_engine\": [\n            \"SaaS-compatible for real-time translation\"\n        ],\n        \"demonstration\": \"Instant and context-aware translation of product narratives\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a clear project requirement, specifying the area (NLP), task (MT), target BLEU score, and the U-Net++ architecture. The high expertise level suggests the user is familiar with the technicalities and requirements. The knowledge about the specialized vocabulary and SaaS compatibility is included. However, the exact technical specifications of the datasets and model are not specified, leaving room for further clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For a high-security museum exhibit management system, develop an advanced SSD-based object detection model that not only recognizes diverse artifacts but also discerns subtle safety hazards. The model must be trained on an exclusive, large-scale dataset combining images from accredited museum collections, undisclosed threat intelligence sources, and ensuring accuracy of at least 95%. Additionally, optimize the model for real-time operation with an end-to-end latency of 25 milliseconds or less, maintaining a strict threat classification precision above 98% in densely populated exhibit areas.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"high-security environment\",\n        \"description\": \"Develop an advanced SSD-based object detection model for a high-security museum exhibit management system, capable of recognizing diverse artifacts and detecting safety hazards. The model should be trained on a large-scale, exclusive dataset combining accredited museum collections, undisclosed threat intelligence, with an accuracy target of at least 95% and real-time operation with an end-to-end latency of 25 milliseconds or less, maintaining a precision above 98% in dense exhibit areas.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"end-to-end latency\",\n                \"value\": 25,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"threat classification precision\",\n                \"value\": 0.98\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Museum Artifact & Threat Detection Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_source\": [\"accredited museum collections\", \"undisclosed threat intelligence sources\"],\n                \"size\": {\n                    \"training\": \"large-scale\",\n                    \"test\": \"appropriately sized for model evaluation\"\n                }\n            },\n            \"description\": \"A comprehensive dataset for training, combining images from accredited museums and undisclosed threat intelligence sources.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced SSD-based object detection model\",\n            \"family\": \"Single-Shot Detection (SSD)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for high accuracy and real-time performance\",\n                \"latency_target\": \"25 milliseconds or less\"\n            },\n            \"description\": \"A state-of-the-art SSD model designed for real-time object detection in a museum exhibit environment, ensuring both artifact recognition and safety hazard detection.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance server or cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time inference\"],\n        \"demonstration\": \"performance under densely populated exhibit areas\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed with a high level of detail, considering the complex nature of the problem and user's expertise. The 'user' section reflects the high expertise level. Performance metrics, latency requirements, and the nature of the dataset were appropriately captured. However, missing details could include specific deployment infrastructure and exact model specifications, which are typically determined based on further consultation.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "A university with a sprawling global alumni network seeks to optimize local chapter cohesion by identifying influential alumni connectors in specific industries, utilizing their unique career paths to develop targeted mentorship and networking events for regional success.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"alumni management\",\n        \"description\": \"A university aims to optimize local chapter cohesion by identifying influential alumni connectors in specific industries. These connectors should be used to facilitate targeted mentorship and networking events for regional success based on their unique career paths.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"node classification accuracy\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"influence score calculation efficiency\",\n                \"value\": null,\n                \"unit\": \"time or FLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"University Alumni Network Data\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"influence\", \"industry\", \"career_path\"],\n            \"specification\": {\n                \"node_types\": [\"alumni\", \"industry\"],\n                \"edge_types\": [\"connects\", \"career_transition\"],\n                \"size\": {\n                    \"total_nodes\": null,\n                    \"important_nodes\": \"alumni connectors\"\n                }\n            },\n            \"description\": \"A graph dataset reflecting the university's alumni network, including career paths and industry connections.\",\n            \"preprocessing\": [\n                \"cleaning\",\n                \"feature extraction (career path vectors)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Influence Maximization Model\",\n            \"family\": \"\",\n            \"type\": \"graph machine learning\",\n            \"specification\": {\n                \"algorithm\": \"e.g. PageRank, LINE, or GraphSAGE\",\n                \"optimizer\": \"\",\n                \"hyperparameters\": {\n                    \"budget\": \"limited resources for influential alumni identification\"\n                }\n            },\n            \"description\": \"A model to identify influential alumni connectors in specific industries using a graph-based approach.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider clustering algorithms to segment alumni based on career paths and industry affiliations.\",\n        \"Efficiently calculate influence scores to prioritize mentorship and networking events.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstration of top influential alumni for each region, along with recommended events\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction indicates a project where advice is sought for identifying influential alumni connectors. The user has a medium level of expertise and the problem is primarily in the recommendation systems area, specifically node classification. The performance metrics are not clearly defined, but the need for optimization suggests the focus on accuracy and efficiency. The dataset is a graph reflecting the alumni network, with preprocessing steps indicated. The 'service' section is left open as the instructions do not specify target device or deployment details.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "link prediction", "instruction": "For the specialized microblogging platform, Tencent Weibo, design a highly granular attention-based graph convolutional neural network (AGCN) model, incorporating user behavior and context-specific features. Strive to enhance the model's discernment of subtle influence patterns and real-time relationship dynamics between users for improved link prediction accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"microblogging\",\n        \"description\": \"Design a highly granular attention-based graph convolutional neural network (AGCN) model for Tencent Weibo, considering user behavior and context-specific features. The goal is to improve the model's ability to detect subtle influence patterns and real-time relationship dynamics for enhanced link prediction accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"link prediction accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tencent Weibo user interaction data\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"user relationships\"],\n            \"specification\": {\n                \"user_behavior_data\": true,\n                \"context_features\": true\n            },\n            \"description\": \"Dataset containing user interactions on the microblogging platform, suitable for training the AGCN model.\",\n            \"preprocessing\": [\"user behavior data preprocessing\", \"context feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"AGCN (Attention-based Graph Convolutional Neural Network)\",\n            \"family\": \"Graph Convolutional Networks (GCN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention_mechanism\": true,\n                \"user_behavior_embedding_depth\": null,\n                \"context_embedding_depth\": null\n            },\n            \"description\": \"An advanced model designed for Tencent Weibo, leveraging user behavior and context-specific features to improve link prediction accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"user behavior feature engineering\", \"context feature refinement\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom model serving platform\",\n        \"inference_engine\": [\"real-time computation\"],\n        \"demonstration\": \"interactive platform showcasing real-time relationship updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear task ('link prediction' on Tencent Weibo using AGCN) and specified expert-level expertise. Performance metrics and task-specific complexities were not provided directly, so I assumed them as 'null' and subject to further requirements. The service section contains assumptions about the deployment environment and the need for real-time computation based on the 'real-time relationship dynamics' requirement.\",\n        \"score\": 0.8\n    }\n}"}
{"task": "object detection", "instruction": "For a bespoke management solution of a top-secret intergalactic artifact display, engineer a cutting-edge, domain-specific YOLOv5 model tailored to detect not only a myriad of extraterrestrial relics but also subtle subatomic security threats. Train the model on a confidential fusion dataset compiled from encrypted celestial museum archives, classified defense research, and ensuring precision above 99% for artifact recognition. Rigorously optimize the model for real-time operation in high-energy gallery settings, targeting an end-to-end latency of 15 milliseconds or less, with a stringent minimum precision of 99.5% in areas of intense artifact congregation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"intelligent art gallery management\",\n        \"description\": \"Engineer a YOLOv5 model specifically for bespoke management of intergalactic artifacts, including detection of extraterrestrial relics and subatomic security threats. Train on a confidential fusion dataset from encrypted archives and defense research, aiming for precision >99% for artifact recognition.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.995,\n                \"targeted\": true\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"end-to-end latency\",\n                \"value\": 0.015,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Confidential Fusion Dataset\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source\": \"encrypted celestial museum archives, classified defense research\"\n            },\n            \"description\": \"A custom dataset compiled for detecting extraterrestrial relics and subatomic threats, with high保密 requirements.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5 (custom)\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"precision_thresholds\": {\n                    \"artifact_detection\": 0.99,\n                    \"subatomic_security\": 0.995\n                },\n                \"latency\": 0.015\n            },\n            \"description\": \"A domain-specific YOLOv5 model designed for real-time artifact and subatomic threat detection in high-energy gallery settings.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance hardware compatible with real-time operation\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for real-time and high-energy environments\"\n        ],\n        \"demonstration\": {\n            \"scenario\": \"intense artifact congregation areas\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided information has been parsed with high confidence. The user is experienced (high expertise) and aims to build a highly specialized model. Performance metrics, complexity targets, and target deployment settings are clearly defined. However, the confidence is somewhat lower due to the absence of concrete details on the training process or specific techniques to ensure high precision and real-time performance.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "object detection", "instruction": "For the upcoming maritime surveillance project, mandate the machine learning team to develop a novel Mask R-CNN model that not only meets standard OVC requirements (95% precision) but must also integrate a real-time adaptive fusion module. This module should analyze and compensate for occlusions, low lighting conditions, and overlapping vessels in high-resolution satellite imagery. The model must demonstrate resilience in extreme weather events and maintain sub-200ms latency on Google's TPUv3 for non-preemptible instances, ensuring concurrent optimization with strict resource sharing with other critical predictive analytics tasks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"maritime surveillance\",\n        \"description\": \"The user wants a novel Mask R-CNN model for maritime surveillance with high requirements such as 95% precision for standard OVC, real-time adaptive fusion for occlusions, low lighting, and overlapping vessels, and resilience in extreme weather events. The model must achieve sub-200ms latency on Google's TPUv3 for non-preemptible instances.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 200,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"type\": \"satellite imagery\"\n            },\n            \"description\": \"High-resolution satellite imagery for maritime scenarios with varying conditions like occlusions and low lighting.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"novel Mask R-CNN\",\n            \"family\": \"Mask R-CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptiveness\": \"real-time adaptive fusion\",\n                \"weather_resilience\": \"extreme weather events\",\n                \"inference_speed\": {\n                    \"environment\": \"Google's TPUv3\",\n                    \"instance_type\": \"non-preemptible\",\n                    \"target_latency\": \"sub-200ms\"\n                },\n                \"resource_sharing\": \"strict\"\n            },\n            \"description\": \"A custom Mask R-CNN for maritime surveillance with real-time fusion and performance optimized for Google's TPUv3.\"\n        }\n    ],\n    \"knowledge\": [\n        \"custom fusion module for occlusions and weather events\",\n        \"strict resource sharing for concurrent optimization\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"Google's TPUv3\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"TPUv3\"\n        ],\n        \"demonstration\": \"real-time adaptive fusion and latency on extreme weather events\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed information includes the user's intent to build a model (expertise assumed high due to the complexity). Performance metric of 95% precision is specified, and the complex fusion and latency requirements are described. Missing parts include dataset specifics and exact deployment endpoint, which can be inferred from the extreme resource requirements and the need for real-time fusion.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, design a sophisticated medical Q&A system for advanced diagnostic assistance, incorporating the massive and constantly updated BioASQ dataset. Mandate the development of a novel fusion model, derived from the cutting-edge BioBERT, that not only delivers precise answers but also generates salient justifications with annotated references from peer-reviewed scientific literature. Incorporate a rigorous A/B testing protocol to track performance enhancements in five consecutive milestones, emphasizing the importance of interpretability and user trust with each incremental improvement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"Design a sophisticated medical Q&A system with advanced diagnostic assistance, utilizing the BioASQ dataset. The system should employ a novel fusion model based on BioBERT, enabling precise answers and generating salient justifications with annotated references from scientific literature.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"massive and constantly updated\",\n                \"update_frequency\": \"not specified\"\n            },\n            \"description\": \"A large and regularly updated dataset for medical question answering, derived from the BioASQ challenge.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"novel fusion model (BioBERT-based)\",\n            \"family\": \"BioBERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"fusion model\",\n                \"interpretability\": true\n            },\n            \"description\": \"A cutting-edge model that combines BioBERT for precise medical QA and generates justifications with annotated references.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"note\",\n            \"content\": \"The system emphasizes interpretability and user trust with each incremental improvement, following a rigorous A/B testing protocol over five milestones.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"not specified\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"A/B testing\"],\n        \"demonstration\": \"user-friendly, interpretability-focused\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON was filled with information according to the schema, considering a high level of expertise for the project manager. The description of the task and requirements are in line with the provided instruction. However, specific performance metrics and deployment details were not provided, leaving them open for further clarification.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for the \"SmartKid's Interactive Learning Environment\" initiative, you are required to commission a team to design and engineer a domain-specific BERT offspring, optimized for the exclusive Children's Educational Question-Answering (CEQA) corpus. Emphasize computational efficiency, child-friendly lexicon, and pedagogical reinforcement. The proposed system must incorporate a state-of-the-art, gamified interface with advanced age-adaptive graphics, ensuring seamless engagement and continuous learning curve progression for preschool to elementary aged kids.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"children's educational\",\n        \"description\": \"Design and engineer a domain-specific BERT offspring optimized for the Children's Educational Question-Answering (CEQA) corpus. Focus on computational efficiency, child-friendly lexicon, and pedagogical reinforcement. The system should include a gamified interface with age-adaptive graphics for seamless engagement and progressive learning for preschool to elementary aged kids.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"efficiency\",\n                \"value\": \"optimized\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational complexity\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Children's Educational Question-Answering (CEQA) corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\"],\n            \"specification\": {\n                \"age_range\": [\"preschool to elementary\"],\n                \"lexical_level\": \"child-friendly\"\n            },\n            \"description\": \"A corpus specifically designed for children's educational question-answering, tailored to the needs of young learners.\",\n            \"preprocessing\": [\"lexicon adaptation for children\"],\n            \"augmentation\": [],\n            \"visualization\": [\"age-adaptive graphics for understanding\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT offspring\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptation\": \"child-friendly\",\n                \"pedagogical reinforcement\": true\n            },\n            \"description\": \"A state-of-the-art BERT model with enhancements for Children's Educational Question-Answering, focusing on efficiency and pedagogy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"gamified interface\", \"age-adaptive graphics\"],\n        \"target_device\": \"smart devices for educational purposes\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for learning environments\"],\n        \"demonstration\": \"engagement and continuous learning progression\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction specifies a clear task of building a domain-specific BERT model with efficiency, child-friendly lexicon, and pedagogical reinforcement. The model requirements and the CEQA corpus indicate high expertise is assumed. Missing details could be filled based on the context of the project manager's role (e.g., target deployment platform).\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager, you're tasked with designing a sophisticated surveillance system for parking lots, requiring not only real-time object recognition but also differentiating between various car models. Compare COCO and Pascal VOC datasets, taking into account their image variety, annotation precision, and how they handle variations in resolution for low-end devices. In-depth research on multiple state-of-the-art detection models like YOLOv5, SSD, RetinaNet, and their adaptations for edge computing is mandatory. Evaluate trade-offs between speed, accuracy, and computational footprint, and provide a comprehensive report with a recommendation for the most efficient and scalable model architecture.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"surveillance\",\n        \"description\": \"Design a sophisticated surveillance system for parking lots with real-time object recognition and car model differentiation. Research COCO and Pascal VOC datasets in terms of image variety, annotation precision, and handling resolution for low-end devices. Investigate YOLOv5, SSD, RetinaNet, and their edge computing adaptations. Evaluate trade-offs between speed, accuracy, and computational footprint and provide a comprehensive report with a recommendation.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": {\n            \"inference_time\": null,\n            \"response_time\": null,\n            \"memory_size\": null\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": [\"COCO\", \"Pascal VOC\"],\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image Variety\": {\"COCO\": \"high\", \"Pascal VOC\": \"moderate\"},\n                \"Annotation Precision\": {\"COCO\": \"high\", \"Pascal VOC\": \"high\"},\n                \"Device Compatibility\": {\n                    \"COCO\": {\"low-end\": null, \"high-end\": null},\n                    \"Pascal VOC\": {\"low-end\": \"limited\", \"high-end\": \"good\"}\n                }\n            },\n            \"description\": {\n                \"COCO\": \"High image variety with precise annotations for object detection.\",\n                \"Pascal VOC\": \"Moderate image variety but offers better compatibility with low-end devices.\"\n            },\n            \"preprocessing\": [\"low-end device adaptation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": [\"YOLOv5\", \"SSD\", \"RetinaNet\"],\n            \"family\": [\"YOLO\", \"Single Shot Detection\", \"Faster R-CNN\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"Speed\": {\"YOLOv5\": \"high\", \"SSD\": \"medium\", \"RetinaNet\": \"low\"},\n                \"Accuracy\": {\"YOLOv5\": \"high\", \"SSD\": \"medium\", \"RetinaNet\": \"high\"},\n                \"Computational Footprint\": {\n                    \"YOLOv5\": \"low\",\n                    \"SSD\": \"moderate\",\n                    \"RetinaNet\": \"high\"\n                }\n            },\n            \"description\": {\n                \"YOLOv5\": \"Adaptation for edge computing with balanced speed and accuracy.\",\n                \"SSD\": \"Faster but less accurate, suitable for trade-offs.\",\n                \"RetinaNet\": \"State-of-the-art accuracy but slower and more resource-intensive.\"\n            }\n        }\n    ],\n    \"knowledge\": [\n        \"Edge computing is crucial to minimize latency and computational power consumption.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adaptation for low-end devices\", \"speed-accuracy trade-offs\"],\n        \"target_device\": \"edge computing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"TensorRT\", \"OpenVINO\"],\n        \"demonstration\": \" Comparative analysis and model recommendation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is consulting to build a system with a focus on model comparison and trade-offs, indicating a high level of expertise. The requirement for analyzing datasets and models as well as research on edge computing adaptations and trade-offs is clearly outlined. However, some performance metrics and target device deployment specifics are not provided, which are needed for a more comprehensive evaluation.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "The project manager demands a cutting-edge deep learning model, capable of not only discerning between cats and dogs with exceptional (99%) precision, but also in unraveling nuanced breed-specific traits, accurately differentiating among endangered subspecies, and demonstrating robustness across a vast and diverse benchmark set of 50,000 high-resolution images, ensuring adaptability and generalization to real-world scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animals, particularly focusing on wildlife and endangered species\",\n        \"description\": \"The project requires a cutting-edge deep learning model that can accurately distinguish between cats and dogs (99% precision) and further classify into breed-specific traits, even for endangered subspecies. The model must handle a large benchmark dataset of 50,000 high-resolution images, ensuring adaptability and generalization to real-world scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"benchmark dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"cat\", \"dog\", \"breed\", \"endangered_subspecies\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"images_count\": 50000,\n                \"diversity\": \"high\"\n            },\n            \"description\": \"A large and diverse dataset consisting of 50,000 high-resolution images for training and testing the model.\",\n            \"preprocessing\": [\"image resizing to a fixed size\"],\n            \"augmentation\": [\"data augmentation techniques to increase diversity\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art deep learning model\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ideally a convolutional neural network (CNN) with advanced features for fine-grained classification\",\n                \"number_of_layers\": null,\n                \"parameters\": null,\n                \"inference_speed\": \"optimized for real-time performance\",\n                \"robustness\": \"high\"\n            },\n            \"description\": \"A deep learning model that meets the criteria for high accuracy, breed-specific trait recognition, and adaptability to diverse environments.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud-based or server for the high computational demands\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"high-performance GPU support\"],\n        \"demonstration\": \"model performance demonstrations on a subset of benchmark images and visual explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"Based on the given information, the 'build' intent and 'high' expertise suggest a need for an advanced and highly optimized model. The project details have been translated into a 'computer vision' task with a high-performance target. The performance metric of 99% precision is clearly defined, while other complexities are left open for further clarification. The confidence is high given the detailed requirements and the exclusion of known issues.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, task the ML team to develop an avant-garde image classifier for haute couture garments on theexclusive Paris Fashion-MNIST dataset. Demand the deployment of cutting-edge Dynamic Routing Capsule Networks with a stringent requirement of 98% accuracy under various photorealistic distortions (e.g. affine transformations). In addition, mandate a comparative study with state-of-the-art Convolutional Neural Networks, including a thorough evaluation of energy efficiency and memory optimization for practical, large-scale deployment in resource-constrained environments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"haute couture garments\",\n        \"description\": \"As a project manager, task the ML team to develop an avant-garde image classifier specifically for haute couture garments using the Paris Fashion-MNIST dataset. The classifier must employ Dynamic Routing Capsule Networks with a target accuracy of at least 98% under various photorealistic distortions like affine transformations. Additionally, mandate a comparative study with state-of-the-art Convolutional Neural Networks, focusing on energy efficiency and memory optimization for practical, large-scale deployment in resource-constrained environments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"unit\": \"energy per inference\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory optimization\",\n                \"unit\": \"memory footprint\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Paris Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_type\": \"haute couture garments\"\n            },\n            \"description\": \"A specialized dataset for haute couture garments, derived from the Fashion-MNIST dataset.\",\n            \"preprocessing\": [\"photorealistic distortions (e.g., affine transformations)\"],\n            \"augmentation\": [\"none, as specified for photorealistic distortions\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dynamic Routing Capsule Networks\",\n            \"family\": \"Capsule Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy under distortions\": \"98%\",\n                \"comparison\": \"state-of-the-art Convolutional Neural Networks\"\n            },\n            \"description\": \"A cutting-edge image classifier employing Dynamic Routing Capsule Networks for haute couture garment classification.\"\n        },\n        {\n            \"name\": \"Convolutional Neural Networks (for comparison)\",\n            \"family\": \"ConvNets\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"Comparative study subject for the image classifier.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Resource-constrained environments\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"specific to the dataset and model requirements\"],\n        \"target_device\": \"resource-constrained environments\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"energy and memory-efficient for practical deployment\"],\n        \"demonstration\": \"comparative study and performance on resource-constrained tasks\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the key aspects of the instruction, indicating a high level of expertise for the project manager. The performance metric, advanced models, and the comparative analysis with ConvNets are captured. However, the confidence is slightly lower due to the missing specific values for energy efficiency and memory optimization, which would require further clarification or assumptions.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for an advanced AI innovation lab, your mandate is to revamp the sophisticated text analysis system for a cutting-edge medical review dataset. The expanded WebMD corpus, consisting of extensive, unstructured patient experiences, necessitates a six-dimensional emotion classification (1-6, representing a gradient of satisfaction) with utmost precision. Strive for top-tier performance, real-time analysis, and context-awareness by leveraging domain-specific ontologies. Mandate the development of explainable AI, and rigorously compare your model's efficiency and interpretability with the latest international research in a series of exhaustive benchmarks. Don't forget to account for scalability and the ethical implications of deployment in your project plan.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"medical review\",\n        \"description\": \"Revamp the sophisticated text analysis system for a cutting-edge medical review dataset, the expanded WebMD corpus. Aim for six-dimensional emotion classification (1-6) with high precision, real-time analysis, and context-awareness using domain-specific ontologies. Incorporate explainable AI and benchmark performance against international research in efficiency and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time analysis\",\n                \"value\": null,\n                \"unit\": \"TBA\"\n            },\n            {\n                \"name\": \"model explainability\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"expanded WebMD corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"emotion classification\"],\n            \"specification\": {\n                \"dimensionality\": 6\n            },\n            \"description\": \"An advanced medical review dataset containing unstructured patient experiences\",\n            \"preprocessing\": [\"domain-specific ontology integration\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks\",\n            \"type\": \"explainable AI\",\n            \"specification\": {\n                \"interpretability\": null,\n                \"scalability\": null\n            },\n            \"description\": \"A model for six-dimensional emotion classification in medical reviews, focusing on precision, real-time analysis, and context-awareness\"\n        }\n    ],\n    \"knowledge\": [\n        \"Domain-specific ontologies for medical context and ethics implications\",\n        \"Exhaustive benchmarks with latest international research\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"context-aware analysis and model interpretation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a high-level project mandate, which suggests a high expertise level. Performance metrics (precision) and complexity (real-time analysis, explainability) are specified. The importance of scalability and ethical implications is also noted. However, specific model names, deployment endpoint, and feature engineering techniques are not provided and would require clarification.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning project manager, your team has been assigned a cutting-edge challenge: to develop a real-time, explainable AI system using the ResNeSt architecture, specifically designed for the challenging DomainNet dataset. This project aims not only to classify an extensive range of objects, from household items to animals, in various styles and resolutions, but also to discern subtle nuances within each class. The system must adapt to occlusions, challenging backgrounds, and changing object scales. To showcase the model's reliability, a minimum F1-score of 90% across all categories is required, and a user-friendly interpretability module must be integrated for non-technical stakeholders to understand the decision-making process. In addition, a thorough analysis of the model's robustness under adversarial attacks should be provided, ensuring the system remains accurate under scrutiny.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"real-life object recognition\",\n        \"description\": \"Develop a real-time, explainable AI system using ResNeSt architecture on the DomainNet dataset. The system must classify objects from various categories, handle occlusions, challenging backgrounds, and changing scales. It should achieve an F1-score of at least 90% and include an interpretability module for non-technical stakeholders.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DomainNet\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": \"extensive\",\n                \"styles_and_resolutions\": \"various\"\n            },\n            \"description\": \"A comprehensive dataset containing diverse objects and categories with real-world challenges like occlusions, backgrounds, and scale variations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNeSt\",\n            \"family\": \"ResNeSt architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptiveness\": \"to handle occlusions, backgrounds, and varying object scales\",\n                \"interpretability\": \"for non-technical stakeholders\"\n            },\n            \"description\": \"A real-time AI system based on ResNeSt for the DomainNet dataset, aiming to classify objects with high F1-score and explainability.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"robustness\",\n            \"description\": \"The model should be robust under adversarial attacks to ensure accuracy and reliability.\"\n        },\n        {\n            \"type\": \"interpretability\",\n            \"description\": \"The interpretability module is a focus for non-technical users.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"type\": \"user-friendly\",\n            \"description\": \"User-friendly interface for stakeholders to understand the decision-making process.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction describes a complex project with specific requirements for architecture, dataset, and performance. High expertise suggests a thorough understanding of the project details. However, some details like deployment endpoint are missing, requiring the assumption that they may be decided later or not mentioned. The confidence score reflects this level of detail.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "The project necessitates the creation of an advanced image recognition system, specifically designed to classify various celestial entities in the Galaxy Zoo dataset. Imposing a constraint, the model must employ a state-of-the-art Swin Transformer for enhanced feature extraction, while maintaining at least 95% accuracy on a validation set not seen during training. This system's efficiency in transfer learning and robustness to noise will be crucial, as it supports groundbreaking astronomical research in real-time.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The project requires an advanced image recognition system for classifying celestial entities, specifically focusing on the Galaxy Zoo dataset. It demands the use of a state-of-the-art Swin Transformer for feature extraction and should achieve at least 95% accuracy on a validation set unseen during training. The system's transfer learning capabilities and robustness to noise are crucial for supporting real-time astronomical research.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"transfer learning efficiency\",\n                \"value\": null, \n                \"unit\": null\n            },\n            {\n                \"name\": \"robustness to noise\",\n                \"value\": null, \n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dimension\": null,\n                \"size\": null\n            },\n            \"description\": \"A dataset containing celestial images for image classification tasks, specifically focusing on various celestial entities.\",\n            \"preprocessing\": [\n                \"Galaxy Zoo specific data preprocessing\"\n            ],\n            \"augmentation\": [\n                \"Swin Transformer compatible data augmentation techniques\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Swin Transformer\",\n            \"family\": \"Swin Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"feature_extraction\": true\n            },\n            \"description\": \"Advanced image recognition system utilizing Swin Transformers for feature extraction and designed to classify celestial entities in the Galaxy Zoo dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Swin Transformers are known for their improved performance and efficiency in image recognition tasks.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"relevant for transfer learning\"],\n        \"target_device\": null,\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"Real-time classification and performance on unseen validation set\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent (expertise), the task (image classification), the dataset (Galaxy Zoo), the specified model (Swin Transformer), accuracy target, and the expected system characteristics. The missing metric values for transfer learning and robustness to noise reflect the need for additional information. The service section is speculative since some details are not given. High confidence level, but with room for completion of missing information.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for the machine learning team, you must develop a highly accurate regression model for predicting wild blueberry yields using the provided Wild Blueberry Yield Dataset. The dataset contains intricate, multi-correlated features and is divided into train, validation, and test sets. Implement advanced feature engineering techniques and optimize the model for explainability, all while maintaining an MAE below 5%. Perform rigorous hyperparameter tuning with a limited time budget and present a detailed performance report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Develop a highly accurate regression model for predicting wild blueberry yields using a provided dataset. The dataset contains multi-correlated features and is divided into train, validation, and test sets. Focus on feature engineering, explainability, and optimize for MAE below 5%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MAE\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wild Blueberry Yield Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"yield\"],\n            \"specification\": {\n                \"is_split\": true,\n                \"train_set_size\": null,\n                \"validation_set_size\": null,\n                \"test_set_size\": null\n            },\n            \"description\": \"A dataset with multi-correlated features for predicting wild blueberry yields, divided into train, validation, and test sets.\",\n            \"preprocessing\": [\"advanced feature engineering techniques\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": true,\n                \"hyperparameter_tuning\": true,\n                \"time_budget\": \"limited\"\n            },\n            \"description\": \"A regression model optimized for high accuracy, explainability, and MAE below 5%.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"advanced\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"requirements\": \"detailed performance report\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the user's instruction well, capturing the intent to build a model, the high level of expertise, and the details of the problem statement, including the performance metric (MAE), feature engineering, and the need for explainability and hyperparameter tuning. The user mentions an exact time budget, and the request for a detailed performance report is clear. However, missing are specifics on the dataset's split sizes and the exact model name and family, which would usually be derived from the user's preferred model or an initial exploration of the data.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For an underwater image segmentation project on CoralNet using a state-of-the-art DeepLabV3+ model, develop a variant that integrates a novel adaptation module to handle extreme water conditions, turbidity, and adapt to diverse lighting. The model must outperform existing benchmarks by at least 5% in F1-score, while ensuring real-time inference for efficient deployment on resource-constrained embedded devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"marine biology\",\n        \"description\": \"The user is working on an underwater image segmentation project for CoralNet. They require a state-of-the-art DeepLabV3+ model variant, with an added novel adaptation module for extreme water conditions, turbidity, and diverse lighting. The main objective is to achieve at least 5% improvement in F1-score compared to existing benchmarks, and ensure real-time inference for deployment on resource-constrained embedded devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"real-time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CoralNet\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"adaptation_module\": \"novel for extreme conditions\"\n            },\n            \"description\": \"Underwater image dataset used for the CoralNet project, with a focus on images captured under challenging conditions.\",\n            \"preprocessing\": [\"novel adaptation module for extreme conditions\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepLabV3+ with novel adaptation module\",\n            \"family\": \"DeepLabV3+\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"embedding_device\": \"resource-constrained embedded devices\",\n                \"latency_constraints\": \"real-time inference\"\n            },\n            \"description\": \"A DeepLabV3+ model enhanced with a novel adaptation module for underwater image segmentation, designed for resource-constrained environments.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"embedded devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time inference\"\n        ],\n        \"demonstration\": \"comparison with existing benchmarks\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's request for a specific model improvement, performance metric targets, and real-time inference. Expertise level is assumed to be high due to the complexity of the problem and the requirement for real-time performance on resource-constrained devices. However, the confidence score might be lower as the user did not specify certain optimization details (like inference time) or a deployment endpoint.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a state-of-the-art explainable AI node classification model using the Heterogeneous Information Network (HIN) extracted from the Wikipedia and DBpedia datasets. The model must integrate both edge and node attributes in a Siamese Hypergraph Neural Network architecture, achieving a minimum F1-score of 92% for unseen node labels. Moreover, optimize the model for resource efficiency, as it will be deployed on a Raspberry Pi for real-time edge computing, and analyze the impact of neighborhood sampling techniques on model performance. Provide a detailed comparative study between different graph convolution methods and highlight the trade-offs between accuracy and computational complexity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graph\",\n        \"description\": \"Develop a state-of-the-art explainable AI node classification model using a Siamese Hypergraph Neural Network with HIN extracted from Wikipedia and DBpedia datasets. The model should integrate both edge and node attributes, targeting a minimum F1-score of 92% for unseen node labels. Optimize the model for resource efficiency suitable for deployment on a Raspberry Pi for real-time edge computing. Analyze the impact of neighborhood sampling techniques on model performance, comparing different graph convolution methods.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.92\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"seconds\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"unit\": \"MB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Heterogeneous Information Network (Wikipedia + DBpedia)\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"node_types\": [\"Wikipedia articles\", \"DBpedia entities\"],\n                \"edge_types\": [\"links between articles/entities\"],\n                \"attribute_types\": [\"node features\", \"edge attributes\"]\n            },\n            \"description\": \"A heterogeneous graph combining Wikipedia and DBpedia data for node classification.\",\n            \"preprocessing\": [\"HIN extraction\", \"feature engineering\"],\n            \"augmentation\": null,\n            \"visualization\": [\n                \"network visualization\",\n                \"attribute visualization\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Siamese Hypergraph Neural Network\",\n            \"family\": \"Hypergraph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Siamese\",\n                \"hypergraph_convolutions\": [\n                    \"edge-centric\",\n                    \"node-centric\"\n                ]\n            },\n            \"description\": \"Integrating edge and node attributes in a Siamese HNN for explainable node classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advantages of using HINs for knowledge representation\",\n        \"Best practices for edge sampling in real-time edge computing\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"Raspberry Pi\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"lightweight, optimized for Raspberry Pi\"],\n        \"demonstration\": \"comparative study on accuracy and computational complexity trade-offs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction has been parsed based on the schema, highlighting a high level of expertise and a need for state-of-the-art model development. The performance metrics and target F1-score are clearly specified, along with resource efficiency requirements and the need for a detailed analysis of different graph convolution methods. The 'service' section expects real-time edge computing deployment and a demonstration of trade-offs, which are included in the response.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Utilize a cutting-edge deep neural network (LSTM with attention mechanism) for interpretable forecasting of DAU on our niche mobile gaming app, considering variable weekly patterns, holidays, and alpha release effects. Ensure regularization techniques are employed to handle sparse data and achieve robust performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"mobile gaming\",\n        \"description\": \"The task is to utilize a cutting-edge deep neural network, specifically an LSTM with attention mechanism, for interpretable forecasting of daily active users (DAU) on a niche mobile gaming app. The model should account for variable weekly patterns, holidays, and the impact of alpha release effects. Regularization techniques are necessary to handle sparse data and achieve robust performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_speed\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory_consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"sparsity_resilience\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"proprietary_mobile_game_data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"DAU\"],\n            \"specification\": {\n                \"time_frequency\": \"daily\",\n                \"data_points\": null,\n                \"holiday_indices\": \"provided\"\n            },\n            \"description\": \"Niche mobile gaming app's DAU data with sparse data and varying patterns reflecting weekly patterns, holidays, and alpha release effects.\",\n            \"preprocessing\": [\n                \"normalize data\",\n                \"handle missing values (imputation or padding)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"data distribution\", \"weekly and holiday patterns\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"LSTM with attention mechanism\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"units\": null,\n                \"number_of_layers\": null,\n                \"attention_type\": null\n            },\n            \"description\": \"Advanced LSTM model with an attention mechanism designed for forecasting DAU in the mobile gaming app context.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"time series feature extraction (seasonality, trend, etc.)\",\n            \"holiday and alpha release feature engineering\"\n        ],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Lite\"],\n        \"demonstration\": \"interpretability through attention weights and feature importance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been parsed as per the schema, including user intent, problem description, model requirements, dataset properties, and service preferences. The user's high expertise is reflected in the choice of a cutting-edge deep model. The missing performance and complexity metric values are expected to be filled based on the user's proficiency. However, since the user did not specify target values, those metrics are set to null.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a seasoned project manager, I need your expertise to guide our machine learning team for an intricate space image classification project. We have a need to identify not only galaxies and stars but also nebulae and exoplanets. Can you recommend a state-of-the-art deep learning model architecture, taking into account the complexities of diverse celestial objects? Additionally, outline the expected challenges in attaining near-human-level accuracy, and propose steps to address those challenges in the development process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"space imagery\",\n        \"description\": \"A project aims to classify space images into galaxies, stars, nebulae, and exoplanets using deep learning. The model should handle the complexities of diverse celestial objects and achieve near-human accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"near-human accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Custom (space image dataset)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"galaxies\", \"stars\", \"nebulae\", \"exoplanets\"],\n            \"specification\": {\n                \"image_type\": \"space\",\n                \"image_dimensions\": \"variable\",\n                \"object_diversity\": \"complex celestial objects\"\n            },\n            \"description\": \"A dataset containing diverse space images of galaxies, stars, nebulae, and exoplanets\",\n            \"preprocessing\": [\n                \"data augmentation for increased diversity\"\n            ],\n            \"augmentation\": [\n                \"random rotation, flipping, Gaussian noise\"\n            ],\n            \"visualization\": [\n                \"object annotations and confusion matrix\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Proposed state-of-the-art deep learning architecture\",\n            \"family\": \"Advanced convolutional neural networks (CNN) or transformer-based models (e.g., Vision Transformers or variants)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"Highly-convoluted layers for object recognition, possibly with transfer learning (e.g., from ImageNet pre-trained models) for better generalization\"\n            },\n            \"description\": \"A model designed for multi-class image classification, robust to the complexity of celestial objects\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"challenge_1\": \"Class imbalance between different celestial objects\",\n            \"addressing\": \"Augment minority classes, use appropriate class weighting in loss function\"\n        },\n        {\n            \"challenge_2\": \"Variation in image quality due to different observation angles and instruments\",\n            \"addressing\": \"Data preprocessing with image enhancement techniques and model robustness to input variations\"\n        },\n        {\n            \"challenge_3\": \"Semantic and fine-grained discrimination between similar celestial objects\",\n            \"addressing\": \"Feature extraction with high-level representations, fine-tuning, or multi-scale approaches\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"adversarial data generation to test robustness\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for GPU performance\"\n        ],\n        \"demonstration\": \"Interactive web-based dashboard with model explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user provided a clear problem statement, indicating a need for consultation on model architecture, expected challenges, and suggestions to address them. Their high expertise level suggests they will have a more in-depth understanding of the details. The gap in specific metrics indicates uncertainty that will be clarified during discussion.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for the machine learning development team, you are tasked with creating a sophisticated diabetic retinopathy classifier using the Kaggle dataset. The project calls for an enhanced VGG-16 architecture that not only distinguishes between healthy and diseased eyes but must also precisely categorize the retinopathy severity into five distinct stages. The model must achieve a minimum accuracy of 95% with an F1-score emphasis on reducing false negatives, as misdiagnoses can have severe consequences. Additionally, optimize the model for real-time deployment in resource-constrained clinical environments, ensuring fast inference times without compromising on accuracy or computational efficiency. Provide a detailed design, implementation plan, and performance benchmarks to demonstrate the efficacy and practicality of the solution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"The project requires a sophisticated diabetic retinopathy classifier using the Kaggle dataset, focusing on an enhanced VGG-16 architecture with five severity stages (healthy vs. stages 1-4). The model must achieve a minimum accuracy of 95% with a strong emphasis on reducing false negatives. It must be optimized for real-time deployment in resource-constrained clinical environments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null,\n                \"additional_info\": \"Emphasis on reducing false negatives\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"real-time\",\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": \"optimized for resource-constrained environments\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Diabetic Retinopathy Kaggle dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"Healthy\", \"Stage 1\", \"Stage 2\", \"Stage 3\", \"Stage 4\"],\n            \"specification\": {\n                \"image_size\": \"\",\n                \"image_channels\": \"\"\n            },\n            \"description\": \"A dataset for diabetic retinopathy classification, including healthy and five severity stages.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"preserving diagnostic relevance\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced VGG-16\",\n            \"family\": \"VGG\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"incorporating advanced techniques for five-class classification\",\n                \"num_layers\": \"\",\n                \"parameters\": \"\"\n            },\n            \"description\": \"A custom VGG-16 architecture designed for robust classification of diabetic retinopathy severity stages.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Emphasis on minimizing false negatives due to medical implications\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time deployment optimizations\"],\n        \"target_device\": \"resource-constrained clinical environments\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for fast inference\"],\n        \"demonstration\": \"Performance benchmarks and detailed design implementation plan\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is clear to build a classifier with specific requirements. High expertise suggests familiarity with ML but requires design and implementation details. The performance metrics, including the accuracy target and emphasis on false negatives, are included. The need for real-time deployment and resource optimization shows a clear vision for practical applicability.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "community detection", "instruction": "For the advanced CiteSeer citation network, apply a deep learning-enhanced Label Propagation algorithm with self-organizing map (SOM) clustering to identify dynamic research communities. Emphasize resilience to scale, fine-grained topic specialization, and ability to autonomously update clusters upon the addition of new nodes using transfer learning, ensuring minimal computational overhead.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"scientific literature\",\n        \"description\": \"Apply a deep learning-enhanced Label Propagation algorithm with self-organizing map (SOM) clustering to the CiteSeer citation network. Focus on scalability, fine-grained topic specialization, and ability to update clusters dynamically with transfer learning, maintaining low computational overhead.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"resilience to scale\",\n                \"value\": null\n            },\n            {\n                \"name\": \"fine-grained topic specialization\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cluster autonomy upon new node addition\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational overhead\",\n                \"value\": null,\n                \"unit\": \"time or FLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CiteSeer citation network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"citation network\",\n                \"nodes\": {\n                    \"type\": \"papers\"\n                },\n                \"edges\": {\n                    \"type\": \"citations\"\n                }\n            },\n            \"description\": \"Advanced citation network data for researching dynamic communities in scientific literature.\",\n            \"preprocessing\": [\"deep learning enhancement\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"deep learning-enhanced Label Propagation with SOM clustering\",\n            \"family\": \"Label Propagation + SOM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer learning\": true,\n                \"dynamic clustering\": true\n            },\n            \"description\": \"A model that combines deep learning, Label Propagation, and SOM clustering for research community detection in CiteSeer while handling scalability and transfer learning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"leveraging transfer learning for adapting to new nodes\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"resilient to node addition\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"autonomously updating clusters on new data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is clear for building a model that addresses the requirements of community detection in a citation network. The expert level suggests a deep understanding of the task. However, some performance and complexity metrics are missing values which would require further clarification from the user. Overall, there's a high confidence for the problem description, but low confidence for quantitative metrics.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the advanced machine learning team, your objective is to develop a high-precision text classification system. Utilizing the diverse and challenging DAIGT dataset, design a multistep pipeline that not only classifies texts into human (0) and AI-generated (1) categories but also evaluates the model's explainability and bias. Strive for state-of-the-art accuracy, while maintaining interpretability and conducting thorough feature engineering for optimal performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"advanced machine learning\",\n        \"description\": \"The objective is to develop a high-precision text classification system, distinguishing between human-written (0) and AI-generated (1) texts. The project requires a multistep pipeline, including explainability evaluation, bias detection, and focus on feature engineering for optimal performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"bias detection\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"human\", \"AI-generated\"],\n            \"specification\": {\n                \"diversity\": \"challenging\",\n                \"classification_categories\": [\"0\", \"1\"]\n            },\n            \"description\": \"A diverse and challenging dataset for text classification, designed to differentiate between human and AI-generated texts.\",\n            \"preprocessing\": [\"feature extraction\", \"tokenization\", \"cleaning\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model explainability\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"feature_engineering\": true\n            },\n            \"description\": \"A deep learning model with a focus on interpretability for text classification\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"explanation\": \"The model should be designed with interpretability in mind to assess and improve transparency.\"\n        },\n        {\n            \"bias_checker\": \"Consider fairness and potential bias in the dataset and model, ensuring bias detection is incorporated into the pipeline.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\",\n            \"feature extraction with interpretable methods\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstrate model performance and explainability with sample texts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has a high level of expertise, indicating a thorough understanding of the project requirements. The instruction clearly specifies the need for a text classification system, dataset, feature engineering, and interpretability, while leaving certain details like specific model name and target device unspecified.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "You must develop a state-of-the-art deep learning architecture for time-series forecasting on the Weather dataset, which not only optimizes for mean squared error (MSE) but also includes a stringent requirement of minimizing the mean absolute percentage error (MAPE). In addition, apply a rolling window technique for validation, ensuring models are robust to seasonality and trends. The dataset has been partitioned into train (70%), validation (15%), and test (15%) subsets, and you must fine-tune hyperparameters using Bayesian optimization, while maintaining a constraint on computational complexity for efficient deployment on resource-constrained devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"weather forecasting\",\n        \"description\": \"Develop a state-of-the-art deep learning architecture for time-series forecasting on the Weather dataset, optimizing for both MSE and MAPE with a focus on seasonality and trends. Use a rolling window technique for validation and employ Bayesian optimization for hyperparameter tuning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational complexity\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Weather\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"partitioning\": {\n                    \"train\": 0.7,\n                    \"validation\": 0.15,\n                    \"test\": 0.15\n                },\n                \"rolling_window\": true\n            },\n            \"description\": \"Weather dataset for time-series forecasting, partitioned into train, validation, and test subsets.\",\n            \"preprocessing\": [\"rolling window technique\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"hyperparameter_optimization\": \"Bayesian\",\n                \"computational_complexity_constraint\": \"resource-constrained devices\"\n            },\n            \"description\": \"A deep learning architecture for weather forecasting, designed for robustness, seasonality, and trend handling, with emphasis on both MSE and MAPE.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient inference\"],\n        \"demonstration\": \"rolling window validation and Bayesian optimization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has a high level of expertise, indicating a more advanced understanding of the project requirements. The instruction specifies a deep learning architecture, performance metrics, and a complex dataset with a defined partition and preprocessing technique. Bayesian optimization for hyperparameter tuning and computational complexity constraints for deployment are clearly mentioned. However, the model name and family are not provided, and the demonstration field assumes the need to show the rolling validation and optimization process.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for the advanced land use detection system, your ML team is tasked with developing a state-of-the-art ViT-based model that processes extensive Landsat 8 Satellite Images. The model must exhibit exceptional transfer learning capabilities, achieving a minimum accuracy of 96% and outperforming benchmark techniques. Additionally, focus on designing a scalable, fault-tolerant model suitable for real-time deployment across multiple cloud platforms (AWS, Azure, and Google Cloud). Ensure detailed, performance-driven documentation, including fine-grained resource utilization profiles, confusion matrix, and AUC-ROC curves, to facilitate efficient monitoring and future enhancements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"remote sensing, land use detection\",\n        \"description\": \"Develop a state-of-the-art ViT-based model for advanced land use detection using Landsat 8 satellite images. The model must demonstrate excellent transfer learning capabilities, targeting a minimum accuracy of 96% and outperforming benchmark techniques. The model must be scalable and fault-tolerant for real-time deployment across AWS, Azure, and Google Cloud.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.96\n            }\n        ],\n        \"complexity_metrics\": {\n            \"name\": [\"memory footprint\", \"inference time\", \"scalability\"],\n            \"value\": [\"ideally optimize for resource efficiency\", \"meets real-time requirements\", \"able to scale across multiple cloud platforms\"]\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Landsat 8 Satellite Images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"land use classes\"],\n            \"specification\": {\n                \"image size\": \"extensive, likely for satellite imagery\",\n                \"modalities\": \"ViT-compatible\"\n            },\n            \"description\": \"A large-scale dataset for land use detection, processed from Landsat 8 satellite data.\",\n            \"preprocessing\": [\"spectral normalization\", \"data augmentation\"],\n            \"augmentation\": [\"geospatial transformations\"],\n            \"visualization\": [\"image patches with ground truth annotations\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ViT-based model\",\n            \"family\": \"Vision Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ViT, transfer learning enabled\",\n                \"benchmark超越\": \"desired\"\n            },\n            \"description\": \"A state-of-the-art model for land use detection using ViT, emphasizing transfer learning and achieving high accuracy.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"land use feature extraction\", \"transfer learning fine-tuning\"],\n        \"target_device\": [\"AWS, Azure, Google Cloud\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"serverless\", \"edge deployment compatible\"],\n        \"demonstration\": \"A detailed process for real-time monitoring and model improvements using resource utilization profiles and performance metrics.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON details a clear request for developing a highly specialized land use detection model with high accuracy. The 'expertise' is assumed to be high due to the project management context and the need for real-time scalability across multiple cloud platforms. The response captures the key performance metrics, dataset specifics, and model requirements, as well as the request for detailed documentation. However, certain metrics such as exact memory footprint or inference time were not provided, so there is some room for clarification from the user.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "As the app's user base grows, we now need a sophisticated image analysis and sentiment engine. Develop a multi-stage categorization system that classifies photos based on product usage, customer satisfaction, and content relevance. The system should not only differentiate between testimonials, requiring further engagement, and gallery-worthy content, but also identify sub-genres within each category for targeted marketing and continuous improvement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"Develop a multi-stage image analysis system to classify photos based on product usage, customer satisfaction, and content relevance. The system should categorize photos into testimonials, requiring further engagement, gallery-worthy content, and identify sub-genres for targeted marketing and continuous improvement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product usage\", \"customer satisfaction\", \"content relevance\", \"testimonial sub-genre\", \"engagement required\", \"gallery-worthy\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing photos for training and testing the multi-stage categorization system.\",\n            \"preprocessing\": [\"resize\", \"image normalization\"],\n            \"augmentation\": [\"rotation\", \"crop\", \"color jitter\"],\n            \"visualization\": [\"example images with annotated categories\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning models\",\n            \"specification\": {\n                \"architecture\": \"multi-stage deep learning\",\n                \"layers\": [\n                    \"pre-processing\",\n                    \"feature extraction\",\n                    \"category-specific layers\"\n                ]\n            },\n            \"description\": \"A multi-layered model for sophisticated image analysis and sentiment classification.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Consider using transfer learning with existing image classification models for faster training.\"\n        },\n        {\n            \"content\": \"Fine-tuning on domain-specific data will improve performance on customer satisfaction and product usage categories.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"custom feature extraction for sentiment analysis\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"proprietary API or third-party platform\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"AWS SageMaker\"],\n        \"demonstration\": \"user-friendly dashboard for real-time classification\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instructions provided a clear request for a complex image analysis and sentiment engine, indicating a high expertise level. However, some specific details, like model name and performance metrics, are not provided, leaving room for further clarification. The service section contains general information, as the platform type and endpoint were not specified.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create a specialized, real-time neural machine translation system using the TEDx Talk collection, specifically for Marathi to Konkani language pairs. Employ a custom-designed EfficientTransformer-XL, optimizing for both low-power Raspberry Pi Zero and ensuring <10% CPU usage during operation, while minimizing <5MB memory footprint for extended translation sessions on portable devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"real-time translation\",\n        \"description\": \"Develop a specialized, real-time neural machine translation system using the TEDx Talk collection, targeting Marathi to Konkani language pairs. The model should be based on a custom-designed EfficientTransformer-XL.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"CPU usage\",\n                \"value\": 0.1,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": 5,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TEDx Talk collection\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": {\n                    \"source\": \"Marathi\",\n                    \"target\": \"Konkani\"\n                },\n                \"collection_size\": \"specific for TEDx Talks\"\n            },\n            \"description\": \"Dataset for Marathi to Konkani translation, extracted from TEDx Talk talks.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientTransformer-XL\",\n            \"family\": \"EfficientTransformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"optimized for Raspberry Pi Zero\",\n                \"low_power\": true\n            },\n            \"description\": \"Custom-designed for real-time Marathi to Konkani translation, with efficiency targeting low CPU usage and small memory footprint.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"Raspberry Pi Zero\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for portable devices\"],\n        \"demonstration\": \"real-time translation with low CPU and memory utilization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction parsed into a JSON format clearly describes a high-level machine learning project. The user's intent is to build, and they possess high expertise in AI. The problem area is natural language processing, with the specific task of machine translation. The system requirements, such as optimizing for low-power Raspberry Pi Zero and minimizing memory usage, are captured in the complexity metrics. The dataset is specified for the TEDx Talk collection, with Marathi to Konkani translation. The model and service requirements are defined accordingly, but some details like the deployment endpoint could be further clarified by the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Utilize the Sylukar Mouse Brain Corpus for a fine-grained node classification task with Graph Fourier Neural Operator (GFNO), optimizing for the detection of ultra-precise neocortical neuron subclasses. Emphasize robustness in differentiating unique cell types while accounting for intricate fiber tracts and hierarchical anatomical hierarchies within the Allen Institute's CCFv2 reconstruction dataset.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"biological research, specifically neuroscience\",\n        \"description\": \"Use the Sylukar Mouse Brain Corpus for a fine-grained node classification task with Graph Fourier Neural Operator (GFNO). The goal is to detect ultra-precise neocortical neuron subclasses. The model should be robust in differentiating unique cell types while considering intricate fiber tracts and hierarchical anatomical structures in the Allen Institute's CCFv2 reconstruction dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sylukar Mouse Brain Corpus\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"neocortical neuron subclasses\"],\n            \"specification\": {\n                \"dataset_format\": \"structured graph\",\n                \"node_type\": \"neurons\",\n                \"edge_type\": \"fiber tracts\",\n                \"anatomical_structure\": \"Allen Institute's CCFv2\"\n            },\n            \"description\": \"Fine-grained data for node classification, focusing on the Sylukar Mouse Brain's neural networks with graph representation from the CCFv2 reconstruction.\",\n            \"preprocessing\": [\"edge filtering for anatomical hierarchy\", \"node feature normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"anatomical and graph structure visualizations\"],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"Allen Institute's CCFv2\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"anatomical hierarchy\"],\n            \"specification\": null,\n            \"description\": \"Hierarchical reconstruction dataset for understanding the context of complex fiber tracts.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Fourier Neural Operator (GFNO)\",\n            \"family\": \"neural networks\",\n            \"type\": \"graph neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"Fourier-based\",\n                \"optimization_target\": \"neocortical neuron subclass detection\"\n            },\n            \"description\": \"A model designed for fine-grained node classification, leveraging GFNO for detecting neuronal subclasses with emphasis on robustness and intricate anatomical structures.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Handle the hierarchical nature of the anatomical data by incorporating anatomical priors.\",\n        \"Ensure model accounts for fiber tract intricacies in the Sylukar corpus and CCFv2 dataset.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"incorporating anatomical hierarchy features\"\n        ],\n        \"target_device\": \"high-performance computing cluster\",\n        \"deployment_endpoint\": \"institute research environment\",\n        \"inference_engine\": [\n            \"optimized for graph-based computations\"\n        ],\n        \"demonstration\": \"visual comparison of classified neuron subclasses to ground truth\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction is clear and detailed, indicating a high level of expertise. The task involves a specific domain and requires advanced techniques like GFNO. The complexity metrics hint at the desire for optimized performance. However, the specific performance metrics and exact implementation details are not provided.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "machine translation", "instruction": "As a project manager, request a state-of-the-art transformer-based Neural Machine Translation (NMT) model, specifically an attention-based Dual Encoder LSTM, utilizing the extensive Europarl v7 corpus. Emphasize on optimizing for real-time scenarios by fine-tuning for fast parallel decoding and targeting sub-300ms latency in a mobile-friendly, low-resource environment for seamless, bilingual communication across diverse devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"bilingual communication\",\n        \"description\": \"As a project manager, seek a state-of-the-art transformer-based Neural Machine Translation model, specifically an attention-based Dual Encoder LSTM, leveraging the Europarl v7 corpus. Focus on optimizing for real-time scenarios by fine-tuning for fast parallel decoding with a target latency of sub-300ms, considering a mobile-friendly, low-resource environment for seamless communication across various devices.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 300,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Europarl v7\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"corpus_size\": \"large\",\n                \"languages\": \"not specified\"\n            },\n            \"description\": \"An extensive corpus for Neural Machine Translation, focusing on multilingual content from the Europarl project.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention-based Dual Encoder LSTM\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer with Dual Encoder LSTM architecture\"\n            },\n            \"description\": \"A state-of-the-art model for NMT, leveraging attention mechanisms within Dual Encoder LSTM for bilingual translation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"fast parallel decoding\"],\n        \"target_device\": \"mobile-friendly, low-resource\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            {\n                \"name\": \"low-latency optimization\"\n            }\n        ],\n        \"demonstration\": \"mobile-friendly, real-time bilingual communication\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clearly a request for a specific model building task with a high level of expertise required due to the complexity and context. The area, downstream task, and corpus are identified, while the performance metrics are left unspecified, which is common in a development process. The complexity metrics include a latency target for real-time usage and a deployment focus on mobile-friendly, low-resource environments. The service section assumes some typical steps for optimizing the model, but specific details on deployment endpoints and engines are missing.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a state-of-the-art hybrid machine learning model, integrating DeepAR and a convolutional neural network (CNN), to forecast not only monthly rainfall amounts for the next year across diverse regions but also detect anomalies in extreme weather patterns. The model should incorporate real-time satellite data, climate indicators, and socio-economic factors to enhance prediction accuracy. Implement a feature autoencoder for dimensionality reduction and novelty detection, ensuring the model adaptively improves its forecasts with every new dataset update using reinforcement learning techniques. Additionally, design a user-friendly interface for data visualization and interpretability of the prediction intervals, along with a comprehensive evaluation framework to compare the model's performance against multiple baselines.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate science and meteorology\",\n        \"description\": \"Develop a state-of-the-art hybrid machine learning model that integrates DeepAR and a CNN for forecasting monthly rainfall amounts over the next year, with anomaly detection for extreme weather patterns. Include real-time satellite data, climate indicators, and socio-economic factors for enhanced accuracy. Implement a feature autoencoder for dimensionality reduction and novelty detection, using reinforcement learning for adaptive improvement with dataset updates. Provide a user-friendly interface for data visualization and model interpretability, and a comprehensive evaluation framework for benchmarking against multiple baselines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Absolute Error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Squared Error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Anomaly Detection Sensitivity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Anomaly Detection Specificity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory Size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Real-time satellite data\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"rainfall data\"],\n            \"specification\": {\n                \"size\": \"varies with real-time updates\"\n            },\n            \"description\": \"Real-time satellite data capturing atmospheric conditions relevant to rainfall\",\n            \"preprocessing\": [\"image preprocessing\", \"feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"Climate indicators\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"climate data\"],\n            \"specification\": {\n                \"dimensions\": \"various\"\n            },\n            \"description\": \"Climate data to incorporate in the forecasting model\",\n            \"preprocessing\": [\"clustering\", \"normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Socio-economic factors\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"economic indicators\"],\n            \"specification\": {\n                \"dimensions\": \"various\"\n            },\n            \"description\": \"Data on socio-economic factors affecting rainfall patterns\",\n            \"preprocessing\": [\"feature selection\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepAR-Convolutional Neural Network (CNN)\",\n            \"family\": \"neural networks\",\n            \"type\": \"hybrid\",\n            \"specification\": {\n                \"DeepAR Components\": {\n                    \"architecture\": \"state-of-the-art\",\n                    \"training\": \"using reinforcement learning for improvement\"\n                },\n                \"CNN Components\": {\n                    \"architecture\": \"including CNN for anomaly detection\",\n                    \"real-time integration\": \"with satellite and climate data\"\n                }\n            },\n            \"description\": \"A hybrid model combining DeepAR for time series forecasting and a CNN for anomaly detection with real-time data inputs.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"dimensionality reduction\",\n            \"reinforcement learning for adaptation\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"proprietary platform\",\n        \"inference_engine\": [\"Autoencoder\", \"Reinforcement Learning Engine\"],\n        \"demonstration\": \"user-friendly interface for model results and interpretation\"\n    },\n    \"knowledge\": [\n        \"Recent advancements in hybrid models and deep learning techniques for improved weather forecasting\",\n        \"Reinforcement learning for model adaptation\"\n    ],\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects a highly complex project requiring expertise in machine learning, including cutting-edge techniques and data sources. The user's high expertise level is assumed. The performance metrics and complexity metrics are left undefined as they need to be specified based on the model's performance and hardware. The missing details in the 'service' section, such as deployment endpoint and user-friendly interface specifics, are placeholders as they would be specific to the project implementation.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "object detection", "instruction": "As a wildlife conservation project manager, you request an advanced, real-time object detection system leveraging the iWildCam dataset. Opt for a state-of-the-art EfficientDet architecture with enhanced interpretability, focusing on biodiversity-specific features, and minimizing latency for seamless deployment on portable, off-grid devices to monitor endangered species in extreme environmental conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"As a wildlife conservation project manager, request an advanced real-time object detection system using the iWildCam dataset. Choose an EfficientDet architecture with enhanced interpretability, targeting biodiversity-specific features, and minimize latency for off-grid deployment on portable devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": \"minimal\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"optimized for off-grid devices\",\n                \"unit\": \"time\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": \"minimized for seamless deployment\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"iWildCam\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"specific_to_biodiversity\": true\n            },\n            \"description\": \"A dataset for wildlife conservation, with a focus on endangered species and real-time monitoring\",\n            \"preprocessing\": [\n                \"enhanced for biodiversity feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientDet\",\n            \"family\": \"EfficientDet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"enhanced\",\n                \"latency\": \"optimized for real-time\"\n            },\n            \"description\": \"A state-of-the-art object detection model with enhanced interpretability, targeting biodiversity features and designed for minimal latency on portable off-grid devices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"iWildCam dataset covers diverse environmental conditions and species for wildlife monitoring\"\n    ],\n    \"service\": {\n        \"target_device\": \"portable, off-grid devices\",\n        \"deployment_endpoint\": \"unclear (may need user input)\",\n        \"inference_engine\": [\n            \"optimized for off-grid environments\"\n        ],\n        \"demonstration\": \"real-time, on-field demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is parsed with a high level of expertise due to the complex nature of the task and requirements. Specifics like the target device and deployment endpoint are open-ended and may require more clarification from the user. The confidence score reflects the confidence in the provided information, with a high emphasis on the technical aspects and the user's request for interpretation and latency optimization.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a seasoned project manager for a cutting-edge machine learning team, your objective is to develop a highly specialized time series classification system using the UWave Gesture Library dataset. This dataset, with its intricate 315-dimensional feature sequences (INPUT_SEQ_LEN) and three distinct partitions (train, validation, and test), requires the design of an advanced model that can accurately classify gestures into one of eight unique classes (label range: 0-7). Emphasize on optimizing performance, robustness, and efficiency while minimizing computational resources. Additionally, devise a novel feature extraction technique and include a thorough evaluation using the chosen metric, precision, and macro-averaged F1 score.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"gesture recognition\",\n        \"description\": \"Develop a highly specialized time series classification system using the UWave Gesture Library dataset. The dataset consists of 315-dimensional feature sequences (INPUT_SEQ_LEN) and has a train, validation, and test partition. The model should classify eight unique gesture classes (0-7) with a focus on performance, robustness, and efficiency. A novel feature extraction technique is required, and the model should be thoroughly evaluated with precision and macro-averaged F1 score as metrics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"macro-averaged F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational resources\",\n                \"value\": \"minimized\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UWave Gesture Library\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"Gesture class labels (0-7)\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 315\n            },\n            \"description\": \"An intricate dataset for gesture recognition, with 315-dimensional feature sequences and partitions for train, validation, and testing.\",\n            \"preprocessing\": [\"novel feature extraction technique\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model complexity\": \"advanced\",\n                \"feature extraction\": \"novel\"\n            },\n            \"description\": \"An advanced time series classification model designed specifically for the UWave Gesture Library dataset with a focus on performance, robustness, and efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using efficient model architectures, such as Transformers or EfficientNet, for time-series processing.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"novel feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Performance metrics to be showcased for each model evaluation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects a high-level understanding of the task, including the target domain, tasks, and key performance indicators. However, specific model architectures, performance metrics, and computational resources optimization methods are not provided, which might require assumptions or input from the user. The confidence score is medium to reflect this dependency on additional details.\",\n        \"score\": 0.7\n    }\n}\n"}
{"task": "question answering", "instruction": "Instruct the project manager to develop a highly scalable and efficient question-answering system using the TriviaQA dataset, incorporating advanced deep learning techniques, specifically a transformer architecture like BERT or a large-scale model like TuringNet. The system must excel in tackling intricate factoid and non-factoid questions, demonstrating exceptional adaptability, while maintaining stringent latency requirements of under 100 milliseconds per response to accommodate high user traffic.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Develop a highly scalable and efficient question-answering system using the TriviaQA dataset. The system should incorporate advanced deep learning techniques, particularly transformer architectures such as BERT or large-scale models like TuringNet. It must excel in handling complex factoid and non-factoid questions while maintaining a latency of under 100 milliseconds to handle high user traffic.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"threshold\": 0.1, \n                    \"unit\": \"seconds\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null, \n                \"unit\": \"high\"\n            },\n            {\n                \"name\": \"memory efficiency\",\n                \"value\": null, \n                \"unit\": \"low memory footprint\"\n            },\n            {\n                \"name\": \"inference speed\",\n                \"value\": {\n                    \"threshold\": 0.1, \n                    \"unit\": \"seconds\"\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TriviaQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A comprehensive dataset for training a question-answering system, with diverse question types.\",\n            \"preprocessing\": [\"BERT tokenization\", \"question and context formatting\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT or TuringNet\",\n            \"family\": \"transformer architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"BERT\": {\n                        \"model_size\": \"large\",\n                        \"inference_speed\": null,\n                        \"memory_consumption\": null\n                    },\n                    \"TuringNet\": {\n                        \"size\": null,\n                        \"latency\": 0.1,\n                        \"resource_consumption\": null\n                    }\n                }\n            },\n            \"description\": \"Advanced deep learning architecture, either BERT or TuringNet, tailored for question answering.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transformer-based models known for their adaptability in handling complex questions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"state-of-the-art acceleration frameworks\"],\n        \"demonstration\": \"demonstrate system's performance and latency with sample question sets\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user clearly specified the intent to build a question-answering system, with requirements for a high-performing transformer architecture and latency constraints. Expertise level is assumed to be high, but some performance metric values are currently unknown as they require model performance testing. Latency is expected to be challenging given the stringent requirement, but clarity is present for the target deployment environment.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "For a high-performance AI project, develop a specialized deep learning convolutional neural network (CNN) architecture, tailored for recognizing subcategories in the CIFAR-100 dataset, ensuring a minimum accuracy of 99% and utilizing data augmentation techniques to enhance generalization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"AI project\",\n        \"description\": \"Develop a high-performance deep learning AI project using a specialized CNN architecture for recognizing subcategories in the CIFAR-100 dataset. Aim for at least 99% accuracy and incorporate data augmentation techniques for better generalization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CIFAR-100\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A benchmark dataset for image classification with subcategories, used for training and testing the specialized CNN architecture.\",\n            \"preprocessing\": [\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\n                \"techniques for enhancing generalization\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"specialized CNN\",\n            \"family\": \"convolutional neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"tailored for CIFAR-100 subcategory recognition\"\n            },\n            \"description\": \"A deep learning architecture specifically designed for CIFAR-100 subcategory recognition, with a high-performance target.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the provided instruction. The high expertise level suggests the user understands the technical requirements, including the architecture and performance metrics. The use of data augmentation and CIFAR-100 benchmark indicates a clear understanding of the project's objectives. However, the user did not specify a target device or deployment details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for the elite AI development team, your objective is to tackle a high-dimensional, real-world challenge in energy demand prediction. Utilizing the Electricity dataset, which has been meticulously stratified into training, validation, and testing sets, design a robust, deep-learning model that can handle complex temporal patterns. The input sequence, consisting of 96 timesteps with 321 distinct features (INPUT_SEQ_LEN=96, INPUT_DIM=321), should forecast not only the subsequent 96 steps but also incorporate seasonality and outliers. Your model's performance must excel in reducing mean squared error (MSE) and mean absolute error (MAE), while demonstrating superior generalization on unseen data. Conduct rigorous experimentation, optimizing hyperparameters and comparing with state-of-the-art methods, before presenting a comprehensive report on your results.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy demand prediction\",\n        \"description\": \"Tackle a high-dimensional, real-world challenge in energy demand prediction using the Electricity dataset. The dataset is stratified into training, validation, and testing sets. The objective is to design a robust deep-learning model capable of handling complex temporal patterns, with a focus on forecasting 96 steps ahead, incorporating seasonality, and outliers. Performance metrics to target are MSE and MAE reduction.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"PRED_SEQ_LEN\": 96\n            },\n            \"description\": \"A dataset for energy demand prediction with 96 timesteps per input sequence and 321 distinct features. Data is stratified into training, validation, and testing sets, considering seasonality and outliers.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep-learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"temporal_pattern_handling\": \"complex\",\n                \"seasonality_inclusion\": true,\n                \"outlier_management\": true\n            },\n            \"description\": \"A deep-learning model designed for energy demand prediction, targeting robustness and superior generalization on unseen data.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal feature extraction\",\n            \"seasonality analysis\",\n            \"outlier detection\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"a comprehensive report with hyperparameter tuning results and comparisons with state-of-the-art methods\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction was parsed based on the given schema. A 'high' expertise level was assumed as it reflects a seasoned project manager with a thorough understanding of the task. The performance metrics, model specifications, and dataset properties were clearly specified. The 'service' section includes steps for rigorous experimentation and report generation. However, the model name and deployment details are not included, as they are not explicitly stated.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for a machine learning team, I seek a sophisticated deep learning model that not only distinguishes between cat and dog images but also identifies their breed with high accuracy, accounting for variations in lighting, pose, and age, using transfer learning and ensemble methods.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal recognition\",\n        \"description\": \"Seek a sophisticated deep learning model for image classification, focusing on distinguishing between cat and dog images with high accuracy. The model must handle variations in lighting, pose, and age, employing transfer learning and ensemble methods.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Generic Cat vs Dog\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"cat\", \"dog\", \"breed\"],\n            \"specification\": {\n                \"image_size\": \"varies (e.g., 224x224 for transfer learning)\",\n                \"age_included\": \"yes\"\n            },\n            \"description\": \"A dataset containing cat and dog images, including variations in lighting, pose, and age for model training and evaluation.\",\n            \"preprocessing\": [\"transfer learning preprocessing\"],\n            \"augmentation\": [\"pose, lighting, and age augmentation\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep convolutional neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer learning\": true,\n                \"ensemble_method\": [\"e.g., VGG, ResNet, InceptionNet\"]\n            },\n            \"description\": \"A deep learning model using transfer learning and ensemble methods for distinguishing between cat and dog images, considering breed identification with high accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for ensemble models\"],\n        \"target_device\": \"cloud or server-based (for efficient ensemble processing)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Serving or equivalent\"],\n        \"demonstration\": \"interactive or API-based demo showcasing breed distinction\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction provided a clear problem statement, specifying accuracy requirements and the use of transfer learning and ensemble methods. The 'expertise' level was assumed to be high due to the project management role. However, the exact names of the models and datasets are not specified. The 'service' and 'confidence' fields were inferred based on the requirements mentioned.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As a project manager for the state-of-the-art machine learning development team, your task is to tackle a highly specialized and intricate time series forecasting challenge. Utilize the intricate Traffic dataset, which consists of diverse, high-resolution, and non-stationary real-world data. The train, validation, and test partitions consist of sequences with a unique pattern (INPUT_SEQ_LEN=96, INPUT_DIM=862) that demand advanced handling of seasonality and anomalies. Your objective is to design an ensemble model employing multiple forecasting techniques, such as LSTMs, attention mechanisms, and Transformer-based architectures, to generate not only the next 96-step future sequence (PRED_SEQ_LEN=96, PRED_DIM=862) but also incorporating multi-step ahead predictions. To ensure robustness, you must optimize for both mean squared error (MSE) and mean absolute error (MAE) with a focus on minimizing forecasting errors and improving generalization across varying conditions. In addition, provide a detailed analysis of your model's interpretability, interpret how the model handles uncertainty, and discuss potential real-world implications for infrastructure planning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"infrastructure planning\",\n        \"description\": \"A highly specialized time series forecasting challenge involving the Traffic dataset with diverse, high-resolution, and non-stationary real-world data. The model must be an ensemble of LSTMs, attention mechanisms, and Transformer-based architectures for multi-step ahead predictions. Focus on optimizing MSE and MAE with robustness against errors and generalization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ],\n        \"interpretability\": {\n            \"explanation\": \"Model interpretability analysis and understanding of how the model handles uncertainty\"\n        },\n        \"real_world_implications\": \"Potential applications for infrastructure planning, such as optimizing traffic flow and resource allocation\"\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Traffic\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 862,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 862\n            },\n            \"description\": \"The dataset contains diverse, high-resolution, and non-stationary real-world traffic data with train, validation, and test partitions.\",\n            \"preprocessing\": [\"Handling seasonality, anomalies, and multi-step predictions\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Ensemble Model (LSTMs, Attention, Transformers)\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_components\": [\"LSTM\", \"attention mechanisms\", \"Transformer\"],\n                \"forecasting_steps\": 96,\n                \"ensemble_size\": null\n            },\n            \"description\": \"A complex model combining LSTMs, attention mechanisms, and Transformer architectures for advanced time series forecasting.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"advanced data preprocessing for seasonality and anomalies\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Model interpretability and real-world implications for infrastructure planning\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed based on the given instruction, interpreting the task as a complex machine learning project. The user's expertise is assumed to be high due to the project manager role. The dataset and model requirements are clearly specified, but some sections like deployment details are left open-ended due to missing information.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "Could you explore the applicability and transfer learning potential of transformer models in developing a sophisticated video forgery detection system for the Celeb-DF dataset? The model should demonstrate advanced discernment capabilities, outperforming a basic random guess algorithm by utilizing multi-level feature extraction and temporal analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"video forensics\",\n        \"description\": \"Investigate the applicability and transfer learning potential of transformer models for developing a sophisticated video forgery detection system on the Celeb-DF dataset. The model should excel in advanced discernment and outperform a basic random guess algorithm by leveraging multi-level feature extraction and temporal analysis.\",\n        \"performance_metrics\": {\n            \"performance_over_random_guess\": {\n                \"name\": \"probability_threshold\",\n                \"value\": 0.0\n            }\n        },\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\"video\"],\n            \"target_variables\": [\"forged vs. authentic\"],\n            \"specification\": {\n                \"length\": \"variable\",\n                \"feature_dimensions\": \"video-based\"\n            },\n            \"description\": \"The Celeb-DF dataset for video forgery detection, containing authentic and forged videos for multimodal analysis.\",\n            \"preprocessing\": [\"frame extraction\", \"multimodal feature extraction\"],\n            \"augmentation\": [\"temporal jittering\", \"multimodal data augmentation\"],\n            \"visualization\": [\"feature embeddings\", \"temporal sequence analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based Video Forgery Detection Model\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-level feature extraction and temporal analysis\"\n            },\n            \"description\": \"A transformer model designed for video forgery detection, capable of outperforming a basic random guess algorithm.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art transformer models in computer vision and multimodal learning\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"multi-scale feature extraction\", \"temporal alignment\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom platform or API\",\n        \"inference_engine\": [\"GPU-accelerated inference\"],\n        \"demonstration\": \"AUC-ROC or confusion matrix results, transfer learning efficacy analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction implies a high-level consult-build scenario where the user is interested in the potential use of transformers and transfer learning for a specific task. They have a high level of expertise, suggesting they're looking for expert advice. The dataset and model requirements, along with the performance metric (outperforming random guess), are included.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for the next-generation medical AI, devise a highly specialized and multi-layered Q&A model that utilizes the extensive BioASQ dataset. Emphasize on integration of domain-specific BioBERT fine-tuning, advanced NER capabilities, and a novel explainability feature. The model must excel in diagnosing complex diseases, distinguish rare conditions, and consistently support its answers with comprehensive, peer-reviewed research citations from PubMed and Scopus indexed publications. Ensure the system meets stringent medical standards and undergo rigorous evaluation with gold-standard annotations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medical AI\",\n        \"description\": \"Design a highly specialized Q&A model for next-generation medical AI, integrating BioBERT fine-tuning, advanced NER, and an explainability feature. The model should excel in diagnosing complex diseases, distinguish rare conditions, and support answers with peer-reviewed citations from PubMed and Scopus.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"diagnosis accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"rare condition detection rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explainability score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"user-friendly\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"complex diseases\", \"rare conditions\"],\n            \"specification\": null,\n            \"description\": \"An extensive medical dataset for fine-tuning and evaluation of the model, focusing on complex diseases and rare conditions.\",\n            \"preprocessing\": [\"BioBERT pretraining\", \"domain-specific data augmentation\"],\n            \"augmentation\": [\"domain-relevant data expansion\"],\n            \"visualization\": [\"model performance on domain-specific subsets\"],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"PubMed and Scopus indexed publications\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"references\"],\n            \"specification\": {\n                \"citations_per_answer\": null\n            },\n            \"description\": \"Comprehensive collection of peer-reviewed research for supporting model responses.\",\n            \"preprocessing\": [\"metadata extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"citation distribution per topic or question\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Domain-specific Q&A model\",\n            \"family\": \"transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"BioBERT fine-tuned\": true,\n                \"NER capabilities\": \"advanced\",\n                \"explainability feature\": true\n            },\n            \"description\": \"A multi-layered model leveraging BioBERT, advanced NER, and explainability for medical Q&A.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integrating medical domain knowledge, rigorous evaluation with gold-standard annotations\",\n        \"Focus on meeting medical standards and handling complex, rare conditions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific feature extraction\", \"citations relevance scoring\"],\n        \"target_device\": \"cloud-based for scalability and high performance\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-based for fast processing\"],\n        \"demonstration\": \"interactive platform for querying and displaying results with supporting citations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided details about the project's core components, the BioBERT fine-tuning and advanced NER, as well as the specific requirements for performance (accuracy, citation support), with a focus on stringent medical standards. The user's high expertise indicates they may need less guidance in these aspects. However, specific metric values, target device, deployment endpoint, and model specifications were not provided and would need to be gathered from further clarification.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for the advanced AI development team, you are now tasked with enhancing the time-series forecasting challenge. The team must handle a multi-modal dataset (combining ILI with temperature data), which has been partitioned into train, validation, and test sets with non-uniform time intervals. The INPUT_SEQ_LEN has been expanded to 48 and includes additional context features (INPUT_DIM=15). The objective is not only to minimize MSE and MAE but also achieve interpretability using feature importance ranking. The model must demonstrate exceptional performance and be adaptable to real-time data with limited computational resources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"advanced AI development\",\n        \"description\": \"Enhance the time-series forecasting challenge with a multi-modal dataset combining ILI and temperature data. The dataset is partitioned into train, validation, and test sets with non-uniform time intervals. INPUT_SEQ_LEN has been expanded to 48 and includes context features (INPUT_DIM=15). The objective is to minimize MSE and MAE while achieving interpretability through feature importance ranking and ensuring exceptional performance under real-time data and limited computational resources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time data handling\",\n                \"value\": null,\n                \"unit\": \"adaptability\"\n            },\n            {\n                \"name\": \"computational resources\",\n                \"value\": null,\n                \"unit\": \"FLOPs or memory footprint\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI-temperature dataset\",\n            \"modality\": [\"time series\", \"multimodal\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 48,\n                \"INPUT_DIM\": 15\n            },\n            \"description\": \"A multi-modal dataset combining ILI (influenza-like illness) with temperature data, with non-uniform time intervals between observations.\",\n            \"preprocessing\": [\"handling multi-modal data\", \"non-uniform time interval handling\"],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance visualization\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"feature importance ranking\",\n                \"real-time adaptability\": \"limited computational resources\"\n            },\n            \"description\": \"A model for multi-modal time-series forecasting with objectives to minimize MSE, MAE, and achieve interpretability through feature importance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction for multi-modal data\",\n            \"transforming non-uniform time intervals\"\n        ],\n        \"target_device\": \"real-time adaptable\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"lightweight, resource-efficient inference\"\n        ],\n        \"demonstration\": \"interpretable model results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the specifics given by the instruction, indicating building a complex project. High expertise suggests extensive knowledge in the field. The multi-modal and real-time data requirements, along with feature interpretability, indicate a deep understanding of the task. However, there are some assumptions made, such as the model name and inference engine, as these details were not provided.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a project manager for the enhanced Bilibili Danmu experience, devise a sophisticated, real-time sequential recommendation system that not only leverages the Bert4Rec architecture but also integrates sentiment analysis and topic modeling from user comments. The model must exhibit advanced context-awareness, handle multilingual input, and demonstrate superior performance in diverse user demographics, all while optimizing for minimal latency in delivering personalized video suggestions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"Bilibili Danmu Experience\",\n        \"description\": \"Design a sophisticated real-time sequential recommendation system that integrates Bert4Rec architecture, sentiment analysis, and topic modeling from user comments. The system must be context-aware, support multilingual input, and demonstrate excellent performance across diverse demographics, while optimizing for low latency in personalized video suggestions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"NDCG\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"minimal\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Bilibili Danmu Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"video recommendations\"],\n            \"specification\": {\n                \"multilingual\": true,\n                \"diverse_user_demographics\": true\n            },\n            \"description\": \"User comments dataset collected from Bilibili, containing text data for sentiment analysis and topic modeling.\",\n            \"preprocessing\": [\"multilingual text cleaning\", \"sentiment lexicon integration\", \"topic modeling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Bert4Rec\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"context-aware, sequential recommendation\",\n                \"handles_multilingual_input\": true,\n                \"advanced_features\": [\"sentiment embeddings\", \"topic embeddings\"]\n            },\n            \"description\": \"A Bert4Rec model enhanced with sentiment and topic modeling capabilities, designed for real-time sequential recommendation on Bilibili Danmu data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Context-awareness is crucial for personalized recommendations.\",\n        \"Multilingual support helps reach a broader user base.\",\n        \"Topic modeling helps identify user preferences in comment data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-aware data preprocessing\", \"real-time updating\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"Bilibili's recommendation API\",\n        \"inference_engine\": [\"GPU-based inference\"],\n        \"demonstration\": \"Live performance metrics and personalized video suggestions demo\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been parsed with high confidence, capturing the user's intent to build a sophisticated recommendation system, their technical expertise, and detailed requirements. The dataset and model properties were derived from the context, and the service requirements align with the objectives. However, specific performance metrics and latency expectations need to be discussed further for a complete understanding.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "For a multi-faceted machine learning challenge, you are tasked with developing a robust time-series classification model. The Ethanol Concentration dataset, featuring high-dimensional data (INPUT_SEQ_LEN=1751, INPUT_DIM=30) with temporally correlated noise, has been meticulously stratified into train, validation, and test sets. Your objective is to design a model that not only exhibits superior accuracy on standardized sequences but also demonstrates generalization ability and resilience to outliers. Additionally, incorporate regularization techniques to prevent overfitting and provide a comprehensive evaluation of performance using AUC-ROC and F1-score.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"multifaceted machine learning challenge\",\n        \"description\": \"You are tasked with developing a robust time-series classification model for the Ethanol Concentration dataset. The dataset has high-dimensional data (INPUT_SEQ_LEN=1751, INPUT_DIM=30) with temporally correlated noise and is stratified into train, validation, and test sets. The model should have high accuracy, generalization ability, and handle outliers. Regularization techniques for preventing overfitting are required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ethanol Concentration\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 1751,\n                \"INPUT_DIM\": 30\n            },\n            \"description\": \"A dataset for time-series classification with high-dimensional data, temporally correlated noise, and stratified into train, validation, and test sets.\",\n            \"preprocessing\": [\" Handling temporally correlated noise\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"regularization\": \"incorporated\"\n            },\n            \"description\": \"A robust time-series classification model designed to handle the Ethanol Concentration dataset with focus on accuracy, generalization, and resilience against outliers.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"incorporating regularization techniques\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"comprehensive evaluation with AUC-ROC and F1-score\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been parsed accurately, capturing the user's intent to build a robust model, their high level of expertise, and the specific requirements for the dataset, model, and performance metrics. The dataset and model sections require some assumptions, such as preprocessing and regularization techniques, but the intent and expectations are clear. The confidence score is 0.95 due to the certainty of the overall interpretation and the minor assumptions made.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "As a project manager for the Tencent Weibo platform, task the machine learning development team to design an advanced Attention-based Graph Neural Network (AGNN) variant, optimizing for edge weight distribution in a high-dimensional social graph. The model must accurately identify subtle influence patterns and community dynamics, considering temporal factors and node attributes, to enhance the overall link prediction performance on the platform.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"social media\",\n        \"description\": \"Design an advanced Attention-based Graph Neural Network (AGNN) variant for the Tencent Weibo platform. Focus on optimizing edge weight distribution in a high-dimensional social graph, while accounting for temporal factors and node attributes to improve link prediction accuracy, particularly for identifying subtle influence patterns and community dynamics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"link prediction accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"optimization for edge weight distribution\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tencent Weibo social graph dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"high-dimensional\": true,\n                \"temporal_factors_included\": true,\n                \"node_attributes_included\": true\n            },\n            \"description\": \"A dataset representing the high-dimensional social graph of Tencent Weibo, incorporating temporal and node attribute information.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Attention-based Graph Neural Network (AGNN)\",\n            \"family\": \"Attention-based Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization_goal\": \"edge_weight_distribution\",\n                \"temporal_factor_integration\": true,\n                \"node_attribute_integration\": true\n            },\n            \"description\": \"An AGNN variant designed for link prediction on Tencent Weibo's social graph, with a focus on subtlety in influence patterns and community dynamics.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed to match the JSON schema. The user's expertise level is assumed to be high due to the project management context. The problem section describes the specifics of the AGNN variant, performance metrics and temporal aspects are taken into account. However, explicit values for performance and complexity metrics are not provided and should be filled based on further analysis.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "Kindly investigate the fine-tuning of MViT models on theCeleb-DF dataset for detecting high-resolution deepfake videos, specifically focusing on Cross-modal fusion and spatio-temporal attention mechanisms. Demonstrate a clear performance improvement over a state-of-the-art baseline, incorporating advanced flow-based motion analysis and considering real-world scenarios with variable lighting conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"deepfake video detection\",\n        \"description\": \"Investigate fine-tuning of MViT models on the Celeb-DF dataset, with a focus on detecting high-resolution deepfake videos. The research should emphasize cross-modal fusion and spatio-temporal attention mechanisms. It should show a performance improvement over a state-of-the-art baseline and account for real-world scenarios with variable lighting conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"ROC AUC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory Usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\n                \"image\",\n                \"video\"\n            ],\n            \"target_variables\": [\"deepfake label\"],\n            \"specifications\": {\n                \"image_resolution\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"video_frame_rate\": null,\n                \"variations\": \"high-resolution, real-world (variable lighting)\"\n            },\n            \"description\": \"A dataset for detecting high-resolution deepfake videos, including real-world scenarios with varying lighting conditions.\",\n            \"preprocessing\": [\n                \"Cross-modal fusion\",\n                \"Spatio-temporal feature extraction\"\n            ],\n            \"augmentation\": [\n                \"Advanced flow-based motion analysis\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MViT\",\n            \"family\": \"Multimodal Vision Transformer\",\n            \"type\": \"neural networks\",\n            \"specifications\": {\n                \"architecture\": \"Fine-tuned MViT for forgery detection\",\n                \"fusion_method\": \"Cross-modal fusion\",\n                \"attention_mechanism\": \"Spatio-temporal\"\n            },\n            \"description\": \"A MViT model tailored for high-resolution deepfake video detection using cross-modal fusion and spatio-temporal attention mechanisms.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art baseline comparison\",\n        \"Real-world scenarios consideration: variable lighting conditions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud-based or edge server\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Real-time or near real-time processing\"],\n        \"demonstration\": {\n            \"results\": \"Performance improvement over the baseline with focus on real-world conditions\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction has been parsed into a JSON format reflecting a high-level build task. The user has high expertise, and the problem focuses on multimodal learning and real-world scenarios. The performance metrics and complexity constraints have been included, but the specific metric values are left unspecified as they need to be derived through the fine-tuning process. The 'service' section's details may need further refinement based on the actual performance and deployment requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a highly specific on-device, real-time neural machine translation application leveraging the TEDx Talk corpus for Marathi to Konkani translation. Mandate the use of a proprietary Enhanced-EfficientTransformer-XL model, focused on achieving breakthrough efficiency with at least 25% energy efficiency on Raspberry Pi Zero, maintaining <5MB memory footprint, and optimizing for sub-10% CPU utilization to ensure seamless, prolonged translation without compromising performance on compact, handheld devices with limited battery life.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"real-time translation\",\n        \"description\": \"Develop a highly specific, on-device neural machine translation app for Marathi to Konkani translation using the TEDx Talk corpus. The application should utilize a proprietary Enhanced-EfficientTransformer-XL model, targeting at least 25% energy efficiency on a Raspberry Pi Zero, maintaining a memory footprint of <5MB, and optimizing CPU utilization to sub-10% for real-time and efficient operation on limited-battery handheld devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": 0.25\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory size\",\n                \"value\": 5,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"CPU utilization\",\n                \"value\": 0.1,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TEDx Talk corpus (Marathi to Konkani)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"TEDx Talk corpus\",\n                \"languages\": [\"Marathi\", \"Konkani\"]\n            },\n            \"description\": \"A specialized corpus for Marathi to Konkani translation tasks sourced from TEDx Talks.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced-EfficientTransformer-XL\",\n            \"family\": \"Enhanced-EfficientTransformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": \"breakthrough\",\n                \"requirements\": {\n                    \"device\": \"Raspberry Pi Zero\",\n                    \"performance\": {\n                        \"energy_efficiency\": 0.25,\n                        \"memory_size\": 5,\n                        \"cpu_usage\": {\n                            \"min\": 0.1\n                        }\n                    }\n                }\n            },\n            \"description\": \"A proprietary neural network model designed for Enhanced-EfficientTransformer-XL, optimized for Marathi to Konkani translation on Raspberry Pi Zero.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"Raspberry Pi Zero\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            {\n                \"name\": \"optimized for Raspberry Pi\",\n                \"requirements\": {\n                    \"battery_life_sensitivity\": true\n                }\n            }\n        ],\n        \"demonstration\": \"sub-10% CPU utilization, real-time translation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction describes a complex machine learning project with high-level details on the application domain, target device, and model specifications. The user's high expertise level indicates a clear understanding of the requirements. However, the JSON lacks explicit details about some service components such as the deployment endpoint and demonstration's exact method.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As the project lead for the AI-driven medical innovation team, your mandate is to develop a highly advanced diabetic retinopathy classifier using the comprehensive Kaggle dataset. The project necessitates an innovative VGG-16 architecture upgrade, which not only discerns between normal and diseased retinas but also maps the severity into a fine-grained classification system of five unique stages, each with subcategories. The model must exhibit a state-of-the-art accuracy of at least 97%, prioritizing both precision and recall, particularly in reducing false negatives to minimize diagnostic errors with serious health implications. Additionally, the design should consider edge computing for real-world deployment in resource-limited ophthalmic clinics, guaranteeing rapid and efficient inferences without any compromise on accuracy or computational resource optimization. Provide a comprehensive blueprint, step-by-step implementation roadmap, and rigorous performance assessments, complete with benchmark comparisons, to substantiate the groundbreaking efficiency and practical feasibility of this life-saving solution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medical\",\n        \"description\": \"Develop a highly advanced diabetic retinopathy classifier using the Kaggle dataset. The project involves upgrading the VGG-16 architecture to distinguish five unique stages with subcategories, targeting state-of-the-art accuracy (97%) and strong precision and recall. Emphasis is on minimizing false negatives and optimizing for edge computing in ophthalmic clinics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null,\n                \"subcategories\": [\n                    {\n                        \"name\": \"false negatives\",\n                        \"value\": null\n                    }\n                ]\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null,\n                \"subcategories\": [\n                    {\n                        \"name\": \"false negatives\",\n                        \"value\": null\n                    }\n                ]\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Kaggle diabetic retinopathy dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"normal\", \"stage1\", \"stage2\", \"stage3\", \"stage4\", \"stage5\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"image_channels\": 3,\n                \"classes\": 6\n            },\n            \"description\": \"A comprehensive dataset for classifying diabetic retinopathy into five stages with subcategories, suitable for VGG-16 architecture enhancement.\",\n            \"preprocessing\": [\"data augmentation (to improve generalization)\"],\n            \"augmentation\": [\"rotation\", \"shearing\", \"cropping\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Innovative VGG-16 architecture\",\n            \"family\": \"VGG\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pre-trained\": \"on ImageNet\",\n                \"layers_upgraded\": \"specific to the diabetic retinopathy task\"\n            },\n            \"description\": \"An advanced VGG-16 architecture with specific layers tailored for the multi-class fine-grained diabetic retinopathy classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilize transfer learning to leverage pre-trained weights.\",\n        \"Consider input size optimization for edge devices.\",\n        \"Implement a mixed precision strategy to reduce computational overhead.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"explainability techniques for medical decision support\"\n        ],\n        \"target_device\": \"edge computing\",\n        \"deployment_endpoint\": \"optimized for ophthalmic clinics\",\n        \"inference_engine\": [\"dynamically adjusted for resource constraints\"],\n        \"demonstration\": \"include sample test cases and performance comparisons on real-world data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON reflects the project details from the instruction, including the high expertise level and the emphasis on accuracy, precision, and recall. The Kaggle dataset source, VGG-16 architecture update, and the focus on edge computing are captured. The need for a comprehensive roadmap and benchmark comparisons is reflected in the service and performance assessment requirements. However, there might be slight ambiguity with performance metrics and inference times due to the lack of specific values, which is typical when they are not directly provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning project manager, your objective is to supervise the development of an intricate deep learning architecture for an image recognition system. The system should excel at identifying celestial objects in the highly diverse Galaxy Zoo dataset, utilizing the Swin Transformer for cutting-edge feature extraction. It must meet or exceed a stringent benchmark of 98% accuracy on an unseen validation set, showcasing exceptional generalization and resistance to distortions. To ensure real-world adaptability and reliability, the model should demonstrate superior transfer learning capabilities and resilience to noisy astronomical data. Timely implementation is critical, as this technology will underpin groundbreaking discoveries in astronomy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The objective is to supervise the development of an intricate deep learning architecture for an image recognition system, targeting celestial object identification in the Galaxy Zoo dataset using Swin Transformer for advanced feature extraction. The model must achieve at least 98% accuracy on an unseen validation set with strong generalization and noise resilience. The project emphasizes transfer learning capabilities and practical application for real-world astronomy advancements.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_complexity\": \"highly diverse\",\n                \"specific_techniques\": \"Swin Transformer\"\n            },\n            \"description\": \"A comprehensive dataset for celestial object recognition with emphasis on the Galaxy Zoo's diverse astronomical data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"noise resilience\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Swin Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced_feature_extraction\": true,\n                \"transfer_learning_capability\": true\n            },\n            \"description\": \"A sophisticated deep learning architecture designed for image recognition with emphasis on Swin Transformer for accurate celestial object identification in noisy astronomical data.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"transfer learning and real-world adaptability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the instruction, which specifies a high-level objective for a project involving a complex deep learning model for astronomical image recognition. The 'expertise' level is set to 'high' since it involves complex architecture development and meeting specific performance metrics. However, the model's actual name and detailed specifications are not provided, so the 'specification' field is left null. The 'confidence' score is reduced due to the lack of explicit details on target device and deployment.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "For a challenging project, design a machine learning pipeline that integrates transformers optimized for cross-modal analysis to detect authenticity in the extensive Celeb-DF dataset. Mandate a minimum 15% improvement over the current baseline, which involves random classification, and assess the model's robustness across varying video manipulations and resolutions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"image and video authenticity\",\n        \"description\": \"Design a machine learning pipeline that integrates transformers optimized for cross-modal analysis to detect authenticity in the Celeb-DF dataset. The target is a 15% improvement over the current random classification baseline, and the model's robustness should be evaluated across different video manipulations and resolutions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"improvement over baseline\",\n                \"value\": 0.15\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\n                \"image\",\n                \"video\"\n            ],\n            \"target_variables\": [\"authenticity\"],\n            \"specification\": {\n                \"video_manipulations\": [\"unknown\"],\n                \"resolutions\": [\"unknown\"]\n            },\n            \"description\": \"A challenging dataset for multimodal forgery detection, featuring extensive images and videos.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"transformers for cross-modal analysis\",\n            \"family\": \"Transformer models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization\": {\n                    \"baseline_improvement\": 15.0\n                }\n            },\n            \"description\": \"An advanced model specifically designed for multimodal analysis with a focus on detecting authenticity.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"cross-modal authentication service\",\n        \"inference_engine\": [\"CPU-optimized\", \"GPU-optimized\"],\n        \"demonstration\": \"Robustness demonstrated through generated visualizations of performance on manipulated videos with varying resolutions.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction specifies a high-level project intent to build, and suggests a high level of expertise. The user requires a machine learning pipeline using transformers for cross-modal analysis in the Celeb-DF dataset. The performance metric requires a 15% improvement over the random baseline, and the model's robustness is to be tested with video manipulations and different resolutions. The model and service sections are incomplete because specific transformer model and deployment details are missing, but they provide enough guidance to infer requirements.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the Project Manager for the innovative AI-driven Time Series Analytics Team, you're now facing a specialized challenge with the \"Smart Grid Datapool\" (INPUT_SEQ_LEN=96, INPUT_FEATURES\tinclude renewable energy metrics, historical consumption, and weather variables, totalling 321 indicators). The data is divided into training, validation, and testing sets, each capturing distinct seasonal patterns and complex interdependencies. Your mandate is to design and engineer a highly sophisticated forecasting model, harnessing advanced deep learning architectures like Transformer-XL with hierarchical attention. The model should forecast the subsequent 96-hour sequence (PRED_SEQ_LEN=96) while incorporating real-time anomalies, external market trends, and prediction uncertainty. Optimize for mean absolute percentage error (MAPE), mean log-normal error (MLE), and computational efficiency, ensuring interpretability by integrating explainable AI techniques. Emphasize model explainability and resilience to micro-variability, all while adhering to stringent latency requirements for utility grid operations. Prove the efficacy of this cutting-edge solution through rigorous A/B testing on multiple regional datasets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart grid\",\n        \"description\": \"As a Project Manager for the AI-driven Time Series Analytics Team, faced with the 'Smart Grid Datapool', the task is to design and engineer a sophisticated forecasting model using Transformer-XL with hierarchical attention. The model must forecast the next 96-hour sequence while considering real-time anomalies, external market trends, and prediction uncertainty, optimizing for MAPE and MLE, and maintaining computational efficiency and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computation time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ],\n        \"requirement\": {\n            \"explainability\": true,\n            \"anomaly_detection\": true,\n            \"micro-variability_resilience\": true,\n            \"latency\": {\n                \"type\": \"string\",\n                \"description\": \"Stricter latency requirements for utility grid operations, specific value required.\"\n            }\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Smart Grid Datapool\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_FEATURES\": 321,\n                \"PRED_SEQ_LEN\": 96\n            },\n            \"description\": \"Data including renewable energy metrics, historical consumption, and weather variables, divided into training, validation, and testing sets with distinct seasonal patterns and complex interdependencies.\",\n            \"preprocessing\": [\"feature_engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL with hierarchical attention\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-XL\",\n                \"attention_type\": \"hierarchical\"\n            },\n            \"description\": \"A highly sophisticated forecasting model for the Smart Grid Datapool.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": true,\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A/B testing on regional datasets\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the user's high expertise and the specific requirements of the project, including advanced model architecture, performance metrics, and interpretability. Some values are left blank for specific metrics and latency, which need to be extracted from the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "Revise the project for a niche luxury grocery store, requiring a highly specialized YOLOv5.1 variant, specifically tailored for the \"ExquisiteGourmetProductDataset\" containing 1,500 distinct premium food categories. Achieve a minimum F1-score of 94% with 93% product detection accuracy. Implement real-time inventory management with a tight latency constraint of 1.25 milliseconds per image capturing detailed artisan goods, ensuring seamless synchronization with the inventory database.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury grocery store\",\n        \"description\": \"Revise the project for a niche luxury grocery store using a highly specialized YOLOv5.1 variant designed for the ExquisiteGourmetProductDataset with 1,500 distinct premium food categories. The model must achieve a minimum F1-score of 94% with 93% product detection accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.94\n            },\n            {\n                \"name\": \"Product detection accuracy\",\n                \"value\": 0.93\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency per image\",\n                \"value\": 0.00125,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ExquisiteGourmetProductDataset\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"distinct_food_categories\": 1500\n            },\n            \"description\": \"A dataset containing 1,500 premium food categories for luxury grocery store use.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5.1 variant\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"customization\": \"highly specialized for ExquisiteGourmetProductDataset\",\n                \"accuracy_target\": 0.94,\n                \"detection_rate_target\": 0.93\n            },\n            \"description\": \"A highly specialized YOLOv5.1 for object detection in luxury grocery context.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Context: The model should be tailored for detailed artisan goods, requiring real-time inventory management and tight latency.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time processing (fast inference)\",\n        \"deployment_endpoint\": \"highly optimized for luxury grocery store\",\n        \"inference_engine\": [\n            \"real-time\",\n            \"optimized for low latency\"\n        ],\n        \"demonstration\": {\n            \"requirement\": \"seamless synchronization with inventory database\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction specifies a clear task for a specialized YOLOv5.1, tailored dataset, performance metrics, and latency requirements. The high expertise level indicates the user has a deep understanding and is seeking optimization. The service section includes real-time processing, deployment, and synchronization constraints. However, the details for feature engineering and inference engine optimizations are not explicitly provided, which might need further clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a specialized deep learning team, you've been assigned to design a highly specialized RNN-T (Recurrent Neural Network Transducer) model for the ETTm2 dataset, a intricate real-life time series dataset with fine-grained noise cancellation. The train, validation, and test splits consist of distinct sequences (96 timesteps each) from 7 diverse input channels, denoted as INPUT_SEQ_LEN and INPUT_DIM. The model must excel in extracting subtle, multi-seasonal patterns over a historical window, forecasting 96 steps ahead, and demonstrate excellence in both quantile-based loss (QLIKE) and mean squared logarithmic error (MSLE). Moreover, mandate the model to possess explainable AI capabilities and adaptive anomaly detection, ensuring it can gracefully handle irregularities without sacrificing peak computational speed on parallel hardware architecture.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-life, fine-grained noise cancellation\",\n        \"description\": \"Design a highly specialized RNN-T model for the ETTm2 dataset, which includes distinct sequence predictions with 96 timesteps and 7 diverse input channels. The model should excel in quantile-based loss (QLIKE) and mean squared logarithmic error (MSLE), have explainable AI capabilities, and adaptive anomaly detection, maintaining high computational speed on parallel hardware.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"QLIKE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MSLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 96,\n                \"description\": \"Contains distinct sequences with 96 timesteps each from 7 diverse input channels\"\n            },\n            \"description\": \"A complex time series dataset for RNN-T model, split into train, validation, and test with noise cancellation requirements.\",\n            \"preprocessing\": [\"noise cancellation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-T\",\n            \"family\": \"Recurrent Neural Network Transducers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainable AI capabilities\": true,\n                \"adaptive anomaly detection\": true,\n                \"computational speed on parallel hardware\": {\n                    \"type\": \"fast\"\n                }\n            },\n            \"description\": \"Specialized model designed for the ETTm2 dataset, with an emphasis on extracting multi-seasonal patterns, QLIKE, and MSLE evaluation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"parallel hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"explainable AI\": true,\n            \"anomaly detection\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response incorporates the provided requirements, such as building a specialized RNN-T model, the complex dataset details, and specific performance metrics. The user's high expertise level is indicated, and the expectation for explainable AI and anomaly detection is included. However, some details like specific computational speed or deployment endpoints are not explicitly provided in the instruction, leaving them speculative.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "visual question answering", "instruction": "For our advanced university research, we aim to develop a state-of-the-art visual question-answering system using the TextVQA dataset. The project necessitates not only a deep comprehension of transformer-based models but also a multi-step reasoning approach to extract, analyze text from images, and generate precise answers, ensuring near-human performance in a competitive benchmark.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"university research\",\n        \"description\": \"The user is working on an advanced university research project to develop a state-of-the-art visual question-answering system, leveraging the TextVQA dataset. The project requires expertise in transformer-based models and includes multi-step reasoning for analyzing text in images and achieving near-human performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"near-human performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TextVQA\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"benchmark\": \"competitive\"\n            },\n            \"description\": \"A dataset for visual question answering that emphasizes deep comprehension and multi-step reasoning.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"transformer-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"multi-step reasoning\": true\n            },\n            \"description\": \"A model designed for visual question answering, capable of extracting and analyzing text from images, targeting near-human performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is to build a complex model for advanced research, indicating high expertise. The project requirements are clearly specified, including the use of a transformer-based model and a multi-step reasoning approach. The TextVQA dataset is identified and the focus on near-human performance is captured. However, some explicit metrics for performance are not provided, like 'near-human performance' and deployment specifics.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a sophisticated translation system using the ParaCrawl dataset, employing a state-of-the-art dual encoder architecture specifically for English to Dutch translations. Ensure semantic accuracy, preservation of dialect nuances, and incorporate sentiment analysis to maintain the emotional essence in the translated text, all within a resource-constrained time frame.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"Create a sophisticated translation system using the ParaCrawl dataset for English to Dutch translations. The system should employ a state-of-the-art dual encoder architecture, focus on semantic accuracy, dialect nuances preservation, and include sentiment analysis to maintain emotional essence in the translation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"semantic accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"dialect nuances preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sentiment analysis preservation\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource utilization\",\n                \"value\": null,\n                \"unit\": \"time or memory\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"English\", \"Dutch\"],\n                \"size\": {\n                    \"train\": \"\",\n                    \"validation\": \"\",\n                    \"test\": \"\"\n                }\n            },\n            \"description\": \"A large-scale dataset for English to Dutch translation, suitable for a dual encoder architecture\",\n            \"preprocessing\": [\"cleaning, tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art dual encoder architecture\",\n            \"family\": \"neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"dual encoder\",\n                \"language_pair\": \"English to Dutch\"\n            },\n            \"description\": \"A specialized architecture for highly accurate and nuanced English to Dutch translations\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"resource-efficient techniques\"\n        ],\n        \"target_device\": \"resource-constrained environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource constraints\"],\n        \"demonstration\": \"showcase resource efficiency while maintaining translation quality\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction indicated a complex task that requires a high level of expertise. Semantic accuracy, dialect nuances, and sentiment analysis are key areas that were mapped to performance metrics. Resource-constrained environment was inferred from the time frame mentioned. However, the user's expertise level is specified as 'high' without further elaboration, so that might need clarification.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "image segmentation", "instruction": "Develop a real-time, 4K coastal surveillance system using the COCO-Sea dataset and Attention U-Net. Ensure pixel-level accuracy while distinguishing land, water, and structures under varying light conditions. Mandate an efficiency boost of at least 30% compared to existing models without sacrificing image quality.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"coastal surveillance\",\n        \"description\": \"Develop a real-time, 4K coastal surveillance system using the COCO-Sea dataset. The system should achieve pixel-level accuracy in classifying land, water, and structures. It must operate under varying light conditions while improving efficiency by at least 30% compared to existing models without compromising image quality.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"pixel-level accuracy\",\n                \"value\": 1.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"efficiency boost\",\n                \"value\": 0.3,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO-Sea\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"land\", \"water\", \"structures\"],\n            \"specification\": {\n                \"image_resolution\": {\n                    \"width\": 4096,\n                    \"height\": 2160,\n                    \"format\": \"4K\"\n                }\n            },\n            \"description\": \"A dataset for coastal surveillance with 4K resolution, designed to handle varying light conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention U-Net\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"image_quality_factor\": \"preserved\",\n                \"efficiency_improvement\": 0.3\n            },\n            \"description\": \"A state-of-the-art model for image segmentation with attention mechanism, designed to distinguish land, water, and structures with real-time performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time system\",\n        \"deployment_endpoint\": \"optimised for real-world deployment\",\n        \"inference_engine\": [\"optimized for speed\"],\n        \"demonstration\": \"demonstrate real-time operation and improved efficiency on the COCO-Sea dataset\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed to cover all aspects of the ML project. The user has a high level of expertise, and their requirements include specific metrics for accuracy and efficiency. The dataset and model properties are well-defined, as is the system's target device and deployment endpoint. The requested demo highlights the real-time and efficient nature of the system.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "You're tackling a specialized deep learning challenge for advanced hydroelectric power plant performance prediction using the ETTm2 dataset, a complex, high-resolution collection with 18 unique attributes (INPUT_DIM=18). The dataset is partitioned into distinct sections for training, calibration with varying seasonality (train[split1, split2]), and rigorous testing. Your model must process an intricate sequence history (INPUT_SEQ_LEN=288, capturing hourly data), anticipating future patterns in the next 72 hours (PRED_SEQ_LEN=144) with accuracy. Emphasize on minimizing mean squared error (MSE) and mean absolute percentage error (MAPE) to demonstrate superior predictive prowess in a noisy, non-stationary environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"hydroelectric power plant performance\",\n        \"description\": \"Tackle a specialized deep learning challenge for advanced hydroelectric power plant performance prediction using the ETTm2 dataset. The dataset has 18 unique attributes (INPUT_DIM=18) and is partitioned for training (split1 and split2), calibration with varying seasonality, and rigorous testing. The model must process a sequence history of 288 hours (INPUT_SEQ_LEN=288) and predict patterns for the next 72 hours (PRED_SEQ_LEN=144) while minimizing MSE and MAPE in a noisy, non-stationary environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 288,\n                \"INPUT_DIM\": 18,\n                \"PRED_SEQ_LEN\": 144,\n                \"seasonality_varying\": \"true\"\n            },\n            \"description\": \"A complex, high-resolution dataset for hydroelectric power plant performance prediction with 18 unique attributes. Split into train, calibration, and test sections for different purposes.\",\n            \"preprocessing\": [\"handling non-stationarity\", \"noise reduction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"sequence_history_length\": 288,\n                \"sequence_input_size\": 18,\n                \"prediction_window\": 72,\n                \"architecture\": \"suitable for time-series forecasting in a noisy, non-stationary environment\"\n            },\n            \"description\": \"A specialized deep learning model for power plant performance prediction, handling complex sequence history and minimizing MSE and MAPE.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature engineering for non-linear relationships\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for noise reduction and non-stationarity\"],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has high expertise, and the instruction provides a clear request to build a specialized model for power plant performance prediction. The dataset, model requirements, and performance metrics are specified in detail. However, there is room for more precision in the 'service' section regarding target device and deployment details, and some inferences were made for the feature engineering and inference engine.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "You are now tackling a specialized deep forecasting challenge using the Electricity Load Forecasting dataset, which consists of minutely-resolution power consumption data. The train, validation, and test partitions have been meticulously stratified based on weather patterns and regional consumption behavior. The input sequences, of length 96 days (INPUT_SEQ_LEN=96, sensing 21 diverse variables, INPUT_DIM=21), require predictive modeling for the subsequent 96-day period (PRED_SEQ_LEN=96) with hourly forecasts of 21 load components. Performance must surpass industry benchmarks with a focus on both Mean Squared Logarithmic Error (MSLE) and Mean Absolute Percentage Error (MAPE). Implement a state-of-the-art deep learning architecture, such as an LSTM-Autoencoder ensemble, and optimize hyperparameters using Bayesian optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"power consumption\",\n        \"description\": \"Tackling a specialized deep forecasting challenge using the Electricity Load Forecasting dataset with minutely-resolution data, stratified by weather patterns and regional consumption behavior. The task involves predicting the next 96-day period with hourly forecasts for 21 load components, targeting to surpass industry benchmarks in terms of MSLE and MAPE.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSLE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity Load Forecasting\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 21,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 21,\n                \"time_resolution\": \"minutely\"\n            },\n            \"description\": \"Dataset with stratified train, validation, and test partitions based on weather and regional consumption behavior.\",\n            \"preprocessing\": [\"sensing diverse variables\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"LSTM-Autoencoder Ensemble\",\n            \"family\": \"LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_method\": \"LSTM-Autoencoder\",\n                \"hyperparameter_optimization\": \"Bayesian\"\n            },\n            \"description\": \"Implementing a state-of-the-art deep learning architecture for power load forecasting with optimized hyperparameters using Bayesian optimization.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed into a high-level JSON format, reflecting a build intent with high expertise. The details cover area, task, dataset specifics (including stratification), performance metrics, and the model requirements (LSTM-Autoencoder ensemble with Bayesian optimization). However, the service deployment and inference engine specifics are not mentioned as the instruction does not provide them.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project manager for a specialized machine learning development team, you've been tasked with the creation of an advanced multilingual transformer model that not only translates between Spanish and English with exceptional accuracy but also seamlessly integrates with a voice-controlled wearable device. The project should utilize the cutting-edge M6 model from DAMO Academy, known for its superior low-resource language translation abilities. The model must demonstrate a state-of-the-art BLEU score of at least 95% on the IWSLT14 evaluation dataset, while maintaining an average response time of 150 milliseconds or less for every sentence on a resource-constrained embedded platform meant for use in remote, real-time smart healthcare applications. In addition to translation speed, emphasize on energy efficiency to ensure the device's battery life lasts up to 10 days under continuous usage. Provide a detailed architecture proposal, a prototype build, and a comprehensive performance benchmarking report in your proposal to the stakeholders.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"smart healthcare and wearables\",\n        \"description\": \"Develop an advanced multilingual transformer model using DAMO Academy's M6 model, focusing on Spanish to English translation with >95% BLEU score on IWSLT14 dataset. The model must have a response time of <150 ms on resource-constrained embedded platforms for real-time smart healthcare applications, and emphasize energy efficiency for battery life of at least 10 days under continuous usage.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"Response time per sentence\",\n                \"value\": 0.15,\n                \"unit\": \"seconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Average energy consumption per translation\",\n                \"value\": null, \n                \"unit\": \"Watts\"\n            },\n            {\n                \"name\": \"Device battery life under continuous usage (days)\",\n                \"value\": 10\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"IWSLT14\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated English sentences\"],\n            \"specification\": {\n                \"language_pair\": \"Spanish to English\"\n            },\n            \"description\": \"Evaluation dataset for Spanish to English translation performance, targeting the M6 model from DAMO Academy.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DAMO Academy M6\",\n            \"family\": \"transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"large\",\n                \"language_pair\": \"Spanish to English\",\n                \"low_resource_capability\": true,\n                \"energy_efficiency\": null\n            },\n            \"description\": \"Advanced multilingual model with focus on Spanish to English translation and low-resource languages\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"model特性\",\n            \"value\": \"M6's low-resource language translation abilities and multilingual support.\"\n        },\n        {\n            \"type\": \"hardware consideration\",\n            \"value\": \"Resource-constrained embedded platform for smart healthcare applications.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained embedded\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"energy-efficient inference\"],\n        \"demonstration\": {\n            \"focus\": [\"translated voice-commands\", \"energy consumption monitoring\", \"battery life metrics\"]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is clear to build a model and has high expertise. Details such as the M6 model's low-resource capabilities and required performance metrics were extracted. However, specific energy efficiency specifications for the model and the deployment endpoint were not directly stated, which might affect the completeness of the response.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "In the context of a high-volume, dynamically evolving retail landscape, develop a real-time, state-of-the-art YOLO-based object detection model for the \"StoreShelf Dataset v3.0\". The model must classify a diverse range of 100 unique products, ensuring precision > 95% and recall > 93% to minimize misclassifications. Design the algorithm to optimize for speed, constraining processing time to an average of less than 1.5 seconds per shelf image while seamlessly integrating with the inventory management system for real-time updates and automatic adjustments to stock levels based on automated scans. Additionally, the system must handle seasonal variations in products and adapt to new SKU introduction without significant impact on performance or accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"Develop a real-time, state-of-the-art YOLO-based object detection model for the StoreShelf Dataset v3.0. The model should classify 100 unique products with precision > 95% and recall > 93%, optimizing for speed (under 1.5 seconds per shelf image), and integrating with inventory management for real-time updates and stock adjustments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.93\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"average inference time\",\n                \"value\": 1.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"StoreShelf Dataset v3.0\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_unique_products\": 100\n            },\n            \"description\": \"A high-volume, dynamic retail dataset with varying product categories for real-time object detection.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO-based object detection model\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed_optimized\": true,\n                \"accuracy_tradeoff\": {\n                    \"precision\": \"95%\",\n                    \"recall\": \"93%\"\n                }\n            },\n            \"description\": \"A real-time object detection model with adaptations for seasonal variations and new SKU introduction.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time processing\",\n        \"deployment_endpoint\": \"inventory management system integration\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time updates and automatic stock level adjustments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the high-level details of the provided instruction, including the task of building a YOLO-based model, the target performance metrics, and the system's real-time integration with inventory management. The expert-level expertise of the user and the requirement for high precision and recall are also incorporated. However, since specific details like processing time optimization techniques or the exact deployment platform were not mentioned, they are left to be inferred.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for a multilingual e-reading platform, your team is tasked with developing a real-time, context-aware translation system. You need to choose a state-of-the-art seq2seq model that balances speed, fluency, and preservation of cultural nuances. The dataset must be diverse and recent, with a minimum of 100K Korean web novel chapters. Also, ensure the system can handle regional dialects and idiomatic expressions while meeting a target latency of under 2 seconds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual e-reading platform\",\n        \"description\": \"As a project manager, your team needs to develop a real-time, context-aware translation system for a multilingual e-reading platform. The system should use a state-of-the-art seq2seq model balancing speed, fluency, and cultural nuance preservation. The dataset must be diverse and recent, with a minimum of 100K Korean web novel chapters and should handle regional dialects and idiomatic expressions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"speed\",\n                \"value\": null\n            },\n            {\n                \"name\": \"fluency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cultural nuance preservation\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 2,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Korean web novel chapters (100K+)\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"minimum_size\": 100000,\n                \"language_focus\": \"Korean\",\n                \"diversity\": \"inclusive of regional dialects and idiomatic expressions\"\n            },\n            \"description\": \"A recent and diverse dataset of Korean web novel chapters for training a translation system that handles regional variations and idiomatic expressions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art seq2seq model\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced_features\": [\"context-aware\", \"speed vs. fluency trade-off\", \"cultural nuances handling\"]\n            },\n            \"description\": \"A seq2seq model optimized for real-time translation with the mentioned requirements\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time, possibly cloud-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for latency\"],\n        \"demonstration\": \"contextual and latency-sensitive\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clear on the task to build a translation system, the user's expertise level is high, and specific details such as model requirements, dataset diversity, and latency targets are included. However, there is a lack of clarity on the specific model family and certain performance metrics, which might need further confirmation from the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager, you seek to enhance the financial news categorization system for a high-profile media intelligence platform. Instruct the machine learning team to work with the Reuters-21578 archive, leveraging the fastText model for superior precision-speed trade-off. Strive for an accuracy of 88%+, specifically focusing on reducing false positives in critical market-moving events by at least 20%, while also optimizing for scalability and real-time processing capabilities. Additionally, outline a detailed plan for comparing with state-of-the-art algorithms and integrating the model with an efficient Newsfeed Algorithm v2.0.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"financial news\",\n        \"description\": \"As a project manager, enhance the financial news categorization system by leveraging the Reuters-21578 archive with the fastText model, targeting an accuracy of 88%+ and a 20% reduction in false positives for critical market-moving events. Focus on scalability and real-time processing. Also, plan for comparing with state-of-the-art algorithms and integrating with Newsfeed Algorithm v2.0.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.88\n            },\n            {\n                \"name\": \"false positive rate reduction\",\n                \"value\": 0.2\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": null,\n                \"unit\": \"required\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"required\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578 archive\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A dataset for financial news categorization using the Reuters-21578 archive, containing articles for model training.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"fastText model\",\n            \"family\": \"word embedding-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"precision_tradeoff\": \"superior\",\n                \"critical_event_false_positive_reduction\": 0.2\n            },\n            \"description\": \"Employ the fastText model for financial news categorization, focusing on precision-speed trade-off and precision in critical market-moving events.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Comparing with state-of-the-art algorithms\",\n        \"Integrating with Newsfeed Algorithm v2.0\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"comparing performance and integration with Newsfeed Algorithm v2.0\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the project manager's request, including the use of the fastText model, accuracy targets, and real-time processing optimizations. The model comparison and integration with Newsfeed Algorithm v2.0 are clearly stated, but the 'service' section requires further details on deployment and the specific target device. The 'expertise' level is assumed to be high due to the project's specific requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the Vaihingen urban greening project, develop a robust SegNet model with a novel fusion technique that differentiates between ten rare vegetation types and assesses their vitality. Ensure scalability across diverse weather patterns, timestamps, and low-light scenarios, while integrating a user-friendly interface for real-time monitoring and smart planning recommendations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban greening\",\n        \"description\": \"Develop a SegNet model for the Vaihingen urban greening project, focusing on the differentiation of ten rare vegetation types and assessing their vitality. The model must be robust to diverse weather patterns, timestamps, and low-light scenarios, and include a user-friendly interface for real-time monitoring and smart planning recommendations.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen urban greening project\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vegetation type\", \"vitality\"],\n            \"specification\": null,\n            \"description\": \"Dataset with images related to the Vaihingen project, capturing rare vegetation types and the varying conditions required for analysis.\",\n            \"preprocessing\": [\"data cleaning\", \"weather normalization\", \"low-light enhancement\"],\n            \"augmentation\": [\"weather simulation, timestamp variation\", \"lighting augmentation\"],\n            \"visualization\": [\"example images\", \"change detection maps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SegNet\",\n            \"family\": \"encoder-decoder networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"including novel fusion technique\",\n                \"ten_rare_vegetation_types\": true,\n                \"robustness\": [\"diverse weather\", \"timestamps\", \"low-light\"],\n                \"interface_type\": \"user-friendly\",\n                \"monitoring_and_recommendation\": true\n            },\n            \"description\": \"A SegNet model designed for the Vaihingen project with a focus on rare vegetation segmentation, vitality assessment, and adaptability to challenging scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Collaborate with domain experts for accurate vegetation type identification\",\n        \"Weather and lighting prediction algorithms for synthetic data augmentation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"data preprocessing for segmentation\", \"ensemble models for improved performance\"],\n        \"target_device\": \"smart monitoring system\",\n        \"deployment_endpoint\": \"project-specific cloud or edge server\",\n        \"inference_engine\": [\"GPU-optimized for real-time processing\"],\n        \"demonstration\": \"example scenarios and user guides\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes all required fields according to the schema, with the given instructions indicating a complex project requiring high expertise. The SegNet model is specified with relevant architectural elements, performance expectations, and a focus on user interface and practical application. Assumptions made include user collaboration with domain experts for accurate classification and leveraging AI algorithms for data augmentation.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Incorporate a multi-layered GCN architecture for the CORA dataset, designed for node classification with high precision (90%+), while mitigating overfitting through regularization techniques. Optimize for minimal training time (1.5-2 hours) on a state-of-the-art GPU, without compromising on computational efficiency, and compare performances with different graph pooling methods.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"graph-based\",\n        \"description\": \"The user aims to incorporate a multi-layered GCN architecture for the CORA dataset, targeting high precision (90%+) and addressing overfitting with regularization techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": 900,\n                \"unit\": \"minutes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CORA\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"node_type\": \"classification\",\n                \"regression_or_classification\": \"classification\",\n                \"specific_dataset_characteristics\": \"CORA dataset\"\n            },\n            \"description\": \"A dataset for node classification with a focus on graph machine learning.\",\n            \"preprocessing\": [\"GCN-friendly data preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multi-layered GCN\",\n            \"family\": \"GCN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-layered\",\n                \"regularization_techniques\": [\"dropout\", \"L2 regularization\"],\n                \"state_of_the_art_GPUs\": [\"NVIDIA A100\"]\n            },\n            \"description\": \"A high-performance model incorporating GCN architecture for node classification in the CORA dataset while meeting time constraints.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"state-of-the-art GPU (e.g., NVIDIA A100)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"computationally efficient, GPU-optimized\"],\n        \"demonstration\": \"Compare performances with different graph pooling methods\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction was parsed accurately, including high expertise, task details, performance targets, and time constraints. The dataset and model requirements were specific to the problem at hand. The service section includes requirements for computational efficiency, GPU optimization, and comparison with graph pooling methods. However, there is no deployment endpoint provided, and the performance comparison might require further clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager seeks to employ the WMT19 multilingual commentary dataset for a sophisticated machine translation project, focusing on Russian-to-English translation. The objective is to design an advanced transformer-based seq2seq model with iterative refinement, incorporating multi-task learning and cross-lingual pretraining. Demands a demonstration of exceptional performance through a rigorous evaluation, aiming for a BLEU score of at least 45, while also optimizing for computational efficiency and handling domain-specific terminologies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual\",\n        \"description\": \"The project manager wants to use the WMT19 multilingual commentary dataset for a Russian-to-English translation task. The objective is to design an advanced transformer-based seq2seq model with iterative refinement, multi-task learning, and cross-lingual pretraining.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 45\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"unit\": \"time or FLOPs\",\n                \"value\": null\n            },\n            {\n                \"name\": \"domain-specific terminology handling\",\n                \"unit\": null,\n                \"value\": \"optimized\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 multilingual commentary dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": \"Russian-to-English\",\n                \"specifics\": \"multilingual commentary data\"\n            },\n            \"description\": \"A dataset for Russian-to-English translation focusing on the WMT19 multilingual commentary data.\",\n            \"preprocessing\": [\"multi-task learning\", \"cross-lingual pretraining\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced transformer-based seq2seq model\",\n            \"family\": \"transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"iterative_refinement\": \"enabled\",\n                \"multi_task_learning\": \"enabled\"\n            },\n            \"description\": \"A sophisticated model for Russian-to-English translation with iterative refinement, multi-task learning, and cross-lingual pretraining.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific terminology handling techniques\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"rigorous evaluation, showcasing exceptional performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a high level of expertise, as the task is complex and requires advanced techniques. The model's specifications and dataset details align with the advanced nature of the project. However, there is a need to further specify computational efficiency metrics or provide guidance on how the user will optimize for it. Additionally, the deployment endpoint is not mentioned, but the expectation of a rigorous evaluation implies it may be considered post-model training.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "Develop a real-time, explainable Q&A model for medical diagnosis, leveraging the recently updated BioASQ v6 dataset. Emphasize interpretability, accuracy, and implementation of a novel fusion technique with SciBERT for improved performance. Ensure integration with a healthcare platform for seamless patient consultations, and ensure compliance with GDPR regulations while incorporating references from at least three reputable sources per response.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medical diagnosis\",\n        \"description\": \"Develop a real-time, explainable Q&A model for medical diagnosis using the BioASQ v6 dataset. Focus on interpretability, high accuracy, and implement a novel fusion technique with SciBERT for enhanced performance. Ensure integration with a healthcare platform for patient consultations, and adhere to GDPR regulations by incorporating references from at least three reputable sources per response.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ v6\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"update_status\": \"recently updated\"\n            },\n            \"description\": \"A medical Q&A dataset for training the model, specifically designed for real-time diagnosis.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Explainable Q&A model\",\n            \"family\": \"SciBERT-based fusion technique\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novel_fusion\": \"true\",\n                \"interpretability_metrics\": null\n            },\n            \"description\": \"A model leveraging SciBERT for medical diagnosis, with a focus on interpretability and improved performance through novel fusion.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"healthcare platform\",\n        \"inference_engine\": [\"compliant with GDPR\"],\n        \"demonstration\": \"seamless patient consultations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a building project, with the user having high expertise. The problem area and downstream task are clear for medical diagnosis and Q&A. However, specific performance metrics (accuracy and interpretability) are not provided. The dataset is specified as BioASQ v6 and is recent. The model requirements and service expectations are well-defined, including the fusion technique, healthcare platform integration, and GDPR compliance. Confidence is high, but missing performance targets call for clarification.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a seasoned project manager for a cutting-edge machine learning team, you're tasked with developing a state-of-the-art time-series classifier for the intricate Ethanol Concentration dataset. This dataset consists of high-resolution, noise-sensitive sensor readings over an extended duration (INPUT_SEQ_LEN=1751 with a dense feature space, INPUT_DIM=3). The challenge lies in forecasting not only categorical labels (0 to 3) but also handling seasonality and complex interdependencies within the time series. Your team must optimize for both accuracy and efficiency, using advanced techniques like deep learning architectures and explainable AI, while ensuring compatibility with real-time streaming data. Additionally, devise a robust testing strategy that accounts for different time horizons and evaluates performance under varying signal-to-noise ratios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"chemical process monitoring\",\n        \"description\": \"Develop a state-of-the-art time-series classifier for the Ethanol Concentration dataset. The dataset contains high-resolution, noise-sensitive sensor readings over an extended duration. The task is to forecast categorical labels (0-3) while handling seasonality and complex interdependencies. Advanced techniques like deep learning and explainable AI must be applied, and compatibility with real-time streaming data should be ensured.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ethanol Concentration\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"categorical labels (0-3)\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 1751,\n                \"INPUT_DIM\": 3,\n                \"signal_to_noise_ratio\": \"variable\"\n            },\n            \"description\": \"A high-resolution sensor data dataset with an emphasis on noise sensitivity and an extended duration, suitable for time-series classification.\",\n            \"preprocessing\": [\"noise reduction\", \"high-pass filtering\"],\n            \"augmentation\": [\"time warping\", \"variable sampling rates\"],\n            \"visualization\": [\"spectrogram analysis\", \"time-lag plots\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art deep learning architecture\",\n            \"family\": \"advanced\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": true\n            },\n            \"description\": \"A deep learning model designed for time-series classification with explainability features, targeting accuracy and efficiency on the Ethanol Concentration dataset.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction from streaming data\"],\n        \"target_device\": \"real-time streaming environment\",\n        \"deployment_endpoint\": \"Edge computing or serverless\",\n        \"inference_engine\": [\"efficient inference algorithms for real-time performance\"],\n        \"demonstration\": \"testing across different time horizons and varying signal-to-noise ratios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction provided a clear task, specifying the complexity and requirements for the project. As a seasoned expert, the user's expectations align with a high level of expertise. The response includes comprehensive sections for dataset, model, and service, reflecting the specific needs of the project. The missing metric values for accuracy and efficiency are placeholders, reflecting the need for actual performance tuning during model development.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For our ambitious smart city initiative, we need a lightweight YOLO-based object detection model specifically tailored for real-time traffic management. It must efficiently detect vehicles (including articulated trucks), diverse pedestrians, and traffic signs on urban streets, maintaining a robust accuracy of 85% or above, and optimizing for a minimum latency of 5 milliseconds per frame to ensure seamless 60 FPS performance across various hardware platforms, including edge devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city, traffic management\",\n        \"description\": \"Develop a lightweight YOLO-based object detection model for real-time traffic management in a smart city context. The model should focus on detecting vehicles (articulated trucks), pedestrians, and traffic signs with a robust accuracy of 85% or above. It must optimize for a minimum latency of 5 milliseconds per frame for 60 FPS performance across various hardware platforms, including edge devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.85\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 5,\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"milliseconds per frame\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vehicles\", \"pedestrians\", \"traffic signs\"],\n            \"specification\": {\n                \"custom_dataset\": \"smart city urban streets\",\n                \"diversity\": \"urban scenes with varying traffic conditions\"\n            },\n            \"description\": \"Dataset for real-world urban street scenes, providing a diverse range of images for training the model on various types of objects.\",\n            \"preprocessing\": [\"data augmentation for varying lighting and weather\"],\n            \"augmentation\": [\"random cropping\", \"color jittering\"],\n            \"visualization\": [\"object bounding boxes and labels\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"lightweight YOLO\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"customized YOLO design for real-time performance and resource efficiency\",\n                \"parameters\": {\n                    \"layers\": \"optimized for hardware constraints\",\n                    \"filters\": \"efficient for the task\"\n                }\n            },\n            \"description\": \"A lightweight YOLO model optimized for real-time traffic management, specifically focusing on detecting vehicles, pedestrians, and traffic signs while achieving high accuracy and low latency.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"traffic flow analysis\", \"data fusion\"],\n        \"target_device\": [\"edge devices\"],\n        \"deployment_endpoint\": \"city traffic management infrastructure\",\n        \"inference_engine\": [\"optimized deep learning inference libraries\"],\n        \"demonstration\": \"live traffic monitoring and predictive analytics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers all the required sections from the schema, including the ambitious smart city context, the specific target objects, and hardware requirements. The 'expertise' is set to 'high' given the complexity of the task. However, the dataset name was not provided, which might need further clarification from the user. The confidence score is 0.95 as the instruction was well-structured, but the details for the model's 'specification' could be more elaborate.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create a novel RNN-based machine translation model using the extensive OPUS corpus for translating English to Russian, focusing on adapting to real-time, informal conversations with an emphasis on colloquial expressions and slang. The model must meet a high benchmark of 33 BLEU score, while maintaining a stringent latency constraint of not more than 250 milliseconds per sentence to guarantee a seamless user experience, even under heavy load conditions. Additionally, design an evaluation strategy to measure the model's performance degradation under different data scarcity scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"real-time, informal conversation\",\n        \"description\": \"Create a novel RNN-based machine translation model using the OPUS corpus for translating English to Russian, focusing on colloquial expressions and slang. The target is to achieve at least 33 BLEU score and maintain a latency of 250 milliseconds per sentence under heavy load conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 33\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 250,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS corpus (English to Russian)\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": \"English to Russian\",\n                \"focus_on\": \"informal conversation, colloquial expressions, and slang\"\n            },\n            \"description\": \"Extensive corpus for training a machine translation model in real-time settings.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-based machine translation model\",\n            \"family\": \"LSTM or Transformer-RNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time适应性\": \"true\",\n                \"data scarcity scenarios evaluation\": true\n            },\n            \"description\": \"A model designed for real-time translation with a focus on informal language.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"subject\": \"Colloquial expressions and slang adaptation\",\n            \"detail\": \"Model training data should include a diverse range of informal language to ensure adaptation.\"\n        },\n        {\n            \"subject\": \"Data scarcity evaluation\",\n            \"detail\": \"Design an evaluation strategy for assessing performance under different amounts of training data.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time, high-performance environment\",\n        \"deployment_endpoint\": \"to be determined\",\n        \"inference_engine\": [\"optimized for latency constraints\"],\n        \"demonstration\": {\n            \"focus\": [\"user experience in high load conditions\", \"response time under various data scarcity\"]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essential requirements of the user's instruction, indicating a high level of expertise and focusing on the technical aspects like BLEU score, latency, and data scarcity evaluation. The user's expectations for a seamless user experience and heavy load conditions are reflected in the service section. However, there's a note that the deployment endpoint is yet to be determined.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As a project manager for the science fair, your team is tasked with developing a sophisticated predictive model. Given historical data on hourly solar energy production for the last month, create an advanced time series analysis that not only forecasts the daily output for the upcoming week but also accounts for weather patterns, seasonal variations, and equipment maintenance schedules. Document the model's accuracy through cross-validation and present a detailed step-by-step explanation in your report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"science fair, solar energy production\",\n        \"description\": \"Develop a sophisticated predictive model for time series analysis. The project requires forecasting the daily solar energy production for the upcoming week, considering historical data for the last month, weather patterns, seasonal variations, and equipment maintenance schedules.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Cross-validation accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Historical hourly solar energy data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"daily output\"],\n            \"specification\": {\n                \"time_period\": \"last month\",\n                \"frequency\": \"hourly\"\n            },\n            \"description\": \"Hourly solar energy production data for the past month, including historical information for model development.\",\n            \"preprocessing\": [\"resampling to daily\", \"handling missing values\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"data exploration plots (line charts, heatmaps)\"\n            ],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"Weather data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"time_coverage\": \"matching solar data period\"\n            },\n            \"description\": \"Weather patterns affecting solar energy production for the same time period as the historical data.\",\n            \"preprocessing\": [\"resampling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"Equipment maintenance schedule\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"data for maintenance impact\"],\n            \"specification\": {\n                \"time_intervals\": \"daily\",\n                \"relevant_factors\": [\"maintenance events\"]\n            },\n            \"description\": \"Table containing maintenance schedule and its potential impact on solar energy production.\",\n            \"preprocessing\": [\"feature engineering (incorporating maintenance events)\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced time series forecasting model\",\n            \"family\": \"neural networks or ensemble models (e.g., ARIMA, LSTM, Prophet)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"accounts for seasonality, weather, and maintenance effects\",\n                \"validation_method\": \"cross-validation\"\n            },\n            \"description\": \"A model designed to incorporate multiple variables for a comprehensive solar energy production forecast.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Include explanations of chosen model, feature engineering steps, and why cross-validation is crucial for accuracy assessment.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \" seasonal decomposition (e.g., seasonal decomposition of trends, STL)\",\n            \"feature extraction from weather data\",\n            \"encoding maintenance schedule\"\n        ],\n        \"target_device\": \"cloud or server-based for efficient cross-validation\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Apache Spark (for distributed computing) or GPU-friendly backends\"\n        ],\n        \"demonstration\": \"Detailed instructions for replicating the model and analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has high expertise, so the instructions are assumed to be clear. The project requires a sophisticated model, so 'high' expertise is appropriate. The task description is comprehensive and specific about the forecasting, seasonality, and data sources. However, specific details for performance and complexity metrics, as well as the deployment endpoint and inference engine, are not provided and can be inferred based on the model requirements.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, request a highly specialized multilingual QA system tailored for the MLQA dataset, demanding proficiency in five diverse languages (English, Spanish, Mandarin, French, and Arabic). Emphasize optimizing performance in low-resource settings with both mBERT and XLM-R architectures, while maintaining state-of-the-art cross-lingual accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"specialized multilingual QA\",\n        \"description\": \"Request a highly specialized multilingual QA system tailored for the MLQA dataset, focusing on English, Spanish, Mandarin, French, and Arabic. The system should optimize performance in low-resource settings and excel with both mBERT and XLM-R architectures while achieving state-of-the-art cross-lingual accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"cross-lingual accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MLQA\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\n                    \"English\",\n                    \"Spanish\",\n                    \"Mandarin\",\n                    \"French\",\n                    \"Arabic\"\n                ],\n                \"low-resource settings\": true\n            },\n            \"description\": \"A multilingual dataset designed for a QA system, emphasizing coverage of five diverse languages.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"mBERT and XLM-R\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"low-resource adaptation\": true\n                }\n            },\n            \"description\": \"A multilingual QA system utilizing mBERT and XLM-R, optimized for low-resource settings.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Low-resource setting adaptation and maintaining state-of-the-art cross-lingual accuracy are key priorities.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the project manager's intent to build a specialized QA system. High expertise suggests advanced knowledge. The problem area is NLP and downstream task is QA with a focus on MLQA dataset and specific language requirements. The absence of explicit model performance values leaves them open for optimization, indicating that state-of-the-art cross-lingual accuracy is to be targeted.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For the upcoming agricultural drone surveillance initiative, enhance the current project by integrating a multi-objective Mask R-CNN model that not only recognizes but also differentiates between various plant diseases, nutrient deficiencies, and stress factors. The model must achieve a minimum F1-score of 95% on the hierarchical PlantVillage dataset, and demonstrate robustness in detecting anomalies from oblique and high-resolution aerial images captured under varying weather conditions. Additionally, incorporate real-time data processing and present a comprehensive evaluation report detailing the model's interpretability and sensitivity to seasonal variations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Enhance the agricultural drone surveillance project by integrating a multi-objective Mask R-CNN model for identifying plant diseases, nutrient deficiencies, and stress factors. The model should achieve at least 95% F1-score on the PlantVillage dataset, and demonstrate robustness in detecting anomalies from oblique and high-resolution aerial images captured under varying weather conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PlantVillage\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_shape\": [\"oblique\", \"high-resolution\"],\n                \"weather_conditions\": [\"varied\"]\n            },\n            \"description\": \"A hierarchical dataset for plant disease recognition with emphasis on identifying plant diseases, nutrient deficiencies, and stress factors.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Anomaly detection\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multi-objective Mask R-CNN\",\n            \"family\": \"Mask R-CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"95%\",\n                \"interpretability\": \"required\",\n                \"seasonal_sensitivity\": \"required\"\n            },\n            \"description\": \"A model designed for multi-object detection with a focus on plant diseases, nutrient deficiencies, and stress factors.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time data processing is necessary for the project\",\n        \"Model interpretability and sensitivity to seasonal variations are crucial evaluation factors.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"Model interpretability and seasonal effect on performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has a high level of expertise and requires a project that involves advanced Mask R-CNN model for multi-object detection. The project specifics align well with the provided requirements, including the performance metrics, dataset, and knowledge needed. The service section covers real-time processing and evaluation demonstration. However, a specific target device is not mentioned, which may need clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a retail analytics expert, create a specialized YOLOv5 model tailored for the \"FusionMart ShelfScan v3.1\" dataset, focusing on identifying 500 distinct products with subcategories like fresh produce, electronics, and home goods. Target a precision of over 98% and recall above 96%, ensuring minimal false positives in crowded, low-light scenarios. Implement a cutting-edge, latency-sensitive design that guarantees an average response time of 0.75 seconds per image, while seamlessly integrating with the \"MerchFlow 360\" inventory system. Account for frequent SKU updates, peak-season demand fluctuations, and incorporate a machine learning-driven dynamic adaptation mechanism to preserve accuracy within a tight resource envelope.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail analytics\",\n        \"description\": \"As a retail analytics expert, the objective is to create a specialized YOLOv5 model for the 'FusionMart ShelfScan v3.1' dataset. The model should target 500 distinct product categories (including subcategories like fresh produce, electronics, and home goods) with precision above 98% and recall above 96%. It must handle crowded, low-light scenarios with minimal false positives and have a latency-sensitive design, aiming for an average response time of 0.75 seconds per image.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.96\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"average response time per image\",\n                \"value\": 0.75,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"FusionMart ShelfScan v3.1\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product ID\", \"subcategories\"],\n            \"specification\": {\n                \"distinct_products\": 500,\n                \"crowded_scenarios\": true,\n                \"low_light_scenarios\": true\n            },\n            \"description\": \"A retail dataset focusing on 500 product categories for object detection in challenging situations like crowded and low-light environments.\",\n            \"preprocessing\": [\"data augmentation (for low-light and crowded scenes)\"],\n            \"augmentation\": [\"image enhancement, object resizing\"],\n            \"visualization\": [\"product distribution and performance analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5 (tailored for FusionMart)\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"targeted\",\n                \"resource constraints\": \"latency-sensitive\"\n            },\n            \"description\": \"A machine learning model specifically designed for the task, incorporating dynamic adaptation and optimized for high precision, recall, and low latency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"SKU updates, peak-season demand fluctuations, machine learning-driven adaptation mechanism, resource-efficient design\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"product embeddings\", \"contextual features\"],\n        \"target_device\": \"latency-sensitive\",\n        \"deployment_endpoint\": \"MerchFlow 360\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"model performance and integration showcase\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the expert's role, the task's focus on accuracy and efficiency, and specific details about the dataset, model requirements, and deployment environment. The information provided is thorough, but the complexity and resource constraints within the tight environment may require additional clarifications. The confidence score is high, given the detail provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project lead for a top-tier machine learning team, your mission is to develop a highly sophisticated RNN-T architecture for the complex ETTm2 dataset, which presents intricate noise mitigation challenges and countless micro-seasonal nuances. Each sequence in the training, validation, and testing partitions (consisting of 96 temporal steps with 7 unique input channels, denoted as INPUT_SEQ_LEN and an expanded INPUT_DIM of 50 dimensions) demands exceptional pattern recognition across diverse timeframes. The model should forecast 112-step ahead with state-of-the-art precision, optimizing for both quantile-based loss (QLIKE) with enhanced fairness and mean absolute percentage error (MAPE) for comprehensive evaluation. Furthermore, mandate the model to incorporate interactive explainability techniques, real-time adaptive anomaly detection, and maintain exceptional computational efficiency on cutting-edge parallel hardware, ensuring seamless performance in the face of unforeseen anomalies without compromising latency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"complex, real-world time-series data\",\n        \"description\": \"Develop a highly sophisticated RNN-T architecture for the ETTm2 dataset with noise mitigation and micro-seasonal nuances. Sequences consist of 96 temporal steps with an expanded INPUT_DIM of 50 dimensions, targeting a 112-step ahead forecast with QLIKE loss and MAPE. The model must include explainability, real-time anomaly detection, and run efficiently on modern parallel hardware.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"QLIKE\",\n                \"value\": null,\n                \"fairness\": \"enhanced\"\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 50,\n                \"PRED_SEQ_LEN\": 112\n            },\n            \"description\": \"A complex dataset with noise mitigation challenges and micro-seasonal variations, split into train, validation, and test partitions.\",\n            \"preprocessing\": [\"noise reduction\", \"data normalization\"],\n            \"augmentation\": [\"time shifts\", \"temporal jittering\"],\n            \"visualization\": [\"sequence patterns\", \"seasonality\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-T architecture\",\n            \"family\": \"recurrent neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A highly sophisticated model tailored for ETTm2, focusing on noise mitigation, micro-seasonality, QLIKE loss, MAPE, explainability, and adaptive anomaly detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advances in parallel hardware for efficient processing\",\n        \"State-of-the-art explainability techniques in deep learning\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"micro-seasonality feature extraction\", \"time-varying embeddings\"],\n        \"target_device\": \"high-performance computing clusters or edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"state-of-the-art parallel libraries\"],\n        \"demonstration\": \"real-time, interactive forecasting and anomaly visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction requires a high expertise level due to the complexities, including sophisticated architecture, advanced metrics, and hardware optimization. The user specifies clear requirements for preprocessing, model performance, and service deployment, ensuring a comprehensive response. However, some metric values and deployment details are not provided, which would typically need further clarification.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "image classification", "instruction": "As an AI project director, your mandate is to orchestrate the design of a highly advanced and scalable deep learning framework for a sophisticated extragalactic object classifier. The system, based on the state-of-the-art Swin Transformer, must not only surpass a demanding benchmark of 99.5% accuracy on the undisclosed subset of the extensive Galaxy Zoo Legacy data, but also excel in handling minute variations, artifacts, and overlapping celestial objects. The project must incorporate techniques for robust noise reduction, domain adaptation, and fine-tuning for niche telescope imaging. Moreover, the model's efficiency in cross-domain transfer learning and resilience to sparse and unreliable real-time astronomical data must be demonstrated in a set of live simulations. Your team is expected to deliver a proof-of-concept within a six-month timeframe, while keeping in mind the potential for groundbreaking discoveries that challenge current cosmological theories.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Design a highly advanced and scalable deep learning framework using the Swin Transformer for an extragalactic object classifier. The system must surpass a 99.5% accuracy on an undisclosed subset of the Galaxy Zoo Legacy data, handle minute variations, artifacts, and overlapping objects, incorporate noise reduction, domain adaptation, and fine-tuning techniques, and demonstrate cross-domain transfer learning and resilience to sparse, unreliable data in live simulations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo Legacy (undisclosed subset)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"classification labels\"],\n            \"specification\": {\n                \"state-of-the-art\": true,\n                \"specific_accuracy_target\": 0.995\n            },\n            \"description\": \"A large-scale astronomical dataset for training and testing, focusing on extragalactic objects.\",\n            \"preprocessing\": [\"noise reduction\"],\n            \"augmentation\": [\"domain adaptation\", \"handling minute variations and overlapping objects\"],\n            \"visualization\": null,\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Swin Transformer-based Classifier\",\n            \"family\": \"Swin Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced_architecture\": true,\n                \"target_accuracy\": 0.995\n            },\n            \"description\": \"A deep learning framework incorporating the Swin Transformer for an extragalactic object classifier.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Robustness to minute variations, artifacts, and overlapping celestial objects\",\n        \"Fine-tuning for niche telescope imaging\",\n        \"Incorporating domain adaptation and cross-domain transfer learning\",\n        \"Resilience to sparse and unreliable real-time astronomical data\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"noise reduction techniques\",\n            \"domain adaptation strategies\"\n        ],\n        \"target_device\": \"high-performance computing infrastructure\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for real-time simulations\"],\n        \"demonstration\": {\n            \"type\": \"live simulations\",\n            \"scope\": \"challenging cosmological theories\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was based on the given instruction, including high-level requirements for the project and the Swin Transformer-based model, as well as specific metrics and techniques. The team's expertise was inferred to be high due to the advanced nature of the project. Some details regarding deployment and real-world application are left open-ended, relying on domain expertise or further clarification.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a maritime AI project manager for an elite coral reef monitoring initiative, task the development team with enhancing the state-of-the-art DeepLabV3+ architecture for an exclusive variant designed specifically for optimizing coral segmentation in-depth marine ecosystems. The new model must incorporate a cutting-edge TurbidityAdaptiveNet layer, addressing photonic challenges caused by extreme water clarity variations. Ensure a minimum 5% improvement in F1-score compared to current best practices, and prioritize real-time, low-latency inference for seamless deployment on resource-limited, embedded platforms targeting wearables in aquatic researchers' gear.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"marine biology, coral reef monitoring\",\n        \"description\": \"Task the development team to enhance DeepLabV3+ architecture for a specialized coral segmentation model in deep marine ecosystems. Incorporate a TurbidityAdaptiveNet layer to tackle photonic challenges due to water clarity variations. Aim for a minimum 5% improvement in F1-score compared to existing methods and focus on real-time, low-latency inference for wearable devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"real-time\",\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"low\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"resource utilization\",\n                \"value\": \"optimized for embedded platforms, wearable devices\",\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\", \"multimodal\"],\n            \"target_variables\": [\"coral segmentation\"],\n            \"specification\": {\n                \"data type\": \"marine ecosystem imagery\",\n                \"input size\": null,\n                \"output size\": null\n            },\n            \"description\": \"Marine ecosystem imagery with coral samples for training and segmentation improvement\",\n            \"preprocessing\": [\"TurbidityAdaptiveNet layer integration\"],\n            \"augmentation\": [],\n            \"visualization\": [\"coral segmentation overlays\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepLabV3+ with TurbidityAdaptiveNet\",\n            \"family\": \"DeepLabV3+\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"exclusive variant with TurbidityAdaptiveNet\",\n                \"improvement\": \"optimizing for coral segmentation and photonic challenges\",\n                \"target\": \"embedded platforms, wearables for marine researchers\"\n            },\n            \"description\": \"Enhanced for coral segmentation and real-time, low-latency performance\"\n        }\n    ],\n    \"knowledge\": [\n        \"Water clarity variations affect coral photonic challenges\",\n        \"Targetted at resource-constrained, wearable platforms\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"TurbidityAdaptiveNet integration\"\n        ],\n        \"target_device\": \"embedded, wearable platforms\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"real-time, optimized for low-latency\"\n        ],\n        \"demonstration\": \"showcase of coral segmentation on wearable devices in marine environments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction describes a specific and advanced machine learning project with clear requirements for model architecture, performance, and deployment. The expertise level of the user is assumed to be high due to the nature of the task. The assumption on missing dataset name, augmentation techniques, and deployment endpoint leaves room for further clarification. The confidence score, though, reflects a high confidence level given the provided information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Incorporate a real-time adaptation module for the machine translation system, capable of learning from live user feedback to refine translations in the e-commerce domain. The system must now achieve a BLEU score of 45+, while seamlessly integrating with both Shopify and Magento APIs. Emphasize on zero-latency translation for live product descriptions to guarantee a cohesive and culturally relevant experience across global marketplaces without human intervention.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Incorporate a real-time adaptation module into the machine translation system for live user feedback in the e-commerce domain. The system should achieve a BLEU score of 45+ and seamlessly integrate with Shopify and Magento APIs, ensuring zero-latency translation for live product descriptions for global markets without human intervention.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU Score\",\n                \"value\": 45.0\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"live_feedback\": true,\n                \"domain适应性\": \"e-commerce\"\n            },\n            \"description\": \"A dynamic dataset for machine translation with a focus on live user feedback and e-commerce domain.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"machine translation with real-time adaptation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"integration\": [\"Shopify API\", \"Magento API\"],\n                \"latency\": \"zero-latency\"\n            },\n            \"description\": \"A machine translation system with real-time adaptation module for live user feedback in the e-commerce domain.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": {\n            \"shopify\": true,\n            \"magento\": true\n        },\n        \"inference_engine\": [\"real-time\", \"streaming\"],\n        \"demonstration\": \"live product description translation, zero-latency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction provided a clear goal for a machine translation system with high-level expertise. The BLEU score, integration with APIs, and emphasis on zero-latency were parsed into performance and service sections. The system's focus on live feedback implies that the dataset is likely infer-searched or might need to be designed to incorporate real-time adaptation. However, the specific models or names were not provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "Act as an AI technology consultant, now requiring you to not only design a custom text classifier for the Polaris AI Generation Attribution dataset but also to integrate it within a secure, real-time monitoring system. Ensure the classifier attains superhuman performance, with a focus on F1 score, while defending against stealthily manipulated inputs and minimizing false positives in both human and AI-generated text, using differentiating features discovered through explainable AI techniques.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI generation attribution\",\n        \"description\": \"Design a custom text classifier for the Polaris AI Generation Attribution dataset and integrate it into a secure, real-time monitoring system. The classifier should aim for superhuman performance with a focus on F1 score, and defend against stealthily manipulated inputs, minimizing false positives for both human and AI-generated text.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"robustness against manipulation\",\n                \"value\": null,\n                \"unit\": \"protected against stealthy manipulation\"\n            },\n            {\n                \"name\": \"false positive reduction\",\n                \"value\": null,\n                \"unit\": \"human and AI-generated text\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Polaris AI Generation Attribution\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"classification labels\"],\n            \"specification\": {\n                \"size\": \"customized for dataset\",\n                \"features\": \"discovering differentiating features through explainable AI\"\n            },\n            \"description\": \"A dataset designed for text classification, with a focus on distinguishing human and AI-generated content\",\n            \"preprocessing\": [\n                \"cleaning, normalization, and feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Text Classifier\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"AI-driven, possibly including deep learning models for high performance\",\n                \"defense mechanisms\": \"Explainable AI techniques for identifying stealthy manipulations\"\n            },\n            \"description\": \"A custom classifier utilizing explainable AI techniques to achieve superhuman F1 score and protect against manipulated inputs.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Explainable AI techniques for feature extraction and adversarial input detection\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"explainable AI techniques, such as LIME, SHAP, or counterfactual explanations\"\n        ],\n        \"target_device\": \"secure, real-time monitoring system\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"AI-powered, possibly using ensemble models or ensemble with human review for enhanced accuracy\"\n        ],\n        \"demonstration\": \"Real-time monitoring and reporting of classification results and defense measures.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction requires a custom text classifier with a focus on F1 score, robustness, and minimizing false positives. Since it's a high expertise level, the user can handle the technical details. The dataset and model fields are tailored based on the requirements. However, specific deployment details and the exact defense mechanisms are not explicitly mentioned, which might be filled through collaboration with the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project, now prioritizing cultural assimilation, requires a sophisticated transformer model upgrade for English-to-Japanese translation. In addition to enhancing general syntactic fluency, the system must demonstrate prowess in unraveling and reproducing intricate colloquial expressions with precision above 98%. Moreover, ensure adaptation to diverse regional dialects and subtle cultural nuances, necessitating meticulous data parsing from the voluminous Common Crawl corpus.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"The project focuses on cultural assimilation and requires a sophisticated transformer model upgrade for English-to-Japanese translation, emphasizing precision above 98% in reproducing intricate colloquial expressions and adapting to regional dialects and cultural nuances. Data should be parsed from the Common Crawl corpus.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_source\": \"Common Crawl\",\n                \"domain\": \"multilingual\",\n                \"lingual_focus\": [\"English\", \"Japanese\"]\n            },\n            \"description\": \"A large corpus parsed from the Common Crawl, containing diverse text data for training a model that captures syntactic fluency and colloquial expressions in multiple Japanese regional dialects.\",\n            \"preprocessing\": [\"data cleaning, language identification, dialect separation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"upgraded transformer model\",\n            \"family\": \"transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-based\",\n                \"novel_features\": \"cultural adaptation, regional dialect handling\"\n            },\n            \"description\": \"A transformer model designed for English-to-Japanese translation, targeting high precision in colloquial expressions and regional variations, while incorporating data from the Common Crawl corpus.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The focus on cultural assimilation necessitates a deep understanding of Japanese colloquialisms and regional dialects.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"language-specific embeddings, dialect detection algorithm\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"example translation outputs showcasing improved colloquial expression reproduction\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the main requirements of the project, focusing on the build intent and detailed specifications for the model and dataset. The user's high expertise level implies they may require more assistance in specific areas like feature engineering and deployment.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning project manager, you're now seeking to develop an advanced image classification system for the Stanford Cars Dataset. The Inception-v3 architecture should not only classify cars based on make, model, and year, but must also discriminate among submodels and variants. Moreover, the model must exhibit robustness under extreme weather conditions and low-resolution images. In addition to standard precision and recall, generate per-class F1 scores and achieve a minimum mean average precision (mAP) of 97%. Ensure the model's real-time performance isn't compromised and present a thorough analysis of transfer learning effects on performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"image recognition\",\n        \"description\": \"Develop an advanced image classification system for the Stanford Cars Dataset, utilizing the Inception-v3 architecture. The system must classify cars based on make, model, year, submodels, and variants, with robustness against extreme weather conditions and low-resolution images.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score (per-class)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean average precision (mAP)\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"ms/epoch\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"make\", \"model\", \"year\", \"submodel\", \"variant\"],\n            \"specification\": null,\n            \"description\": \"A dataset for advanced image classification, focusing on cars with detailed information.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Inception-v3\",\n            \"family\": \"Inception\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Inception-v3\",\n                \"weather robustness\": true,\n                \"low-resolution robustness\": true\n            },\n            \"description\": \"A state-of-the-art model for image classification with Inception-v3 base, targeting fine-grained recognition tasks including car variations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning will be used, and its impact on performance will be thoroughly analyzed.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"description\": \"The project will include a demonstration showcasing real-time performance and the transfer learning analysis results.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the user's instruction for a machine learning project manager, with a high level of expertise. The Inception-v3 architecture is specified, along with the requirement for per-class metrics, robustness, and a minimum mAP. The challenge of transfer learning and its effect on performance analysis is highlighted. However, since the target device and deployment specifics are not provided, the 'service' section might be incomplete.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "The mission necessitates the creation of a high-precision Deep Learning Image Categorizer, leveraging the advanced DECam Deep Learning (DECam DL) Transformer variant, designed specifically for the diverse and challenging astrophysical data from the SDSS Galaxy Zoo Extended Project. Emphasize on model resilience to minute celestial features, cross-epoch generalization, and validation on previously unexplored celestial objects, to streamline professional astronomers' surveys.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The goal is to create a high-precision Deep Learning Image Categorizer, specifically using the DECam Deep Learning Transformer variant, for the SDSS Galaxy Zoo Extended Project. The model should exhibit resilience to subtle celestial features, achieve cross-epoch generalization, and be validated on previously unseen celestial objects to enhance astronomers' survey efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SDSS Galaxy Zoo Extended Project\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_type\": \"astronomical\",\n                \"diversity\": \"challenging\",\n                \"unexplored_objects\": true\n            },\n            \"description\": \"A diverse and challenging dataset from the SDSS Galaxy Zoo Project, designed for the DECam DL Transformer variant for astrophysical image classification.\",\n            \"preprocessing\": [\n                {\n                    \"step\": \"Data augmentation for subtle celestial features\"\n                }\n            ],\n            \"augmentation\": [\n                \"transformations that preserve astronomical context\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DECam DL Transformer\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"advanced DECam DL Transformer\",\n                \"resilience\": \"subtle celestial features\"\n            },\n            \"description\": \"An advanced DECam DL Transformer tailored for high-precision image classification in the challenging astrophysical domain.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            {\n                \"step\": \"Generalization across different observational epochs\"\n            }\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"focus\": \"Illustrate improved categorization of unseen celestial objects\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction details a specific task of building a high-precision image classifier, suggests the DECam Deep Learning Transformer, and outlines requirements for model resilience and validation. The user's high expertise level indicates a deeper understanding. However, performance metrics and target device specifics are not explicitly specified, leaving them open for further inquiry.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create a sophisticated multilingual translation model using the ParaCrawl dataset, employing a dual encoder architecture. Emphasize on preserving not only precise semantics but also nuanced emotions and cultural context in English-to-Dutch translations, while optimizing for performance on various linguistic nuances and dialects.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual\",\n        \"description\": \"Create a sophisticated multilingual translation model specifically for English-to-Dutch translations, leveraging the ParaCrawl dataset. The model must employ a dual encoder architecture, focusing on preserving precise semantics, nuanced emotions, and cultural context, while optimizing for performance across linguistic nuances and dialect variations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"semantic preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"emotion preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cultural context preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"linguistic nuances handling\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": [\"English\", \"Dutch\"],\n                \"specific_data_split\": \"multilingual\"\n            },\n            \"description\": \"A multilingual dataset for English-to-Dutch translation, capturing diverse linguistic nuances and dialects through the ParaCrawl corpus.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"dual encoder architecture\",\n            \"family\": \"neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"dual encoder\",\n                \"specific_details\": \"emphasizing on precise semantics, emotion, and cultural context preservation\"\n            },\n            \"description\": \"A specialized dual encoder model for English-to-Dutch translation, designed to handle linguistic nuances and dialect variations.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction was parsed into a high-level JSON structure, indicating a build intent for a multilingual translation model. The specific dataset (ParaCrawl), dual encoder architecture, and the emphasis on preserving emotions and cultural context were captured. However, there are no explicit complexity metrics or service details, as they were not provided in the input.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the satellite land classification task using Sentinel-2 imagery, you need to evaluate and compare various segmentation models, considering real-time performance and their capacity to distinguish between intricate classes like freshwater, dense forests, sparse vegetation, urban centers, and barren lands. Optimize for efficiency without compromising accuracy, aiming for a rapid development cycle while ensuring satisfactory pixel-level precision.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"remote sensing, satellite imagery\",\n        \"description\": \"Evaluate and compare various segmentation models for satellite land classification using Sentinel-2 imagery. Focus on real-time performance, precision for classes such as freshwater, dense forests, sparse vegetation, urban centers, and barren lands. Optimize for efficiency and rapid development while maintaining satisfactory pixel-level accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"pixel-level precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2 imagery (specifically for land classification)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"freshwater\", \"dense forests\", \"sparse vegetation\", \"urban centers\", \"barren lands\"],\n            \"specification\": {\n                \"image_size\": null,\n                \"bands\": null,\n                \"data_format\": null\n            },\n            \"description\": \"Data from Sentinel-2 satellite for land classification task, focusing on detailed classes.\",\n            \"preprocessing\": [\n                \"georeferencing\",\n                \"radiometric calibration\",\n                \"data normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"example segmentation masks\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": null,\n                \"params\": null,\n                \"FLOPs\": null\n            },\n            \"description\": \"Potential models for satellite land classification, emphasizing on efficiency and accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using lightweight neural networks or model pruning techniques.\",\n        \"Explore transfer learning from pre-trained models on similar datasets.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\"\n        ],\n        \"target_device\": \"real-time satellite data processing platform\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"TensorRT\",\n            \"OpenVINO\"\n        ],\n        \"demonstration\": \"Demo of model performance and its effects on classification speed\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is parsed with a consult-build intent, indicating that guidance and comparison of models is needed. The user's high expertise suggests they may have more specific requirements that would be beneficial to include in the model specifications and performance metrics. The service section mentions a real-time platform and potential deployment engines, but precise details are not provided as they would depend on further consultation.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, you're tasked with developing a cutting-edge machine translation system for niche languages, focusing initially on Swahili-to-English translation. Utilize the extensive and diverse Masakhane dataset, integrating both Transformer and Recurrent Neural Network architectures to optimize for efficiency. The target is to achieve a benchmark uplift of 15% in BLEU scores, ensuring state-of-the-art accuracy and outperforming existing systems by a significant margin. Mandate a comprehensive evaluation of model robustness and low-resource adaptability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics, translation\",\n        \"description\": \"As a project manager, develop a cutting-edge machine translation system focusing on Swahili-to-English. Use the Masakhane dataset, integrating Transformer and RNN architectures for optimal efficiency. Target a 15% BLEU score uplift over existing systems with emphasis on model robustness and low-resource adaptability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score uplift\",\n                \"value\": 0.15\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Masakhane\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"Swahili\", \"English\"],\n                \"size\": \"extensive and diverse\"\n            },\n            \"description\": \"A comprehensive Swahili-to-English dataset for machine translation, featuring extensive and diverse data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-RNN fusion\",\n            \"family\": \"neural networks\",\n            \"type\": [\"Transformer\", \"Recurrent Neural Network\"],\n            \"specification\": {\n                \"model efficiency optimization\": \"Transformer and RNN integration\"\n            },\n            \"description\": \"A state-of-the-art machine translation model that combines Transformer and RNN architectures to target Swahili-to-English translation with a focus on efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Focus on model robustness and low-resource adaptability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Model performance comparison with existing systems\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed based on the schema. The user's high expertise level indicates a comprehensive understanding of the project. Performance metrics are clearly specified, including a target BLEU score uplift. The Masakhane dataset and the use of Transformer and RNN architectures are incorporated. Low-resource adaptability is an important aspect, but without more specific details, that is represented in the knowledge field.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Task the machine learning team to develop a comprehensive coastal surveillance system utilizing the COCO-Sea dataset, requiring them to design an advanced Attention U-Net model that not only distinguishes between land, water, and infrastructures but also adapts to variable lighting conditions. The project must optimize efficiency for real-time 4K drone footage analysis while maintaining high accuracy and minimizing latency for seamless monitoring operations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"coastal surveillance\",\n        \"description\": \"Task the machine learning team with developing a comprehensive coastal surveillance system, using the COCO-Sea dataset. The team must design an advanced Attention U-Net model capable of differentiating between land, water, and infrastructures, while also handling variable lighting conditions. The system must be efficient for real-time 4K drone footage analysis, achieving high accuracy and minimizing latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specific to variable lighting conditions\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing speed\",\n                \"value\": null,\n                \"unit\": \"frames per second\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO-Sea\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"4K\",\n                \"variable_lighting_conditions\": true\n            },\n            \"description\": \"A dataset designed for coastal surveillance, featuring real-world 4K drone footage with varying lighting conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"variable lighting condition adaptation\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention U-Net\",\n            \"family\": \"U-Net\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced_features\": [\"attention mechanism\"]\n            },\n            \"description\": \"An advanced Attention U-Net model tailored for coastal surveillance, capable of segmentation and adapting to variable lighting conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"The model should incorporate techniques to enhance performance under varying lighting conditions.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"drone systems\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for real-time 4K video analysis\"\n        ],\n        \"demonstration\": {\n            \"requirements\": \"Smooth operation, seamless monitoring, and high accuracy visualizations\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON captures the essence of the task, including the requirement for a high-expertise-level project, attention U-Net model, and real-time processing. Some performance metrics are yet to be specified, but the high expertise level suggests they will be set according to the project's needs. The COCO-Sea dataset's properties and 4K drone footage requirements are clearly conveyed. The confidence score is relatively high, but there is room for further clarification on specific performance expectations and latency.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for a hybrid team, you must develop a multi-class classification model using a limited labeled dataset of 1000 cat images with various breeds. In addition to species identification, the model must also distinguish between domestic and wild cats, and account for variations in lighting and image quality. Provide a detailed architecture and evaluation plan.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal classification\",\n        \"description\": \"The project requires the development of a multi-class classification model for distinguishing between different cat breeds, domesticated, and wild cats, with considerations for variations in lighting and image quality, using a limited labeled dataset of 1000 cat images.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Limited Cat Images\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"species\",\n                \"domestic_wild\"\n            ],\n            \"specification\": {\n                \"number_of_images\": 1000,\n                \"cat_breeds\": [\"varied\"],\n                \"lighting_variations\": [\"variations\"],\n                \"image_quality\": [\"limited\"]\n            },\n            \"description\": \"A dataset of 1000 cat images with diverse breeds, lighting conditions, and image quality.\",\n            \"preprocessing\": [\"image normalization\", \"augmentation (variations in lighting and image quality)\"],\n            \"augmentation\": [\"lighting adjustments, noise addition\"],\n            \"visualization\": [\n                \"data distribution across classes and quality\",\n                \"training and validation split\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Multi-Class Cat Classification Model\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"DenseNet, ResNet, or a custom architecture for multi-class classification\",\n                \"layers\": \"architecture-specific, e.g., number of layers and layers' details\",\n                \"hyperparameters\": {\n                    \"batch size\": null,\n                    \"learning rate\": null,\n                    \"optimizer\": null\n                }\n            },\n            \"description\": \"A model designed to handle multi-class classification with limited labeled data, focusing on breed, domestic-wild distinction, and resilience to variations in image quality.\"\n        }\n    ],\n    \"knowledge\": [\n        \"As a high-expertise user, prior knowledge of transfer learning techniques and data augmentation strategies will be crucial for working with limited data.\",\n        \"Incorporating pre-trained models on similar tasks can help improve performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"transfer learning, if necessary\"],\n        \"target_device\": \"hybrid team's computing resources (potentially cloud or high-performance computing for limited data)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"TensorFlow Serving, or a custom model server, for scalable deployment\"\n        ],\n        \"demonstration\": \"Model performance evaluation with metrics and a comparison to baseline methods\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the given instruction with a high expertise level assumed. The user wants to build a multi-class classification model and includes specific requirements for dataset and model architecture. The model requirements suggest transfer learning and real-world considerations. The 'service' and 'confidence' sections are built on the assumption of best practices for handling limited labeled data and managing deployment. However, specific architecture, hyperparameters, and deployment details are not provided, which would require more information from the user.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a highly-optimized node classification model on the NELL dataset by integrating a state-of-the-art Hypergraph Neural Network architecture. Emphasize on handling noise resilience, multi-relational complexity, and ensure scalability. Target an accuracy of at least 90%, while conducting an extensive hyperparameter fine-tuning study with cross-validation, and document the trade-offs and model performance variations in a comprehensive report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graphs\",\n        \"description\": \"Develop a highly-optimized node classification model using a state-of-the-art Hypergraph Neural Network architecture on the NELL dataset. Focus on noise resilience, multi-relational complexity, and ensure scalability. Target accuracy of at least 90%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_format\": \"hypergraph\",\n                \"nodes_type\": \"knowledge entities\",\n                \"edges_type\": \"relations\"\n            },\n            \"description\": \"A knowledge graph dataset for node classification, emphasizing noise resilience and multi-relational complexity.\",\n            \"preprocessing\": [\"noise removal\", \"relation type encoding\"],\n            \"augmentation\": [],\n            \"visualization\": [\"hyperparameter tuning, model architecture comparison\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art Hypergraph Neural Network\",\n            \"family\": \"Hypergraph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"HyperGCN, GAT-Hyper, or other advanced hypergraph models\",\n                \"noise_reduction techniques\": [\"dropout\", \"graph regularization\"],\n                \"multi-relational handling\": \"joint learning or separate encoders for each relation type\"\n            },\n            \"description\": \"Integrating state-of-the-art for noise resilience, multi-relational complexity, and scalability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using techniques like Graph Convolutional Filters, Attention Mechanisms, and meta-path based strategies for better performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"hyperparameter tuning using GridSearchCV, RandomSearch, or Bayesian Optimization\",\n            \"cross-validation\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"lightweight libraries for graph processing\"],\n        \"demonstration\": \"Model performance presentation with sensitivity analysis and trade-off comparisons\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response parses the given instruction by specifying a high-level intent to build, a focused task of node classification, and the use of a high-expertise level. The description covers NELL's graph structure and requirements for performance, noise resilience, and multi-relational handling. The 'model' section details the use of a state-of-the-art Hypergraph Neural Network, and the 'service' section covers the suggested hyperparameter tuning and model deployment aspects. The assumption for target device and endpoint is based on the assumption that these details were not provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a cutting-edge time series forecasting model for the mobile app's daily active users, incorporating autoregressive integrated moving average (ARIMA) with exogenous variables. Implement a rolling window technique and a Bayesian hyperparameter optimization to ensure adaptability to unforeseen trends, seasonality, and prevent overfitting, while delivering real-time predictions within a latency constraint of 1 minute.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"mobile app (daily active users)\",\n        \"description\": \"Task is to develop a cutting-edge time series forecasting model, specifically using ARIMA with exogenous variables for mobile app's daily active users. It requires a rolling window technique, Bayesian hyperparameter optimization, and real-time predictions with a latency constraint of 1 minute.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 60,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"daily active users\"],\n            \"specification\": {\n                \"dimension\": null,\n                \"length\": null,\n                \"data_format\": \"daily user counts\"\n            },\n            \"description\": \"Dataset containing mobile app daily active users data with exogenous variables.\",\n            \"preprocessing\": [\n                \"cleaning\",\n                \"normalization\"\n            ],\n            \"augmentation\": [\n                \"seasonality-aware synthetic data generation\"\n            ],\n            \"visualization\": [\n                \"time series plot with exogenous variables\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ARIMA with Exogenous Variables\",\n            \"family\": \"classical machine learning\",\n            \"type\": \"statistical models\",\n            \"specification\": {\n                \"forecast_method\": \"ARIMA with exogenous variables\",\n                \"rolling_window\": true,\n                \"hyperparameter_optimization\": \"Bayesian\",\n                \"overfitting_protection\": \"rolling window, Bayesian optimization\"\n            },\n            \"description\": \"ARIMA model designed for real-time forecasting with exogenous factors and adaptive optimization techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Bayesian hyperparameter optimization can adapt to unknown trends and prevent overfitting.\",\n        \"Rolling window technique ensures adaptability to time-varying conditions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection based on exogenous variables\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"mobile app backend\",\n        \"inference_engine\": [\"real-time inference engine\"],\n        \"demonstration\": \"live daily active user predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON response covers the user's intention to build a machine learning model, their assumed level of expertise, and the specific details of the time-series forecasting project, including the model, dataset, performance metrics, and service requirements. However, performance metrics are left as null since they depend on model performance and actual data. A high expertise level suggests some understanding but may need detailed guidance.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a senior project manager for a space-themed image classification project, I need the team to investigate not only a specific, recently published NASA dataset (require source verification) containing diverse celestial objects, but also evaluate the effectiveness of multiple deep learning models like ResNet, Inception, and VGG16. Prove the scalability and possible transfer learning approaches for enhancing accuracy, while considering computational complexity and real-time constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"space-themed\",\n        \"description\": \"As a senior project manager, the team needs to investigate a specific NASA dataset for celestial object classification and evaluate the effectiveness of ResNet, Inception, and VGG16 models. The project also requires scalability, transfer learning approaches to improve accuracy, and consideration of computational complexity and real-time constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational complexity\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            },\n            {\n                \"name\": \"real-time constraint\",\n                \"value\": null,\n                \"unit\": \"latency\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NASA celestial object dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source verification\": true\n            },\n            \"description\": \"A recent NASA dataset with diverse celestial objects for image classification\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNet\",\n            \"family\": \"Residual Networks\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"Inception\",\n            \"family\": \"Inception-v2\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"VGG16\",\n            \"family\": \"VGG16\",\n            \"type\": \"neural networks\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Proof of scalability and transfer learning effects on accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes the user's intent to build a project and high level of expertise. The problem description captures the image classification task on a NASA dataset, evaluation of multiple models, and the requirements for scalability, transfer learning, and complexity consideration. Performance metrics and complexity metrics are specified. However, the specific value metrics are not provided and should be verified from the data. The 'service' section needs specific details on target device and deployment if known.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "question answering", "instruction": "The project must develop a real-time, sentiment-aware Q&A system for e-commerce customer service using the AmazonQA dataset. Utilize a state-of-the-art DistilBERT model that demonstrates superior interpretability, emotion recognition, and achieve sub-millisecond response times while optimizing for both accuracy and system throughput.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"e-commerce customer service\",\n        \"description\": \"The project requires a real-time, sentiment-aware Q&A system for e-commerce using the AmazonQA dataset. The system should be based on a state-of-the-art DistilBERT model and demonstrate interpretability, emotion recognition, and prioritize sub-millisecond response times while optimizing for accuracy and system throughput.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"system throughput\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": {\"unit\": \"milliseconds\", \"value\": 1},\n                \"target\": \"sub-millisecond\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AmazonQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\", \"question\", \"answer\"],\n            \"specification\": {\n                \"sentiment_coverage\": null,\n                \"question_length\": null,\n                \"answer_length\": null\n            },\n            \"description\": \"A dataset for e-commerce sentiment-aware Q&A with questions and answers, including sentiment labels.\",\n            \"preprocessing\": [\"sentiment analysis\", \"tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model performance w.r.t sentiment\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DistilBERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"superior\",\n                \"emotion_recognition\": \"state-of-the-art\"\n            },\n            \"description\": \"A real-time, sentiment-aware Q&A system based on the DistilBERT model for AmazonQA dataset\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interpretablility and emotion recognition using DistilBERT is an advantage.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual embeddings\", \"entity recognition\"],\n        \"target_device\": \"real-time\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for sub-millisecond response times\"],\n        \"demonstration\": \"Emphasis on interactive and fast performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided information was parsed based on the schema, with the project details regarding the model, dataset, and specific task (Q&A) detailed. The 'expertise' field is set to 'high' due to the requirement for state-of-the-art model and real-time response. Some performance metrics are left unspecified, as exact values are not provided in the instruction. However, the user's intent to build and prioritize system throughput are clear. The confidence score is set to 0.89 due to the missing performance metrics, which might impact the overall evaluation and optimization.\",\n        \"score\": 0.89\n    }\n}\n"}
{"task": "node classification", "instruction": "For the PubMed dataset, design a highly efficient GraphSAGE model that not only excels in node classification but also exhibits transfer learning capabilities, adaptability to evolving domains, and must surpass 90% accuracy. Rigorously optimize for real-world deployment, targeting near-zero inference time (ideally below 10ms) on resource-constrained devices with a memory footprint below 25MB.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"bioinformatics (PubMed dataset)\",\n        \"description\": \"Design a GraphSAGE model that demonstrates exceptional efficiency, transfer learning capabilities, adaptability, and must achieve at least 90% accuracy on the PubMed dataset. The model should be optimized for real-world deployment with near-zero inference time (less than 10ms) and a memory footprint of below 25MB.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 10,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": 25,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PubMed\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"node_type\": \"graphs\",\n                \"node_features\": {},\n                \"edge_features\": {},\n                \"task_size\": {}\n            },\n            \"description\": \"A bioinformatics dataset focusing on PubMed publications, requiring a GraphSAGE model for node classification.\",\n            \"preprocessing\": [\"dataset split (train, validation, test)\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GraphSAGE\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": true,\n                \"domain_adaptability\": true,\n                \"optimizer\": \"rigorous\",\n                \"architectural_adaptation\": {}\n            },\n            \"description\": \"A GraphSAGE model specifically designed for the PubMed dataset, targeting high efficiency, transfer learning, and real-world deployment constraints.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Efficient model architecture design, transfer learning techniques, and domain adaptation strategies for GraphSAGE\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"graph feature extraction\"],\n        \"target_device\": \"resource-constrained devices\",\n        \"deployment_endpoint\": \"lightweight deployment platform\",\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": \"use case demonstrations with target deployment scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a GraphSAGE model, specifies the high expertise level, and details the problem statement. Performance metrics, complexity metrics, and optimization requirements are included. However, some specific pre-existing model or data preprocessing steps are assumed based on common practices rather than being explicitly mentioned. The confidence score indicates a high level of understanding, but the potential for missing information is acknowledged.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project manager, you are tasked with overseeing the development of a cutting-edge transformer-based machine translation system that focuses on the diverse and expansive Common Crawl dataset. The objective is not only to facilitate fluent translations between English and Japanese but to tackle the complexities of colloquial language expressions, slang, and regional variations, aiming for a minimum accuracy rate of 98%. Additionally, outline strategies for continuous improvement and incorporate cross-lingual transfer learning to enhance performance in underrepresented dialects.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics and translation\",\n        \"description\": \"The project involves the development of a transformer-based machine translation system, focusing on the Common Crawl dataset. The goal is to handle colloquial language, slang, and regional variations with a minimum accuracy of 98% for English to Japanese translations. The system should also include strategies for continuous improvement and cross-lingual transfer learning to support underrepresented dialects.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl (specifically for English and Japanese)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"English\", \"Japanese\"],\n                \"linguistic_variations\": true,\n                \"colloquial_language\": true,\n                \"slang_included\": true,\n                \"regional_variations\": true\n            },\n            \"description\": \"A large-scale dataset with diverse and extensive content for training the machine translation system on English and Japanese, considering colloquial expressions and regional variations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural machine translation\",\n            \"specification\": {\n                \"architectural advancements\": \"handling complexities of colloquial language and slang\",\n                \"cross-lingual transfer learning\": true\n            },\n            \"description\": \"A cutting-edge transformer-based system designed to translate between English and Japanese, with an emphasis on handling colloquial expressions and regional variations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Strategies for continuous improvement: analyze and update the model regularly with new data, fine-tune based on user feedback, and incorporate NLP advancements.\",\n        \"Cross-lingual transfer learning: leveraging existing models in related languages to enhance the underrepresented dialects\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"update with new data\", \"user feedback-based fine-tuning\", \"NLP improvements\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Highlight system's performance on colloquial expressions and regional variations through case studies or samples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON was parsed based on the provided information, assuming a high level of expertise for a project manager. The problem description covers the translation task with a clear focus on linguistics and minimum accuracy requirement. However, specific device deployment details are not available and would be needed for a more accurate response.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a linguistics expert, our team has been tasked with addressing the intricacies of multilingual customer interactions in diverse markets. We must integrate a state-of-the-art, context-aware neural machine translation model that not only translates queries but also understands regional dialects and colloquialisms to ensure precise, culturally-sensitive responses. What specific steps and API integration must we undertake for a seamless and efficient non-English support system?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual customer interactions\",\n        \"description\": \"As a linguistics expert, the goal is to develop a state-of-the-art, context-aware neural machine translation model for multilingual customer interactions, considering regional dialects and colloquialisms to ensure culturally-sensitive translations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cultural sensitivity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"training_data_types\": [\n                    \"multilingual\",\n                    \"dialects\",\n                    \"colloquialisms\"\n                ]\n            },\n            \"description\": \"A diverse collection of multilingual data with regional dialects and colloquialisms for model training.\",\n            \"preprocessing\": [\n                \"context-aware preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art context-aware NMT\",\n            \"family\": \"neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"context-awareness\": true,\n                \"handling_of_dialects\": true\n            },\n            \"description\": \"A model capable of accurate and culturally-sensitive translation of queries, taking into account regional variations and colloquial language.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Expertise in multilingual NLP, regional dialects, and colloquialism analysis required\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual adaptation for target languages\",\n            \"post-processing for improved response quality\"\n        ],\n        \"target_device\": \"cloud or server\",\n        \"deployment_endpoint\": {\n            \"API integration\": {\n                \"provider\": \"custom or established NMT service\",\n                \"methods\": [\n                    \"REST\",\n                    \"gRPC\"\n                ]\n            }\n        },\n        \"inference_engine\": [\"GPU-driven for faster processing\"],\n        \"demonstration\": \"Design interactive multilingual support system with clear user interface for cultural sensitivity\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed information reflects the expert level and task nature, focusing on integrating a state-of-the-art NMT model with cultural adaptability. However, the absence of specific model performance targets or a precise dataset name suggests room for more detailed information. The service requirements include integration and deployment aspects, but with some assumptions.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Apply a specialized Long Short-Term Memory (LSTM) with bidirectional and state-of-the-art attention mechanism for a small, niche dataset of a specific building's energy consumption. The model must accurately forecast hourly consumption for the upcoming 72 hours, while satisfying an aggressive real-time performance benchmark of 10 milliseconds average response time per prediction, ensuring seamless operation on resource-constrained embedded devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"building energy management\",\n        \"description\": \"Apply a specialized LSTM model with bidirectional and state-of-the-art attention mechanism for a small niche dataset of a specific building's energy consumption. The goal is to forecast hourly consumption for the next 72 hours with an aggressive real-time performance target of 10 milliseconds average response time per prediction.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"average response time\",\n                \"value\": 10,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Niche Building Energy Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"energy consumption\"],\n            \"specification\": {\n                \"length\": {\n                    \"past\": {\n                        \"hours\": 72,\n                        \"predictions\": 72\n                    }\n                },\n                \"size\": {\n                    \"input\": null,\n                    \"output\": null\n                }\n            },\n            \"description\": \"A small, niche dataset focusing on a specific building's energy consumption for hourly forecasting.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Specialized LSTM with BI-Attention\",\n            \"family\": \"LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"bidirectional\": true,\n                \"attention\": \"state-of-the-art\"\n            },\n            \"description\": \"A model specifically designed for forecasting with a focus on real-time performance for embedded devices.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"embedded\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time capable\"],\n        \"demonstration\": \"real-time energy consumption forecasting on resource-constrained devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was parsed based on the provided details about the LSTM model, the performance requirements, and the data characteristics. The user's high expertise level suggests they can handle the technical aspects. However, missing performance metrics and dataset details might be inferred, as the user didn't specify them explicitly.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design a cutting-edge multilingual neural machine translation system, leveraging the extensive TED Talks corpus. Mandate the deployment of Transformer-XL for high-accuracy English to Portuguese translations, prioritizing portability for smartphones with constraints on energy efficiency, low-latency performance, and minimizing the impact on battery life and device storage.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Design a cutting-edge multilingual neural machine translation system, focusing on English to Portuguese translation using Transformer-XL. The deployment must prioritize portable and energy-efficient deployment for smartphones, with low-latency performance and minimal impact on battery life and device storage.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"Watts\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"battery impact\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"storage footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TED Talks corpus\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language\": [\"English\", \"Portuguese\"],\n                \"corpus_size\": \"\"\n            },\n            \"description\": \"Leveraged for a multilingual neural machine translation system, focusing on extensive English to Portuguese translations.\",\n            \"preprocessing\": [\"Multilingual data preprocessing\"],\n            \"augmentation\": [\"None, preserving linguistic integrity\"],\n            \"visualization\": [\"Model performance monitoring\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-XL\",\n                \"language_pairs\": [\"English-Portuguese\"],\n                \"translated_sequences\": null\n            },\n            \"description\": \"Used for high-accuracy English to Portuguese translations in a portable, energy-efficient and low-latency setup.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Multilingual Transformer pretraining, adaptable to low-resource languages.\",\n        \"Efficient transformer designs to reduce computational needs.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Model pruning for size reduction\",\n            \"Model quantization for efficient inference\"\n        ],\n        \"target_device\": \"smartphones\",\n        \"deployment_endpoint\": \"Optimized cloud or lightweight edge server\",\n        \"inference_engine\": [\"Mobile-friendly inference libraries\"],\n        \"demonstration\": \"Demonstration of real-time translation on a smartphone with resource constraints\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has a high level of expertise and the instruction is specific, targeting a high-performance multilingual translation system with constraints on deployment. Missing performance metric values indicate need for training or evaluation. The service section covers device-specific requirements and deployment strategy.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "node classification", "instruction": "Utilize the ultra-high-resolution Allen Brain Connectivity Atlas 2018, combining millions of connectomic data points from mice, to develop a state-of-the-art GWNN model. Emphasize on differentiating various interneuron subtypes with exceptional accuracy and precision, ensuring minimal false positives and false negatives. Incorporate advanced topology management techniques for handling intricate and fine-grained spatial dependencies within the brain's neural network architecture.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"The user wants to develop a state-of-the-art Graph Weighted Neural Network (GWNN) model using the ultra-high-resolution Allen Brain Connectivity Atlas 2018 data. The model should focus on differentiating interneuron subtypes with high accuracy and precision, minimizing false positives and false negatives.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 1.0\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"false_positive_rate\",\n                \"value\": 0.0\n            },\n            {\n                \"name\": \"false_negative_rate\",\n                \"value\": 0.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"memory_usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Allen Brain Connectivity Atlas 2018\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [\n                \"interneuron_subtypes\"\n            ],\n            \"specification\": {\n                \"data_points\": \"millions\",\n                \"node_type_details\": \"connectomic data points from mice\"\n            },\n            \"description\": \"A high-resolution dataset for graph analysis in neuroscience, focusing on interneuron subtypes\",\n            \"preprocessing\": [\n                \"data normalization\",\n                \"noise reduction techniques\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"topology visualizations\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art GWNN\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"topology_management\": \"advanced techniques for spatial dependencies\"\n            },\n            \"description\": \"A GWNN designed to differentiate interneuron subtypes with high accuracy and precision, leveraging the Allen Brain Connectivity Atlas 2018.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"nature_of_knowledge\": \"advanced topology management\"\n        },\n        {\n            \"data_resolution\": \"ultra-high resolution\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"fine-grained feature extraction\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"neuroscience-specific platform or cloud service\",\n        \"inference_engine\": [\n            \"optimized for interneuron subtype classification\"\n        ],\n        \"demonstration\": \"high-resolution visualizations and performance curves\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON captures the core elements of the user's instruction, focusing on model development, data source, and requirements for accuracy and spatial dependencies. The user's high expertise level suggests that they have in-depth knowledge of the problem and the domain. Some areas such as performance metrics and inference time are estimated based on the general goals provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As the senior event security expert for an international music festival, you're mandated to develop a cutting-edge computer vision system using deep learning techniques. The system must detect unique behavioral patterns in real-time, differentiate between normal crowd dynamics and potential threats, and function seamlessly in challenging lighting conditions, with a target accuracy of 99%. Additionally, integrate advanced facial recognition and anomaly detection algorithms, ensuring minimal false positives for a highly secure and scalable solution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"event security\",\n        \"description\": \"Develop a cutting-edge computer vision system using deep learning for real-time event security at an international music festival. The system must detect unique behavioral patterns, differentiate between normal crowd dynamics and threats, and operate in low-light conditions with a target accuracy of 99%. Include advanced facial recognition and anomaly detection with minimal false positives for a secure and scalable solution.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Custom Music Festival Dataset\",\n            \"modality\": [\n                \"image\",\n                \"video\"\n            ],\n            \"target_variables\": [\"threat\", \"facial features\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing real-time event recordings from an international music festival with varying lighting conditions.\",\n            \"preprocessing\": [\"low-light enhancement\"],\n            \"augmentation\": [\"real-time video synthesis\"],\n            \"visualization\": [\"live monitoring\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Computer Vision System\",\n            \"family\": \"deep learning-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"combination of object detection, behavioral analysis, and facial recognition\",\n                \"real-time capabilities\": true,\n                \"resilience to lighting conditions\": \"challenging\"\n            },\n            \"description\": \"A state-of-the-art system leveraging deep learning to detect behavioral patterns and facial recognition, with strict performance requirements for accuracy and minimal false positives.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"anomaly detection algorithms\",\n            \"real-time data preprocessing\"\n        ],\n        \"target_device\": \"cloud-optimized for real-time processing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-powered for efficient deep learning computations\"\n        ],\n        \"demonstration\": \"live, interactive system demonstration with a focus on security and scalability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear task with high-level requirements, indicating a high level of expertise. The problem statement has been parsed to include the target accuracy, unique dataset needs, and the integration of advanced algorithms. The areas of feature engineering, target device, and inference engines align with the complexities mentioned. However, the specific deployment endpoint and demonstration details are not explicitly stated and require some assumptions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "Your objective is to develop a highly specialized sentiment analysis system for the medical domain using the \"PubMed-MEDICAL\" dataset. The task involves fine-grained classification of online patient reviews into five discreet emotion categories (positive, very positive, neutral, slightly negative, and negative), with each labeled as 0 to 4. Emphasize on precision and recall alongside overall macro-averaged F1-score, ensuring interpretability of results while training a deep bidirectional transformer model.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"medical\",\n        \"description\": \"The objective is to develop a sentiment analysis system for medical domain online patient reviews. The task is to classify into five emotion categories: positive, very positive, neutral, slightly negative, and negative with corresponding numerical labels from 0 to 4.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score (macro-average)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PubMed-MEDICAL\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"num_classes\": 5,\n                \"emotion_categories\": [\"positive\", \"very positive\", \"neutral\", \"slightly negative\", \"negative\"]\n            },\n            \"description\": \"The dataset for medical sentiment analysis, focusing on fine-grained classification of patient reviews.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"deep bidirectional transformer\",\n            \"family\": \"transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_name\": \"customized\",\n                \"interpretability\": true\n            },\n            \"description\": \"A deep bidirectional transformer model designed for fine-grained sentiment analysis in the medical domain, emphasizing interpretability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction was parsed into a valid JSON response with high expertise level, reflecting a complex problem in the NLP domain. Performance metrics such as precision, recall, and macro-averaged F1-score were specified, along with requirements for interpretability and a custom deep bidirectional transformer model. The user-extracted dataset and the target device details are left to be provided separately.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Develop a high-resolution urban green space segmentation model using the Vaihingen dataset, specifically enhancing a custom SegNet architecture. Incorporate real-time adaptability for lighting and season variations, with the additional challenge of differentiating between native and invasive plant species, and prioritize interpretability for generating actionable urban planning recommendations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"Develop a high-resolution urban green space segmentation model using the Vaihingen dataset with a customized SegNet architecture. The model should incorporate real-time adaptability for lighting and season variations, and specifically distinguish between native and invasive plant species. Interpretability is a priority to generate actionable planning recommendations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity (native species)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sensitivity (invasive species)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"green space\", \"native plants\", \"invasive plants\"],\n            \"specification\": {\n                \"image_resolution\": null,\n                \"size\": null,\n                \"modalities\": {\n                    \"image\": {\n                        \"color_channels\": null,\n                        \"data_types\": [\"RGB\"]\n                    }\n                }\n            },\n            \"description\": \"The Vaihingen dataset for urban green space segmentation, containing images with native and invasive plant species.\",\n            \"preprocessing\": [\"lighting normalization\", \"seasonal adaptation\"],\n            \"augmentation\": [\"random brightness, contrast\", \"seasonal transformations\"],\n            \"visualization\": [\"class activation maps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"customized SegNet\",\n            \"family\": \"SegNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"customized\",\n                \"real-time_adaptability\": true,\n                \"interpretability\": true\n            },\n            \"description\": \"A SegNet-based model optimized for high-resolution urban green space segmentation, with emphasis on adaptability and differentiation between native and invasive plant species.\"\n        }\n    ],\n    \"knowledge\": [\"Incorporate domain-specific knowledge on urban plants and their characteristics for enhanced model performance\"],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for interpretability\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"UrbanPlanningAPI\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"generate and annotate heatmaps for decision-making\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the main aspects of the user's request, such as the task, dataset, and model preferences. The user's expertise level is inferred as high due to the specific requirements. There might be room for clarification on exact performance metrics and model architecture details, but the core information is well represented. The confidence score is high given the clarity of the request.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "As a project manager for the Machine Learning team, I require the development of a state-of-the-art Graph Convolutional Network (GCN) model optimized for intricate node classification tasks on the diverse CORA dataset. The model must exhibit exceptional performance, targeting a minimum accuracy of 90% while mitigating overfitting through advanced regularization techniques. Additionally, prioritize efficiency by constraining the training duration to a maximum of 1.5 hours on a high-end, latest-gen GPU, ensuring optimal resource utilization without compromising results.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"graph data analysis\",\n        \"description\": \"As a project manager, a state-of-the-art Graph Convolutional Network (GCN) model is needed for intricate node classification tasks on the CORA dataset. The model should achieve at least 90% accuracy and prevent overfitting using advanced regularization techniques. Efficiency is a priority, with a max training duration of 1.5 hours on the latest-gen GPU.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": 900, \n                \"unit\": \"minutes\", \n                \"constraint\": \"maximum\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CORA\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"node_types\": null,\n                \"edge_types\": null,\n                \"features_count\": null,\n                \"graph_size\": null\n            },\n            \"description\": \"A diverse dataset for node classification tasks in graph machine learning.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art GCN\",\n            \"family\": \"GCNs\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": null,\n                \"flops\": null,\n                \"inference_speed\": null,\n                \"overfitting_avoidance\": \"advanced regularization techniques\"\n            },\n            \"description\": \"An advanced GCN model targeting at least 90% accuracy on the CORA dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-end GPU\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"latest-gen GPU\"],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction was clearly mapped to the JSON schema. Expertise is high due to the project manager role. The minimum accuracy, advanced regularization techniques, and the training time constraint were included as performance and complexity metrics. The target dataset, CORA, and specific model type were also defined.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Develop a specialized convolutional neural network (CNN) architecture, focusing on spectral fusion and transfer learning with VGG16, for classifying the CIFAR-100 subset with an additional constraint of 99.5% top-5 accuracy, ensuring interpretability and efficient memory usage on embedded hardware.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"image recognition\",\n        \"description\": \"The user wants to develop a specialized CNN architecture, with a focus on spectral fusion and transfer learning using VGG16, targeting the CIFAR-100 subset. They aim for a high accuracy of 99.5% top-5, and emphasize interpretability and efficient memory usage on embedded hardware.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"top-5 accuracy\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": null, \n                \"unit\": \"none (desired property)\"\n            },\n            {\n                \"name\": \"memory usage (embedded hardware)\",\n                \"value\": null, \n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CIFAR-100 subset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"label\"],\n            \"specification\": null,\n            \"description\": \"A subset of the CIFAR-100 dataset for image classification tasks\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Spectral Fusion CNN with VGG16\",\n            \"family\": \"CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer learning\": \"VGG16\",\n                \"spectral fusion\": true,\n                \"interpretability\": true\n            },\n            \"description\": \"A specialized CNN that integrates spectral fusion with VGG16 for CIFAR-100 classification, aiming for high performance and embedded hardware efficiency.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"embedded hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"include interpretability techniques for model predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction was parsed according to the schema, with the 'intent' as 'build' for creating a model. Given the complexity and specific requirements, the 'expertise' level is assumed to be 'high'. The problem area and downstream task are derived from the context. Performance metric is explicitly mentioned for top-5 accuracy. Interpretability is a desired property and efficient memory usage is a constraint for embedded hardware. The CIFAR-100 subset is directly searched, and the model specification mentions VGG16 and spectral fusion.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a time-series forecasting AI research project, you're tasked with developing an advanced deep learning model for the Electricity dataset. The dataset has undergone a rigorous 70-15-15 train-validation-test split, maintaining a stringent input sequence length of 96 time steps (INPUT_SEQ_LEN=321) and 321 dimensions per observation. Your objective is to design a predictive model that not only forecasts the subsequent 96 steps (PRED_SEQ_LEN=321) with unprecedented accuracy but also exhibits robustness to seasonality and outliers. In addition to minimizing mean squared error (MSE) and mean absolute error (MAE), the model must showcase improved forecasting efficiency by outperforming state-of-the-art methods and providing interpretability of the underlying patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"electricity forecasting\",\n        \"description\": \"Develop an advanced deep learning model for the Electricity dataset with a 70-15-15 train-validation-test split. The input sequence length is 96 time steps (INPUT_SEQ_LEN=321) and each observation has 321 dimensions. The objective is to forecast the next 96 steps (PRED_SEQ_LEN=321) with high accuracy, robustness against seasonality and outliers, and outperform state-of-the-art methods. Additionally, interpretability of the underlying patterns is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"forecast efficiency\",\n                \"value\": null,\n                \"unit\": \"% improvement over state-of-the-art\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"PRED_SEQ_LEN\": 96,\n                \"test_split_ratio\": \"70-15-15\"\n            },\n            \"description\": \"A dataset for time-series forecasting with a 70-15-15 split, input and output sequence dimensions specified, and a focus on electricity data.\",\n            \"preprocessing\": [\"seasonality removal\", \"outlier detection\"],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance\", \"pattern recognition\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Deep Learning Model (specifically for Electricity data)\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A deep learning model tailored for electricity forecasting, aiming for improved accuracy, robustness, and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"To tackle seasonality, incorporate techniques such as seasonal decomposition or recurrent models.\",\n        \"For robustness against outliers, utilize robust loss functions or anomaly detection layers.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"robust data normalization\", \"feature scaling\"],\n        \"target_device\": \"high-performance computing environment\",\n        \"deployment_endpoint\": \"customized API for real-time predictions\",\n        \"inference_engine\": [\"TensorFlow serving\", \"Apache Spark for distributed inference\"],\n        \"demonstration\": \"Include case studies and visualizations to demonstrate model performance and interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction contains detailed requirements, including a high expertise level, specific dataset characteristics, performance targets, and engineering considerations. The confidence score is lower due to the lack of specific model architecture details, which could be inferred if more context or guidelines were provided.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager is tasked with developing a state-of-the-art interactive question-answering platform, targeted at elementary school children. Utilize the exclusive KidsQA dataset, focusing on a specifically tailored BERT variant that enhances pedagogical comprehension and linguistic simplicity. The system must incorporate visually appealing and intuitive design elements to boost engagement and foster learning through play. Additionally, ensure the model's responses adhere to age-appropriate lexicon and pedagogical principles.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education, specifically for elementary school children\",\n        \"description\": \"Create a state-of-the-art interactive question-answering platform designed for elementary school children. Utilize the KidsQA dataset, and customize a BERT variant to enhance pedagogical understanding and simplify language. The platform must have visually appealing and intuitive design for increased engagement and learning through play.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"custom_BERT_variant\": \"tailored for pedagogical simplicity and age-appropriate\"\n            },\n            \"description\": \"A dataset specifically designed for an interactive platform targeting young children, with focus on question-answer pairs.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"custom BERT variant\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pedagogical_comprehension\": true,\n                \"linguistic_simplicity\": true\n            },\n            \"description\": \"A BERT model optimized for educational content, ensuring responses align with age-appropriate language and pedagogical principles.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Focus on engaging visuals and intuitive design for child-friendly interaction\",\n        \"Adopt educational lexicon and principles in model responses\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"age-appropriate visual design\",\n            \"user-friendly interface\"\n        ],\n        \"target_device\": \"multimedia-enabled devices (PC, tablets, or smart devices for young children)\",\n        \"deployment_endpoint\": \"cloud-based for easy access and scalability\",\n        \"inference_engine\": [\"GPU-based for faster model execution\"],\n        \"demonstration\": \"Design demonstrations showcasing learning through play and appropriate content\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction parsed the user's intent for building a question-answering platform, considered the high expertise level for advanced customization. Specific model and dataset requirements were included, as well as design considerations and deployment strategy. Knowledge points were captured. However, performance metrics were not specified in detail, indicating that they may be optional or part of the client's ongoing optimization process.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a machine learning project manager, you are overseeing the development of a cutting-edge deep forecasting system for real-world traffic data. The Traffic dataset, now with additional noise and irregularities, has been divided into three non-overlapping segments: a historical train set (for model learning), a validation set (hyperparameter tuning benchmark), and an unseen test set for robust performance evaluation. Each data point consists of a time-varying 96-step pattern (INPUT_SEQ_LEN=96, INPUT_DIM=862) representing traffic patterns. The challenge is to design and implement an advanced neural architecture that not only predicts the subsequent 96-step trajectory (PRED_SEQ_LEN=96, PRED_DIM=862) but also demonstrates seasonality awareness and adaptability to unforeseen events. The team must optimize for both mean squared error (MSE) and mean absolute error (MAE), aiming for state-of-the-art performance across multiple horizons within the given constraints. Instruct the team to present a step-by-step explanation of their model's reasoning and validation process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-world traffic data\",\n        \"description\": \"As a machine learning project manager, overseeing the development of a deep forecasting system for the Traffic dataset with added noise and irregularities. The dataset is divided into train, validation, and test sets for model learning, hyperparameter tuning, and robust evaluation. Each data point is a 96-step pattern with 862 features, and the task is to predict a 96-step trajectory while demonstrating seasonality and adaptability to unseen events. Optimization goals are MSE and MAE, targeting state-of-the-art performance across multiple horizons while respecting given constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Traffic\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 862,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 862\n            },\n            \"description\": \"Traffic dataset with noise and irregularities, divided into train, validation, and test segments for sequential data processing.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced_neural_architecture\": \"Seasonality-aware and adaptable to unforeseen events\",\n                \"design_details\": \"Step-by-step explanation of reasoning and validation process\"\n            },\n            \"description\": \"Advanced neural architecture for real-world traffic data forecasting, with a focus on seasonal patterns and handling unexpected events.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"description\": \"A detailed explanation of the model's reasoning and validation process, highlighting how it addresses seasonality and unforeseen events.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is parsed with a high level of expertise due to the project manager role, and the given dataset and performance metrics are clear. The model section captures the challenge of seasonality and adaptability. However, without explicit details about target device or deployment, those remain placeholders.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a sophisticated project manager, oversee the development of an advanced deep learning traffic forecasting system, addressing complex scenarios with synthetic anomalies in the high-dimensional Traffic dataset (now with increased resolution, 96-hour patterns, and 862 diverse indicators). The data is segmented into disjoint training (80% historical), validation (10% for dynamic hyperparameter tuning), and an unseen, real-world stress-test set. Design a neural network architecture that not only forecasts the next 96-step trajectory with enhanced multi-scale temporal understanding but also incorporates event-driven adaptability and handles spatiotemporal anomalies. The team must optimize for both regression metrics, MSE and MAE, while targeting benchmark performance for short- to long-term predictions, considering real-world scalability and explainability. Demand a detailed, interpretable model walkthrough and rigorous validation protocol, including sensitivity analysis and comparison with state-of-the-art methods.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"traffic management\",\n        \"description\": \"Develop an advanced deep learning traffic forecasting system for complex scenarios with synthetic anomalies. The dataset is the high-resolution Traffic dataset, containing 96-hour patterns and 862 indicators. The data is divided into training, validation, and unseen stress-test sets. The model should forecast the next 96-step trajectory, have multi-scale temporal understanding, event-driven adaptability, and handle spatiotemporal anomalies. Optimize for MSE and MAE, targeting benchmark performance and considering scalability and explainability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Traffic (high-resolution)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"length\": 96,\n                \"indicators\": 862,\n                \"resampling\": \"96-hour patterns\"\n            },\n            \"description\": \"Dataset with disjoint training, validation, and stress-test sets, including synthetic anomalies and 862 diverse indicators.\",\n            \"preprocessing\": [\"data segmentation\", \"resampling\"],\n            \"augmentation\": [],\n            \"visualization\": [\"96-hour patterns\", \"spatiotemporal anomalies\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Traffic Forecasting Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"multi-scale understanding\": true,\n                    \"event-driven adaptability\": true,\n                    \"anomaly handling\": true\n                },\n                \"interpretability\": \"detailed walkthrough\"\n            },\n            \"description\": \"A model that forecasts future traffic trajectory with explainability and handles spatiotemporal anomalies in a scalable way.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"dynamic hyperparameter tuning\",\n            \"real-world scalability testing\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"interpretable model walkthrough\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been parsed into a high-level JSON format. The user's expertise level was inferred as high due to the project manager role. The deep learning model requirements, dataset details, and performance metrics are clearly stated, as well as the service requirements and validation protocol. A few areas where assumptions were made include the exact deployment endpoint and specific details of the model architecture.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Design a novel deep learning model that combines quantum-enhanced convolutional neural networks (Q-CNN) with a hierarchical attention mechanism, integrating Google's SqueezeNet and ResNet50 for ultra-fast classification of a niche subset of ImageNet22K, while maintaining a target accuracy of 99.7%, ensuring hardware compatibility with resource-constrained RISC-V microcontrollers and implementing real-time adversarial defense mechanisms.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"narrowed ImageNet22K\",\n        \"description\": \"Design a novel deep learning model combining quantum-enhanced convolutional neural networks (Q-CNN) with a hierarchical attention mechanism, using Google's SqueezeNet and ResNet50 for ultra-fast classification. The goal is to achieve 99.7% accuracy, targeting resource-constrained RISC-V microcontrollers, and incorporating real-time adversarial defense mechanisms.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.997\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed compatible with RISC-V microcontrollers\",\n                \"value\": true\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"subset of ImageNet22K\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"predefined_models\": [\n                    \"SqueezeNet\",\n                    \"ResNet50\"\n                ],\n                \"size\": {\n                    \"custom\": \"ultra-fast\"\n                }\n            },\n            \"description\": \"A niche subset of ImageNet22K for ultra-fast image classification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-enhanced CNN with Hierarchical Attention\",\n            \"family\": [\n                \"Q-CNN\",\n                \"Hierarchical Attention\"\n            ],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model融合\": [\n                    \"SqueezeNet\",\n                    \"ResNet50\"\n                ],\n                \"accuracy_goal\": 0.997,\n                \"defense\": \"real-time adversarial defense\"\n            },\n            \"description\": \"A deep learning model combining Q-CNN and hierarchical attention, leveraging SqueezeNet and ResNet50 for efficient image classification.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"RISC-V microcontrollers\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"efficient for microcontrollers\"\n        ],\n        \"demonstration\": \"real-time, including adversarial defense\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction implies a complex and specialized project, which suggests a high level of expertise. The model requirements, performance metrics, and constraints were translated into the JSON schema accordingly. However, the deployment endpoint and inference engine specifics were not explicitly stated, so these are left as placeholders.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "For the PubMed dataset, develop a GraphSAGE model that integrates unsupervised pre-training, active learning, and a dynamic domain adaptation mechanism. The model should achieve at least 95% accuracy while demonstrating generalizability across diverse medical subdomains. Moreover, optimize for deployment on resource-restricted edge devices with stringent constraints: sub-10ms inference time and memory utilization below 15MB, ensuring minimal latency and hardware impact.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"medical\",\n        \"description\": \"Develop a GraphSAGE model for the PubMed dataset that integrates unsupervised pre-training, active learning, and a dynamic domain adaptation mechanism. The model aims for at least 95% accuracy with generalizability across diverse medical subdomains.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": 10,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"memory_size\",\n                \"value\": 15,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PubMed\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A biomedical graph dataset suitable for node classification tasks, such as integrating unsupervised pre-training and domain adaptation.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GraphSAGE\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pre-training\": \"unsupervised\",\n                \"active_learning\": true,\n                \"domain_adaptation\": \"dynamic\"\n            },\n            \"description\": \"A GraphSAGE model with integrated strategies for PubMed dataset, targeting high accuracy and deployment on edge devices.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource-restricted deployment\"],\n        \"demonstration\": \"minimal latency and hardware impact\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided detailed requirements for a GraphSAGE model on the PubMed dataset, specifying accuracy, pre-training, active learning, and domain adaptation. Resource constraints for deployment on edge devices were clearly stated. The 'expertise' level indicates a high understanding of the task and requirements.\",\n        \"score\": 1.0\n    }\n}\n"}
{"task": "question answering", "instruction": "The project's objective is to design a cutting-edge Q&A system for e-commerce customer support that dynamically analyses emotions and responds instantly. The system must employ a DistilBERT model, enhanced with interpretability techniques, and handle a diverse array of queries. The goal is to achieve not just high precision and millisecond response times, but also seamless scalability, efficiency, and effective integration with existing infrastructure. Ensure continuous monitoring of sentiment shifts and adaptivity to evolving customer needs, using the comprehensive AmazonQA dataset.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"e-commerce customer support\",\n        \"description\": \"The objective is to design a Q&A system that dynamically analyzes emotions and provides instant responses using a DistilBERT model, enhanced with interpretability techniques. The system must handle diverse queries, target millisecond response times, and have high scalability, efficiency, and integration with existing e-commerce infrastructure. Sentiment monitoring and adaptability to evolving customer needs are essential, based on the AmazonQA dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AmazonQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"question-answering\",\n                \"content_source\": \"e-commerce\",\n                \"sentiment_analysis\": true\n            },\n            \"description\": \"A comprehensive dataset for e-commerce Q&A, including sentiment analysis, for the dynamic system.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DistilBERT with interpretability enhancements\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"DistilBERT\",\n                \"interpretability_techniques\": [\"attention interpretability\"]\n            },\n            \"description\": \"A state-of-the-art Q&A system with interpretability, for analyzing customer emotions and answering queries.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware feature extraction\",\n            \"sentiment-aware feature generation\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom scalable infrastructure\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"real-time sentiment analysis and adaptive responses\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the given user's intent to build a high-level Q&A system, the requirements for DistilBERT model with interpretability, and the essential performance and scalability criteria. The AmazonQA dataset is specified as the source, and service requirements for a cloud-based setup with a scalable infrastructure and GPU-accelerated inference are included. Some aspects, like precision and desired response times, are left undefined as they would typically be filled in based on the specific model's performance and benchmarking results.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a retail analytics consultant, design a highly-specialized YOLOv5 architecture for the \"FusionMart ShelfScan v3.1\" dataset. The model should distinguish between at least 500 diverse product categories, including niche subgroups like organic produce, niche electronics, and niche home goods. Strive for a precision benchmark of 99.5% with a recall rate of 99.8%, ensuring minimal false positives even in challenging conditions like high crowd density and low lighting. Implement an ultra-low-latency solution, aiming for an average inference time of 0.5 milliseconds per image, while maintaining seamless integration with the \"MerchFlow 360\" inventory system. Account for instant SKU revisions, dynamic forecasting of seasonal fluctuations, and incorporate a self-evolving machine learning model that adaptively optimizes accuracy without compromising resource efficiency in a rapidly evolving retail landscape. Additionally, devise a system to handle real-time data spikes and optimize energy consumption during peak demand.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail analytics\",\n        \"description\": \"Design a highly-specialized YOLOv5 architecture for the 'FusionMart ShelfScan v3.1' dataset. The model should identify at least 500 diverse product categories, including niche subgroups, with a precision of 99.5% and recall of 99.8%. It must handle high crowd density and low lighting conditions with minimal false positives. Aim for an average inference time of 0.5 milliseconds and seamless integration with 'MerchFlow 360'. The model should adapt to SKU revisions, dynamic seasonal fluctuations, and self-evolve for improved accuracy with resource efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.995\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.998\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"average inference time\",\n                \"value\": 0.0005,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"FusionMart ShelfScan v3.1\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product category\"],\n            \"specification\": {\n                \"diverse categories\": 500,\n                \"subgroups\": [\"organic produce\", \"niche electronics\", \"niche home goods\"],\n                \"challenging conditions\": [\"high crowd density\", \"low lighting\"]\n            },\n            \"description\": \"A retail dataset for object detection, specifically focusing on identifying various product categories with high accuracy.\",\n            \"preprocessing\": [\n                \"image cleaning and normalization\"\n            ],\n            \"augmentation\": [\n                \"data augmentation to handle challenging conditions\"\n            ],\n            \"visualization\": [\n                \"data analysis and model performance visualization\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5 (highly-specialized)\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": {\n                    \"target\": \"optimized for niche categories and precision\"\n                },\n                \"latency\": {\n                    \"average\": 0.5,\n                    \"unit\": \"milliseconds\"\n                }\n            },\n            \"description\": \"Adapts YOLOv5 to handle the specifics of the FusionMart ShelfScan v3.1 dataset and requires low-resource efficiency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"dynamic SKU and inventory management\",\n            \"seasonal forecasting\",\n            \"self-evolving model optimization\"\n        ],\n        \"target_device\": \"ultra-fast, compatible with 'MerchFlow 360'\",\n        \"deployment_endpoint\": \"optimized for real-time data spikes\",\n        \"inference_engine\": [\n            \"optimized for low-latency and energy consumption\"\n        ],\n        \"demonstration\": \"incorporates seamless integration with MerchFlow 360 and real-time performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction has been parsed into a JSON reflecting a high-level task, with the need for a highly-specialized YOLOv5 model, detailed performance and complexity requirements, and specific service requirements. The user's high expertise level suggests they might require more guidance or a customized solution.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a cutting-edge, hierarchical Graph Convolutional Network (GCN) model with attention mechanism for the diverse and extensive CORA citation network, targeting node classification with exceptional accuracy above 92%. Implement advanced regularization strategies like dropout and early stopping to prevent overfitting, while maintaining a stringent training time constraint of 1.6-1.8 hours on the most advanced GPU. Rigorously conduct ablation studies on different graph pooling techniques, such as mean, max, and spatial pooling, to assess their impact on performance, computational efficiency, and generalization, ensuring optimal computational footprint and scalability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"citations\",\n        \"description\": \"Create a cutting-edge, hierarchical GCN model with attention mechanism for the CORA citation network. Aim for exceptional accuracy (92% or above) while applying advanced regularization like dropout and early stopping to prevent overfitting. The training time constraint is 1.6-1.8 hours on the most advanced GPU. Conduct ablation studies on graph pooling techniques (mean, max, and spatial pooling).\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.92\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": 1.6,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"GPU performance\",\n                \"value\": \"most advanced\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CORA\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A diverse and extensive citation network for node classification tasks.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Graph Convolutional Networks (GCN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention\": true,\n                \"regularization\": [\"dropout\", \"early stopping\"],\n                \"time_constraint\": \"1.6-1.8 hours\"\n            },\n            \"description\": \"Hierarchical GCN with attention mechanism for node classification on the CORA network.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Ablation studies on graph pooling techniques, optimizing computational footprint and scalability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"most advanced GPU\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Rigorous evaluation of performance, computational efficiency, and generalization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON covers the core details from the instruction, including high-level requirements (intent, expertise), task (GCN model for node classification), dataset (CORA), performance objectives (accuracy, training time), and additional details like regularization methods and ablation study. However, specific model name, feature engineering, and deployment endpoint are not provided but can be inferred as part of the requirements or as assumptions based on the complexity of the task and user's expertise.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Create a scalable, 4K real-time coastal surveillance system employing COCO-Sea dataset and a specialized Deep Feature Fusion (DFF) variant of Attention U-Net. Achieve sub-pixel precision in distinguishing land, water, and maritime structures, while withstanding extreme weather conditions and handling low light with HDR enhancement. Mandate a 30% efficiency improvement with a negligible drop in image clarity, maintaining sub-threshold latency for live operations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"coastal surveillance\",\n        \"description\": \"Create a scalable real-time coastal surveillance system using the COCO-Sea dataset. A specialized Deep Feature Fusion (DFF) variant of Attention U-Net should be employed. The system should achieve sub-pixel precision in differentiating land, water, and maritime structures, even under extreme weather and low light conditions with HDR enhancement. A 30% efficiency improvement is required without significantly sacrificing image clarity, maintaining sub-threshold latency for live operations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sub-pixel precision\",\n                \"value\": 1.0\n            },\n            {\n                \"name\": \"HDR enhancement\",\n                \"value\": null\n            },\n            {\n                \"name\": \"efficiency improvement\",\n                \"value\": 0.3\n            },\n            {\n                \"name\": \"image clarity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency threshold\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"efficiency\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"sub-threshold\",\n                \"unit\": \"delay\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO-Sea\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"color_depth\": 3,\n                \"size\": {\n                    \"width\": \"4K\",\n                    \"height\": \"4K\"\n                }\n            },\n            \"description\": \"A dataset for coastal surveillance, featuring real-time 4K images with diverse coastal scenes.\",\n            \"preprocessing\": [\n                \"HDR enhancement\",\n                \"low light handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DFF-Attention U-Net\",\n            \"family\": \"neural networks\",\n            \"type\": \"U-Net\",\n            \"specification\": {\n                \"architecture\": \"DFF variant\",\n                \"performance_goal\": \"30% efficiency improvement\"\n            },\n            \"description\": \"A specialized DFF Attention U-Net designed for real-time coastal surveillance with sub-pixel accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"scalable, potentially on edge or cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Live operations with real-time updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent to build a system and high expertise suggest a full understanding of the requirements. The metrics provided indicate the performance objectives and the dataset is specified with preprocessing steps. However, the target device and deployment endpoint are not explicitly defined, and they might be derived from the 'scalable' mention. The confidence score reflects the clarity of the given information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "As a project manager for the elite Ph.D. program, direct a team to engineer a highly specialized Visual Turing Test, leveraging the SAI-360 TextVQA dataset. Mandate the integration of cutting-edge M6 model variants, attention-based feature extraction, and iterative fusion techniques for fine-grained image-text alignment. Strive for a 97%+ accuracy on COCO-QA, ensuring human-level precision and a thorough understanding of diverse scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"artificial intelligence research\",\n        \"description\": \"The task is to engineer a highly specialized Visual Turing Test, targeting the SAI-360 TextVQA dataset. The project requires integration of M6 model variants, attention-based feature extraction, and iterative fusion techniques for fine-grained image-text alignment. The goal is to achieve a minimum accuracy of 97% on COCO-QA, ensuring human-level precision and comprehensive scenario understanding.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SAI-360 TextVQA\",\n            \"modality\": [\"image\", \"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_split\": [\"train\", \"validation\", \"test\"],\n                \"model_input_format\": \"image-text pairs\",\n                \"target_object\": \"answer to the question\"\n            },\n            \"description\": \"A specialized dataset for visual question answering, requiring the M6 model and attention-based techniques.\",\n            \"preprocessing\": [\"attention-based feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6 model variants\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"M6\",\n                \"model_depth\": \"advanced\",\n                \"attention_mechanism\": true\n            },\n            \"description\": \"Integration of multiple M6 model variants for fine-grained image-text alignment\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art M6 models, attention-based feature extraction, and iterative fusion techniques\",\n        \"Ensuring human-level precision and comprehensive understanding across diverse scenarios\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"attention-based feature extraction\", \"iterative fusion\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"potentially a specialized AI inference platform\"],\n        \"demonstration\": \"Fine-grained image-text alignment and high accuracy on COCO-QA\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed based on the schema, with high expertise assumed for the project manager. The problem description captured the image-text task, target accuracy, and the specific requirements (M6 models, attention, and fusion). The dataset section reflects a need for the dataset in a user-uploaded format. The confidence score is high given the clear specification of the task and requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the Project Manager for a top-tier deep learning research team, your mandate is to develop a state-of-the-art RNN-T model specifically tailored for the ETTm2 dataset, a complex, high-resolution sensor network data with sub-millisecond noise cancellation. The project requires the model to process 7 unique channel inputs, each with a fine-tuned sequence of 960 sampled values (INPUT_SEQ_LEN * 10). The model must discern intricate, multi-year weather patterns across a sliding, season-aware window of 36,000 timesteps, forecasting a challenging 96-hour ahead horizon. Performance must surpass industry benchmarks with robust quantile-based loss (QLIKE+) and a novel hybrid MSLE metric. Additionally, the model must integrate LIME-friendly explainability, seamlessly adapt to real-time anomalies using DNN-based anomaly detection, and maintain peak efficiency on cutting-edge GPU clusters while optimizing for real-time latency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"sensor network data, weather forecasting\",\n        \"description\": \"Develop a state-of-the-art RNN-T model for the ETTm2 dataset, with a focus on sub-millisecond noise cancellation, 7-channel input processing, a 960-length sequence, and a 36,000-timestep sliding window for season-aware weather forecasting. The model should exceed industry benchmarks using QLIKE+ and a hybrid MSLE metric, incorporate LIME-friendly explainability, and adapt to real-time anomalies with DNN-based detection while maintaining peak efficiency on GPU clusters and optimizing for low latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"QLIKE+\",\n                \"value\": null\n            },\n            {\n                \"name\": \"hybrid MSLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"GPU efficiency\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"Real-time latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 960,\n                \"INPUT_DIM\": 7,\n                \"INPUT_CHANNEL\": 7,\n                \"SLIDING_WINDOW_SIZE\": 36000,\n                \"PRED_SEQ_LEN\": 96,\n                \"DATA_SAMPLING_RATE\": \"10\"\n            },\n            \"description\": \"A complex, high-resolution sensor network dataset with sub-millisecond noise cancellation.\",\n            \"preprocessing\": [\"sub-millisecond noise cancellation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-T Model\",\n            \"family\": \"Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"input_channels\": 7,\n                \"sequence_length\": 960,\n                \"window_size\": 36000,\n                \"forecast_horizon\": 96,\n                \"loss_function\": \"QLIKE+ and hybrid MSLE\"\n            },\n            \"description\": \"A tailored RNN-T model for the ETTm2 dataset with LIME explainability and DNN-based anomaly detection.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"season-aware window\", \"LIME explainability\"],\n        \"target_device\": \"cutting-edge GPU clusters\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"DNN-based anomaly detection\"],\n        \"demonstration\": \"integration with real-time anomaly detection and low latency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the task, indicating the high complexity due to the requirement for a cutting-edge model, multiple performance metrics, and real-time adaptations. However, specific numerical values for metrics or efficiency are not provided and would need to be filled based on the desired benchmarks and efficiency targets.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "object detection", "instruction": "Develop an exclusive AI-based inventory management system for a high-end luxury food boutique, demanding a refined YOLOv5.1.1 variant trained explicitly on the \"LuxuryRareFood Archival Dataset\" comprising 2,000 unique premium ingredients. Target a F1-score of 95% with at least 94% precision in detecting gourmet delicacies in real-time. Emphasize sub-millisecond performance, with a strict latency benchmark of 0.5 milliseconds per image capture, while maintaining detailed recognition of artisanal specialties. Seamless synchronization with the exclusive product database must be guaranteed, accounting for seasonal variations and manual updates with atomic transactions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury food boutique inventory management\",\n        \"description\": \"Develop an exclusive AI-based inventory management system using a refined YOLOv5.1.1 variant, specifically trained on the 'LuxuryRareFood Archival Dataset' with 2,000 unique premium ingredients. The system should achieve a minimum F1-score of 95% and precision of 94% for detecting gourmet delicacies in real-time, while maintaining sub-millisecond performance of 0.5 milliseconds per image capture and detailed recognition of artisanal specialties. Seamless synchronization with the exclusive product database is required, accounting for seasonal variations and manual updates with atomic transactions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.94\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency per image\",\n                \"value\": 0.005,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LuxuryRareFood Archival Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": 2000,\n                \"unique_ingredients\": true\n            },\n            \"description\": \"A high-end luxury food boutique dataset for gourmet delicacies, specifically targeting 2,000 unique premium ingredients\",\n            \"preprocessing\": [\"customized fine-tuning\"],\n            \"augmentation\": [\"subtle aesthetic adjustments\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"refined YOLOv5.1.1 variant\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"YOLOv5.1.1\",\n                \"pretrained_model\": \"customized LuxuryRareFood Dataset\",\n                \"inference_speed\": \"sub-millisecond\"\n            },\n            \"description\": \"A specialized YOLO model designed for gourmet delicacy detection in a luxury food boutique context\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonal data filtering\",\n            \"atomic transactions\"\n        ],\n        \"target_device\": \"high-performance embedded systems\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time processing\"],\n        \"demonstration\": \"demonstrate real-time gourmet delicacy detection and inventory updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction parsed into a JSON with a clear intent to build a high-performance inventory system. The model requirements are specified, with target F1-score and precision. Performance latency and synchronization with the product database are key considerations. The system's expertise level is assumed high due to the complex nature of the problem. However, the deployment endpoint is not provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for an elite machine learning team, you've been tasked with designing a cutting-edge AI research project that pushes the boundaries of image recognition. Your team must design a quantum-enhanced deep learning model, integrating both hybrid quantum-classical neural networks (QCNNs) and advanced swarm intelligence algorithms, specifically tailored to identify and classify ultra-fine subcategories within the HyperGuacamole dataset, a highly specialized collection of 100,000 images with encrypted and dynamically changing patterns. The model must achieve a staggering 99.9% top-1 accuracy under zero-shot learning conditions and demonstrate robustness against adversarial attacks. To validate its performance, the project should also include the implementation of nested validation strategies and real-time adaptability to new unseen subcategories discovered in the wild.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"image recognition\",\n        \"description\": \"Design a cutting-edge AI research project to develop a quantum-enhanced deep learning model integrating hybrid quantum-classical neural networks (QCNNs) and advanced swarm intelligence algorithms. The focus is on the HyperGuacamole dataset, which contains 100,000 images with encrypted and dynamically changing patterns. The model should achieve 99.9% top-1 accuracy under zero-shot learning and be robust against adversarial attacks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"top-1 accuracy\",\n                \"value\": 0.999\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HyperGuacamole\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"total_images\": 100000,\n                \"pattern_type\": [\"encrypted\", \"dynamically changing\"]\n            },\n            \"description\": \"A highly specialized dataset of 100,000 images with ultra-fine subcategories and encrypted, dynamically changing patterns.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-enhanced deep learning\",\n            \"family\": \"Quantum-Classical Hybrid\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"techniques\": [\"QCNNs\", \"swarm intelligence\"],\n                \"zero_shot_learning\": true,\n                \"adversarial_resistance\": true\n            },\n            \"description\": \"A model integrating hybrid quantum-classical neural networks and advanced swarm algorithms, designed for high accuracy and robustness.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Zero-shot learning: Zero-shot scenarios for new subcategories discovered in the wild\",\n        \"Adversarial attacks: The model should be resistant to attacks aimed at misclassification\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"nested validation strategies and real-time adaptability to new unseen subcategories\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The high expertise level indicates a deep understanding of the project requirements. The user has specified that the model should achieve a challenging top-1 accuracy and be resistant to adversarial attacks, along with other advanced features. The missing parts include target device and deployment details, as they were not explicitly stated.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As the project manager for the innovative land use recognition system, your AI team must not only devise a next-generation Vision Transformer (ViT) model that integrates massive Landsat 8 imagery with advanced feature extraction techniques. The model must achieve a remarkable transfer learning efficiency, consistently maintaining an accuracy of at least 98% across diverse geographical regions and surpassing established methods by 2%. Moreover, design a distributed, redundant architecture that guarantees real-time processing on-premises and in-hybrid cloud environments, demonstrating seamless integration with AWS Lambda, Azure Functions, and Google Cloud's serverless offerings. In addition to delivering clean, battle-tested code, provide comprehensive documentation highlighting resource efficiency, performance metrics like precision-recall curves, and an intricate ensemble of evaluation metrics, including micro- and macro-F1 scores, for continuous monitoring and evolutionary improvements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"land use recognition\",\n        \"description\": \"As a project manager, your AI team must develop a next-generation Vision Transformer (ViT) model using massive Landsat 8 imagery and advanced feature extraction. The model should have at least 98% accuracy across diverse regions, outperform existing methods by 2%, and feature a distributed, redundant architecture for real-time processing in on-premises and hybrid cloud environments (AWS Lambda, Azure Functions, Google Cloud). Comprehensive documentation, resource efficiency, and detailed performance metrics are required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"micro-F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"macro-F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": \"yes\",\n                \"unit\": \"on-premises & hybrid cloud\"\n            },\n            {\n                \"name\": \"seamless integration\",\n                \"value\": [\n                    \"AWS Lambda\",\n                    \"Azure Functions\",\n                    \"Google Cloud's serverless\"\n                ]\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Landsat 8\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": \"massive\",\n                \"feature_extraction_techniques\": \"advanced\"\n            },\n            \"description\": \"Integrates massive Landsat 8 imagery for model training.\",\n            \"preprocessing\": null,\n            \"augmentation\": null,\n            \"visualization\": null,\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Vision Transformer (ViT)\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning_efficiency\": \"remarkable\",\n                \"performance_target\": \"98%\",\n                \"improvement_over-established\": \"2%\"\n            },\n            \"description\": \"Integrating massive imagery and advanced feature extraction for high accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Ensemble methods\",\n        \"Performance monitoring\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time\", \"distributed\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"AWS Lambda, Azure Functions, Google Cloud's serverless offerings\",\n        \"inference_engine\": [\n            \"AWS Lambda\",\n            \"Azure Functions\",\n            \"Google Cloud Functions\"\n        ],\n        \"demonstration\": {\n            \"nature\": \"real-time, continuous improvement\",\n            \"platforms\": \"on-premises and hybrid cloud\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction indicates a complex machine learning project that demands a high level of expertise, specified by a high expertise level in the user field. The project's requirement for a remarkable transfer learning efficiency, real-time processing, and diverse performance metrics suggests a high level of specificity. However, it lacks some details such as target device or specific model architecture details, which might be deduced through consultation. The missing components are marked as null.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "As a seasoned project manager for the elite AI research team, your mission is to conceive a multifaceted text classifier that distinguishes between intricate human-generated (0) and advanced AI-produced (1) texts, leveraging the extensive DAIGT dataset. Mandate a sophisticated pipeline encompassing feature extraction, interpretability analysis, bias mitigation, and regular performance optimization. Strive for cutting-edge F1 scores, ensuring model transparency and thorough benchmarking across various iterations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"The mission is to create a multifaceted text classifier that distinguishes between human-generated and AI-produced texts, using the DAIGT dataset. The classifier should include feature extraction, interpretability analysis, bias mitigation, and regular performance optimization for high F1 scores. Emphasis is on model transparency and thorough benchmarking across multiple iterations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"specifics\": \"human-generated vs AI-produced texts\"\n            },\n            \"description\": \"A dataset for distinguishing human-generated and AI-produced texts, specifically designed for advanced text classification tasks.\",\n            \"preprocessing\": [\n                \"feature extraction\",\n                \"normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"interpretability analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"required\",\n                \"bias_mitigation\": \"mandatory\",\n                \"performance_optimization\": \"essential\"\n            },\n            \"description\": \"A sophisticated text classifier with a focus on interpretability, bias mitigation, and performance optimization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cutting-edge machine learning techniques\",\n        \"State-of-the-art interpretability methods\",\n        \"Best practices for bias mitigation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"deep learning features\",\n            \"linguistic features\"\n        ],\n        \"target_device\": \"high-performance computing environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Apache Serving\"],\n        \"demonstration\": \"iteration-based performance comparison and transparency report\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provides ample detail for a complex project with a high-level requirement. The user has high expertise, which is reflected in the project scope. The performance metric F1 score is not provided, but should be discussed with the user for a specific value. The specified dataset and model characteristics align with the request.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "node classification", "instruction": "As a project manager for the machine learning development team, devise a highly sophisticated and interpretable AI system that specializes in classifying nodes within a complex, domain-specific Heterogeneous Information Network (HIN) derived from scholarly Wikipedia and comprehensive DBpedia repositories. Mandate the integration of distinct edge and node characteristics within a Siamese Hypergraph Neural Network (SHNN) architecture, targeting a minimum F1-score of 95% for previously unseen label classifications. Rigorously optimize the model's efficiency, considering its deployment on a cutting-edge Raspberry Pi tailored for real-time edge computation in resource-constrained environments. Scrutinize the influence of advanced neighborhood sampling techniques on model performance, and conduct an in-depth analysis comparing various graph convolution methods, emphasizing the intricate trade-offs between enhanced accuracy and computational resource utilization. Ensure the study provides concrete evidence-based recommendations for future iterations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"scholarly knowledge graphs (Wikipedia, DBpedia)\",\n        \"description\": \"Develop a sophisticated and interpretable AI system for classifying nodes in a complex Heterogeneous Information Network (HIN) based on scholarly sources. Use a Siamese Hypergraph Neural Network (SHNN) architecture, integrating edge and node characteristics, with a target F1-score of at least 95% for unseen node classifications. Focus on optimizing efficiency for deployment on Raspberry Pi for real-time edge computation in resource-constrained environments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Scholarly HIN derived from Wikipedia and DBpedia\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"node_characteristics\": \"\",\n                \"edge_characteristics\": \"\",\n                \"SHNN_structure\": \"\"\n            },\n            \"description\": \"A complex HIN derived from scholarly sources, containing rich node and edge attributes for analysis.\",\n            \"preprocessing\": [\"HIN construction\", \"Feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Hypergraph visualization\",\n                \"Neighborhood sampling comparison\"\n            ],\n            \"source\": \"user-link\",\n            \"description_detail\": \"Contains diverse node types and edge types with rich domain-specific attributes.\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Siamese Hypergraph Neural Network (SHNN)\",\n            \"family\": \"Hypergraph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Siamese, integrates edge and node characteristics\"\n            },\n            \"description\": \"A deep learning model designed for node classification in HIN, targeting high interpretability and optimized for deployment on Raspberry Pi.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Raspberry Pi's resource constraints\",\n        \"Advanced neighborhood sampling techniques\",\n        \"Graph convolution methods comparison\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"Raspberry Pi\",\n        \"deployment_endpoint\": \"Real-time edge computation\",\n        \"inference_engine\": [\"Optimized for Raspberry Pi\"],\n        \"demonstration\": {\n            \"content\": \"Comparison of model performance with/without advanced neighborhood sampling and diverse graph convolution methods\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clear about the project requirements, including the high-level goal, domain, target task, model architecture, performance metrics, and specific challenges like efficiency optimization for Raspberry Pi and analysis of neighborhood sampling and graph convolution trade-offs. The 'expertise' level is assumed to be high given the project manager role.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "object detection", "instruction": "For the \"Oceanic Integrity Protection System,\" we need a state-of-the-art Mask R-CNN designed specifically for Ultra-Deep Sea Vehicle Identification (UDSVI), trained on a confidential data set containing 10,000 unique maritime vessels with detailed hull shapes, keel lengths, and real-time navigation angles. The model must exhibit precision above 98% in extreme weather conditions and crowded sea lanes, while maintaining an average latency of less than 180 milliseconds on the latest, high-throughput NVIDIA A100 Tensor Core GPUs, ensuring efficient resource management without hindering other AI-assisted maritime surveillance tasks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"maritime\",\n        \"description\": \"For the Oceanic Integrity Protection System, a state-of-the-art Mask R-CNN is required. It is specifically designed for Ultra-Deep Sea Vehicle Identification (UDSVI), trained on a confidential data set of 10,000 unique maritime vessels with detailed features (hull shapes, keel lengths, and real-time navigation angles).\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.18,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"GPU performance\",\n                \"value\": \"NVIDIA A100 Tensor Core\",\n                \"unit\": \"GPU\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"confidential maritime data set\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"hull shapes\", \"keel lengths\", \"real-time navigation angles\"],\n            \"specification\": {\n                \"size\": 10000,\n                \"data_type\": \"confidential\"\n            },\n            \"description\": \"A data set containing unique maritime vessels for UDSVI training with detailed features.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art Mask R-CNN\",\n            \"family\": \"Carnegie Mellon University's Mask R-CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_conditions\": \"extreme weather and crowded sea lanes\",\n                \"inference_speed\": {\n                    \"gpu\": \"NVIDIA A100 Tensor Core\",\n                    \"latency\": \"less than 180ms\"\n                }\n            },\n            \"description\": \"Customized for UDSVI, targeting precision above 98% under specific conditions.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"NVIDIA A100 Tensor Core GPU (ultra-high throughput)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"requirements\": \"efficient resource management compatible with AI-assisted maritime surveillance\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction was parsed into a JSON structure. A high expertise level indicates a detailed understanding, which influenced the details in the 'model' and 'complexity_metrics' sections. Specific conditions for performance and resource management are included in the response. Missing elements are assumed to be unknown but would be crucial for complete fulfillment of the project.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, develop a sophisticated medical Q&A system for precision diagnostics, using the vast BioASQ dataset and progressively refining a highly-optimized BioBERT+ framework. Mandate real-time adaptation, explicit result explanations with annotated references from peer-reviewed journals, and continuous A/B testing to achieve a 20% accuracy boost in each model iteration.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medical precision diagnostics\",\n        \"description\": \"As a project manager, aim to develop a sophisticated medical Q&A system that leverages the BioASQ dataset and refines a BioBERT+ framework. Key requirements include real-time adaptation, explicit result explanations with annotated references from peer-reviewed journals, and continuous A/B testing for a 20% accuracy improvement in each model iteration.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null,\n                \"increase_by\": \"20%\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"vast\",\n                \"domain\": \"medical\"\n            },\n            \"description\": \"The BioASQ dataset for training and evaluating the medical Q&A system\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BioBERT+\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization\": \"highly-optimized\"\n            },\n            \"description\": \"A foundation for the medical Q&A system using a BioBERT+ framework.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"explicit result explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clear about the high-level build project, specifying BioBERT+ and BioASQ dataset. The requirement for real-time adaptation, explicit explanations, and A/B testing for accuracy improvement indicates a high level of expertise. The metrics and other specifications are derived directly from the instruction. However, the 'complexity_metrics' section and specific deployment details are not provided.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project lead for the cutting-edge AI-driven time-series analysis team, you are now confronted with a multifaceted challenge that necessitates the development of an advanced, interpretable, and scalable forecasting model. The Electricity dataset, a large and diverse set (with 96 sequential inputs and 321 unique features, INPUT_SEQ_LEN=96, INPUT_DIM=321), has been meticulously divided into distinct training, validation, and testing partitions. Your objective is to design and implement a state-of-the-art deep-learning architecture that seamlessly integrates seasonal patterns, multiple temporal dependencies, and real-time external factors, forecasting not just a 96-step ahead prediction (PRED_SEQ_LEN=96, PRED_DIM=321) but also accounting for long-term trends and short-term volatility. The model must optimize for both mean squared error (MSE) and mean absolute error (MAE) with exceptional performance, ensuring adaptability, stability, and generalizability across all data splits, while adhering to stringent constraints on computational resources and model explainability. Your strategy must balance innovation with practicality, targeting the most efficient solution for this intricate, high-dimensional, and rapidly evolving forecasting problem in the industry.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy sector (electricity data)\",\n        \"description\": \"Develop an advanced, interpretable, and scalable forecasting model for an AI-driven time-series analysis team. The Electricity dataset (96 sequential inputs, 321 features) is divided into train, validation, and test sets. The model must handle seasonal patterns, multiple temporal dependencies, and real-time external factors, predicting a 96-step ahead forecast while accounting for long-term trends and short-term volatility. Performance objectives include minimizing MSE and MAE, and the model must be adaptable, stable, and generalize well across splits. It must adhere to resource constraints and model explainability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational resources\",\n                \"value\": null,\n                \"unit\": \"CPU FLOPs or GPU hours\"\n            },\n            {\n                \"name\": \"model explainability\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specifications\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321\n            },\n            \"description\": \"A large and diverse dataset with 96 sequential inputs and 321 features, divided into training, validation, and testing sets.\",\n            \"preprocessing\": [\n                \"seasonal decomposition\",\n                \"feature extraction (for external factors)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specifications\": {\n                \"architecture\": \"state-of-the-art, integrates seasonal patterns, multiple temporal dependencies, and real-time external factors\",\n                \"explainability techniques\": \"such as interpretable model layers or post-hoc explanations\"\n            },\n            \"description\": \"A scalable forecasting model with high performance, designed to address the Electricity dataset's requirements and the complex nature of the problem.\"\n        }\n    ],\n    \"knowledge\": [\n        \"include recent advancements in interpretable deep learning\",\n        \"account for external factors via feature engineering\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance computing or cloud environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time, scalable engine\"],\n        \"demonstration\": \"transparent report on model performance and interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction provided comprehensive details about the project's requirements, indicating a high level of expertise. The JSON response reflects this by including specific model characteristics and potential resources, and emphasizes on adaptability and explainability. The performance metrics and complexity constraints are clearly stated, and the missing values for specific numbers will be determined during implementation.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "Act as a project manager for an elite AI development team that must design a multi-label text classifier, Polaris AIguard, specifically for the PAGA dataset. In addition to differentiating human-written and AI-generated content (labels 0 and 1), ensure the model can identify subcategories within AI-generated text (2-5). Mandate not only high precision and recall, but also resistance to adversarial attacks, while implementing explainable AI techniques and conducting performance analysis on a dynamically evolved subset of the dataset.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI safety and security\",\n        \"description\": \"The project is about designing a multi-label text classifier named Polaris AIguard, focusing on the PAGA dataset. The classifier should distinguish between human-written and AI-generated content, with specific subcategories for AI-generated text (labels 0, 1, 2, 3, 4, and 5). The model must exhibit high precision and recall, and be resilient to adversarial attacks. Additionally, explainable AI techniques should be incorporated, and performance analysis should be conducted on an evolving dataset subset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"adversarial resistance\",\n                \"value\": null,\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PAGA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"human-written\", \"AI-generated\", \"subcategories (2-5)\"],\n            \"specification\": {\n                \"content_type\": \"multi-label\",\n                \"category_distribution\": null\n            },\n            \"description\": \"A dataset designed for developing Polaris AIguard, specifically focused on human-AI content differentiation and subcategories within AI-generated content.\",\n            \"preprocessing\": [\"multi-label data preparation\"],\n            \"augmentation\": [\"adversarial attack simulation\"],\n            \"visualization\": [\"model performance on evolving subsets\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Polaris AIguard\",\n            \"family\": \"multi-label text classification\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"explainable AI, multi-label classification\",\n                \"adversarial defense mechanisms\": null\n            },\n            \"description\": \"A multi-label text classifier designed for human-AI differentiation and subcategory detection in AI-generated text, with resilience to adversarial attacks.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating explainable AI methods such as LIME, SHAP, or counterfactual explanations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": null,\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [],\n        \"demonstration\": \"Performance analysis on evolving dataset subsets\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON represents a clear and detailed plan, incorporating the user's high expertise level for an AI development team. Performance metrics include precision and recall but not specific values, allowing for flexibility based on the project's progress. Adversarial resistance is a notable requirement, along with explainable AI. However, the target device, deployment endpoint, and the evolving dataset demonstration specifics are missing and would need to be filled out with additional information.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "machine translation", "instruction": "The recently assigned mission for the advanced machine learning development team is to revamp a state-of-the-art emotion-aware neural machine translation (NMT) system, specializing in highly technical medical terminology from rare domain-specific corpora, particularly in biochemistry and pharmacology. This new model must not only guarantee a precise translation with an unprecedented benchmark of 99% concordance with original source content, but also accurately interpret and incorporate the specialized terminology's contextual usage across international medical forums, ensuring global doctors can collaborate seamlessly without misunderstandings. Furthermore, the project necessitates the implementation of a novel evaluation metric that quantifies not only translation accuracy but also the system's ability to maintain the integrity of complex medical concepts and their implications in diverse patient care scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"medical, biochemistry, pharmacology\",\n        \"description\": \"The advanced ML team is assigned to revamp a state-of-the-art emotion-aware NMT system, targeting precise translation of technical medical terms from rare domain-specific corpora in biochemistry and pharmacology. The model must achieve 99% concordance with the original source, interpret and handle context in international medical forums, and incorporate a new evaluation metric for maintaining the integrity of complex medical concepts in diverse patient care scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"concordance\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"custom biochemistry/pharmacology corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": [\"biochemistry\", \"pharmacology\"],\n                \"contextual_usage\": true\n            },\n            \"description\": \"Rare domain-specific corpora for biochemistry and pharmacology, focusing on highly technical medical terminology.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"emotion-aware NMT system (Revamped)\",\n            \"family\": \"neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"99% concordance\",\n                \"context_interpretation\": true,\n                \"novel_metric\": true\n            },\n            \"description\": \"A specialized NMT model for medical terminology translation with emphasis on precision, context awareness, and complex medical concept integrity.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"contextual usage demonstration in international medical forums\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON was parsed based on the user's instruction. The 'expertise' is assumed to be high due to the complexity of the task. 'application_domain' includes the specific medical fields, and a new evaluation metric is mentioned. However, the 'deployment_endpoint' is not specified, and it might require clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a seasoned project manager for the advanced machine learning team, your mandate is to develop a sophisticated image classification system using the Stanford Cars Dataset. Our objective is to leverage the cutting-edge Inception-v3 architecture to create a model that not only discerns between makes, models, and years of cars but also demonstrates exceptional resilience to occlusions, weather effects, and dynamic backgrounds. To showcase the model's prowess, we require a comprehensive performance analysis, including a per-class breakdown of precision and recall, with an aspirational benchmark of 98% overall accuracy under real-world conditions. Emphasize the importance of transfer learning from noisy web data and adherence to state-of-the-art techniques for adversarial robustness.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"automotive\",\n        \"description\": \"Develop a sophisticated image classification system using the Stanford Cars Dataset. Utilize Inception-v3 architecture with focus on classifying car makes, models, and years. The model should be resilient to occlusions, weather effects, and dynamic backgrounds. Performance analysis should include a per-class breakdown of precision and recall, with an aspirational 98% overall accuracy under real-world conditions. Emphasize the importance of transfer learning from noisy web data and adversarial robustness techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"overall_accuracy\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"class_wise_metrics\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Dataset\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\"car make\", \"model\", \"year\"],\n            \"specification\": null,\n            \"description\": \"A dataset for image classification of cars, focusing on distinguishing between various car attributes.\",\n            \"preprocessing\": [\"data augmentation\", \"transfer learning with web data\"],\n            \"augmentation\": [\"occlusions, weather effects, dynamic backgrounds\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Inception-v3\",\n            \"family\": \"Inception\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": true,\n                \"adversarial_robustness\": true\n            },\n            \"description\": \"Leveraging the Inception-v3 architecture for the image classification system, with emphasis on state-of-the-art techniques and transfer learning from noisy web data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning is crucial for adapting existing models to a smaller dataset with limited annotations.\",\n        \"Adversarial robustness techniques will ensure model resilience against real-world adversarial attacks.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or server environment\",\n        \"deployment_endpoint\": \"potentially a cloud-based platform\",\n        \"inference_engine\": [\"highly optimized for image classification tasks\"],\n        \"demonstration\": \"Focus on showcasing results under real-world conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON accurately captures the user's intent to build a complex image classification system, including specific dataset and model requirements. The high expertise level suggests a deeper understanding is expected for this project. Performance metrics are set with an ambitious goal, yet achievable with a sophisticated approach. The reference to web data transfer learning and adversarial robustness is well captured. Some assumptions have been made about the service section, specifically the deployment endpoint.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Utilize the extensive OpenSubtitles corpus to develop a state-of-the-art translation system, specifically designed for real-time conversational exchanges between English and Mandarin speakers. The model, based on advanced BERT, must adeptly handle informal language, colloquial expressions, and idiomatic nuances to guarantee precise and culturally relevant translations, necessitating a multi-stage training and validation process to optimize performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"real-time conversational exchanges\",\n        \"description\": \"Task is to utilize the OpenSubtitles corpus for developing a state-of-the-art translation system, targeting English to Mandarin speakers with an advanced BERT-based model. The model should handle informal language, colloquial expressions, and idiomatic nuances for precise and culturally relevant translations. A multi-stage training and validation process is required for optimal performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Semantic Similarity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Culturally Relevant Translation Accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory Footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"English\", \"Mandarin\"],\n                \"corpus_size\": null,\n                \"informal_text_ratio\": null,\n                \"colloquial_expressions\": null\n            },\n            \"description\": \"The dataset for developing a translation system using the OpenSubtitles corpus, with a focus on English to Mandarin conversations.\",\n            \"preprocessing\": [\n                \"Text cleaning, tokenization\"\n            ],\n            \"augmentation\": null,\n            \"visualization\": [\n                \"Corpus analysis, data distribution\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced BERT for Conversational Translation\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-based, fine-tuned for real-time\",\n                \"language_pair\": \"English to Mandarin\",\n                \"handling_nuances\": \"Colloquial expressions, idiomatics\"\n            },\n            \"description\": \"Advanced BERT model for real-time translation, specifically designed to handle informal English and Mandarin conversations.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"subject\": \"Multi-stage training\",\n            \"description\": \"Include pre-training, fine-tuning, and continuous evaluation.\"\n        },\n        {\n            \"subject\": \"Domain-specific adaptations\",\n            \"description\": \"Potentially, applying domain-adaptive training or transfer learning techniques.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Language adaptation techniques\", \"Contextual sensitivity\"],\n        \"target_device\": \"edge or mobile for real-time processing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Distributed inference, possibly with GPU support\"\n        ],\n        \"demonstration\": \"Create a user-friendly interface for interactive translation during conversations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provides sufficient context for a high-expertise level, emphasizing the need for a state-of-the-art model and detailed requirements like handling informal language and idiomatic nuances. The multi-stage training and specific characteristics of the BERT model are included. However, missing specific metrics and complexity metric values make the confidence slightly lower.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "As we strive to enhance our customer experience catering to diverse dietary needs, devise a machine learning solution for our menu image database. Not only should the system accurately recognize dishes based on ingredients, but also identify local, seasonal, and exotic ingredients, while maintaining a high level of classification accuracy under 5% error and ensuring real-time updating for menu changes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"catering/dietary services\",\n        \"description\": \"Develop a machine learning solution for a menu image database to accurately recognize dishes based on ingredients, identify local, seasonal, and exotic ingredients, while maintaining an accuracy of less than 5% error.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"classification accuracy\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Menu Image Database\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"ingredients\"],\n            \"specification\": {\n                \"database_structure\": {\n                    \"local_inclusions\": \"specific descriptions of local ingredients\",\n                    \"seasonal_inclusions\": \"indications of seasonal dishes\"\n                },\n                \"dynamic_updates\": \"real-time\"\n            },\n            \"description\": \"A database containing menu images with ground-truth ingredient labels, including local, seasonal, and exotic ingredients.\",\n            \"preprocessing\": [\"recipe analysis\", \"image segmentation\"],\n            \"augmentation\": [\"recipe variation for diverse image representation\"],\n            \"visualization\": [\"ingredient distribution within dishes\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced image recognition model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_details\": \"to address multi-label classification\",\n                \" Ingredient-specific modules\": \"for local, seasonal, and exotic ingredient recognition\"\n            },\n            \"description\": \"A machine learning model designed for ingredient-based dish recognition with high accuracy and real-time update capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Seasonality and local availability should influence the model's training data selection.\",\n        \"Advanced feature extraction methods are crucial for recognizing diverse ingredients.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"recipe-based feature extraction\", \"multi-label classification\"],\n        \"target_device\": \"high-performance, real-time processing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"CPU-optimized\", \"GPU acceleration (if available)\"],\n        \"demonstration\": \"Live demonstrations showcasing ingredient recognition and updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has a high level of expertise, and the requirements are clearly stated, including a need for high accuracy and real-time updates. The system requirements cover preprocessing, augmentation, and demonstration. However, the exact model name and family are unspecified, and the deployment endpoint is left open.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a highly specialized satellite image analysis project manager, I need a proficient machine learning team to develop an efficient and fast semantic segmentation model using the Sentinel-2 data. Our focus should be on deep learning algorithms optimized for land type classification, particularly identifying pristine lakes, dense coniferous forests, and densely populated urban areas. The model must demonstrate high accuracy within a reasonable computational time frame, ensuring real-time processing for remote monitoring applications.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"remote sensing and satellite imagery\",\n        \"description\": \"Develop a proficient machine learning model for semantic segmentation using Sentinel-2 satellite data, focusing on land type classification such as pristine lakes, dense coniferous forests, and densely populated urban areas. The model should prioritize efficiency and real-time processing for remote monitoring applications.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational time frame\",\n                \"value\": \"real-time processing\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2 data\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_format\": \"satellite imagery\",\n                \"bands\": [\n                    \"optical, thermal, and spectral data\"\n                ]\n            },\n            \"description\": \"A dataset containing Sentinel-2 satellite data for land type classification tasks, particularly for pristine lakes, dense coniferous forests, and densely populated urban areas.\",\n            \"preprocessing\": [\n                \"image normalization, cleaning, and preprocessing for satellite imagery\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"optimized deep learning algorithms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specific to land type classification\",\n                \"algorithm\": \"focus on optimization for real-time processing\"\n            },\n            \"description\": \"A machine learning model, using deep learning algorithms, designed for land type classification in satellite imagery with a focus on pristine lakes, dense coniferous forests, and densely populated urban areas.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Experience in satellite image analysis and domain-specific deep learning optimizations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for remote sensing\"],\n        \"target_device\": \"optimization for real-time processing on edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for resource-constrained environments\"\n        ],\n        \"demonstration\": \"real-time processing demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a build project with a focus on high accuracy and real-time processing. The user's expertise is assumed to be high due to their project management role. The `feature_engineering` and `inference_engine` sections require some assumptions, but are relevant to the problem and are based on the goal of real-time processing.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "The legal project requires a sophisticated question-answering system that not only processes and decodes complex legal documents from the LegalQA corpus but also performs multi-level analysis of legal terminology using a customized DeBERTa model. Ensure GDPR-compliant data handling, rigorous benchmarking for precision, and the ability to generate transparent explanations for each response to uphold confidence and legal rigor.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"legal\",\n        \"description\": \"The project demands a sophisticated question-answering system that processes and decodes complex legal documents from the LegalQA corpus and conducts multi-level analysis of legal terminology using a customized DeBERTa model. GDPR-compliant data handling is crucial, and the system must demonstrate high precision through rigorous benchmarking. Transparency is essential, requiring the ability to generate explanations for each response to ensure confidence and maintain legal rigor.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LegalQA corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"document_complexity\": {\n                    \"description\": \"Legal document complexity\"\n                },\n                \"size\": {\n                    \"description\": \"Size of the corpus\"\n                }\n            },\n            \"description\": \"A corpus containing complex legal documents for question-answering\",\n            \"preprocessing\": [\"GDPR-compliant\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"customized DeBERTa\",\n            \"family\": \"DeBERTa\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"legal_terminology_analysis\": true,\n                \"gdpr_compliance\": true,\n                \"explanation_generation\": true\n            },\n            \"description\": \"A DeBERTa model tailored for multi-level legal terminology analysis and GDPR-compliant question answering\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"legal entity linking\",\n            \"entity extraction\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"GDPR-compliant storage and API endpoint\",\n        \"inference_engine\": [\"GPU inference\"],\n        \"demonstration\": \"User-friendly interface showcasing explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON reflects a clear intent to build a sophisticated legal QA system, specifying the required skills (high expertise) and the task at hand (NLP, question-answering). GDPR compliance, precision benchmarking, and explanation generation are key requirements. The dataset is specified as LegalQA corpus, with necessary preprocessing steps and a clear source. The model uses a customized DeBERTa for its functionality, and the service section covers aspects such as deployment and demonstration. Missing metric values in 'performance_metrics' are to be filled during the model training phase.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a seasoned project manager for the machine learning development team, you are tasked with overseeing a multifaceted regression project. Utilizing the Concrete Strength Dataset with its intricate array of numerical attributes, you must create an ensemble model. Divide the data into train, validation, and test sets, ensuring stratified sampling. The goal is not just to develop a single model, but to fine-tune multiple models using different algorithms and regularization techniques. Your objective is to minimize the RMSE, demanding a thorough feature selection process and iterative performance optimization. Emphasize the importance of model explainability and provide a comprehensive report on the trade-offs between complexity and accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"materials science or construction\",\n        \"description\": \"Oversee a multifaceted regression project using the Concrete Strength Dataset. Create an ensemble model with stratified data split into train, validation, and test sets. Aim to minimize RMSE through fine-tuning multiple models with different algorithms and regularization techniques, emphasizing feature selection and iterative optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Concrete Strength Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"concrete strength\"],\n            \"specification\": null,\n            \"description\": \"Dataset containing numerical attributes for a concrete strength prediction project.\",\n            \"preprocessing\": [\"stratified sampling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": {\n        \"items\": [\n            {\n                \"name\": \"\",\n                \"family\": \"ensemble\",\n                \"type\": \"ensemble models\",\n                \"specification\": {\n                    \"algorithms\": [],\n                    \"regularization\": []\n                },\n                \"description\": \"Create and fine-tune multiple ensemble models using different algorithms and regularization techniques.\"\n            }\n        ],\n        \"description\": \"Ensemble models, including thorough feature selection and iterative performance optimization for high accuracy\"\n    },\n    \"knowledge\": [\n        \"Trade-offs between model complexity and accuracy are critical. Model explainability is a priority.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Include comprehensive reports on model performance, trade-offs, and explainability.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a high-level regression project involving an ensemble model, Concrete Strength Dataset, and a focus on performance optimization, feature selection, and model explainability. The user's high level of expertise suggests they need expert guidance, but missing specifics may require clarification on the preferred algorithms, regularization techniques, or deployment details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the elite AI development team, you are tasked with enhancing the existing text classification project for British Airways reviews. In addition to improving accuracy using state-of-the-art techniques, ensure the model handles sentiment shifts and sarcasm, while maintaining interpretability. The new constraint is to achieve a minimum F1-score of 0.90 and use explainable AI methods for model validation, all under a tight deadline of 30 days. Good luck!", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"customer reviews (British Airways)\",\n        \"description\": \"The task is to enhance the existing text classification project for British Airways reviews, focusing on improving accuracy with state-of-the-art techniques, detecting sentiment shifts and sarcasm, and maintaining interpretability. The model should have a minimum F1-score of 0.90, and use explainable AI methods for model validation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability constraint\",\n                \"value\": \"tight\",\n                \"unit\": \"time constraints\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"British Airways review data\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\", \"sarcasm\"],\n            \"specification\": null,\n            \"description\": \"Dataset containing British Airways reviews, including sentiment and sarcasm labels.\",\n            \"preprocessing\": [\n                \"text cleaning\",\n                \"tokenization\"\n            ],\n            \"augmentation\": [\n                \"synonym replacement\",\n                \"sentence permutation\"\n            ],\n            \"visualization\": [\n                \"feature importance visualization\",\n                \"confusion matrix\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art text classification model\",\n            \"family\": \"neural networks (preferably deep learning)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability methods\": \"explainable AI techniques (e.g., SHAP, LIME)\"\n            },\n            \"description\": \"A model to classify British Airways reviews with high accuracy, handling sentiment shifts and sarcasm, and utilizing explainable AI for validation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Previous experience with handling sentiment shifts, sarcasm, and interpretability in NLP models\",\n        \"Deadline constraint of 30 days\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction for sentiment and sarcasm detection\"\n        ],\n        \"target_device\": \"cloud-based deployment\",\n        \"deployment_endpoint\": \"specific BA's AI platform or a cloud provider (AWS, Google Cloud, or Azure)\",\n        \"inference_engine\": [\n            \"real-time processing\"\n        ],\n        \"demonstration\": \"Interactive explanations of model predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essential details from the instruction, indicating a build intent with high expertise, specifying the NLP area, task, and performance metrics. The model requirements, deadline, and specific service needs are addressed based on the context. However, the source of the dataset is not explicitly mentioned (assuming user-link).\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for a specialized AI development team, could you propose a deep-learning model with high accuracy and interpretability, considering diverse cat breeds, color variations, and age groups, that can efficiently classify a large and unsorted dataset of cat images captured in various environments?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"AI development - cat image classification\",\n        \"description\": \"Propose a deep-learning model for classifying cat images based on breed, color variations, and age groups. The model should have high accuracy and interpretability while handling a large, unsorted, and environment-diverse dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Unsorted Cat Images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\", \"color\", \"age\"],\n            \"specification\": {\n                \"unsorted\": true,\n                \"diverse\": {\n                    \"environments\": true,\n                    \"breeds\": true,\n                    \"color_variations\": true,\n                    \"age_groups\": true\n                }\n            },\n            \"description\": \"A large and unsorted dataset containing cat images from various environments, with diverse cat breeds, color variations, and age groups.\",\n            \"preprocessing\": [\"image normalization\", \"data augmentation\"],\n            \"augmentation\": [\"cropping\", \"flipping\"],\n            \"visualization\": [\"feature importance\", \"confusion matrix\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Interpretable Deep Learning Model (Proposed)\",\n            \"family\": \"Convolutional Neural Networks (e.g., Inception, ResNet) with attention mechanisms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_target\": null,\n                \"interpretability_techniques\": [\"attention maps\", \"model explanation using LIME\"]\n            },\n            \"description\": \"A deep learning model tailored for high accuracy and interpretability with emphasis on breed, color, and age classification of cats.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Involves transfer learning for better performance on a smaller labeled dataset\",\n        \"Consider using ensemble models for improved accuracy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"feature selection\"],\n        \"target_device\": \"cloud or high-performance computing\",\n        \"deployment_endpoint\": \"cloud-based platform or dedicated server\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"ONNX\"],\n        \"demonstration\": \"Data-driven visualization of model performance and interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction implies a high-level requirement for building a deep learning model, targeting accuracy, interpretability, and handling diverse cat image data. The user's high expertise level suggests they understand the technical details. However, specific performance targets are not clearly provided, so they're marked as null. Knowledge and service requirements reflect the need for guidance in specific techniques and deployment setup.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "For the mobile app's user activity forecasting project, develop a cutting-edge time series analysis model that incorporates autoregressive integrated moving average (ARIMA), exponential smoothing, and seasonal decomposition of time series (STL). Ensure the model can handle non-linearities, handle missing data gracefully, and validate performance using cross-validation techniques, while striving for a balance between bias and variance to prevent overfitting in real-time data streams.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"mobile user activity forecasting\",\n        \"description\": \"Develop a cutting-edge time series analysis model for a mobile app's user activity forecasting project. The model should integrate ARIMA, exponential smoothing, and STL. It must handle non-linearities, missing data, and use cross-validation for performance validation. Aim for a balanced model to prevent overfitting in real-time data streams.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Percentage Error (MAPE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Squared Error (MSE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mobile User Activity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"real-time\": true,\n                \"data_distribution\": null,\n                \"data_format\": \"streaming\"\n            },\n            \"description\": \"A time series dataset containing mobile app user activity data, suitable for real-time forecasting.\",\n            \"preprocessing\": [\n                \"handle missing data\",\n                \"normalize or scale\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ARIMA, Exponential Smoothing, and STL Integration\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"nonlinearities\": true,\n                \"cross-validation_technique\": true,\n                \"overfitting_prevention\": true\n            },\n            \"description\": \"A state-of-the-art time series analysis model using ARIMA, exponential smoothing, and STL for a mobile app's user activity forecasting, ensuring real-time handling of non-linearities, missing data, and overfitting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using appropriate feature engineering for real-time stream data\",\n        \"Employ sliding window technique for cross-validation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"window-based feature extraction\",\n            \"rolling averages\"\n        ],\n        \"target_device\": \"mobile and real-time\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"real-time stream processing\"],\n        \"demonstration\": \"Include examples of real-time forecast and model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction was parsed into a high-level JSON structure reflecting the user's intent to build a model with specific requirements for handling real-world challenges like non-linearities and overfitting. The dataset is described as mobile user activity data, and preprocessing steps for handling missing data are specified. Performance validation and model specifications are outlined, and additional considerations for the service, like real-time feature engineering and deployment, are included.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a cutting-edge ML team, you are tasked with developing a state-of-the-art time-series forecasting model for the ETTm2 dataset. The dataset is divided into train, validation, and test sets with specific input dimensions (96 observations, 7 variables) and target sequences (96 steps, 7 future values). Your team must not only minimize mean squared error (MSE) and mean absolute error (MAE) but also optimize for efficiency and interpretability, ensuring real-time predictions while adhering to strict latency constraints of <= 100 milliseconds. Design an innovative model that demonstrates superior performance and adaptability to changing data patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"cutting-edge ML for time-series data\",\n        \"description\": \"As a project manager, the goal is to develop a state-of-the-art time-series forecasting model for the ETTm2 dataset. The dataset is divided into train, validation, and test sets with 96 observations and 7 variables in the input and 96 steps and 7 future values in the target. Key performance metrics are MSE, MAE, and efficiency and interpretability with a latency constraint of <= 100 milliseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 0.1, \n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 7\n            },\n            \"description\": \"A divided dataset with train, validation, and test sets, each having specific input and target sequence sizes.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": {},\n                \"efficiency\": {}\n            },\n            \"description\": \"A state-of-the-art time-series forecasting model designed for the ETTm2 dataset with a focus on performance, efficiency, and adaptability to changing data patterns.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model should consider advanced techniques like autoencoder-based forecasting, interpretable machine learning, and efficient neural network architectures like Transformer variants for real-time predictions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Real-time, low-latency inference\"\n        ],\n        \"demonstration\": \"Demonstration of real-time predictions and adaptability with changing data patterns\"\n    },\n    \"confidence\": {\n        \"rationale\": \"Based on the provided information, the user's intent is to build a high-level ML project, targeting a state-of-the-art model for time-series forecasting. The complexity of the task suggests a high level of expertise. The specific performance metrics, efficiency, and real-time latency constraint have been incorporated. However, the model name and family, as well as the exact deployment details and some aspects of feature engineering and demonstration, are missing. They may need to be gathered or clarified with additional information.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "Utilizing the PubMed dataset, design a state-of-the-art GraphSAGE model specifically for Transfer Learning in biological node classification, optimizing for not only accuracy but also domain adaptability. mandate a model compression technique for efficient deployment on resource-constrained devices, targeting real-time inference with a strict requirement of sub-25ms latency per unseen node classification request.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"biological\",\n        \"description\": \"Design a state-of-the-art GraphSAGE model for Transfer Learning in biological node classification, with a focus on optimizing accuracy and domain adaptability. Incorporate model compression techniques for efficient deployment on resource-constrained devices, targeting real-time inference with a latency requirement of sub-25ms per unseen node classification request.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"domain adaptability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": 25\n                },\n                \"constraint\": \"sub-25ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PubMed\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node labels\"],\n            \"specification\": null,\n            \"description\": \"A graph dataset for biological node classification, possibly containing a large number of nodes and node features for Transfer Learning.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GraphSAGE\",\n            \"family\": \"GraphSAGE\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": true,\n                \"model_compression\": true\n            },\n            \"description\": \"A state-of-the-art GraphSAGE model tailored for transfer learning in biological node classification, with a focus on both accuracy and efficiency.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time, sub-25ms latency\"],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction indicates a build task with a high level of expertise, requiring a complex graph machine learning model for biological node classification. The emphasis on accuracy, domain adaptability, model compression, and low-latency deployment is captured in the corresponding sections. However, specific performance metrics and feature engineering details are not provided, leaving those values as null or placeholders.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "community detection", "instruction": "Utilize a Label Propagation algorithm on the CiteSeer citation network for discovering dynamic, self-organizing research communities with minimal supervision. Implement an incremental learning feature that seamlessly integrates new nodes without requiring significant recalibration or retraining, ensuring scalability and adaptability over time.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"scientific research\",\n        \"description\": \"Apply a Label Propagation algorithm on the CiteSeer citation network to discover dynamic, self-organizing research communities. Include an incremental learning feature to handle new nodes without major recalibration or retraining, ensuring scalability and adaptability.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": \"required\",\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"adaptability\",\n                \"value\": \"required\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CiteSeer citation network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"citation network\",\n                \"size\": {\n                    \"nodes\": \"unknown\",\n                    \"edges\": \"unknown\"\n                }\n            },\n            \"description\": \"A network representing citations between papers in the CiteSeer dataset.\",\n            \"preprocessing\": [\"node feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [\"graph visualization\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Label Propagation Algorithm\",\n            \"family\": \"Graph Mining Algorithms\",\n            \"type\": \"statistical models\",\n            \"specification\": {\n                \"incremental_learning\": \"true\"\n            },\n            \"description\": \"An unsupervised learning model designed for community detection in the CiteSeer network.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"incremental learning\"\n        ],\n        \"target_device\": \"cloud or distributed system\",\n        \"deployment_endpoint\": \"not specified\",\n        \"inference_engine\": [\"batch processing\"],\n        \"demonstration\": \"demonstrate incremental learning on new nodes\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clear about the algorithm, dataset, and desired incremental learning feature. The user's high expertise suggests they may require more guidance on technical aspects. The missing performance metrics can be inferred from the emphasis on scalability and adaptability.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the advanced machine learning team, your objective is to tackle the sophisticated TextAuthenticity Classification Challenge using the exclusive DAIGT-DeepFusion dataset. This involves not only training a state-of-the-art model but also conducting data augmentation, fine-tuning with transfer learning, and optimizing for not just accuracy but also precision and recall. The task demands multi-stage validation and comparison with existing benchmark techniques.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"authenticity detection\",\n        \"description\": \"Tackle the sophisticated TextAuthenticity Classification Challenge using the exclusive DAIGT-DeepFusion dataset. This project involves training a state-of-the-art model, conducting data augmentation, fine-tuning with transfer learning, and optimizing for accuracy, precision, and recall. Multi-stage validation and comparison with benchmark techniques are required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT-DeepFusion\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"text_features\": {\n                    \"type\": \"custom\",\n                    \"description\": \"Customized dataset with unique features for text authenticity detection\"\n                }\n            },\n            \"description\": \"The exclusive DAIGT-DeepFusion dataset for text authenticity classification, demanding a multi-modal and sophisticated approach.\",\n            \"preprocessing\": [\"data cleaning\", \"tokenization\"],\n            \"augmentation\": [\"synonym replacement\", \"random sentence insertion\", \"back translation\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"custom or pre-trained\",\n                \"description\": \"Advanced model designed for text classification, likely involving transfer learning or transformers\"\n            },\n            \"description\": \"A cutting-edge model for text authenticity classification\"\n        }\n    ],\n    \"knowledge\": [\n        \"Apply transfer learning techniques for adapting pre-trained models to the specific task\",\n        \"Explore ensemble models for improving accuracy and robustness\",\n        \"Investigate recent NLP advancements and relevant research papers\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"ensembling techniques\",\n            \"feature selection\"\n        ],\n        \"target_device\": \"cloud or server\",\n        \"deployment_endpoint\": \"specific platform or API for model deployment\",\n        \"inference_engine\": [\n            \"GPU-accelerated inference\",\n            \"C++ for fast response times\"\n        ],\n        \"demonstration\": \"Live examples or visual explanations of classification outcomes\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has high expertise and the instruction clearly outlines a project with multiple steps, including model selection, data augmentation, and metric optimization. However, specific metric values and deployment details are not provided and should be obtained from the user or inferred based on their expertise level.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the \"High-Volume Text Categorization Challenge,\" you need to choose between implementing a BERT-based ensemble model with fine-tuning on the 20 Newsgroups dataset, taking into account specific subcategories like 'alt.atheism' and 'sci.space', while estimating the training time for various GPU configurations (Tesla V100, A100, and TPUv3) to optimize performance within a tight deadline.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"high-volume text categorization\",\n        \"description\": \"Choose a BERT-based ensemble model for the 'High-Volume Text Categorization Challenge' focusing on the 20 Newsgroups dataset, specifically addressing subcategories like 'alt.atheism' and 'sci.space'. Account for the training time on different GPU configurations (Tesla V100, A100, TPUv3) to meet a tight deadline.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": null,\n                \"unit\": \"GPU hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"category\"],\n            \"specification\": {\n                \"subcategories\": [\n                    \"alt.atheism\",\n                    \"sci.space\"\n                ]\n            },\n            \"description\": \"A dataset for text categorization with subcategories of interest\",\n            \"preprocessing\": [\n                \"BERT pre-processing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-based ensemble model\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble\": true,\n                \"fine-tuning\": true\n            },\n            \"description\": \"An ensemble model built on BERT, fine-tuned for text classification on the 20 Newsgroups dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": [\"Tesla V100\", \"A100\", \"TPUv3\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"deadline consideration\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON accurately captures the user's intention to build a model and their high expertise level. The problem description includes the specific task and dataset requirements, mentioning 20 Newsgroups with subcategories and the importance of training time optimization. The performance metrics are not specified but implied to be optimized for the given context. The 'service' section includes the GPU configurations for training time estimation and a mention of the tight deadline in the demonstration requirements.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image segmentation", "instruction": "As a project manager for a leading AI research team, I require a state-of-the-art deep learning model that not only efficiently segments cats in images with high precision, but also demonstrates superior object detection in low-light conditions, occlusions, and can handle diverse breeds and resized inputs with minimal latency. Kindly provide a detailed proposal outlining the architecture, dataset augmentation techniques, and expected accuracy metrics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"A project manager seeks a state-of-the-art deep learning model for cat segmentation with high precision in images, particularly focusing on object detection in low-light, occlusions, diverse breeds, and handling resized inputs with minimal latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": {\n                    \"情境\": \"high\",\n                    \"target_conditions\": [\"low-light\", \"occlusions\", \"diverse breeds\"]\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"target\": \"minimal\"\n                },\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"cat segmentation\"],\n            \"specification\": {\n                \"data_augmentation\": {\n                    \"techniques\": [\n                        \"low-light simulations\",\n                        \"occlusion simulators\",\n                        \"data augmentation for diverse breeds\"\n                    ]\n                },\n                \"handling_resized_inputs\": \"true\"\n            },\n            \"description\": \"A dataset to be created or chosen that supports the requirements of high precision cat segmentation and robustness to varying conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"low-light simulations\",\n                \"occlusion simulators\",\n                \"diverse breeds-specific augmentation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"detailed proposal needed (e.g., YOLOv5, Mask R-CNN, etc.)\",\n                \"specifics\": {\n                    \"robustness\": [\"low-light conditions\", \"occlusions\", \"diverse breeds\"],\n                    \"efficient inference\": \"true\"\n                }\n            },\n            \"description\": \"A model that efficiently segments cats with high precision, especially in challenging scenarios.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"AI research environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"description\": \"Visual demonstrations of model performance on low-light, occluded, and diverse breed images\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intent, expected expertise, and requirements for the deep learning model, including detailed specifications for the architecture, dataset augmentation, and performance metrics. The confidence is high, assuming the proposal includes the requested details and the missing model and dataset names will be filled in by the engineer.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Aim to develop a highly optimized, bidirectional LSTM-based translation model using Europarl v7, targeting simultaneous, fluent Spanish-English translations with a stringent 95% accuracy rate. Ensure the model is lightweight for low-latency operation, with an absolute response time ceiling of 250 milliseconds on a resource-constrained embedded device.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"The user aims to develop a highly optimized bidirectional LSTM-based translation model for simultaneous Spanish-English translations, targeting 95% accuracy. The model must be lightweight for low-latency operation on a resource-constrained embedded device with a maximum response time of 250 milliseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time (absolute)\",\n                \"value\": 250,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"model size or computational resources\",\n                \"value\": null, \n                \"unit\": \"optimized for lightweight\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Europarl v7\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language pair\": [\"Spanish\", \"English\"]\n            },\n            \"description\": \"The dataset for training the bidirectional LSTM model, targeting Spanish-English language pairs.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"optimized bidirectional LSTM\",\n            \"family\": \"LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"bidirectional\",\n                \"lightweight\": true\n            },\n            \"description\": \"A lightweight LSTM model designed for efficient Spanish-English translation with the specified requirements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"embedded\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for low-latency\"],\n        \"demonstration\": \"require fluent, simultaneous translation with minimal latency on embedded device\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed with the given information, including the user's high expertise level, the specific task (language translation), and the high accuracy target. The performance metrics and response time complexity are clearly stated, while the need for the model to be lightweight and efficient for resource-constrained devices is also noted. However, there are no details about the source of the Spanish-English translation task within the Europarl v7 dataset, which may require a 'direct-search' source.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "object detection", "instruction": "As a project manager for a next-generation retail inventory management solution, you're tasked with developing a highly sophisticated YOLO-based model that leverages the \"StoreShelf Dataset 3.0\". This advanced system must not only detect and accurately classify a minimum of 150 distinct product categories in real-time, but also conduct fine-grained recognition to distinguish sub-variants. The model's precision and recall must surpass industry standards at 95% each, while maintaining a strict requirement of processing 100 shelf images simultaneously, ensuring inventory updates are instantaneous (<500 milliseconds per shelf) to streamline supply chain operations. Please provide a detailed project plan and architecture blueprint for this high-performance AI system.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail inventory management\",\n        \"description\": \"As a project manager, develop a YOLO-based system using the StoreShelf Dataset 3.0 for real-time detection and classification of at least 150 product categories and their sub-variants, with precision and recall of 95% each. The model must handle 100 shelf images simultaneously, processing within 500 milliseconds per shelf for efficient inventory updates.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"simultaneous image processing\",\n                \"value\": 100,\n                \"unit\": \"images\"\n            },\n            {\n                \"name\": \"processing time per shelf\",\n                \"value\": 0.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"StoreShelf Dataset 3.0\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"distinct_product_categories\": 150,\n                \"sub-variant recognition\": true\n            },\n            \"description\": \"A dataset for developing a YOLO-based retail inventory management system with real-time product detection and classification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO-based advanced system\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"YOLOvX or YOLOv4 (or higher)\",\n                \"training_dataset_size\": \"StoreShelf Dataset 3.0\",\n                \"parameter_count\": null,\n                \"flops\": null\n            },\n            \"description\": \"A high-performance AI system using YOLO for real-time product detection and classification in retail inventory management.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Industry standards for precision and recall: 95% each\",\n        \"Targeted shelf image processing rate: 100 images simultaneously\",\n        \"Inference time requirement: 500 milliseconds per shelf\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Data preprocessing for YOLO input\",\n            \"Transfer learning for faster model development\",\n            \"Fine-tuning for sub-variant recognition\"\n        ],\n        \"target_device\": \"edge devices or cloud for real-time processing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"CUDA-based for efficient GPU computations\"],\n        \"demonstration\": \"A live demo showcasing the system's performance and functionality\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction was parsed based on the provided details. The high expertise level suggests a technical project, and performance metrics were set according to the requirements. The plan includes details like model architecture, dataset specifics, and service components. However, there is no deployment endpoint specified, which could be obtained from further communication.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "object detection", "instruction": "For our global wildlife preservation initiative, we require a task-oriented object detection model, specifically an Enhanced EfficientDet variant, trained on the diverse and high-resolution iWildCam dataset. The model must not only exhibit exceptional species recognition and tracking but also demonstrate adaptability in low-light conditions and withstand real-time deployment on resource-constrained, off-grid devices. Moreover, please provide benchmark comparisons with existing models and an energy efficiency analysis to ensure optimal performance for prolonged ecological monitoring missions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"global wildlife conservation\",\n        \"description\": \"A task-oriented object detection model is needed for our global wildlife preservation effort. The model should be an Enhanced EfficientDet variant, trained on the iWildCam dataset, ensuring species recognition and tracking, low-light performance, and real-time deployment on off-grid devices. Benchmarks against existing models and an energy efficiency analysis are essential for optimal performance in prolonged ecological monitoring missions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"species recognition accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"tracking performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"J/FLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"iWildCam\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"resolution\": \"high\",\n                \"diversity\": \"diverse\"\n            },\n            \"description\": \"A high-resolution and diverse wildlife dataset suitable for training an Enhanced EfficientDet model for object detection in various environments, including low-light conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"low-light simulations\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced EfficientDet\",\n            \"family\": \"EfficientDet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptability\": \"low-light conditions, real-time deployment\",\n                \"resource_consumption\": \"resource-constrained devices\"\n            },\n            \"description\": \"A task-oriented object detection model using Enhanced EfficientDet, designed for species recognition, tracking, and deployment in low-light conditions with optimized energy efficiency for off-grid devices.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"off-grid devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"energy-efficient inference methods\"],\n        \"demonstration\": \"deployed ecological monitoring missions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user instruction provided detailed requirements for a machine learning model and additional analysis. The expertise level is high, indicating a technical understanding. However, specific benchmark comparison numbers and energy efficiency values are not provided, leaving those as null values for now. The confidence score is 0.8, reflecting that the data is mostly complete, but lacking some quantitative metrics.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "text classification", "instruction": "The project for high-precision financial events detection involves fine-tuning a tailored ETL process to ingest Reuters-21578's real-time articles. The goal is to employ a state-of-the-art BERT model enhanced with FinBERT, aiming for 90% accuracy in discerning intricate financial topics like mergers, stock valuations, and crisis events, with a stringent focus on reducing false positives that might sway market sentiment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"financial events detection\",\n        \"description\": \"The project involves fine-tuning an ETL process for real-time Reuters-21578 articles to detect high-precision financial events with a 90% accuracy target. This task requires discerning intricate topics like mergers, stock valuations, and crisis events. The focus is on reducing false positives to minimize market sentiment sway.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"false positive rate\",\n                \"value\": 0.0,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"financial event type\", \"textual content\"],\n            \"specification\": null,\n            \"description\": \"A real-time dataset containing articles from Reuters-21578, tailored for financial event detection and finetuning.\",\n            \"preprocessing\": [\"ETL process adaptation for real-time ingestion\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT with FinBERT enhancement\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model performance on financial topics\": {\n                    \"accuracy\": 0.9\n                },\n                \"false positive reduction\": {\n                    \"mechanism\": \"enhanced with FinBERT\"\n                }\n            },\n            \"description\": \"A state-of-the-art BERT model tailored for detecting intricate financial events, with a focus on minimizing false positives.\"\n        }\n    ],\n    \"knowledge\": [\n        \"FinBERT is trained on financial domain-specific text, improving performance on financial event detection.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"finetuning ETL process\",\n            \"domain-specific embedding integration\"\n        ],\n        \"target_device\": \"real-time processing or cloud\",\n        \"deployment_endpoint\": \"financial event detection API\",\n        \"inference_engine\": [\"real-time, low-latency inference\"],\n        \"demonstration\": \"live examples of detected financial events with sentiment analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure is parsed based on the provided information. The user's high level of expertise is indicated for the complexity of the task. Performance metrics and specificity on reducing false positives are included. However, the deployment endpoint and inference engine are assumptions based on typical high-accuracy NLP projects, and the confidence score might be lowered if more specific details were not provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Develop a comprehensive urban greenspace segmentation project using the Vaihingen dataset, employing a state-of-the-art fusion of SegNet and a deep conditional Generative Adversarial Network (GAN). The model should not only distinguish between various vegetation types but also classify their precise health stages, accounting for architectural occlusions and dynamic changes in lighting and seasons. Additionally, design a user-friendly interactive dashboard to present real-time insights for informative urban planning strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"The project aims to develop a comprehensive urban greenspace segmentation using the Vaihingen dataset, employing a fusion of SegNet and a deep conditional GAN. The model should classify vegetation types and their health stages, considering architectural occlusions, and dynamic lighting and seasonal changes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vegetation types\", \"health stages\"],\n            \"specification\": {\n                \"image_size\": null,\n                \"number_of_classes\": null\n            },\n            \"description\": \"A dataset for urban greenspace segmentation, featuring images of urban environments with varying architectures, lighting, and seasonal conditions.\",\n            \"preprocessing\": [\"data augmentation for handling occlusions and seasonal variations\"],\n            \"augmentation\": [\"architectural occlusions\", \"seasonal changes\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SegNet-GAN fusion\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"SegNet-GAN fusion architecture\",\n                \"number_of_layers\": null,\n                \"number_of_parameters\": null,\n                \"FLOPs\": null\n            },\n            \"description\": \"A state-of-the-art model that combines SegNet and a deep conditional GAN for complex urban greenspace segmentation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Dynamic lighting and season changes require data preprocessing and model robustness against such variations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"developing a feature extraction layer for health stages and handling occlusions\"\n        ],\n        \"target_device\": \"cloud-based, potentially supporting interactive web applications\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": {\n            \"description\": \"An interactive dashboard with real-time segmentation results and health stage analysis for urban planners.\",\n            \"features\": [\"real-time updates\", \"visualized insights\", \"customizable for different scenarios\"]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear task, specifying the dataset, model details, and the need for an interactive dashboard. The expert-level expertise indicates a deeper understanding. However, the performance metrics and complexity constraints are not specified, and the model architecture is assumed to be well-defined for a state-of-the-art fusion. The confidence is high, but the gaps include performance expectations and target device deployment specifics.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "The project manager needs to supervise the implementation of an advanced image classification system utilizing the Fashion-MNIST dataset. Mandate the team to develop a state-of-the-art Capsule Network model surpassing a benchmark of 92%, while ensuring robustness against affine transformations. In addition, mandate a comprehensive analysis comparing its performance with advanced CNN architectures, focusing on computational efficiency and interpretability trade-offs, and showcasing the practical advantages under real-world constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"The project requires the development of an advanced image classification system using the Fashion-MNIST dataset. The focus is on implementing a Capsule Network model that surpasses a benchmark accuracy of 92% and exhibits robustness against affine transformations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.92\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"unit\": \"FLOPs\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"unit\": \"model complexity\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": \"28x28\",\n                \"number_of_classes\": 10\n            },\n            \"description\": \"Fashion-MNIST dataset for image classification tasks in fashion items.\",\n            \"preprocessing\": [\"affine transformation robustness\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model comparison\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Capsule Network\",\n            \"family\": \"CapsNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"capabilites for robustness against affine transformations\"\n            },\n            \"description\": \"Advanced Capsule Network model for image classification, targeting state-of-the-art performance.\"\n        },\n        {\n            \"name\": \"advanced CNN architectures\",\n            \"family\": \"comparison models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"focus\": \"computational efficiency and interpretability trade-offs\"\n            },\n            \"description\": \"Compared models for their efficiency and interpretability against the Capsule Network.\"\n        }\n    ],\n    \"knowledge\": [\n        \"trade-offs between computational efficiency and interpretability\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient\", \"interpretable\"],\n        \"demonstration\": {\n            \"real-world constraints\": \"comparison showcasing practical advantages\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes all the necessary details based on the instruction. The user's high expertise indicates they may require less guidance. However, the specific computational efficiency and interpretability values for the models are not provided, requiring them to be deduced from the context. The score reflects this assumption.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager overseeing a cutting-edge machine learning team, we are tasked with tackling a sophisticated time series forecasting challenge. The Traffic dataset, a rich compilation of real-world, complex patterns, has been artfully divided into train, validation, and test subsets to test our model's adaptability. Each input sequence (INPUT_SEQ_LEN=96) consists of an impressive array of 862-dimensional variables, and our goal is to predict not just the subsequent sequence (PRED_SEQ_LEN=96), but to do so with exceptional accuracy over multiple horizons. We demand state-of-the-art performance, utilizing advanced algorithms, and optimizing for both mean squared error (MSE) and mean absolute error (MAE) as key evaluation criteria. Moreover, devise a robust model architecture that can handle seasonality, trends, and potential anomalies, while ensuring scalability and efficient resource management.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-world, complex traffic patterns\",\n        \"description\": \"The team is assigned to tackle a sophisticated time series forecasting challenge using the Traffic dataset. It is divided into train, validation, and test subsets. The input sequences have 862 dimensions and the task is to predict multiple horizons accurately. Key evaluation criteria are mean squared error (MSE) and mean absolute error (MAE). The model must handle seasonality, trends, and anomalies while ensuring scalability and efficient resource management.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Traffic\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 862,\n                \"PRED_SEQ_LEN\": 96\n            },\n            \"description\": \"Real-world, complex Traffic dataset divided into train, validation, and test subsets, with high-dimensional input sequences and multiple forecast horizons.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced algorithms for time series forecasting\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"handles seasonality\": true,\n                \"handles trends\": true,\n                \"anomaly detection\": true,\n                \"scalability\": true,\n                \"efficient resource management\": true\n            },\n            \"description\": \"A model that optimizes for state-of-the-art performance, handling seasonality, trends, and anomalies, while ensuring scalability and efficient use of resources.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction has been parsed into a JSON format, capturing the advanced requirements such as handling complex patterns, multiple horizons, and high-level performance metrics. The user's expertise level has been set to high, reflecting their role as a seasoned project manager. The 'service' section could benefit from additional information on deployment and resource management, but that is expected based on the general nature of the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for a specialized deep learning team, you are tasked with developing a state-of-the-art text classification system for the arXiv-BoolQ dataset. This dataset consists of intricate, domain-specific passages from scientific articles and accompanying questions that require nuanced understanding of the content. The challenge is to design a model capable of differentiating between factual entailment (0 - not inferable) and verifiable claims (1 - inferable), with a focus on precision and F1-score. Collaborate on optimizing a transformer-based architecture using specialized NER and cross-attention mechanisms, ensuring explainability and model robustness while adhering to tight resource constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"scientific articles\",\n        \"description\": \"Develop a state-of-the-art text classification system for the arXiv-BoolQ dataset focusing on factual entailment vs verifiable claims. The model should leverage transformer-based architecture with NER and cross-attention mechanisms, prioritize precision and F1-score, and exhibit explainability and robustness under tight resource constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv-BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"factual entailment\", \"verifiable claims\"],\n            \"specification\": {\n                \"domain\": \"scientific articles\",\n                \"passage_length\": null,\n                \"question_length\": null,\n                \"label_types\": [\"not inferable\", \"inferable\"]\n            },\n            \"description\": \"Dataset with intricate, domain-specific passages from scientific articles and accompanying questions for factual entailment and verifiable claims classification.\",\n            \"preprocessing\": [\"NER tagging\", \"cross-attention encoding\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model interpretability\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"transformer-based architecture (NER + cross-attention)\",\n            \"family\": \"transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": true,\n                \"robustness\": true,\n                \"resource constraints\": {\n                    \"GPU requirements\": \"limited\",\n                    \"GPU memory footprint\": \"low\"\n                }\n            },\n            \"description\": \"A state-of-the-art model designed for text classification, leveraging NER and cross-attention mechanisms.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"ner\", \"cross-attention pretraining\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorRT\", \"on-demand GPU\"],\n        \"demonstration\": \"model interpretability with attention maps\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is clear to build a model, and high expertise indicates a good understanding of the technical aspects. The description covers the problem domain, performance metrics, complexity, dataset, and model specifications. The NER and cross-attention requirements, along with resource constraints, are noted. However, specific metric values and deployment endpoint are not provided, which might be unknown or not applicable without further context.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager is tasked with developing an innovative low-resource language translation solution, specifically targeting Quechua to Spanish translation. Utilizing the JW300 dataset constraint, opt for an advanced LSTM architecture fortified with advanced attention mechanisms. The model must showcase proficiency in capturing intricate linguistic subtleties, aiming for a minimum BLEU score of 35, while simultaneously prioritizing compactness to ensure seamless mobile compatibility and optimal performance on memory-constrained devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"low-resource languages\",\n        \"description\": \"The project involves developing an innovative low-resource language translation solution, focusing on Quechua to Spanish translation. The solution should be based on the JW300 dataset constraint and employ an advanced LSTM architecture enhanced with advanced attention mechanisms. The model must excel in capturing complex linguistic nuances with a minimum BLEU score of 35 while prioritizing compactness for mobile compatibility and optimal performance on memory-limited devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 35.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Model size (compactness)\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"Inference speed\",\n                \"value\": null,\n                \"unit\": \"ms per translation\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"JW300\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": \"Quechua to Spanish\",\n                \"restriction\": \"low-resource\"\n            },\n            \"description\": \"A dataset designed for Quechua to Spanish translation, particularly suitable for a low-resource scenario.\",\n            \"preprocessing\": [\n                \"tokenizer_for_LSTM\",\n                \"attention_mechanism_inclusion\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced LSTM with attention\",\n            \"family\": \"LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"LSTM_layers\": \"advanced\",\n                    \"attention_types\": [\"global\", \"local\"]\n                },\n                \"inference_requirements\": {\n                    \"mobile_compatibility\": true\n                }\n            },\n            \"description\": \"An LSTM model with advanced attention mechanisms tailored for Quechua to Spanish translation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Tokenizer for LSTM compatibility\",\n            \"Incorporation of attention mechanisms\"\n        ],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Optimized for memory-constrained environments\"],\n        \"demonstration\": \"Mobile-friendly and efficient\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction is well-structured, indicating a clear build task with a specific advanced model architecture, dataset, and performance expectations. High expertise suggests the user has a good understanding of the requirements. The 'service' section includes relevant expectations for feature engineering, device compatibility, and deployment. However, the specific inference engine details are not provided, leaving room for further clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a high-impact healthcare forecasting team, you're tasked with developing an advanced time-series forecasting model for the ILI dataset. This dataset, divided into train, validation, and test sets with intricate seasonality patterns, consists of complex 7-dimensional data points (INPUT_SEQ_LEN=36) representing past patient visits. Your mandate is not only to design an algorithm that forecasts the subsequent 24-step (PRED_SEQ_LEN=24) trend with exceptional accuracy, but also to incorporate novel features like moving averages and exponential smoothing. The model must minimize mean squared error (MSE) and mean absolute error (MAE) while demonstrating strong generalization on unseen test data. Your challenge: create a cutting-edge model that outperforms benchmark techniques and ensures real-time monitoring for public health interventions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Develop an advanced time-series forecasting model for the ILI dataset, with intricate seasonality patterns and 7-dimensional data points. The task is to forecast 24-step trends (PRED_SEQ_LEN=24), while incorporating novel features like moving averages and exponential smoothing. Performance targets are high accuracy and strong generalization in minimizing MSE and MAE.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model_complexity\",\n                \"value\": {\n                    \"optimization_for\": \"real-time monitoring\",\n                    \"tradeoff\": \"accuracy vs. computational_speed\"\n                },\n                \"unit\": \"N/A\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 36,\n                \"INPUT_DIM\": 7\n            },\n            \"description\": \"A healthcare dataset with intricate seasonality patterns, split into train, validation, and test sets.\",\n            \"preprocessing\": [\"moving averages\", \"exponential smoothing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novel_features\": [\"moving averages\", \"exponential smoothing\"],\n                \"sequence_length\": 24,\n                \"data_complexity\": \"7-dimensional\"\n            },\n            \"description\": \"An advanced time-series forecasting model with a focus on accuracy and real-time monitoring for public health interventions.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"moving averages\", \"exponential smoothing\"],\n        \"target_device\": \"real-time\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"public health interventions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear task for building a high-expertise model with specific requirements on accuracy, performance metrics, dataset, and novel features. The service section includes real-time monitoring and deployment considerations, but lacks an exact endpoint and engine. The inferred benchmarking for performance is assumed based on the 'outperform' clause.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a state-of-the-art RNN-based sequence-to-sequence model for English to Russian translation, utilizing the extensive and diverse OPUS corpus. Emphasize on domain adaptation for conversational and informal online chat content, targeting BLEU scores above 35 and demonstrating exceptional proficiency in recognizing and preserving slang and idiomatically nuanced expressions. Mandate a stringent real-time constraint of under 250 milliseconds latency to guarantee a seamless, interactive user interface without compromising quality. Additionally, explore potential strategies for transfer learning and fine-tuning to enhance translation accuracy in specific subdomains.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"English to Russian translation, particularly for conversational and informal online chat content\",\n        \"description\": \"Create a state-of-the-art RNN-based sequence-to-sequence model for English to Russian translation. Utilize the OPUS corpus, focusing on domain adaptation for conversational chat content. Aim for BLEU scores above 35 and preserve slang and nuanced expressions. Ensure a real-time constraint of less than 250 milliseconds latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 35\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.25,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": \"English to Russian\",\n                \"conversational_domain\": true,\n                \"diversity\": \"extensive and diverse\"\n            },\n            \"description\": \"A large corpus containing English and Russian text data, especially suited for conversational and informal online chat content.\",\n            \"preprocessing\": [\"domain adaptation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-based sequence-to-sequence model\",\n            \"family\": \"LSTM or GRU\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention\": true,\n                \"transfer_learning\": true,\n                \"fine_tuning\": true\n            },\n            \"description\": \"Targeted for English to Russian translation with domain adaptation and real-time constraints, emphasizing slang and nuanced expression recognition.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Explore transfer learning and fine-tuning strategies for subdomain-specific improvements in translation accuracy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or edge\",\n        \"deployment_endpoint\": \"real-time interactive service\",\n        \"inference_engine\": [\"optimized for low-latency\"],\n        \"demonstration\": \"interactive chat interface demonstrating real-time, high-quality translation with emphasis on slang and nuances\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's intent (build), high expertise level, task description, performance metric (BLEU score), latency constraint, and dataset (OPUS corpus) with specifics. However, certain sections like feature engineering and deployment endpoint are speculative based on the provided information. High latency target might require further clarification on the target environment.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager is tasked with developing a cutting-edge multilingual QA system for the MLQA dataset. Mandate the team to utilize an advanced mBART model fine-tuned on five diverse languages (English, Spanish, Mandarin, French, and Russian), ensuring exceptional cross-lingual performance, especially in under-explored linguistic environments, with a stringent goal of at least 95% accuracy across all language variations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"cross-lingual\",\n        \"description\": \"The project manager needs to develop a multilingual QA system for the MLQA dataset. The team is required to use an advanced mBART model fine-tuned on five languages (English, Spanish, Mandarin, French, and Russian), aiming for exceptional cross-lingual performance, particularly in under-explored languages, with a target accuracy of at least 95% across all language variations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MLQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_variations\": [\"English\", \"Spanish\", \"Mandarin\", \"French\", \"Russian\"]\n            },\n            \"description\": \"A multilingual question answering dataset for the development of a cutting-edge system supporting cross-lingual understanding.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"mBART\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine-tuned_languages\": [\"English\", \"Spanish\", \"Mandarin\", \"French\", \"Russian\"]\n            },\n            \"description\": \"An advanced mBART model fine-tuned on five languages for multilingual QA system, ensuring exceptional cross-lingual performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"cross-lingual\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is accurately capturing the user's requirements for a high-expertise project manager to develop a multilingual QA system with mBART model and specified performance target. The dataset, performance metrics, and complexity metrics align with the instruction. The only assumption made is the target device and the demonstration requirement, as they were not explicitly provided in the input.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As a project manager for the machine learning team, you're tasked with developing a real-time, anomaly-detection system. The system should not only flag sudden spikes or drops in website traffic but also analyze the contributing factors, distinguish normal behavior patterns from anomalies, and provide actionable insights for security and user experience optimization. Additionally, incorporate multivariate analysis and incorporate real-time geographical data to identify regional trends.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"web traffic and security\",\n        \"description\": \"As a project manager, you need to develop a real-time, anomaly-detection system for website traffic. The system should detect sudden spikes or drops, analyze contributing factors, distinguish normal behavior from anomalies, and provide actionable insights for security and user experience optimization. It should incorporate multivariate analysis and real-time geographical data for regional trend analysis.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Website Traffic Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"website traffic\"],\n            \"specification\": {\n                \"dimension\": [\"time\", \"geographical location\"],\n                \"data format\": \"real-time\",\n                \"attributes\": [\"multivariate\", \"geographical details\"]\n            },\n            \"description\": \"A time-series dataset capturing website traffic patterns, including real-time data and geographical information.\",\n            \"preprocessing\": [\"feature scaling\", \"multivariate feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time-series heatmaps\", \"geographical patterns\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Real-time Anomaly Detection System\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"combining ML models for multivariate analysis\",\n                \"time complexity\": \"real-time processing\",\n                \"geospatial analysis\": \"incorporates regional trends\"\n            },\n            \"description\": \"An advanced ensemble model for real-time anomaly detection, capable of analyzing website traffic patterns and providing actionable insights.\"\n        }\n    ],\n    \"knowledge\": [\"Consideration of statistical baselines for anomaly detection\", \"Incorporation of domain expertise for better trend analysis\"],\n    \"service\": {\n        \"feature_engineering\": [\"handling missing data\", \"normalization\", \"real-time update pipeline\"],\n        \"target_device\": \"cloud-based infrastructure\",\n        \"deployment_endpoint\": \"web-based UI or API for real-time insights\",\n        \"inference_engine\": [\"streaming data processing\", \"ML model serving\"],\n        \"demonstration\": \"live demo with mock data showcasing the system's performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction is clearly specified for a build task and requires a high level of expertise. The problem details cover multivariate analysis, real-time analysis, and regional trends. The performance metrics are not specified as the focus is on the detection system, but we can infer the need for optimization. The missing part can be completed with assumptions on feature engineering for real-time data and deployment.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design and implement a highly sophisticated LSTM-CRF based seq2seq model with attention mechanism for low-resource English to Russian translation. Utilize the vast and diverse OPUS-XLcollection, focusing specifically on colloquial and slang-rich social media data from Twitter and Reddit. Target a demanding BLEU score of 40+, ensuring not just word-by-word accuracy but also adeptness at handling cultural references and idiomatic expressions. Enforce an ultra-tight real-time constraint of less than 150 milliseconds to guarantee an ultra-responsive, user-friendly interface without diluting translation precision. Additionally, investigate and propose novel transfer learning techniques for domain-specific adaptation in subdomains like gaming, tech, and entertainment, with a rigorous evaluation of their impact on performance and computational efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"low-resource English to Russian translation\",\n        \"description\": \"Design and implement a sophisticated LSTM-CRF based seq2seq model with attention mechanism for low-resource English to Russian translation. Focus on colloquial and slang-rich social media data from Twitter and Reddit, targeting a BLEU score of 40+ while handling cultural references and idiomatic expressions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 40.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 150,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS-XL\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source_lang\": \"English\",\n                \"target_lang\": \"Russian\",\n                \"type\": \"colloquial and slang-rich social media data\",\n                \"sources\": [\"Twitter\", \"Reddit\"],\n                \"domain\": [\"gaming\", \"tech\", \"entertainment\"]\n            },\n            \"description\": \"Vast and diverse collection of low-resource English to Russian text data, focusing on social media data for domain adaptation.\",\n            \"preprocessing\": [\"Handling slang and colloquialism\", \"Domain adaptation data selection\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"LSTM-CRF seq2seq with attention\",\n            \"family\": \"LSTM-CRF\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"use_attention\": true,\n                \"domain_adaptation\": true\n            },\n            \"description\": \"A highly specialized model for English to Russian translation, addressing low-resource scenario and designed for fast and accurate translation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Investigation of novel transfer learning techniques for domain-specific adaptation\",\n        \"Impact analysis on performance and computational efficiency\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Domain-specific adaptation techniques\"],\n        \"target_device\": \"ultra-responsive (real-time constraint)\",\n        \"deployment_endpoint\": \"user-friendly interface with precision preserved\",\n        \"inference_engine\": [],\n        \"demonstration\": \"User-friendly translation interface showcasing fast and accurate translation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a detailed requirement for a high-end machine learning project, specifying a LSTM-CRF model, transfer learning, and real-time constraint. Expertise level is assumed high due to the complexity of the task. However, some details like the source of transfer learning models and exact deployment endpoint are not explicitly mentioned.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a distinguished project manager leading a trailblazing AI research team, we are now charged with conquering a multifaceted temporal forecasting challenge in the ambit of high-dimensional spatiotemporal data. The newly-enhanced SmartCity dataset, featuring intricate real-world dynamics across 10,000 diverse locations, has been meticulously partitioned into nested cross-validation sets for a stringent test of our deep learning prowess. Each input sequence (INPUT_SEQ_LEN=480), encompassing a staggering 1,500 interdependent features, requires precise forecasting of the subsequent sequence (PRED_SEQ_LEN=480) with exceptional precision for multiple horizons, factoring in long-term dependencies, irregularities, and abrupt changes. Our objective is to achieve groundbreaking results by leveraging novel transformer architectures, optimizing for F1-score, mean absolute percentage error (MAPE), and a custom resilience metric. Additionally, design an adaptive and resilient model infrastructure that accounts for geographical variations, dynamically adjusts to changing trends, and minimizes resource usage while guaranteeing parallelization and distributed computing efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"SmartCity\",\n        \"description\": \"Conquer a multifaceted temporal forecasting challenge in high-dimensional spatiotemporal data using the newly-enhanced SmartCity dataset. The dataset features intricate dynamics across 10,000 diverse locations and is partitioned for nested cross-validation. The task requires precise forecasting of long sequences with 1,500 interdependent features, optimizing for F1-score, MAPE, and a custom resilience metric. Use novel transformer architectures and design an adaptive model infrastructure accounting for geographical variations and distributed computing efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Custom resilience metric\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SmartCity (enhanced)\",\n            \"modality\": [\"multimodal\", \"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 480,\n                \"INPUT_DIM\": 1500,\n                \"PRED_SEQ_LEN\": 480\n            },\n            \"description\": \"Dataset with 10,000 locations, real-world dynamics, and nested cross-validation splits for performance evaluation.\",\n            \"preprocessing\": [\n                \"Feature extraction\",\n                \"Normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Novel Transformer architecture\",\n                \"optimizer\": \"For optimizing performance\"\n            },\n            \"description\": \"Design to account for long-term dependencies, irregularities, and abrupt changes, leveraging transformer architecture.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"geographical feature integration\", \"dynamic trend adaptation\"],\n        \"target_device\": \"Cloud and distributed computing infrastructure\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Parallelization\", \"Distributed computing\"],\n        \"demonstration\": \"Account for geographical variations and changing trends in model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON was parsed based on the provided user's role as a project manager and their high expertise level. The problem, dataset, and model requirements are clearly outlined, with emphasis on novel transformer architectures, cross-validation, and the specified performance metrics. Some details are left implicit, like the specific model name and deployment endpoint, but the overall structure is comprehensive.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Create an advanced urban greenspace analysis project utilizing the Vaihingen dataset, where a novel combination of SegNet, GAN, and a temporal attention mechanism is mandated. The model must detect not only diverse vegetation classes but also grade their vitality levels with high accuracy, considering architectural obstructions and real-time weather effects. Plus, design an intuitive, interactive dashboard for policymakers, integrating a real-time monitoring system with predictive analytics for adaptive urban planning in a sustainable manner.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban greenspace analysis\",\n        \"description\": \"Create an advanced urban greenspace analysis project using the Vaihingen dataset. The project must involve a novel combination of SegNet, GAN, and a temporal attention mechanism for detecting diverse vegetation classes and grading their vitality levels with high accuracy. It should account for architectural obstructions and real-time weather effects. Additionally, the project should include an interactive dashboard for policymakers with a real-time monitoring system and predictive analytics for adaptive urban planning in a sustainable manner.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"vegetation detection accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"vitality level grading accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen\",\n            \"modality\": [\"image\", \"time series\"],\n            \"target_variables\": [\"vegetation classes\", \"vitality levels\"],\n            \"specification\": {\n                \"data_format\": \"satellite or aerial imagery\",\n                \"time_series_length\": \"required for temporal attention mechanism\"\n            },\n            \"description\": \"The Vaihingen dataset for urban greenspace analysis, including satellite or aerial imagery for segmentation and time series for weather and environmental factors.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"data normalization\",\n                \"temporal data processing\"\n            ],\n            \"augmentation\": [\n                \"spatial augmentation (accounting for architectural obstructions)\",\n                \"weather-based augmentation\"\n            ],\n            \"visualization\": [\"model performance visualizations\", \"data exploratory visualizations\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SegNet-GAN with Temporal Attention\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"SegNet\", \"GAN\", \"Temporal Attention Mechanism\"],\n                \"parameters\": {\n                    \"segmentation_network\": {\n                        \"output_depth\": \"multiple vegetation classes and vitality levels\"\n                    },\n                    \"gan_generator\": \"conditioned on vegetation and vitality labels\",\n                    \"temporal_attention_module\": \"adopts real-time data inputs\"\n                }\n            },\n            \"description\": \"A novel model that combines SegNet, GAN, and a temporal attention mechanism for urban greenspace analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"novel architectures and techniques for vegetation detection and vitality grading\",\n        \"integrating real-time weather data\",\n        \"architectural obstructions and their impact on data processing\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for segmentation\", \"time series feature engineering\"],\n        \"target_device\": \"cloud or edge\",\n        \"deployment_endpoint\": \"planned for policymakers\",\n        \"inference_engine\": [\"real-time monitoring and predictive analytics engine\"],\n        \"demonstration\": \"interactive and intuitive dashboard for decision-makers\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction was clearly detailed, specifying a high-level project and requiring expert expertise. The model's architecture and dataset requirements are accurately represented, but missing specific performance metrics and some feature engineering details. The dashboard and service requirements, while comprehensive, are not fully specified. Overall, the JSON response covers the main aspects while leaving room for further clarification.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "In your capacity as the Project Manager for \"The Comprehensive Text Categorization Endeavor,\" assess a specialized Multipart BERT fusion model tailored for deciphering nuanced themes in 20 Newsgroups' distinct subsets like 'religion: atheism' and 'science: astronomy'. Furthermore, conduct a multi-faceted analysis comparing training time for different cutting-edge GPU technologies (P100, A100, and TPUv3), factoring in hardware acceleration, memory efficiency, and optimizing for a stringent deadline while ensuring minimal latency in real-world deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"comprehensive text categorization\",\n        \"description\": \"Assess a specialized Multipart BERT fusion model for deciphering nuanced themes in the 20 Newsgroups' subsets such as 'religion: atheism' and 'science: astronomy'. Conduct a multi-faceted comparison of training time for P100, A100, and TPUv3 GPU technologies, optimizing for minimal latency in real-world deployment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"training time efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"GPU hardware acceleration\",\n                \"value\": null,\n                \"unit\": \"compute power\"\n            },\n            {\n                \"name\": \"memory efficiency\",\n                \"value\": null,\n                \"unit\": \"memory footprint\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\"theme or category\"],\n            \"specification\": {\n                \"subset\": [\"religion: atheism\", \"science: astronomy\"]\n            },\n            \"description\": \"A dataset containing distinct subsets for thematic analysis, focusing on religion and science.\",\n            \"preprocessing\": [\"BERT-specific text preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multipart BERT fusion model\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT fusion, tailored for nuanced theme extraction\"\n            },\n            \"description\": \"A specialized model for text classification in specific 20 Newsgroups subsets.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider hardware specifics such as tensor cores, memory bandwidth, and parallel processing for comparing training time.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": [\"P100\", \"A100\", \"TPUv3\"],\n        \"deployment_endpoint\": \"optimized for real-world deployment\",\n        \"inference_engine\": [\"GPU-accelerated\", \"TPU-accelerated\"],\n        \"demonstration\": \"analysis report showcasing efficiency and latency trade-offs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction includes clear requirements for model assessment, dataset, and GPU comparison. It implies a high level of expertise in the context of a project manager. However, actual metric values for training time, latency, and complexity metrics are unknown and need to be filled with specific analysis results.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "community detection", "instruction": "Develop a specialized, edge-oriented Label Propagation algorithm tailored for the CiteSeer citation network in the field of machine learning, focusing on citation-based community detection in bioinformatics. Implement a real-time incremental learning mechanism that efficiently updates clusters with novel publications in computational genomics, maintaining stability and performance amidst high data throughput.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Create a specialized edge-oriented Label Propagation algorithm tailored for the CiteSeer citation network in machine learning, focusing on citation-based community detection within bioinformatics, with a real-time incremental learning mechanism for updating clusters with novel publications in computational genomics while maintaining stability and performance under high data throughput.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"cluster stability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"performance under high throughput\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time incremental learning time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CiteSeer\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"citation network\",\n                \"domain\": \"machine learning\",\n                \"bioinformatics_focused\": true\n            },\n            \"description\": \"A citation network dataset in the field of machine learning, specifically tailored for bioinformatics application.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Edge-oriented Label Propagation\",\n            \"family\": \"Label Propagation\",\n            \"type\": \"graph machine learning\",\n            \"specification\": {\n                \"incremental_learning\": true,\n                \"realtime\": true\n            },\n            \"description\": \"A specialized algorithm designed for edge-oriented community detection in the CiteSeer citation network with real-time incremental learning for computational genomics.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge device\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"live clustering updates and stability demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction is parsed based on the given schema. The user's intent is to build a specialized model, indicating high expertise. The problem description and requirements are clear, including the need for a real-time incremental learning mechanism and maintaining performance with high data throughput. However, some performance metrics are not provided, so the values are null. The user has specified an edge device for deployment, but exact values for complexity metrics and some service requirements are missing. Assuming edge device characteristics.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for an elite deep learning team at a tech pioneer, your mandate involves creating a highly specialized deep reinforcement learning (DRL) model for the unique ETTm2 time-series dataset. This dataset comprises three subsets: 'past weather patterns' (96 hourly readings, 15 diverse meteorological variables), 'environmental sensors' (60-minute samples, 11 context-specific readings), and 'economic indicators' (daily records, 5 industry-specific metrics). The model should be split into train, validation, and testing sets, with input sequences of 1500 timestamps and targets spanning 1200 steps, forecasting minute-level shifts in 17 interconnected variables. Your team must not only push the boundaries of accuracy, reducing normalized root mean squared error (NRMSE) and mean absolute percentage error (MAPE) to below 2%, but also optimize for model efficiency with a focus on hardware-efficient quantization and pruning techniques. Additionally, the model must deliver interpretable insights through explainable AI methods while ensuring real-time forecasts with sub-millisecond latency (<1ms) for seamless integration into a globally dispersed renewable energy management system. Design a robust, adaptive, and modular architecture that showcases breakthrough performance amidst ever-evolving data irregularities and sector trends.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"renewable energy management\",\n        \"description\": \"Create a highly specialized deep reinforcement learning (DRL) model for the ETTm2 time-series dataset, consisting of past weather patterns, environmental sensors, and economic indicators. The model should target minute-level forecasting of 17 interconnected variables with sub-millisecond latency. Performance metrics include reducing NRMSE and MAPE below 2% while focusing on hardware efficiency with quantization and pruning techniques. The model must deliver interpretable insights for real-time integration into a renewable energy management system.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"NRMSE\",\n                \"value\": 0.02\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": 0.02\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference latency\",\n                \"value\": 0.001,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2 (past weather patterns)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"15 diverse meteorological variables\"],\n            \"specification\": {\n                \"sequence_length\": 1500,\n                \"timestamp_per_sequence\": 96,\n                \"variable_dimensions\": 15\n            },\n            \"description\": \"96 hourly readings of 15 meteorological variables from the past weather patterns subset\",\n            \"preprocessing\": [\"Data normalization\", \"Sequence shaping\"],\n            \"augmentation\": [\"None (due to real-world data quality)\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"ETTm2 (environmental sensors)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"11 context-specific readings\"],\n            \"specification\": {\n                \"sequence_length\": 1500,\n                \"timestamp_per_sequence\": 60,\n                \"variable_dimensions\": 11\n            },\n            \"description\": \"60-minute samples of 11 environmental sensor readings\",\n            \"preprocessing\": [\"Data normalization\", \"Sequence shaping\"],\n            \"augmentation\": [\"None (due to real-world data quality)\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"ETTm2 (economic indicators)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"5 industry-specific metrics\"],\n            \"specification\": {\n                \"sequence_length\": 1500,\n                \"timestamp_per_sequence\": 1,\n                \"variable_dimensions\": 5\n            },\n            \"description\": \"Daily records of 5 industry-specific economic metrics\",\n            \"preprocessing\": [\"Data normalization\", \"Sequence shaping\"],\n            \"augmentation\": [\"None (due to real-world data quality)\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Highly-Specialized DRL Model\",\n            \"family\": \"Deep Reinforcement Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Adaptive and modular architecture\",\n                \"quantization\": \"Quantization for hardware efficiency\",\n                \"pruning\": \"Pruning techniques to optimize model size\"\n            },\n            \"description\": \"A DRL model designed for real-time, accurate, and interpretable renewable energy forecasting with sub-millisecond latency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Innovative interpretation methods to extract insights from model predictions\",\n        \"Adaptation to evolving data irregularities and sector trends\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Data normalization, shaping for input sequence\"],\n        \"target_device\": \"Edge devices with high-performance GPUs for real-time forecasting\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Quantized models for efficient deployment\"],\n        \"demonstration\": \"Design includes a seamless integration demonstration for renewable energy management system\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a detailed scope for a deep reinforcement learning model in the renewable energy domain, specifying performance targets, dataset composition, and requirements for model efficiency and interpretability. The 'expertise' field suggests a high level of technical understanding. The 'service' and 'confidence' fields are deduced based on the information provided, but some details like the deployment endpoint could be more specific.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Develop a specialized deep learning model for our restaurant's extensive image database, focusing on global fusion cuisine. The system must discern not only the conventional ingredients but also distinguish rare, locally sourced, and seasonal components, with a stringent requirement of <1% misclassification rate. Incorporate real-time adaptive technology to accommodate rapid menu transformations, ensuring prompt updates for dietary restrictions and seasonal ingredient availability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"restaurant\",\n        \"description\": \"Create a specialized deep learning model for a restaurant's image database, focusing on global fusion cuisine. The model should accurately identify both conventional and rare, locally sourced, and seasonal ingredients with a strict misclassification rate below 1%. Real-time adaptive technology is necessary for accommodating menu transformations and dietary restrictions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"misclassification rate\",\n                \"value\": 0.01\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Restaurant Image Database (Global Fusion Cuisine)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"ingredients\"],\n            \"specification\": {\n                \"image dimensions\": \"variable\",\n                \"recipe categories\": [\"conventional\", \"rare\", \"locally sourced\", \"seasonal\"]\n            },\n            \"description\": \"A comprehensive image database containing fusion cuisine dishes with diverse ingredients, including rare and seasonal components.\",\n            \"preprocessing\": [\n                \"image resizing\",\n                \"normalization\"\n            ],\n            \"augmentation\": [\n                \"random rotations\",\n                \"flipping\",\n                \"color jitter\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Fusion Cuisine Classifier\",\n            \"family\": \"convolutional neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"global feature fusion, possibly using attention mechanisms\"\n            },\n            \"description\": \"A specialized deep learning model designed to classify global fusion cuisine images, distinguishing conventional and unique ingredients with a focus on real-time adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptation to real-time menu changes using techniques like transfer learning and finetuning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Incremental learning\",\n            \"Online model updating\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"RESTful API or custom web portal\",\n        \"inference_engine\": [\n            \"GPU-based for optimal performance\",\n            \"Lightweight for real-time updates\"\n        ],\n        \"demonstration\": \"Interactive ingredient identification feature in the restaurant's mobile app or website.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instructions are detailed, specifying the area of computer vision, the target task of image classification, and high expertise level of the user. Performance metrics, dataset attributes, and specific model requirements are included. Some assumptions have been made (e.g., real-time adaptive technology and deployment endpoint), but they are reasonable based on the context. The confidence score is high given the clarity of the instructions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "The recently appointed task for the pioneering AI research and development unit is to enhance a cutting-edge emotion-sensitive neural machine translation (NMT) framework, specifically tailored for translating intricate medical jargon from obscure domain collections in biochemistry and pharmacology. The system must exceed a stringent benchmark of 99.5% conformance with the original text while accounting for nuanced context in rare medicinal discourse from a diverse range of international medical archives. This project mandates the creation of a groundbreaking evaluation framework that measures not only translation fidelity but also the system's capacity to preserve the precision and clinical significance of complex terminologies in real-world scenarios involving multicultural patient care. Additionally, the output must adhere to the latest EHR interoperability standards to ensure seamless medical information exchange among global healthcare professionals.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"medical\",\n        \"description\": \"The task is to enhance a state-of-the-art emotion-sensitive neural machine translation (NMT) framework, focusing on translating intricate medical jargon from biochemistry and pharmacology domains. The system must achieve at least 99.5% conformance with the original text and account for nuanced context in rare medicinal discourse. The evaluation should also consider preserving precision and clinical significance in diverse, real-world patient care scenarios, adhering to EHR interoperability standards.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"conformance\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": [\"biochemistry\", \"pharmacology\"],\n                \"rare_medical_discourse\": true,\n                \"international_sources\": true\n            },\n            \"description\": \"A dataset of obscure domain collections in biochemistry and pharmacology, containing intricate medical jargon and multicultural archive examples.\",\n            \"preprocessing\": [\"emotion-sensitive text processing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"emotion-sensitive NMT\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural machine translation\",\n            \"specification\": {\n                \"evaluation_framework\": \"context-preservation and clinical significance\",\n                \"interoperability\": \"EHR standards\"\n            },\n            \"description\": \"A cutting-edge NMT framework tailored for translating complex medical terminology from biochemistry and pharmacology domains.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The project requires not only high translation accuracy but also the preservation of contextual precision and clinical importance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual translation\", \"cultural adaptation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"specific to showcase translation examples and adherence to EHR standards\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The information was parsed based on the given complex and specific task requirements. High expertise indicates a deeper understanding of the problem. However, the exact model name and some deployment details are missing; they could be inferred if not directly provided.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "For the elite AI development team, design a next-generation text classifier that not only discerns between intricate human and AI-generated content but also differentiates between diverse subcategories of AI outputs (e.g., chatbots, GPT models). Enhance the project with novelty detection, continuous adversarial training, and real-time drift analysis. Aim for exceptional F1 scores, while maintaining explainability, fairness, and rigorous comparison with state-of-the-art methods, all within a strict latency constraint of 100 milliseconds per prediction.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI development and content authentication\",\n        \"description\": \"Design a next-generation text classifier capable of distinguishing between human and AI-generated content, including subcategories like chatbots and GPT models. Incorporate novelty detection, continuous adversarial training, and real-time drift analysis. Aim for high F1 scores, with a focus on explainability, fairness, and state-of-the-art comparisons, while adhering to a strict latency constraint of 100 milliseconds per prediction.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A dataset for text classification and feature extraction from human and AI-generated content, including diverse AI outputs.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"next-generation text classifier\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": true,\n                \"fairness\": true,\n                \"state_of_the_art_comparison\": true\n            },\n            \"description\": \"A model that enhances text classification capabilities with novelty detection, adversarial training, and real-time drift analysis while maintaining high F1 scores and a latency of 100 milliseconds.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Novelty detection, adversarial training, and real-time drift analysis techniques will be employed.\"\n        },\n        {\n            \"content\": \"Focus on explainability, fairness, and state-of-the-art comparisons to ensure model performance and ethical practices.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"novelty detection\",\n            \"adversarial training\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time streaming\"\n        ],\n        \"demonstration\": \"Explainable predictions with minimal latency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction specifies a complex NLP project with high-level requirements for the text classifier. The high expertise level suggests a deep understanding, and the mention of strict latency constraint indicates a clear understanding of the application's requirements. However, the dataset name, model specification, and deployment details are assumed based on the high-level details given.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a sophisticated traffic flow analytics project, you are shepherding the team in the creation of an advanced deep learning system that masterfully handles complex, noisy, and irregular traffic data. The Traffic dataset, with distinct partitions - a historic (96-step patterns, INPUT_SEQ_LEN=96, INPUT_DIM=862), validation, and undisclosed test sets, necessitates the development of a multi-layered architecture. This architecture should not only forecast the next 96 steps (PRED_SEQ_LEN=96, PRED_DIM=862) but also decipher nuanced seasonal patterns and swiftly adapt to unexpected events with resilience. The team must design models that balance performance optimization for both mean squared error (MSE) and mean absolute error (MAE), targeting state-of-the-art results across various prediction horizons while maintaining resource efficiency and real-time responsiveness. Demand a detailed, step-by-step strategy outlining data preprocessing techniques, feature engineering, architecture design decisions, regularization methods, and a rigorous validation process that incorporates cross-validation and adaptability validation for unforeseen scenarios. The final report should include a comprehensive evaluation of model robustness and explainability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"traffic flow analytics\",\n        \"description\": \"The project requires an advanced deep learning system for sophisticated traffic flow analytics. The Traffic dataset consists of historic, validation, and undisclosed test sets with 96-step patterns (INPUT_SEQ_LEN=96, INPUT_DIM=862), needing a multi-layered architecture. The model must forecast the next 96 steps (PRED_SEQ_LEN=96, PRED_DIM=862), detect seasonal patterns, and adapt to unexpected events. The aim is to optimize for MSE and MAE, targeting state-of-the-art results and resource efficiency while maintaining real-time responsiveness. A detailed strategy is needed for preprocessing, feature engineering, architecture design, regularization, cross-validation, and adaptability validation, along with model robustness and explainability assessment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Traffic\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 862,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 862\n            },\n            \"description\": \"The Traffic dataset for traffic flow analysis, divided into historic, validation, and test sets with complex data patterns.\",\n            \"preprocessing\": [\n                \"noise reduction\",\n                \"outlier detection\",\n                \"imputation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning (multi-layered architecture)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"layers\": [\"recurrent\", \"convolutional\", \"attention mechanisms\"],\n                \"regularization techniques\": [\"dropout\", \"LSTM cells\"],\n                \"adaptability\": \"incorporates adaptive learning\"\n            },\n            \"description\": \"Advanced architecture targeting high performance, robustness, and explainability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"smoothing\",\n            \"feature extraction (seasonality, trend)\",\n            \"feature selection\"\n        ],\n        \"target_device\": \"cloud or edge\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow serving\", \"real-time streaming\"],\n        \"demonstration\": \"robustness and explainability demos\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed based on the given instruction. The user's intent is clearly a 'build' project, and their expertise is assumed to be high due to the complexity of the task. The description, requirements, and metrics have been extracted and represented in the JSON format. However, specific model names, deployment endpoint, and inference engine details were not provided and are left as placeholders. Confidence is relatively high, but with some ambiguity related to missing explicit details.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for a cutting-edge time-series forecasting AI initiative, your team is now assigned to revolutionize the field by developing a sophisticated, hybrid deep learning model. Using the Electricity dataset, which has been meticulously divided into train (70%), validation (15%), and test (15%) sets with high resolution, ensure each input sequence is a lengthy 96 steps (INPUT_SEQ_LEN=321) and possesses 321-dimensional observations. Your challenge is to design a state-of-the-art model that forecasts a massive 321-step ahead (PRED_SEQ_LEN=321) with unparalleled precision, while demonstrating adaptability to complex seasonality, resistance to outliers, and preserving interpretability. Moreover, your model must excel in concurrent metrics like Mean Absolute Percentage Error (MAPE) and Symmetric Mean Absolute Percentage Error (SMAPE), while exceeding contemporary benchmarks and providing actionable insights into the driving patterns within the data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"electricity forecasting\",\n        \"description\": \"Develop a sophisticated, hybrid deep learning model for the time-series forecasting initiative with the Electricity dataset. The dataset is divided into train, validation, and test sets (70%, 15%, 15%) with high resolution, with 96-step (INPUT_SEQ_LEN=321) inputs and 321-dimensional observations. The target is to forecast 321 steps ahead (PRED_SEQ_LEN=321) while demonstrating adaptability to seasonality, resistance to outliers, and interpretability. The model must excel in metrics like MAPE and SMAPE and surpass benchmarks, providing actionable insights into driving patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Absolute Percentage Error (MAPE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Symmetric Mean Absolute Percentage Error (SMAPE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 321,\n                \"INPUT_DIM\": 321,\n                \"PRED_SEQ_LEN\": 321\n            },\n            \"description\": \"The Electricity dataset divided into train, validation, and test sets (70%, 15%, 15%) with high resolution. Each input sequence is 96 steps long and has 321-dimensional observations.\",\n            \"preprocessing\": [\n                \"Resampling to high resolution\",\n                \"Normalizing features\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Visual inspection of seasonality and outliers\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Deep Learning Model\",\n            \"family\": \"sophisticated deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Custom hybrid model combining deep learning (e.g., RNN or LSTM) with interpretability techniques\"\n            },\n            \"description\": \"A state-of-the-art model designed for electricity forecasting, incorporating adaptability, resistance to outliers, and interpretability while targeting high performance in MAPE and SMAPE metrics.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"Feature engineering for complex seasonality and outlier handling\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Apache MXNet\"],\n        \"demonstration\": \"Interactive dashboard showcasing model performance and data patterns\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes all relevant information from the given instruction, with the user's high expertise level indicated. The model's specifications, dataset details, and performance metrics are well-defined. However, specific values for performance metrics are not provided as they depend on model performance, which is not detailed in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the high-resolution satellite land classification project, mandate a comprehensive evaluation of cutting-edge deep learning models, which must demonstrate exceptional real-time adaptability, nuanced differentiation between complex classes (precisely defining coastal wetlands, various forest types, sparse shrublands, extensive urban areas, and arid terrain), and a balance between computational efficiency and robustness. Strive for a rapid iterative development process that guarantees both pixel-level accuracy and efficiency, while adhering to strict latency constraints in a dynamically evolving satellite data landscape.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"remote sensing, land classification\",\n        \"description\": \"As a project manager, mandate a comprehensive evaluation of cutting-edge deep learning models for high-resolution satellite land classification. The models should exhibit exceptional real-time adaptability, nuanced differentiation between complex classes such as coastal wetlands, forest types, sparse shrublands, urban areas, and arid terrain. Focus on a balance between computational efficiency and robustness, while ensuring rapid iterative development with pixel-level accuracy and adherence to strict latency constraints in a constantly changing satellite data landscape.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"pixel-level accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time adaptability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency constraints\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"high-resolution satellite imagery\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"land classes: coastal wetlands, forest types, sparse shrublands, urban areas, arid terrain\"],\n            \"specification\": {\n                \"image resolution\": null,\n                \"data size\": null,\n                \"dynamic nature\": \"evolving\"\n            },\n            \"description\": \"A collection of high-resolution satellite images for land classification, accounting for the dynamically changing nature.\",\n            \"preprocessing\": [\"adaptation for real-time processing\"],\n            \"augmentation\": [\"data augmentation techniques for robustness\"],\n            \"visualization\": [\"pixel-level accuracy visualization\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"advanced deep learning models\",\n            \"family\": \"cutting-edge\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptability\": null,\n                \"real-time inference\": null,\n                \"complex_class Differentiation\": null\n            },\n            \"description\": \"Investigation of deep learning models demonstrating exceptional real-time adaptability and high-resolution classification performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"efficient and adaptive feature extraction\"],\n        \"target_device\": \"real-time processing and edge computing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for latency\"],\n        \"demonstration\": \"iterative process with pixel-level accuracy and efficiency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction clearly indicates a building task, with high expertise expected from the user. The problem details, including the areas of focus and the requirement for pixel-level accuracy, demonstrate a specific task. However, some performance and complexity metrics require context or estimation, like efficiency, robustness, and latency. The model family is specified as 'advanced deep learning models,' but individual models need more detailed specifications for a complete response.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "image classification", "instruction": "As a seasoned project manager for a cutting-edge AI team, your objective is to design an enhanced image recognition system for the Stanford Cars Dataset using Inception-v3. The model should classify not only primary car attributes but also pinpoint submodels and variants, with a focus on recognizing vehicles in diverse weather conditions, low-resolution images, and under challenging lighting. In addition to the standard evaluation metrics, you must achieve a minimum per-class F1 score of 98% and maintain real-time performance, while conducting a comprehensive study on the impact of fine-tuning and progressive transfer learning strategies on accuracy and efficiency, without sacrificing speed. Provide a detailed performance report showcasing the trade-offs between efficiency and accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"automotive\",\n        \"description\": \"Design an enhanced image recognition system for the Stanford Cars Dataset using Inception-v3. The system should classify primary car attributes, submodels, and variants, focusing on diverse weather conditions, low-resolution images, and challenging lighting. It must achieve at least 98% per-class F1 score and run in real-time.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"per-class F1 score\",\n                \"value\": 0.98,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"primary car attributes\", \"submodels\", \"variants\"],\n            \"specification\": {\n                \"additional_details\": {\n                    \"weather conditions\": [\"diverse\", \"low-resolution\", \"challenging lighting\"]\n                }\n            },\n            \"description\": \"A dataset for advanced image recognition with a focus on diverse conditions and model variety.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"fine-tuning\", \"progressive transfer learning\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Inception-v3\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time performance\": true,\n                \"study_topics\": [\"fine-tuning\", \"progressive transfer learning\"]\n            },\n            \"description\": \"Enhanced Inception-v3 model for improved recognition in varied conditions and diverse vehicles.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The system should consider the trade-offs between accuracy and efficiency in the context of fine-tuning and progressive transfer learning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"performance report showcasing trade-offs between efficiency and accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON covers the user's objective to build an advanced image recognition system, with a high expertise level. The problem definition includes the specific dataset, advanced requirements, and the planned study on transfer learning strategies. The model and service section captures the complexity requirements and real-time performance focus. However, the specific performance report is not included in the response as it would typically be created after model training and analysis.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Request a highly scalable and fine-tuned translation solution using the ParaCrawl dataset, combining the power of a novel tri-encoder architecture for English-to-Dutch translations. Emphasize on maintaining not only syntactic fidelity but also cultural context, collocation preservation, and a nuanced understanding of regional dialects. Moreover, incorporate real-time sentiment analysis with sentiment transfer, ensuring the emotional congruence between original and translated content, while adhering to strict resource limitations and meeting tight project deadlines.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"translation\",\n        \"description\": \"Request a highly scalable and fine-tuned translation solution using the ParaCrawl dataset. Focus on English-to-Dutch translations with a novel tri-encoder architecture, emphasizing syntactic fidelity, cultural context, collocation preservation, and a nuanced understanding of regional dialects. Include real-time sentiment analysis with sentiment transfer to ensure emotional congruence between original and translated content. The solution should meet strict resource limitations and project deadlines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"syntactic fidelity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cultural context preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"collocation preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sentiment transfer accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"requests per second\"\n            },\n            {\n                \"name\": \"resource utilization\",\n                \"value\": null,\n                \"unit\": \"GPU memory\"\n            },\n            {\n                \"name\": \"deployment time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"project deadline adherence\",\n                \"value\": null,\n                \"unit\": \"percent\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language pair\": \"English-to-Dutch\",\n                \"architecture\": \"tri-encoder\"\n            },\n            \"description\": \"A large-scale dataset for English-to-Dutch translations with a focus on a novel tri-encoder architecture.\",\n            \"preprocessing\": [\"combined with cultural context\", \"collocation filtering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"novel tri-encoder architecture\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"focus on sentiment-aware translation\"\n            },\n            \"description\": \"A fine-tuned tri-encoder model for English-to-Dutch translations with sentiment analysis and sentiment transfer capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"importance of maintaining cultural context and regional dialects\",\n        \"sentiment transfer for emotional congruence\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time sentiment analysis\", \"sentiment transfer\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource constraints\"],\n        \"demonstration\": \"focus on maintaining emotional and contextual consistency in translated content\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the provided information. The user's high expertise suggests they might need less guidance, and we've inferred some performance metrics that are key for this translation task. The ParaCrawl dataset link and project resource and deadline constraints are assumed as they were not directly specified. The confidence score could be lower due to the lack of specific resource usage details.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "image classification", "instruction": "In your capacity as a project manager of a cutting-edge AI research team, your multifaceted task is to shepherd the development of a highly specialized deep learning pipeline. The system, based on the advanced Swin Transformer, must not only achieve state-of-the-art performance in recognizing a myriad of celestial objects from the challenging and diverse Galaxy Zoo dataset but also surpass a benchmark of 99% accuracy on an unseen and noise-polluted validation set. The architecture should exhibit exceptional fine-tuning potential, noise resilience, and adaptability to rapidly evolving astronomical data. Furthermore, it should incorporate innovative techniques to handle cluttered backgrounds and resolve ambiguities in classification. Your project's success will pave the way for transformative discoveries while maintaining a tight deadline of 6 months to ensure timely impact in the scientific community.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"As a project manager, you're tasked with developing a specialized deep learning pipeline using the Swin Transformer, targeting state-of-the-art celestial object recognition in the Galaxy Zoo dataset. The system must achieve at least 99% accuracy on a noisy validation set, exhibit fine-tuning potential, noise resilience, and adaptability to evolving data, and handle cluttered backgrounds and classification ambiguities.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"is_challenging_and_diverse\": true,\n                \"noise_polluted\": true\n            },\n            \"description\": \"A comprehensive dataset for detecting celestial objects in a complex, challenging environment with a focus on noise resilience.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Swin Transformer-based System\",\n            \"family\": \"Swin Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"state_of_the_art_performance\": true,\n                \"fine_tuning_potential\": true\n            },\n            \"description\": \"A highly specialized deep learning pipeline with Swin Transformer architecture designed for astronomical object recognition, focused on noise resilience and adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"handle cluttered backgrounds and classification ambiguities, real-time adaptation to new astronomical data, and need for fine-tuning potential\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"noise reduction techniques\", \"adaptation algorithms\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"astronomical research platform\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"Areal-time demonstration of the pipeline's performance on the latest astronomical data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project's requirements, including the advanced Swin Transformer, performance benchmark, and project constraints. However, some assumptions about the need for noise reduction techniques and real-time adaptation were made as those were not explicitly mentioned in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a project director for the enhanced, immersive Bilibili Global Comments Platform, design a cutting-edge recommendation engine that fuses Bert4Rec, sentiment analysis from multiple languages, and topic modeling on a high-velocity, user-generated content stream. The system must exhibit deep contextual understanding across cultural nuances, deliver tailored video suggestions with real-time AI refinement, and ensure ultra-low latency for user satisfaction, considering varying demographics and optimizing for scalability in a dynamically evolving platform.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"social media\",\n        \"description\": \"Design a cutting-edge recommendation engine for the enhanced Bilibili Global Comments Platform. The system should fuse Bert4Rec, perform sentiment analysis in multiple languages, and incorporate topic modeling on high-velocity user-generated content. It should exhibit deep contextual understanding across cultural nuances, provide real-time, tailored video suggestions, and ensure ultra-low latency for high user satisfaction, while accounting for diverse demographics and scalability in a dynamically evolving platform.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\"unit\": \"milliseconds\", \"value\": \"ultra-low\"}\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\"unit\": \"milliseconds\", \"value\": \"optimized\"},\n                \"reason\": \"For ultra-low latency requirements.\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": {\"unit\": \"MB\", \"value\": \"optimized for scalability\"},\n                \"reason\": \"To ensure system can handle dynamically evolving content volume.\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Bilibili Global Comments Stream\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"velocity\": \"high\",\n                \"language_diversity\": \"true\"\n            },\n            \"description\": \"User-generated content stream on Bilibili with high velocity and multiple language support.\",\n            \"preprocessing\": [\"BERT4Rec-specific tokenization\", \"Multilingual sentiment analysis tokenization\", \"Topic modeling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Bert4Rec (enhanced)\",\n            \"family\": \"neural networks\",\n            \"type\": \"recommendation models\",\n            \"specification\": {\n                \"model_fusion\": [\"BERT\", \"Topic modeling\"],\n                \"language_capabilities\": \"multilingual\",\n                \"contextual Understanding\": \"deep\"\n            },\n            \"description\": \"A recommendation engine integrating Bert4Rec with sentiment analysis and topic modeling on the user-generated content stream.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cultural nuances are critical to consider for effective recommendations\",\n        \"Real-time AI refinement for continuous improvement\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Dynamic context adaptation\",\n            \"Personalization based on demographic shifts\"\n        ],\n        \"target_device\": \"real-time\",\n        \"deployment_endpoint\": \"dynamic API for real-time video suggestions\",\n        \"inference_engine\": [\"optimized for high-performance computing clusters\"],\n        \"demonstration\": \"Live A/B testing, user feedback incorporation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction describes a complex and specialized ML project. The high expertise level indicates the user has a deep understanding of the project requirements. Performance metrics are not provided, but the intent to 'build' suggests that they will be included during the model development process. It also specifies key features like contextual understanding, low latency, and scalability. The missing 'accuracy' and 'precision' can be assumed to be sought for sequential recommendations.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project manager for an elite machine learning research team, you've been presented with a groundbreaking challenge: to develop a zero-emission smart grid management system using cutting-edge, explainable AI. Utilize the recently acquired Oceanographic Dynamics dataset, characterized by its extensive, ultra-high-resolution temporal and spatial data with unpredictable oceanic patterns and extreme weather events. The dataset comprises complex sequences with a unique cryptographic pattern (INPUT_SEQ_LEN=512, INPUT_DIM=7,000) that necessitates the implementation of novel, secure forecasting algorithms combining RNN-XOR, quantum-inspired neural networks, and hybrid edge-cloud computing. The task is to design a distributed forecasting model that forecasts not only the next 512-time steps for marine energy production (PRED_SEQ_LEN=512, PRED_DIM=7,000) but also optimizes the energy flow for variable renewable sources and predicts potential disruptions to minimize grid instability. To guarantee reliability and resilience, evaluate the model's performance using Wasserstein distance and asymmetric uncertainty metrics, prioritizing both precision and adaptability under extreme conditions. Additionally, present a pedagogical explanation of your model's decision-making process, discuss its eco-friendly impact, and propose practical applications for sustainable marine resource management.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart grid management, marine energy production\",\n        \"description\": \"Develop a zero-emission smart grid management system using explainable AI. The task is to create a distributed forecasting model for marine energy production based on the Oceanographic Dynamics dataset. The model should combine RNN-XOR, quantum-inspired neural networks, and employ hybrid edge-cloud computing to handle complex data with an unpredictable cryptographic pattern (INPUT_SEQ_LEN=512, INPUT_DIM=7000). Performance must be evaluated using Wasserstein distance and asymmetric uncertainty metrics, prioritizing precision and adaptability under extreme weather events.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Wasserstein distance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Asymmetric uncertainty metrics\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time on edge devices\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"Memory footprint for secure algorithms\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Oceanographic Dynamics\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"marine energy production\", \"environmental disruptions\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 512,\n                \"INPUT_DIM\": 7000,\n                \"PRED_SEQ_LEN\": 512,\n                \"PRED_DIM\": 7000\n            },\n            \"description\": \"A dataset with ultra-high-resolution, complex sequences characterized by unpredictable oceanic patterns and extreme weather events.\",\n            \"preprocessing\": [\"novel cryptographic pattern decryption\"],\n            \"augmentation\": [\"RNN-XOR, quantum-inspired NN\"],\n            \"visualization\": [\"model decision-making process\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Distributed forecasting model\",\n            \"family\": \"RNN-XOR, quantum-inspired neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"edge-cloud computing\": true,\n                \"algorithm_combination\": \"RNN-XOR + quantum-inspired NN\"\n            },\n            \"description\": \"A model that forecasts marine energy production for 512-time steps, optimizes energy flow for variable renewables, and predicts disruptions with explainable AI for smart grid management.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Model decisions are made using combined RNN-XOR and quantum-inspired neural networks, considering extreme conditions and cryptographic patterns.\"\n        },\n        {\n            \"description\": \"The model's eco-friendly impact: reduces emissions and improves renewable energy management, contributing to sustainable marine resource management.\"\n        },\n        {\n            \"description\": \"Practical applications: supports real-time decision-making in marine energy production, forecasting, and grid stability.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"hybrid edge-cloud setup\"],\n        \"target_device\": \"hybrid edge-cloud infrastructure\",\n        \"deployment_endpoint\": \"dedicated sustainable energy platform\",\n        \"inference_engine\": [\"secure, quantum-inspired algorithms\"],\n        \"demonstration\": \"pedagogical explanation for decision-making process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers all necessary information from the instruction, indicating a high-level requirement. Performance metrics and complexity metrics are specified, and the model's unique combination and adaptability are highlighted. However, some metric values are left unspecified, as the instruction does not provide exact figures.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a proficient project manager for the top-tier deep-tech R&D team, your task is to develop a cutting-edge energy demand forecasting model for the volatile smart-grids market. Using the 'Advanced Electricity Insights' dataset, which is divided into specialized subsets: past, present validation, and futuristic testing samples, design a hybrid LSTM-CNN model with specialized seasonality detection module. The model must process intricate 1,536-hour sequences (EXTENDED_INPUT_SEQ_LEN=1,536, EXTENDED_INPUT_DIM=321), accounting for anomalies and demand peaks, by forecasting the next 1,536 hours with high precision. Strive for breakthrough performance by minimizing normalized root mean squared error (NRMSE) and mean absolute percentage error (MAPE) while demonstrating adaptability across diverse geographies. Conduct exhaustive hyperparameter fine-tuning, incorporating ensemble techniques, and compare against innovative techniques from top academic conferences. Prepare a comprehensive, visually-rich report showcasing breakthrough results and industry implications.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart-grids energy demand\",\n        \"description\": \"Develop a cutting-edge energy demand forecasting model for the volatile smart-grids market using the 'Advanced Electricity Insights' dataset. The model should be a hybrid LSTM-CNN with a seasonality detection module, able to process 1,536-hour sequences with an EXTENDED_INPUT_SEQ_LEN of 1,536 and EXTENDED_INPUT_DIM of 321. The model must account for anomalies and demand peaks, targeting high precision. Performance metrics include NRMSE and MAPE minimization, with adaptability across diverse geographies. Hyperparameter tuning, ensemble techniques, and comparison with academic conference innovations are required. A comprehensive report with visualizations and industry implications is expected.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"NRMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model adaptability\",\n                \"value\": null,\n                \"unit\": \"geographies\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Advanced Electricity Insights\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"EXTENDED_INPUT_SEQ_LEN\": 1536,\n                \"EXTENDED_INPUT_DIM\": 321\n            },\n            \"description\": \"A dataset divided into past, validation, and testing subsets for energy demand forecasting. Includes specialized subsets for seasonality detection and diverse geographies.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"hybrid LSTM-CNN\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A model with LSTM and CNN components for energy demand forecasting, including a seasonality detection module and adaptability to diverse data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"incorporates ensemble techniques\",\n        \"compares with top academic conference innovations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"hyperparameter fine-tuning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"comprehensive, visually-rich report\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the high-level requirements for a complex ML project with a specific task, dataset, and performance indicators. The user's expertise level is assumed to be high, and the missing values in 'target_device', 'deployment_endpoint', and 'inference_engine' are assumed to be unclear from the instruction. However, the absence of ensemble comparison specifics and the presentation requirements for demonstration adds room for improvement.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for the cutting-edge AI team, commission the design of a highly optimized CNN architecture, integrating advanced transfer learning techniques, for discriminating between 100 nuanced subcategories within CIFAR-100. Demand not only a peak accuracy of 99%, but also necessitate the implementation of state-of-the-art data augmentation strategies to foster robustness and outperform benchmarks, ensuring scalability for future enhancements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"image recognition\",\n        \"description\": \"As a project manager, commission the design of a highly optimized CNN architecture that integrates advanced transfer learning for 100 nuanced subcategories in CIFAR-100 dataset. Aim for peak accuracy of 99% and mandate the implementation of state-of-the-art data augmentation strategies for robustness and benchmark超越. Ensure scalability for future enhancements.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CIFAR-100\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_classes\": 100,\n                \"subcategories\": 100\n            },\n            \"description\": \"A dataset for image classification with 100 nuanced subcategories, used for developing an optimized CNN architecture with advanced transfer learning.\",\n            \"preprocessing\": [],\n            \"augmentation\": {\n                \"type\": \"state-of-the-art\"\n            },\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": true,\n                \"number_of_layers\": null,\n                \"architecture_details\": \"\"\n            },\n            \"description\": \"An optimized CNN architecture using transfer learning techniques for CIFAR-100 with 99% accuracy target and advanced data augmentation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"future enhancements and scalability demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a highly optimized CNN for image classification. The expert level of expertise indicates the user expects detailed design and advanced techniques. The performance metric for accuracy, 99%, is clearly defined, and the request for state-of-the-art data augmentation highlights the need for robustness and benchmark超越. The scalability for future enhancements is noted.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a visionary project manager for a cutting-edge machine learning team, you've been commissioned to develop a groundbreaking algorithm that not only detects and classifies diabetic retinopathy into five subcategories with nanoscale precision but also predicts potential disease progression. The team must enhance the state-of-the-art Transformers architecture with the addition of Explainable AI (XAI) layers, enabling clinicians to understand the reasoning behind diagnoses. The model must achieve a groundbreaking accuracy of 99.5% while maintaining an F1-score of at least 0.98, minimizing false negatives and life-altering misdiagnoses in remote and resource-limited settings. Furthermore, devise a distributed deployment strategy using quantum-inspired techniques for ultra-fast and energy-efficient inference in real-time, without any compromise on accuracy. Prepare a comprehensive research report, PoC implementation, and a comparative analysis with traditional machine learning models to showcase the groundbreaking nature and practical significance of your solution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medicine, healthcare\",\n        \"description\": \"Develop a groundbreaking algorithm to detect and classify diabetic retinopathy into five subcategories with nanoscale precision and predict potential disease progression. Enhance Transformers architecture with Explainable AI (XAI) layers. Aim for at least 99.5% accuracy and 0.98 F1-score, minimizing false negatives in remote settings. Implement a distributed deployment strategy using quantum-inspired techniques for real-time, ultra-fast, and energy-efficient inference.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.995\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"Real-time with quantum-inspired techniques\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"Quantum-inspired for ultra-fast and energy-efficient\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Diabetic Retinopathy dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"Diabetic Retinopathy severity, disease progression\"],\n            \"specification\": {\n                \"image_size\": null,\n                \"label_categories\": 5,\n                \"remote_data_distribution\": true\n            },\n            \"description\": \"Dataset for detecting and classifying diabetic retinopathy with potential progression\",\n            \"preprocessing\": [\"XAI-ready image preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Algorithmic reasoning visualizations\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformers with XAI layers\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art, XAI integrated\",\n                \"parameters\": null,\n                \"F1_score_target\": 0.98,\n                \"nanoscale_precision\": true\n            },\n            \"description\": \"Enhanced with Explainable AI for reasoning transparency\"\n        }\n    ],\n    \"knowledge\": [\n        \"Quantum-inspired deployment strategies for real-time and energy efficiency\",\n        \"Explainable AI for clinician understanding of diagnoses\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Quantum-inspired, optimized feature extraction\"\n        ],\n        \"target_device\": \"Edge devices in remote healthcare settings\",\n        \"deployment_endpoint\": \"Quantum-based, distributed inference platform\",\n        \"inference_engine\": [\"Quantum-inspired AI engines\"],\n        \"demonstration\": \"Demonstrations on limited-resource settings\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction clearly indicates a complex machine learning project with specific requirements such as accuracy targets and XAI layers. The need for distributed deployment with quantum-inspired methods highlights the innovative nature. High expertise level implies a need for detailed guidance.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Require the ML team to design an innovative image classifier, specifically targeting high-end fashion attire on the Paris Fashion-MNIST dataset. They must employ the most recent Dynamic Routing Capsule Networks with a stringent condition of achieving a minimum accuracy of 98% in recognizing haute couture designs, even under extreme photorealistic distortions like affine transformations, light occlusions, and color shift. Additionally, mandate a multi-step research comparing their model to top-performing Convolutional Neural Networks, including a detailed analysis of not just accuracy but also computational efficiency, memory footprint, and hardware compatibility. Prioritize energy conservation and memory optimization for successful large-scale implementation in resource-constrained, wearable and IoT devices with limited computational power.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"high-end fashion attire\",\n        \"description\": \"Design an innovative image classifier targeting high-end fashion attire on the Paris Fashion-MNIST dataset, specifically utilizing Dynamic Routing Capsule Networks. The model must achieve at least 98% accuracy in recognizing haute couture designs despite extreme photorealistic distortions like affine transformations, light occlusions, and color shift.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null, \n                \"unit\": \"operations per second\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null, \n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"hardware compatibility\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Paris Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"haute couture class\"],\n            \"specification\": {\n                \"distortions\": [\n                    \"affine transformations\", \n                    \"light occlusions\", \n                    \"color shift\"\n                ]\n            },\n            \"description\": \"A dataset for high-end fashion attire classification, focusing on recognizing haute couture under challenging conditions.\",\n            \"preprocessing\": [\"data augmentation for photorealistic distortions\"],\n            \"augmentation\": [\n                \"affine transformations\",\n                \"light occlusions\",\n                \"color shift\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dynamic Routing Capsule Networks\",\n            \"family\": \"Capsule Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A state-of-the-art model for image classification, specifically tailored to handle extreme photorealistic distortions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Energy conservation and memory optimization are crucial for large-scale implementation in resource-constrained devices.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"energy-efficient\", \"memory-optimized\"],\n        \"target_device\": [\"wearable\", \"IoT devices\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for limited computational power\"\n        ],\n        \"demonstration\": \"including comparison with top-performing Convolutional Neural Networks\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON captures the details of the task, including the target dataset, model requirements, and performance metrics. The user's high level of expertise suggests they may expect detailed guidance and assistance in the process. The missing values in complexity metrics and feature engineering steps denote they would need to be filled based on the team's analysis.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, develop a highly specialized medical Q&A system for accurate and time-sensitive disease diagnosis. Utilize the comprehensive BioASQ dataset, demanding the BioBERT model to undergo domain-specific adaptation with interpretable outputs. Mandate not only consistent accuracy improvements but also real-time updates and active monitoring of-source reliability, ensuring gold-standard information for each iteration, adhering to JAMA publication guidelines.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medical\",\n        \"description\": \"As a project manager, develop a specialized medical Q&A system with BioBERT for disease diagnosis, focusing on domain-specific adaptation for interpretable outputs. The system should prioritize accuracy improvements, real-time updates, and active monitoring of source reliability, meeting JAMA publication guidelines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"disease entities\", \"diagnosis\"],\n            \"specification\": {\n                \"data_format\": \"domain-specific for medical\",\n                \"examples_count\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                }\n            },\n            \"description\": \"A comprehensive medical dataset for BioBERT adaptation and disease diagnosis tasks\",\n            \"preprocessing\": [\n                \"domain adaptation\",\n                \"label extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"model interpretability\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BioBERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"domain_adaptation\": true,\n                \"interpretable_outputs\": true\n            },\n            \"description\": \"A BioBERT model designed for domain-specific adaptation in the medical Q&A system\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Adherence to JAMA publication guidelines and gold-standard information per iteration.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"real-time updates\",\n            \"active monitoring\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"API for disease diagnosis\",\n        \"inference_engine\": [\n            \"optimized for medical Q&A\"\n        ],\n        \"demonstration\": \"Interactive medical Q&A with explanations for diagnosis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction was parsed into a JSON response, reflecting the user's requirements for a specialized medical Q&A system using BioBERT for disease diagnosis. High expertise level was assumed. The performance metrics were left unspecified for the accuracy improvement target. The BioASQ dataset is linked via a URL, and the system requirements, like real-time updates and adherence to JAMA guidelines, were captured. However, some missing details in the original instruction (like dataset specification and specific accuracy value) are likely to require further clarification.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for a luxury automaker's AI division, design a state-of-the-art parking lot monitoring system using real-time, edge-computing technology. Compare the COCO and Pascal VOC datasets, focusing on their subsets for commercial vehicle subclasses, annotation fidelity for complex logo recognition, and adaptability to low-resolution imagery captured by 4K drone-mounted cameras. Investigate cutting-edge object detection models like YOLOv5-Nano, SSD-Lite, and a hybrid RetinaNet+EfficientDet for minimal latency under limited GPU resources. Analyze computational efficiency, accuracy trade-offs, and propose a multi-object detection solution with explainable AI for future firmware updates.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury automaker's AI division\",\n        \"description\": \"Design a state-of-the-art parking lot monitoring system using real-time edge computing. Compare COCO and Pascal VOC datasets for commercial vehicle subclasses, focusing on annotation fidelity for complex logo recognition and adaptability to low-resolution imagery from 4K drone cameras. Investigate YOLOv5-Nano, SSD-Lite, and a hybrid RetinaNet+EfficientDet for minimal latency under GPU resource constraints.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"GPU resource usage\",\n                \"value\": null,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"commercial vehicle subclasses\",\n                \"complex logo recognition\"\n            ],\n            \"specification\": {\n                \"subset\": \"commercial vehicle subclasses\",\n                \"image resolution\": \"low\"\n            },\n            \"description\": \"COCO dataset for comparing object detection performance on commercial vehicle subclasses, focusing on annotation fidelity for logo recognition in low-resolution imagery.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Pascal VOC\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"commercial vehicle subclasses\",\n                \"complex logo recognition\"\n            ],\n            \"specification\": {\n                \"subset\": \"commercial vehicle subclasses\",\n                \"image resolution\": \"low\"\n            },\n            \"description\": \"Pascal VOC dataset for similar analysis as COCO, focusing on commercial vehicle subclasses and logo recognition in low-resolution imagery.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5-Nano\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"latency\": null,\n                \"GPU resource usage\": null\n            },\n            \"description\": \"Investigating for minimal latency and resource efficiency in a state-of-the-art object detection model.\"\n        },\n        {\n            \"name\": \"SSD-Lite\",\n            \"family\": \"SSD\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"latency\": null,\n                \"GPU resource usage\": null\n            },\n            \"description\": \"Another cutting-edge model for object detection, suited for low-latency requirements.\"\n        },\n        {\n            \"name\": \"RetinaNet+EfficientDet hybrid\",\n            \"family\": [\n                \"RetinaNet\",\n                \"EfficientDet\"\n            ],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"latency\": null,\n                \"GPU resource usage\": null\n            },\n            \"description\": \"Exploring a hybrid model combining two architectures to optimize computational efficiency and accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Edge computing technology\",\n        \"Low-resolution imagery challenges\",\n        \"4K drone camera requirements\",\n        \"Explainable AI for firmware updates\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge computing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"explaining model decisions for future firmware updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction indicates a high-level project with a focus on AI in the automotive industry. The system comparison, datasets analysis, and specific model choices indicate a high expertise level. However, performance metrics are not specified for COCO, Pascal VOC, and individual models, leaving those open for further clarification. Also, the firmware update discussion suggests interest in explainable AI, but no exact requirements provided.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager, you are now directing a pioneering machine learning team to confront a multifaceted challenge involving multivariate, high-frequency electricity consumption forecasting. Utilizing the Electricity dataset, a complex and sprawling collection (with sequences of 96 time steps, 321 diverse features, and distributed as train, validation, and test sets), the objective is to design an innovative, modular forecasting system. The model must not only anticipate the subsequent 96-step sequence with precision (PRED_SEQ_LEN=96, PRED_DIM=321) but also integrate sophisticated seasonality adjustments, deep cross-impact analysis, and real-time external data assimilation. Strive for minimized mean squared error (MSE), mean absolute error (MAE), and model interpretability, all while optimizing for computational efficiency and demonstrating exceptional generalization across varying data distributions. Develop a methodologically rigorous, yet intuitive, cutting-edge solution to this intricate time-series forecasting dilemma, ensuring seamless scalability and adaptability to real-world variations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"electricity consumption\",\n        \"description\": \"A machine learning team needs to develop an innovative, modular forecasting system for multivariate, high-frequency electricity consumption. The dataset includes sequences of 96 time steps and 321 features. The model must predict future sequences (PRED_SEQ_LEN=96, PRED_DIM=321), incorporate seasonality adjustments, cross-impact analysis, and real-time external data integration. Performance goals include minimized MSE, MAE, interpretability, and optimized computational efficiency. The solution should demonstrate exceptional generalization and scalability to real-world variations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 321\n            },\n            \"description\": \"A complex dataset with sequences of 96 time steps and 321 diverse features, distributed as train, validation, and test sets for electricity consumption forecasting.\",\n            \"preprocessing\": [\n                \"seasonality adjustments\",\n                \"data standardization\"\n            ],\n            \"augmentation\": [\n                \"real-time external data assimilation\"\n            ],\n            \"visualization\": [\n                \"model interpretability visualization\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"modular and interpretable\"\n            },\n            \"description\": \"A cutting-edge, methodologically rigorous solution for high-frequency electricity consumption forecasting, incorporating seasonality, cross-impact analysis, and real-time data assimilation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integrate techniques for handling varying data distributions\",\n        \"Ensure scalability and adaptability to real-world variations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or server for optimal performance\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time forecasting\"],\n        \"demonstration\": \"Methodology and results explanation video or interactive demo\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction was thoroughly parsed, considering the high level of expertise. Details on the dataset and model requirements, as well as the performance and complexity metrics, are clearly defined. However, the specific model name and family, as well as the deployment endpoint and inference engine, are not provided and may require additional clarification.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "object detection", "instruction": "For a niche gourmet market, develop an advanced YOLOv5.1 model, specifically tailored for the \"BoutiqueFoods Shelf Insights Enhanced Dataset\". It must distinguish among 200 exclusive luxury product categories, maintaining a precision of over 97% and recall above 96% post-optimization. Additionally, the system must feature a highly efficient inventory management module with real-time synchronization, ensuring that every update for exquisite items is synced within milliseconds, max 100 milliseconds per captured high-definition shelf image, while adhering to strict energy efficiency guidelines (power consumption < 5 watts during operation).", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"gourmet products, inventory management\",\n        \"description\": \"Develop an advanced YOLOv5.1 model for the 'BoutiqueFoods Shelf Insights Enhanced Dataset', targeting 200 exclusive luxury product categories. The model must achieve precision above 97% and recall above 96% post-optimization, with a focus on real-time inventory management and high energy efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.96\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time inference\",\n                \"value\": 100,\n                \"unit\": \"milliseconds\",\n                \"max_value\": 100\n            },\n            {\n                \"name\": \"power consumption\",\n                \"value\": 5,\n                \"unit\": \"watts\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoutiqueFoods Shelf Insights Enhanced Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_categories\": 200,\n                \"data_quality\": \"high definition\"\n            },\n            \"description\": \"A dataset for gourmet products, focusing on luxury categories with enhanced shelf insights\",\n            \"preprocessing\": [\"data augmentation for niche categories\"],\n            \"augmentation\": [\"random cropping, resizing, and flipping\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"custom YOLOv5.1\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"YOLOv5.1\",\n                \"target_categories\": 200\n            },\n            \"description\": \"An advanced YOLOv5.1 model designed for boutique gourmet products with advanced inventory management features\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"real-time object recognition, inventory update mechanism\"],\n        \"target_device\": \"real-time synchronization (e.g., edge device)\",\n        \"deployment_endpoint\": \"custom API endpoint for boutique food market\",\n        \"inference_engine\": [\"optimized for low power consumption\"],\n        \"demonstration\": \"model performance demonstration with high-definition images\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON was constructed based on the provided requirements, with user expertise set to 'high' due to the need for advanced model development. The model and dataset details have been tailored to meet the constraints. However, the actual pre-trained model for the luxury product categories might not be present in the YOLOv5.1 library, so a custom model might require adaptation or fine-tuning. The energy efficiency and real-time synchronization requirements are clearly specified.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager, guide your AI team to design an intricate hybrid deep learning model for the ETTm2 dataset, which demands not only sophisticated autoregressive abilities but also hybrid attention mechanisms. The data, containing train, validation, and test sets with INPUT_SEQ_LEN (96) and diverse INPUT_DIM (7) features, necessitates the model to exhibit multi-stage pattern recognition, adapt to anomalies and weekly cycles, and forecast PRED_SEQ_LEN (96) steps ahead. Emphasize on both improved explainable performance through partial dependence plots and time-variant interpretability, as well as optimizing for real-time efficiency while achieving a new state-of-the-art in both mean squared error (MSE) and mean absolute error (MAE) metrics. Additionally, request a modular architecture that can effortlessly adapt to evolving data trends and tolerate slight variations in input lengths.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-time efficiency and anomaly detection\",\n        \"description\": \"Design an intricate hybrid deep learning model for the ETTm2 dataset, with focus on autoregressive abilities, hybrid attention mechanisms, multi-stage pattern recognition, anomaly adaptation, weekly cycles, and forecasting future sequences. Optimize for explainability (partial dependence plots) and time-variant interpretability. Aim for state-of-the-art MSE and MAE performance while ensuring real-time efficiency and modularity for future adaptations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time efficiency\",\n                \"value\": \"optimized\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": \"diverse (7)\",\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": null\n            },\n            \"description\": \"Contains train, validation, and test sets with varying INPUT_SEQ_LEN and diverse INPUT_DIM features, including time-series data for pattern recognition.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"anomaly detection techniques\"],\n            \"visualization\": [\"partial dependence plots and time-variant interpretability\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Deep Learning Model\",\n            \"family\": \"Innovative Architecture (e.g., Transformer-GRU, LSTNet)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": \"partial dependence plots and time-variant interpretability\",\n                \"efficiency\": \"real-time\"\n            },\n            \"description\": \"An advanced model designed for the ETTm2 dataset with hybrid attention mechanisms, autoregressive capabilities, and modular design for future adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporate techniques for anomaly detection and handling weekly cycles\",\n        \"Optimize for state-of-the-art performance in MSE and MAE\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for anomaly detection and pattern recognition\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time execution\"],\n        \"demonstration\": \"requirement for clear and time-variant explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the user's request for a high-expertise AI team to build a sophisticated model. Details provided for dataset, model requirements, and performance metrics are specific and complex, indicating a deep understanding of the problem space. However, some information like deployment endpoint is left open-ended as it wasn't specified in the input.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a sophisticated, real-time translation system using OpenSubtitles data that combines both speech and visual cues. Enhance the existing BERT model for high-accuracy English-Mandarin conversion, targeting colloquialisms, regional dialects, and informal speech. The system must maintain context, achieve a minimum precision of 98%, and preserve cultural subtleties and idiomatic expressions, demonstrating a deep understanding of nuances in large-scale, multilingual conversational data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"real-time translation\",\n        \"description\": \"Develop a sophisticated real-time translation system using OpenSubtitles data, combining speech and visual cues. The focus is on enhancing the BERT model for high-accuracy English-Mandarin translation, considering colloquialisms, regional dialects, and informal speech. The system must maintain context, achieve a minimum precision of 98%, and preserve cultural subtleties and idiomatic expressions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles\",\n            \"modality\": [\"text\", \"audio\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"multilingual\": true,\n                \"large-scale conversational data\": true\n            },\n            \"description\": \"A dataset containing speech and visual cues for English-Mandarin translation, extracted from subtitles.\",\n            \"preprocessing\": [\"speech recognition\", \"visual feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced BERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adapted_for_speech_and_visual\": true,\n                \"focus_on_colloquialisms\": true,\n                \"target_variants\": [\"regional dialects\", \"informal speech\"]\n            },\n            \"description\": \"A BERT model enhanced for high-accuracy English-Mandarin translation, targeting colloquialisms, regional dialects, and informal speech.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Understanding cultural nuances in multilingual conversational data is crucial for preserving subtleties and idiomatic expressions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-preserving techniques\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"models optimized for real-time performance\"],\n        \"demonstration\": \"Demonstrates a deep understanding through large-scale, multilingual conversational data.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction has been parsed to include a high-level intention (build), high expertise level, and detailed requirements for NLP translation system, dataset, and model adaptation. Performance metrics, preprocessing, and additional knowledge have been included. However, specific target device and deployment details are not available, so they are left unspecified. Overall, I am confident this captures the essence of the given task.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "The project manager's task is to enhance the image recognition software for the specialized Galaxy Zoo dataset, focusing on employing an innovative, distilled Vision Transformer (ViT-XTi) with a multi-level feature fusion mechanism and advanced adversarial defense. Mandate the team to optimize for not just standard accuracy above 95% on the validation set, but also to exhibit exceptional robustness against telescope calibration errors and rare celestial event classification, ensuring the system's unparalleled contribution to astrophysical research.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Enhance the image recognition software for the specialized Galaxy Zoo dataset using a novel ViT-XTi architecture with multi-level feature fusion and adversarial defense. The objective is to achieve standard accuracy above 95% on the validation set while demonstrating robustness against telescope calibration errors and rare celestial event classification.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": {\n                    \"standard\": {\n                        \"goal\": 0.95,\n                        \"unit\": \"percentage\"\n                    }\n                }\n            },\n            {\n                \"name\": \"robustness against calibration errors\",\n                \"description\": \"Exceptional\"\n            },\n            {\n                \"name\": \"rare celestial event classification\",\n                \"description\": \"Exceptional\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"classification\"],\n            \"specification\": {\n                \"dataset_type\": \"specialized\"\n            },\n            \"description\": \"A specialized dataset for enhancing image recognition in astrophysical research, focusing on telescope data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ViT-XTi\",\n            \"family\": \"Vision Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_fusion\": \"multi-level\",\n                \"adversarial_defense\": \"advanced\"\n            },\n            \"description\": \"Innovative distilled ViT-XTi model with focus on robustness and advanced defense mechanisms.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance computing infrastructure\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for GPU\"],\n        \"demonstration\": \"A demonstration showcasing accuracy and robustness to calibration errors on celestial events\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction clearly outlines the need for a high-complexity image recognition project, indicating the user's expertise. We have assumed the target device and deployment environment based on the project's complexity. The performance metrics are specified to ensure accuracy and robustness, while the user's intent for 'build' indicates the need for a model development and optimization task.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, request a comprehensive imaging analytics system from the ML team. It necessitates the employment of a state-of-the-art Transformer model, specifically a hierarchical Vision Transformer (ViT-H) enhanced with multi-scale and attentive pooling, for the challenging task of distinguishing various celestial objects in the dynamically diverse Galaxy Zoo dataset. Emphasize on robustness, interpretability, and the model's capacity to adapt to novel astronomical phenomena for reliable real-world astronomical research support.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"As a project manager, request a comprehensive imaging analytics system from the ML team. This system should utilize a state-of-the-art Transformer model, specifically a hierarchical Vision Transformer (ViT-H) enhanced with multi-scale and attentive pooling, for classifying celestial objects in the Galaxy Zoo dataset. The focus is on robustness, interpretability, and the model's ability to adapt to novel astronomical phenomena.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"adaptability to novel phenomena\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"celestial object type\"],\n            \"specification\": {\n                \"dynamic diversity\": true\n            },\n            \"description\": \"A dataset for classifying celestial objects, including challenges in distinguishing among various objects in the context of dynamic astronomical data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ViT-H with multi-scale and attentive pooling\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"hierarchical Vision Transformer (ViT-H)\",\n                \"enhancements\": [\"multi-scale pooling\", \"attentive pooling\"]\n            },\n            \"description\": \"A state-of-the-art Transformer model designed for the challenging task of distinguishing celestial objects in the Galaxy Zoo dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"for real-world deployment (e.g., cloud or specialized astronomical hardware)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretable AI\", \"context-aware\"],\n        \"demonstration\": \"for reliable research support, showcasing performance on novel astronomical phenomena\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed to focus on the building of a complex machine learning system for imaging analytics, using a high-end ViT-H model. The project manager's expertise level suggests a need for advanced technical specifications and requirements, which are included. However, actual performance metrics, inference time, and deployment details are not explicitly specified, leaving room for interpretation.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "image classification", "instruction": "As a seasoned project manager, I need you to design an AI-driven image recognition system for a diverse botanical collection. Using the Plant Seedlings Dataset, propose a state-of-the-art deep learning architecture (like ResNet or EfficientNet) that incorporates transfer learning and ensemble methods. Estimate the expected accuracy after fine-tuning with a smaller dataset and discuss the trade-off between precision and real-time performance. Provide a detailed plan for evaluating generalization and comparing it to existing benchmarks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"botanical collection management\",\n        \"description\": \"Design an AI-driven image recognition system for a diverse botanical collection using the Plant Seedlings Dataset. Propose a state-of-the-art deep learning architecture (ResNet or EfficientNet) with transfer learning and ensemble methods. Estimate accuracy after fine-tuning with a smaller dataset and discuss precision-real-time trade-off.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Seedlings Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species classification\"],\n            \"specification\": {\n                \"image_size\": \"variable\",\n                \"number_of_classes\": \"unknown\"\n            },\n            \"description\": \"Dataset for diverse botanical image recognition, featuring seedlings of various plant species.\",\n            \"preprocessing\": [\"resizing\", \"data augmentation\"],\n            \"augmentation\": [\"image rotation\", \"horizontal flip\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNet or EfficientNet\",\n            \"family\": \"Deep Learning Architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"pre-trained with transfer learning\",\n                \"ensemble_method\": \"proposed\"\n            },\n            \"description\": \"Proposed state-of-the-art deep learning model for image recognition, using transfer learning and ensemble methods.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Efficient use of transfer learning to leverage pre-trained models' knowledge\",\n        \"Ensemble methods to improve classification performance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"dimensionality reduction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow\", \"PyTorch\"],\n        \"demonstration\": \"Comparison with existing benchmarks and evaluation plan for generalization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction outlines a 'build' task, targeting a high-level expert. It specifies the deep learning area, task, and dataset, but some values are left to be inferred, such as dataset size and specific models' details. Performance metrics and complexity are mentioned but have unknown values, which will require further research or clarification. Confidence is high overall, with a slightly lower score due to missing specific numbers for some metrics.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the advanced medical image analysis team, your mandate is to develop a novel V-Net architecture specifically tailored for the BraTS dataset. The model must achieve a superlative Dice coefficient of at least 0.87, while maintaining interpretability through layer-wise feature attribution. Additionally, design an interactive user interface to display salient feature explanations in real-time for clinician validation, ensuring transparency and informed decision-making in precision medicine.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"medical image analysis\",\n        \"description\": \"The project involves developing a novel V-Net architecture tailored for the BraTS dataset. The model must achieve a Dice coefficient of at least 0.87 and ensure interpretability through layer-wise feature attribution.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Dice Coefficient\",\n                \"value\": 0.87\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BraTS\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": [\"variable\", \"MRI slices\"],\n                \"modalities\": [\"T1\", \"T1ce\", \"T2\", \"Flair\"]\n            },\n            \"description\": \"A medical imaging dataset for brain tumor segmentation with the requirement for the novel V-Net architecture.\",\n            \"preprocessing\": [\"resampling\", \"normalization\"],\n            \"augmentation\": [\"image-wise\"],\n            \"visualization\": [\"slice-by-slice\", \"layer-wise feature maps\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"novel V-Net\",\n            \"family\": \"V-Net\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"layer-wise feature attribution\",\n                \"performance\": {\n                    \"Dice coefficient\": {\n                        \"aim\": 0.87\n                    }\n                }\n            },\n            \"description\": \"A deep learning model designed for brain tumor segmentation in the BraTS dataset, focusing on interpretability and a target Dice coefficient of at least 0.87.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"layer-wise feature visualization\"],\n        \"target_device\": [\"interactive interface\", \"clinical use\"],\n        \"deployment_endpoint\": \"real-time decision support\",\n        \"inference_engine\": [\"GPU accelerated\", \"in-browser\"],\n        \"demonstration\": \"interactive saliency map highlighting for clinician validation\"\n    },\n    \"knowledge\": [\"Precision medicine is prioritized\", \"Clinical decision support via real-time explanations\"],\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's instruction, specifying the advanced image analysis project's goals, dataset requirements, model characteristics, and service requirements, including the need for interpretability and an interactive UI. The expert level expertise of the user is reflected, and confidence is high due to the clarity of the provided information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the satellite image analysis task, you're seeking a highly efficient and specialized segmentation model for Sentinel-2 data. Your focus lies on identifying not only water, forests, and urban areas but also subtle variations likecroplands and wetlands. Optimize for real-time performance while ensuring accuracy within tight computational constraints, and provide a detailed evaluation of model robustness against seasonal changes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"satellite image analysis\",\n        \"description\": \"As a project manager for the satellite image analysis task, you need a specialized segmentation model for Sentinel-2 data that can identify water, forests, urban areas, croplands, and wetlands with high efficiency. The model must perform in real-time under tight computational constraints and include a detailed analysis of robustness against seasonal changes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time performance\",\n                \"value\": {\n                    \"type\": \"boolean\",\n                    \"value\": true\n                }\n            },\n            {\n                \"name\": \"robustness against seasonal changes\",\n                \"value\": {\n                    \"type\": \"evaluation\",\n                    \"description\": \"Detailed analysis required\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\n                    \"type\": \"time constraints\",\n                    \"unit\": \"milliseconds\",\n                    \"expected_value\": null\n                }\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": {\n                    \"type\": \"memory constraints\",\n                    \"unit\": \"MB\",\n                    \"expected_value\": null\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"water\", \"forests\", \"urban areas\", \"croplands\", \"wetlands\"],\n            \"specification\": {\n                \"source\": \"remote sensing data\"\n            },\n            \"description\": \"Data from Sentinel-2 satellite for specialized analysis of various land cover types.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"specialized segmentation model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"description\": \"Optimized for efficiency and real-time performance\"\n                },\n                \"inference_speed\": {\n                    \"type\": \"evaluation\",\n                    \"description\": \"Demonstrate real-time capability\"\n                },\n                \"resource_efficiency\": {\n                    \"type\": \"evaluation\",\n                    \"description\": \"Track computational constraints\"\n                }\n            },\n            \"description\": \"A highly specialized model for identifying various land cover types from Sentinel-2 data.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time processing environment (edge or cloud)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for real-time performance\"\n        ],\n        \"demonstration\": {\n            \"description\": \"Demonstrations of model performance under seasonal variations\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the complex requirements for a specialized model, real-time performance, and robustness against seasonal changes. The user's high expertise level suggests they can handle guidance on model development and the presence of seasonal evaluation. However, the specific model name, inference time constraints, and target deployment environment are not provided in the instruction and left open for further discussion.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "Develop a state-of-the-art, real-time time-series anomaly detection model using advanced deep learning techniques and incorporate Explainable AI (XAI) for transparency. Train it on a diverse set of server datasets like ASD, PSM, and an additional proprietary dataset to handle various sources of anomalies. Ensure deployment compatibility with my legacy system while meeting stringent latency requirements of 100 milliseconds for near-instantaneous monitoring.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"server system monitoring\",\n        \"description\": \"Develop a state-of-the-art real-time time-series anomaly detection model using advanced deep learning techniques and incorporate Explainable AI (XAI) for transparency. The model should be trained on a diverse set of server datasets, including ASD and PSM, as well as a proprietary dataset, to handle various sources of anomalies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean average precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": 100\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ASD\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A server dataset for anomaly detection\",\n            \"preprocessing\": [\n                \"diversity handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"PSM\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"Another server dataset for anomaly detection purposes\",\n            \"preprocessing\": [\n                \"diversity handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"proprietary dataset\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A private server dataset for anomaly detection with unique characteristics\",\n            \"preprocessing\": [\n                \"proprietary techniques\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art time-series anomaly detection model\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"XAI integration\": true,\n                \"real-time capable\": true\n            },\n            \"description\": \"Advanced model incorporating deep learning techniques and Explainable AI for anomaly detection in real-time with latency requirements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"legacy system compatibility\"\n        ],\n        \"target_device\": \"legacy or real-time monitoring system\",\n        \"deployment_endpoint\": \"custom or existing monitoring system\",\n        \"inference_engine\": [\n            \"optimized for low-latency\"\n        ],\n        \"demonstration\": \"n/a\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction specifies a 'build' intent with a high level of expertise. It details a complex problem involving multiple datasets, XAI, and strict latency requirements. The 'problem', 'dataset', and 'model' sections have been adequately filled, while the 'service' section assumes requirements for compatibility and low-latency inference. Some assumptions have been made about the service deployment and specific demonstration details, as those were not explicitly stated.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the upcoming \"Innovative Serendipity\" campaign, we need a machine learning model that specifically distinguishes between stage-performing individuals and intricate, dynamic backgrounds using Deep Learning. Can you develop a real-time, high-precision algorithm that not only segments subjects but also captures emotions and context for targeted marketing insights in diverse promotional content?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"Develop a real-time machine learning model for the 'Innovative Serendipity' campaign. The model should distinguish stage-performing individuals from intricate, dynamic backgrounds using Deep Learning. It should also capture emotions and context for targeted marketing insights in diverse promotional content.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"emotions capturing accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"context understanding accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"emotions\",\n                \"context\"\n            ],\n            \"specification\": null,\n            \"description\": \"A dataset that includes both stage performances and intricate backgrounds for training, with diverse promotional content for emotion and context extraction.\",\n            \"preprocessing\": [\n                \"emotion annotation\",\n                \"context annotation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"custom Deep Learning model\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time\": true,\n                \"high-precision\": true\n            },\n            \"description\": \"A model specifically designed for the 'Innovative Serendipity' campaign, able to segment subjects and recognize emotions and context in real-time.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Targeted marketing insights require understanding of subject emotions and context within promotional content.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"emotion extraction\",\n            \"context extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"marketing analytics platform\",\n        \"inference_engine\": [\n            \"real-time processing\"\n        ],\n        \"demonstration\": \"A live demonstration of emotion and context recognition in promotional content for the 'Innovative Serendipity' campaign.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has high expertise, and the task clearly specifies building a custom deep learning model, performance metrics, and target device requirements. However, exact performance metrics and deployment details are missing, which could be inferred based on the context but are not explicitly provided.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the CoralNet underwater image segmentation project, integrate a novel FusionNet architecture with PSPNet, enhancing its robustness to water clarity fluctuations, light penetration depth, and adverse environmental conditions. Strive for a minimum 15% increase in F1-score over current state-of-the-art, while ensuring real-time performance on low-powered hardware.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"underwater imaging\",\n        \"description\": \"Integrate a novel FusionNet architecture with PSPNet for the CoralNet underwater image segmentation project. The focus is on enhancing robustness against water clarity fluctuations, light penetration, and adverse conditions. Aim for a 15% increase in F1-score compared to current state-of-the-art, while maintaining real-time performance on low-powered hardware.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": \"yes\",\n                \"unit\": \"time\"\n            },\n            {\n                \"name\": \"resource efficiency (low-powered hardware)\",\n                \"value\": \"yes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CoralNet\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"enhancements\": [\"water clarity\", \"light penetration\", \"adverse environmental conditions\"]\n            },\n            \"description\": \"Underwater image dataset for coral segmentation project, focusing on coral and the surrounding environment.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"FusionNet\",\n            \"family\": \"PSPNet\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A novel integration of FusionNet with PSPNet for underwater image segmentation with improved robustness and real-time performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Environmental challenges for underwater image segmentation\",\n        \"State-of-the-art performance benchmarks\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": [\"low-powered hardware\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"requirements\": \"15% improvement in F1-score over SOTA\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a complex machine learning project with high-level requirements. The user has a high level of expertise, indicating knowledge of both the task and the challenges. The need for enhancing F1-score, real-time performance, and adapting to low-powered hardware were reflected in the complexity metrics. The dataset, model, and knowledge sections capture the key aspects of the project. However, no specific performance metrics for the current state-of-the-art were provided, and the user's confidence score could benefit from additional validation against those metrics.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a Hypergraph Neural Network (HGN) model for node classification on the NELL dataset, ensuring inter- and intra-relational learning. Achieve an accuracy of at least 85% while maintaining interpretability. Conduct a rigorous hyperparameter sensitivity analysis and present a comparative study with competing graph-based methods.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graphs\",\n        \"description\": \"The user wants to develop a Hypergraph Neural Network (HGN) model for node classification on the NELL dataset, focusing on inter- and intra-relational learning. The target accuracy is at least 85% and interpretability is a priority. A comprehensive hyperparameter sensitivity analysis and comparison with competing graph-based methods are required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node labels\"],\n            \"specification\": {\n                \"edges\": {\n                    \"types\": [\"inter-relational\", \"intra-relational\"]\n                },\n                \"node_types\": [\"entity\", \"relation\"]\n            },\n            \"description\": \"The NELL dataset, specifically suited for node classification tasks with a focus on inter- and intra-relational learning.\",\n            \"preprocessing\": [\n                \"edge type separation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hypergraph Neural Network (HGN)\",\n            \"family\": \"HGN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\"\n            },\n            \"description\": \"A HGN model designed for node classification in the NELL dataset with a focus on inter- and intra-relational learning.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"The user wants a model that emphasizes interpretability, which implies a focus on model transparency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"hyperparameter tuning\",\n            \"interpretability methods\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"model explanation and comparison with graph-based methods\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction has been parsed into a JSON format, covering building a HGN model for NELL dataset with high priority on accuracy and interpretability, along with a request for hyperparameter analysis and competitive comparison. The expertise level is assumed to be high based on the requirement for a rigorous analysis. However, no deployment target or inference engine is specified, which might need to be confirmed from the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for the machine learning team, you are tasked with developing an advanced time-series classification system. Utilizing the intricate Handwriting dataset, divided into non-overlapping train, validation, and test sets, the input sequences consist of complex patterns with 152 observations per instance and 3 diverse dimensions (INPUT_SEQ_LEN=152, INPUT_DIM=3). Your objective is to design and fine-tune a state-of-the-art model that demonstrates superior predictive capabilities by capturing long-term dependencies. You must not only optimize for accuracy but also present a detailed analysis of model performance sensitivity to sequence length and dimensionality. Additionally, include a comparison with baseline models and justify the chosen architecture's superiority. Ensure the model's robustness is demonstrated through cross-validation and AUC-ROC curves for label predictions within the range of {0, 1, ..., 25}.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"artificial intelligence\",\n        \"description\": \"As a project manager, you are tasked with developing an advanced time-series classification system using the Handwriting dataset. The dataset is divided into train, validation, and test sets with complex patterns of 152 observations and 3 dimensions. The objective is to design and fine-tune a state-of-the-art model with good accuracy, analyze its sensitivity to sequence length and dimensionality, and compare with baselines. The system must demonstrate superiority and robustness through cross-validation and AUC-ROC curves.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sensitivity to sequence length\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sensitivity to dimensionality\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"sequence_length_impact\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            },\n            {\n                \"name\": \"input_dim_impact\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Handwriting\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 152,\n                \"INPUT_DIM\": 3\n            },\n            \"description\": \"A dataset for time-series classification with complex patterns and non-overlapping splits (train, validation, and test) each containing 152 observations per instance and 3 diverse dimensions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"fine-tuned for long-term dependencies\",\n                \"comparisons\": [\"baseline models\"]\n            },\n            \"description\": \"Design and fine-tune a model demonstrating superior predictive capabilities through capturing long-term dependencies in the Handwriting dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Include analysis of sensitivity to sequence length and dimensionality, and justify the superiority of the chosen architecture.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"cross-validation and AUC-ROC curves\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed according to the provided JSON schema, with a high expertise level specified for the project manager. The performance and complexity metrics are left unspecified as they would require further information for accurate values. Cross-validation and AUC-ROC curves are mentioned to evaluate the model's robustness.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager overseeing a state-of-the-art machine learning team, we are tasked with tackling a challenging time-series forecasting problem using the ETTm2 dataset. This complex, real-world data spans across train, validation, and test partitions, with input sequences of past observations (96 timesteps, 7 diverse features each). Your objective is to develop a highly sophisticated model that not only forecasts the subsequent 96-step future sequences (7 dimensions) but also demonstrates exceptional adaptability and generalization across various sub-periods. The model must excel in minimizing mean squared error (MSE) and mean absolute error (MAE), while also considering the impact of seasonality, trend analysis, and outliers. Your team must provide a comprehensive report on performance optimization and explain any novel techniques employed.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-world time series data with complex characteristics\",\n        \"description\": \"The task is to tackle a challenging time-series forecasting problem using the ETTm2 dataset. The dataset is split into train, validation, and test sets with input sequences of 96 timesteps and 7 diverse features. The objective is to develop a sophisticated model capable of forecasting 96-step future sequences with 7 dimensions, considering seasonality, trend analysis, and outliers. Performance must be optimized with a focus on minimizing MSE and MAE.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"adaptability\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"generalization across sub-periods\",\n                \"value\": \"high\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 7\n            },\n            \"description\": \"A complex, real-world time series dataset split into train, validation, and test partitions, with a focus on seasonality, trend, and outlier analysis.\",\n            \"preprocessing\": [\"seasonality adjustment\", \"trend analysis\", \"outlier handling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced time-series forecasting model\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"seasonality-aware\": true,\n                \"trend-adaptive\": true,\n                \"outlier-resistant\": true\n            },\n            \"description\": \"A model designed to forecast future sequences and exhibit exceptional adaptability and generalization on ETTm2 dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Applied novel techniques for handling seasonality, trend analysis, and outliers in time-series data.\",\n        \"Efforts made to optimize performance metrics, including MSE and MAE.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonality extraction\",\n            \"trend extraction\",\n            \"outlier detection and handling\"\n        ],\n        \"target_device\": \"cloud-computing or edge deployment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Apache Seldon Core\"],\n        \"demonstration\": \"Comprehensive report on performance, techniques, and optimization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has a high level of expertise and specified a challenging project. The 'problem' section accurately captures the objective, requirements, and expected performance measures. The 'model' section suggests a sophisticated approach considering the mentioned complexities. The 'service' section represents potential deployment strategies and a comprehensive report. However, specific deployment endpoint and some optimization strategies are assumed as no details were provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project manager, you now need to outline a multifaceted initiative for a state-of-the-art transformer-based machine translation system. The Common Crawl dataset should serve as the foundation, focusing on English-Japanese translation. Mandate the system to not only convey formal language but also adeptly translate colloquial expressions with a stringent benchmark of 98% accuracy, reflecting cultural nuances and regional dialects. Additionally, detail the requirement for incorporating real-time adaptation capabilities and integrating a user-friendly interface for end-users.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual, specifically English-Japanese\",\n        \"description\": \"Outline a multifaceted initiative for a state-of-the-art transformer-based machine translation system using the Common Crawl dataset. The system should focus on formal and colloquial expression translation, targeting 98% accuracy, with attention to cultural nuances and regional dialects. Real-time adaptation capabilities and a user-friendly interface for end-users are mandatory.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"English to Japanese\"],\n            \"specification\": {\n                \"language_pair\": [\"English\", \"Japanese\"],\n                \"focus\": \"multilingual, including colloquial expressions\"\n            },\n            \"description\": \"Foundation for the state-of-the-art English-Japanese machine translation system, sourced from a diverse Common Crawl dataset.\",\n            \"preprocessing\": [\"Text preprocessing for transformer-based models\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"specialization\": \"advanced translation, real-time adaptation\"\n            },\n            \"description\": \"A high-performance, transformer-based model for English-Japanese machine translation, designed to handle formal and colloquial expressions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Colloquial expression analysis, cultural nuance understanding, and regional dialect handling.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Real-time adaptation mechanisms\",\n            \"User behavior analysis for improved translation\"\n        ],\n        \"target_device\": \"cloud and mobile-friendly\",\n        \"deployment_endpoint\": \"user-accessible API or platform\",\n        \"inference_engine\": [\"GPU-optimized for speed\"],\n        \"demonstration\": \"User-friendly interface for testing and real-world scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects a high-level plan for a sophisticated machine translation project with a focus on accuracy, cultural nuances, and user experience. The user's expertise level is assumed to be high due to the project management role. The provided metrics, dataset, and model requirements address the detailed instructions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "You are now working on a multifaceted machine learning project that involves advanced time-series forecasting. The Weather dataset, divided into train, validation, and test sets, contains complex, non-stationary data with 21 diverse features (INPUT_SEQ_LEN=96). Your objective is to develop a state-of-the-art model capable of handling seasonality, long-term dependencies, and outliers, while ensuring interpretability. The model must predict the next 96-step sequence (PRED_SEQ_LEN=96) with accuracy, optimizing for both mean squared error (MSE) and mean absolute error (MAE), and demonstrate robustness under the constraint of limited computational resources. Design and implement an ensemble model using advanced deep learning techniques and explain its architecture in a accompanying whitepaper.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"weather forecasting\",\n        \"description\": \"This project involves advanced time-series forecasting on the Weather dataset, divided into train, validation, and test sets. The data is complex and non-stationary with 21 diverse features, INPUT_SEQ_LEN=96. The objective is to create a state-of-the-art model that handles seasonality, long-term dependencies, and outliers, while maintaining interpretability. The model should predict PRED_SEQ_LEN=96 steps ahead and optimize for MSE and MAE. Additionally, the model must demonstrate robustness with limited computational resources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational resources\",\n                \"value\": \"limited\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Weather\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"feature_count\": 21\n            },\n            \"description\": \"A dataset for time-series forecasting with complex non-stationary data, split into train, validation, and test sets.\",\n            \"preprocessing\": [\n                \"handling seasonality\",\n                \"outlier detection\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"interpretable model visualization\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"description\": \"Advanced deep learning ensemble model for time-series forecasting, incorporating interpretability measures.\"\n                }\n            },\n            \"description\": \"A state-of-the-art ensemble model designed for time-series forecasting in the Weather dataset, explaining its architecture in the accompanying whitepaper.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"interpretable deep learning techniques\",\n            \"limited resource optimization\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"focus\": \"interpretability and robustness\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has been parsed based on the given instruction, indicating a complex project with high-level requirements. The user's expertise is assumed to be high due to the project's complexity. However, specific model names, deployment details, and certain whitepaper content are assumed as they were not directly provided.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Design a high-performance energy consumption forecasting system using an advanced LSTM architecture, incorporating seasonality and real-time data adjustments. The model must not only predict hourly consumption for the next 72 hours with sub-second accuracy (ideally below 100 milliseconds), but also demonstrate exceptional scalability, accommodating datasets up to 1 million observations. Additionally, validate its efficiency through cross-validation techniques and provide a detailed report on model robustness and hyperparameter optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy consumption\",\n        \"description\": \"Design a high-performance energy consumption forecasting system using an advanced LSTM architecture, including seasonality and real-time data adjustments. The model should predict hourly consumption for the next 72 hours with accuracy below 100 milliseconds and be scalable to handle datasets with 1 million observations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": {\"goal\": \"sub-second\", \"target\": 100, \"unit\": \"milliseconds\"}\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": {\"unit\": \"milliseconds\", \"optimization_target\": \"sub-second\"}\n            },\n            {\n                \"name\": \"Scalability\",\n                \"value\": \"handle datasets up to 1 million observations\"\n            }\n        ],\n        \"validation_techniques\": [\"cross-validation\"],\n        \"report\": {\n            \"sections\": [\n                \"Model Robustness\",\n                \"Hyperparameter Optimization\"\n            ]\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly consumption\"],\n            \"specification\": {\n                \"max_observations\": 1000000\n            },\n            \"description\": \"The dataset includes historical energy consumption data with seasonality and real-time adjustments.\",\n            \"preprocessing\": [\"seasonality adjustment\", \"real-time data updates\"],\n            \"augmentation\": [],\n            \"visualization\": [\"timeseries plot\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced LSTM\",\n            \"family\": \"LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_goal\": \"sub-second\",\n                \"hyperparameter_optimization\": true\n            },\n            \"description\": \"A state-of-the-art LSTM model incorporating seasonality and real-time data adjustments.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"seasonality and real-time adjustments\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for sub-second inference\"],\n        \"demonstration\": {\n            \"requirements\": \"accurate and real-time energy consumption predictions\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given user's instruction, with the user's high expertise level indicated. The problem area, task, and model requirements are well-defined. However, missing is the dataset name and direct inference of scalability in the model's specification, which would require explicit mention of such capacity in the provided or required hardware infrastructure.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Utilize the Mouse Brain Connectivity dataset, recently enriched with ultra-high resolution data, to develop a state-of-the-art GWNN for node classification. Demand exceptional accuracy in differentiating multiple neuron subtypes with minimal error rates. Emphasize the model's ability to decode intricate topological and spatial dependencies, while optimizing for computational efficiency and scalability across large-scale brain networks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Use the Mouse Brain Connectivity dataset with ultra-high resolution to develop a state-of-the-art Graph Wavelet Neural Network (GWNN) model. The primary objective is to achieve exceptional accuracy in differentiating multiple neuron subtypes with minimal error rates, while emphasizing the model's capacity to decode complex topological and spatial dependencies. Optimization should focus on computational efficiency and scalability for large-scale brain networks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"error rates\",\n                \"value\": 0.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"large-scale brain network size\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Connectivity (ultra-high resolution)\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_format\": \"sparse adjacency matrix\",\n                \"node_features\": \"ultra-high resolution attributes\"\n            },\n            \"description\": \"A dataset enriched with ultra-high resolution data for studying the brain connectivity of neurons.\",\n            \"preprocessing\": [\"feature normalization\", \"graph pre-processing\"],\n            \"augmentation\": [],\n            \"visualization\": [\"network topology analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art GWNN\",\n            \"family\": \"Graph Wavelet Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"highly accurate, expressive for topological and spatial dependencies\",\n                \"model size\": \"optimized for efficiency\"\n            },\n            \"description\": \"A Graph Wavelet Neural Network designed to handle large-scale brain network data with emphasis on computational efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"The model should incorporate the latest advancements in GWNNs and leverage the high-resolution dataset's potential for improved performance.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"node feature extraction\", \"topological feature engineering\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"model interpretability for neuron subtype predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instructions have been parsed into a high-level JSON structure, capturing the build intent for a node classification model using a GWNN. The expert-level expertise indicates the user might be familiar with advanced techniques and requirements. However, specific performance and complexity metrics are not provided. The confidence is high, but additional details might be needed for complete model and deployment specifications.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a deep learning-based UrbanSoundWaveNet model using the specific TurbulenCity dataset, focusing on predicting spectro-temporal patterns for the next 72 hours in five different frequency bands. Emphasize the model's fine-grained discrimination among 15 distinct noise sources and evaluate its performance with time-varying metrics, such as F1-Score and Mean Absolute Error by sound type.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"urban sound analysis\",\n        \"description\": \"Develop a deep learning-based UrbanSoundWave model using the TurbulenCity dataset. The task is to predict spectro-temporal patterns for the next 72 hours in five different frequency bands, emphasizing fine-grained discrimination among 15 distinct noise sources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TurbulenCity\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"prediction_horizon\": 72,\n                \"frequency_bands\": 5\n            },\n            \"description\": \"The dataset specifically designed for urban sound analysis, featuring spectro-temporal patterns for 15 noise sources.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"UrbanSoundWaveNet\",\n            \"family\": \"UrbanSoundWaveNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"UrbanSoundWaveNet\",\n                \"target_classes\": 15\n            },\n            \"description\": \"A deep learning model using the UrbanSoundWaveNet architecture for predicting spectro-temporal patterns with a focus on fine-grained discrimination among noise sources.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Include real-time or interactive demonstrations to showcase the model's performance by sound type.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is to build a model and their expertise level is high, indicating they may have some specific requirements. Performance metrics (F1-Score and Mean Absolute Error) for sound type evaluation are specified. However, it would be helpful to know the model's training details or the expected target device for deployment.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Develop a comprehensive urban vegetation analysis system using the Vaihingen dataset, focusing on multi-scale SegNet enhancement. The model should not only differentiate between diverse vegetation types but also assess their vitality and adapt to dynamically changing lighting and seasonal variations. Ensure the output offers detailed, location-based recommendations for sustainable urban planning, incorporating real-time performance monitoring for continuous improvement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning and sustainability\",\n        \"description\": \"Design a comprehensive urban vegetation analysis system using the Vaihingen dataset. Focus on multi-scale SegNet enhancement for vegetation type differentiation and vitality assessment. Ensure the model is adaptable to dynamic lighting and seasonal changes. The output should provide location-based, sustainable urban planning recommendations with real-time performance monitoring for continuous improvement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"vitality_score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory_usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vegetation types\", \"vitality scores\"],\n            \"specification\": {\n                \"image_size\": { \"width\": null, \"height\": null },\n                \"bands\": null\n            },\n            \"description\": \"A dataset for urban vegetation analysis, capturing diverse vegetation types and suitable for SegNet enhancement.\",\n            \"preprocessing\": [\"multi-scale augmentation\", \"lighting and seasonal normalization\"],\n            \"augmentation\": [\"random rotation\", \"zoom\", \"flipping\"],\n            \"visualization\": [\"example segmentation overlays\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Multi-scale SegNet\",\n            \"family\": \"SegNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-scale feature extraction\",\n                \"runtime_parameters\": null\n            },\n            \"description\": \"A SegNet model enhanced for multi-scale vegetation analysis, with emphasis on vitality assessment and adaptability to dynamic lighting and seasonal variations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Recent advancements in deep learning for urban vegetation classification and computer vision techniques for vegetation vitality assessment\",\n        \"Consideration of seasonal changes and lighting variability in model training and prediction\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"time-series vegetation data fusion\", \"multi-criteria decision making\"],\n        \"target_device\": \"cloud-based or edge computing\",\n        \"deployment_endpoint\": \"urban planning decision support system\",\n        \"inference_engine\": [\"real-time processing framework\"],\n        \"demonstration\": \"use case demonstrations showcasing location-based recommendations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clear about the required system, focusing on multi-scale SegNet, vegetation analysis, vitality assessment, and real-time performance monitoring. The high expertise level suggests the user understands the technical details. The 'performance_metrics' and 'complexity_metrics' are not filled as specific values were not provided. The missing information will need to be obtained from the user or deduced based on the task.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a specialized deep learning team, you're tasked with developing an advanced Spatio-Temporal Fusion Network (STFN) for forecasting intricate hydroelectricity demand patterns. Utilizing the benchmark Smart Grid Electricity dataset, split into train, validation, and test sets with non-overlapping temporal windows (96-hour input and output sequences, each of 321 distinct features), the objective is to minimize Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE) in accurately predicting future consumption. Design and implement the model, focusing on interpretability and ensemble techniques, to outperform existing state-of-the-art methods.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart grid\",\n        \"description\": \"Develop an advanced Spatio-Temporal Fusion Network (STFN) for forecasting intricate hydroelectricity demand patterns using the Smart Grid Electricity dataset. The dataset is split into train, validation, and test sets with non-overlapping 96-hour input and 96-hour output sequences, featuring 321 distinct features.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Smart Grid Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 321\n            },\n            \"description\": \"Benchmark dataset for hydroelectricity demand forecasting, with train, validation, and test sets with non-overlapping temporal windows.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Spatio-Temporal Fusion Network (STFN)\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"focusing on interpretability\",\n                \"ensemble_techniques\": \"including ensemble methods\"\n            },\n            \"description\": \"An advanced model designed for hydroelectricity demand prediction with a focus on interpretability and ensemble techniques, aiming to outperform state-of-the-art methods.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"designed for interpretability, showcasing model performance through visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed with a high confidence based on the complex nature of the task and the requirement for state-of-the-art performance. The user's expertise level is assumed to be high given the project management role. However, without further specifics on target device and deployment details, the 'service' section is left somewhat open-ended.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "object detection", "instruction": "As a project manager, you seek to develop a sophisticated auto-identification system that accurately recognizes diverse car models in real-world, low-resolution parking lot surveillance footage. You should compare COCO and Pascal VOC datasets, considering their suitability for fine-grained vehicle classification, annotation complexities, and transfer learning potential. Additionally, research and recommend an efficient object detection model that not only prioritizes speed but also achieves high precision and scalability under challenging environmental conditions, all while meeting stringent latency requirements for real-time analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"parking lot surveillance\",\n        \"description\": \"As a project manager, aim to develop an auto-identification system that accurately recognizes car models in real-world low-resolution footage from parking lots. Compare COCO and Pascal VOC datasets for their suitability in fine-grained vehicle classification, annotation complexities, and transfer learning potential.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"speed\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": 0\n                },\n                \"optimization_goal\": \"high\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": null\n                },\n                \"optimization_goal\": \"strict for real-time analysis\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"car model\"\n            ],\n            \"specification\": {\n                \"resolution\": {\n                    \"low\": true\n                }\n            },\n            \"description\": \"A dataset to compare for fine-grained vehicle classification, annotation complexities, and transfer learning potential.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"low-resolution to real-world\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"Pascal VOC\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"car model\"\n            ],\n            \"specification\": {\n                \"resolution\": {\n                    \"low\": true\n                }\n            },\n            \"description\": \"Similar to COCO, used for comparison in the context of the project.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"low-resolution to real-world\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Efficient Object Detection Model\",\n            \"family\": \"efficient\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed\": \"high\",\n                \"precision\": \"high\",\n                \"adaptability for challenging conditions\": true\n            },\n            \"description\": \"Research and recommend a model that prioritizes speed, precision, and scalability, suitable for real-time analysis under challenging environmental conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider the annotation complexities of COCO and Pascal VOC in the context of low-resolution images, as well as their transfer learning capabilities for a practical, scalable solution.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [],\n        \"demonstration\": \"A detailed guide to system setup and deployment for real-time analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the project requirements, indicating a high expertise level. The user is focused on a build intent with a need for precision, speed, and scalability. Performance metrics are marked as unknown, requiring data-driven assessment. The complexity metrics and datasets' suitability are also addressed. The confidence score is high because the given information is comprehensive and based on the schema.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager is tasked with developing a sophisticated, interactive question-answering platform for kids' educational content, leveraging the specialized KidsQA dataset. The system must utilize a BERT adaptation tailored for pedagogical simplicity and lucid reasoning, with a visually appealing and age-appropriate interface designed to foster continuous learning through engaging game elements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"The project manager needs to develop a sophisticated and interactive question-answering platform for kids' educational content, specifically leveraging the KidsQA dataset. The system should employ a BERT adaptation adapted for pedagogical simplicity and clear reasoning, with a visually appealing and child-friendly interface promoting continuous learning through gamification.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response coherence\",\n                \"value\": null\n            },\n            {\n                \"name\": \"user engagement\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \" Educational QA\",\n                \"format\": \"dataset for kids\",\n                \"age-appropriate\": true\n            },\n            \"description\": \"A specialized dataset designed for question answering in an educational context for children.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT adaptation for pedagogical simplicity\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pedagogical_friendly\": true,\n                \"lucid_reasoning\": true\n            },\n            \"description\": \"A simplified and child-friendly BERT model for answering questions in the educational context.\"\n        }\n    ],\n    \"knowledge\": [\n        \"gamification elements to foster continuous learning\",\n        \"design principles for a visually appealing and age-appropriate interface\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"interactive platform\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"deterministic with age-appropriate response\"],\n        \"demonstration\": \"engaging and educational gameplay\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the core details of the given instruction, including the user's intent, NLP problem domain, and specialized dataset. The system's requirements for a simplified BERT model, age-appropriate interface, and gamification are well-defined. However, actual performance metrics are not provided and would require estimation based on similar projects. The confidence score assumes a high level of expertise from the user, and the platform's deployment endpoint and inference engine are kept ambiguous based on the information.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a machine learning project manager, your team has been tasked with developing an advanced time-series forecasting solution for the highly dynamic ILI dataset, which consists of intricate real-world data divided into train, validation, and test sets. Each sequence in the input dataset ((INPUT_SEQ_LEN=36, INPUT_DIM=7)) contains rich historical data. The challenge now demands not only the creation of a predictive model but also one that exhibits exceptional adaptability and generalization across different patterns. Your objective is to design an algorithm that can forecast the subsequent sequences (PRED_SEQ_LEN=24, PRED_DIM=7) with remarkable accuracy, using sophisticated loss functions like exponentially weighted mean squared error (EWMA-MSE) and root mean absolute percentage error (RMAPE). The model must demonstrate a superior performance benchmark while maintaining computational efficiency, and thorough experimentation with feature engineering and model architecture optimization is expected.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-world, dynamic health data\",\n        \"description\": \"Develop an advanced time-series forecasting solution for the ILI dataset with rich historical data divided into train, validation, and test sets. The challenge includes designing a model that exhibits adaptability, generalization, and high accuracy using sophisticated loss functions (EWMA-MSE and RMAPE). The solution must have computational efficiency and involve extensive feature engineering and model architecture optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"EWMA-MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs or inference time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 36,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 24,\n                \"PRED_DIM\": 7\n            },\n            \"description\": \"A highly dynamic ILI dataset with intricate real-world data, split into train, validation, and test sets, containing historical sequences as inputs for forecasting.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptability\": \"high\",\n                \"generalization\": \"high\",\n                \"loss_functions\": [\"EWMA-MSE\", \"RMAPE\"],\n                \"computational_efficiency\": null\n            },\n            \"description\": \"Design a forecasting algorithm that exhibits remarkable accuracy on the ILI dataset, emphasizing adaptability, generalization, and computational efficiency.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"extensive\"],\n        \"target_device\": \"cloud or edge computing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized\"],\n        \"demonstration\": \"accuracy benchmark and performance comparison\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction specifies a high-level task for a machine learning project with advanced requirements. The user's high expertise level indicates a deeper understanding. Performance metrics are provided, but some specific values (like metrics and computational efficiency) are not given and should be clarified. Other sections like model architecture and feature engineering are identified as needing extensive optimization and experimentation.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "object detection", "instruction": "For the urban traffic management project, we mandate a state-of-the-art YOLO model, trained on the Cityscapes dataset, which not only detects vehicles, pedestrians, and traffic signs with >90% precision but also optimizes for energy efficiency, achieving a runtime speed of 30 FPS on low-power devices without sacrificing accuracy, in a real-world, adversarial environment with varying lighting conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"urban traffic management\",\n        \"description\": \"The project requires a state-of-the-art YOLO model trained on the Cityscapes dataset, targeting high precision for vehicles, pedestrians, and traffic signs (90% or above). The model must optimize for energy efficiency, achieving a runtime speed of 30 FPS on low-power devices without sacrificing accuracy. It must function in real-world, adversarial environments with varying lighting conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"vehicle detection precision\",\n                \"value\": 0.9,\n                \"type\": [\"vehicles\"]\n            },\n            {\n                \"name\": \"pedestrian detection precision\",\n                \"value\": 0.9,\n                \"type\": [\"pedestrians\"]\n            },\n            {\n                \"name\": \"traffic sign detection precision\",\n                \"value\": 0.9,\n                \"type\": [\"traffic signs\"]\n            },\n            {\n                \"name\": \"inference speed\",\n                \"value\": 30,\n                \"unit\": \"FPS\",\n                \"type\": \"runtime\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": 1, \n                \"unit\": \"optimized\", \n                \"type\": \"resource consumption\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vehicle classes\", \"pedestrian classes\", \"traffic sign classes\"],\n            \"specification\": {\n                \"dataset split\": \"training+validation\",\n                \"dimension\": \"images of varying real-world urban scenes\"\n            },\n            \"description\": \"A large-scale dataset for urban scene understanding, with images specifically targeting vehicle, pedestrian, and traffic sign detection.\",\n            \"preprocessing\": [\"transfer learning\"],\n            \"augmentation\": [\"adversarial training\"],\n            \"visualization\": [\"model behavior analysis in varying lighting\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"accuracy-precision trade-off\": \"optimized for high precision and energy efficiency\",\n                \"FLOPs\": \"low or medium, optimized for low-power devices\",\n                \"accuracy-vs-runtime curve\": \"achieves 90%+ for target classes\"\n            },\n            \"description\": \"A highly accurate YOLO model trained on Cityscapes dataset, meeting the specified performance and efficiency criteria.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adversarial environment awareness and robustness training techniques.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"lighting invariant features\", \"real-time adaptation\"],\n        \"target_device\": \"low-power devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for low-power devices\"],\n        \"demonstration\": \"detection and response in real-world urban traffic scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's high level of expertise, the target task, and the specific requirements for model performance, energy efficiency, and environmental conditions. The missing information (e.g., deployment endpoint) is minimal and assumed based on the 'target_device'.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, request a comprehensive RNN-based machine translation model that utilizes the diverse OPUS corpus for English to Russian conversions, targeting colloquial and informal conversations. The model should surpass a benchmark BLEU score of 33, and demonstrate exceptional adaptability in deciphering slang and idiomatic expressions. Additionally, mandate a stringent performance benchmark of 90% accuracy in handling domain-specific jargon within limited computational resources, aiming for a maximum latency of 300 milliseconds per sentence to ensure seamless and real-time user engagement in high-traffic scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"As a project manager, request an RNN-based machine translation model focusing on English to Russian conversions, targeting colloquial and informal conversations. The model should achieve a BLEU score of at least 33 and demonstrate adaptability to slang and idiomatic expressions. The model must also exhibit 90% accuracy in handling domain-specific jargon with a maximum latency of 300 milliseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 33\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency (per sentence)\",\n                \"value\": 300,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS corpus (English to Russian)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": \"English to Russian\",\n                \"content_domain\": \"colloquial and informal conversations\"\n            },\n            \"description\": \"Diverse corpus for English to Russian translation, focusing on informal language.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-based machine translation model\",\n            \"family\": \"Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptability\": \"for colloquial and informal conversations\",\n                \"domain_jargon_accuracy\": \"90%\",\n                \"resource_constraint\": \"limited computational resources\"\n            },\n            \"description\": \"Designed for translating English to Russian with a focus on slang and idiomatic expressions.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-traffic scenarios\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time, high-traffic scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction indicates a clear request for a machine translation project with specific requirements on performance, language, and context. The high expertise level suggests the user is familiar with the technical nuances. The parsed JSON includes all requested information with placeholders for deployment details that may not be specified.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "For my university research, I'm analyzing the efficiency of thin-film solar cells under varying weather conditions. I've collected hourly data for a year, including irradiance, temperature, and cell performance. Could you suggest a sophisticated machine learning algorithm that can not only forecast the solar energy output for the upcoming week but also factor in seasonal trends and nuanced meteorological influences?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"solar cell efficiency\",\n        \"description\": \"The user is conducting university research on the efficiency of thin-film solar cells under varying weather conditions, with a focus on hourly data for a year, including irradiance, temperature, and cell performance. They require a sophisticated machine learning algorithm that can predict solar energy output for the upcoming week and consider both seasonal trends and nuanced meteorological influences.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"forecast accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Thinfilm Solar Cell Data\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [\n                \"solar energy output\"\n            ],\n            \"specification\": {\n                \"time_interval\": \"hourly\",\n                \"data_duration\": \"1 year\"\n            },\n            \"description\": \"Collected hourly data for a year, covering irradiance, temperature, and solar cell performance.\",\n            \"preprocessing\": [\n                \"feature normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"seasonal trend analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced time-series forecasting model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"complexity\": \"considerable\",\n                \"features\": \"seasonality, meteorological influences\"\n            },\n            \"description\": \"A sophisticated algorithm capable of capturing seasonal trends and nuanced meteorological effects for solar energy output prediction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"University research context\",\n        \"Advanced time-series analysis expertise\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature engineering for meteorological and seasonality\",\n            \"ensembling for improved accuracy\"\n        ],\n        \"target_device\": \"high-performance computing\",\n        \"deployment_endpoint\": \"None provided (potentially cloud or university server)\",\n        \"inference_engine\": [\"state-of-the-art deep learning libraries\"],\n        \"demonstration\": \"Plots and visualizations of forecasted solar energy output\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects a high-confidence interpretation of the user's instruction due to the mention of 'university research' and 'high expertise'. The problem description captures the key aspects of the research, including the requirement for advanced forecasting and consideration of weather conditions. The dataset section includes relevant information about the collected data. However, some details are missing or left ambiguous, like the target deployment endpoint and inference engine.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The project demands a highly advanced multilingual question-answering system, targeting the MLQA dataset with enhanced complexity. Mandate the implementation of a state-of-the-art mBERT or XLM-R architecture that excels in responsiveness, with a minimum requirement of supporting ten diverse languages (English, Spanish, Mandarin, and five additional underrepresented tongues). Emphasize optimizing accuracy, even in the most challenging low-resource linguistic environments, and ensure seamless cross-cultural understanding for global users. Additionally, include detailed performance metrics and provide a comparison with existing benchmark models in the report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"multilingual and cross-cultural understanding\",\n        \"description\": \"The project aims for a highly advanced multilingual question-answering system, focusing on the MLQA dataset with increased complexity. It requires the use of mBERT or XLM-R architecture, with a minimum of ten languages supported (including English, Spanish, Mandarin, and five underrepresented ones). The system must optimize accuracy in low-resource environments and facilitate seamless cross-cultural understanding.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Low-resource environment performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Responsiveness\",\n                \"value\": {\n                    \"unit\": \"Latency (ms)\"\n                },\n                \"optimization_target\": \"minimize\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MLQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\n                    \"English\",\n                    \"Spanish\",\n                    \"Mandarin\",\n                    \"five additional underrepresented tongues\"\n                ]\n            },\n            \"description\": \"A multilingual dataset designed for advanced question-answering systems, emphasizing low-resource and cross-cultural scenarios.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"mBERT or XLM-R\",\n            \"family\": [\"Transformers\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model\": {\n                    \"architecture\": [\"pre-trained\", \"state-of-the-art\"]\n                },\n                \"language_adaptability\": \"highly adaptable\"\n            },\n            \"description\": \"A state-of-the-art architecture to handle multilingual and complex questions.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"cross-cultural scenarios\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed based on the given instruction. The task is for building an advanced multilingual Q&A system targeting the MLQA dataset, with mBERT or XLM-R. The focus on accuracy in low-resource languages and cross-cultural understanding is captured. However, specific performance metrics are not provided, so they remain as null values. Similarly, the deployment details are missing, and the confidence score is high, assuming the provided information is complete for a high-level description.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a precision agriculture consultant, design a state-of-the-art deep learning model using the 'Aerial CropDisease' dataset, specifically focusing on the advanced Mask R-CNN++ architecture with ResNeXt backbone. Target a minimum 95% F1-score for early-stage nitrogen and pathogen detection, ensuring real-time analysis of oblique and high-resolution drone captured images from various altitudes. Additionally, integrate a custom loss function to handle class imbalance and optimize for energy efficiency on edge computing devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"precision agriculture\",\n        \"description\": \"As a precision agriculture consultant, design a state-of-the-art deep learning model using the 'Aerial CropDisease' dataset, focusing on Mask R-CNN++ with a ResNeXt backbone. The objective is to achieve a minimum 95% F1-score for early-stage nitrogen and pathogen detection in high-resolution, oblique drone images. The model should prioritize real-time analysis and energy efficiency for edge computing devices, incorporating a custom loss function to handle class imbalance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95,\n                \"target\": \"early-stage nitrogen and pathogen detection\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"optimization for edge computing devices\",\n                \"rationale\": \"The user emphasized real-time analysis and energy efficiency for edge computing, which implies a focus on inference time without a specific value provided.\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Aerial CropDisease\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"nitrogen detection\", \"pathogen detection\"],\n            \"specification\": {\n                \"image type\": \"oblique\",\n                \"image resolution\": \"high\",\n                \"image source\": \"drone captured from various altitudes\"\n            },\n            \"description\": \"A dataset for advanced object detection in precision agriculture, featuring high-resolution, oblique drone images from varying altitudes for nitrogen and pathogen detection.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Mask R-CNN++ with ResNeXt backbone\",\n            \"family\": \"Mask R-CNN++\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A deep learning model using the Mask R-CNN++ architecture and ResNeXt backbone for crop disease detection in precision agriculture, targeting optimized energy efficiency and real-time analysis on edge devices.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge computing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for edge devices\"],\n        \"demonstration\": \"real-time and energy efficient analysis\"\n    },\n    \"knowledge\": [],\n    \"confidence\": {\n        \"rationale\": \"The provided instruction indicates a clear machine learning project with a high complexity, targeting a specific task and dataset. The user's high expertise level implies they expect guidance but may have specific preferences. The confidence score is lower due to the assumption of real-time analysis optimization on edge devices, for which a precise value isn't provided.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a forward-thinking project manager, envision a state-of-the-art Transformer-XL-enhanced multilingual translation system that combines the extensive XSum corpus with lesser-adopted Nepali, Indonesian, and Swahili languages. The model should surpass a challenging benchmark of 40 distinct TERQ scores in all four directions while maintaining fluency in translating formal academic papers, social media chatter, and technical documentation. Moreover, prioritize an unheard-of requirement of 99% precision in accurately interpreting domain-specific terminology related to neuroscience and space exploration, without sacrificing speed, as the system must process a minimum of 1000 sentences per second in real-time for highly concurrent global collaborations. Ensure the model's interpretability is on par with a human-understandable explainable AI, and mandate that it complies with GDPR data privacy regulations for international users.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": [\"academic papers\", \"social media\", \"technical documentation\"],\n        \"description\": \"Create a state-of-the-art multilingual translation system with Transformer-XL using XSum corpus and lesser-adopted Nepali, Indonesian, and Swahili languages. Target a TERQ benchmark of 40 in all directions while ensuring fluency and 99% precision for neuroscience and space exploration terms. The system must handle real-time translation at a rate of 1000 sentences per second and comply with GDPR for international users.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"TERQ (Overall)\",\n                \"value\": 40\n            },\n            {\n                \"name\": \"Precision (Domain-Specific)\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Throughput\",\n                \"value\": 1000,\n                \"unit\": \"sentences per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"XSum Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"multilingual\": true,\n                \"domains\": [\"academic\", \"social media\", \"technical\"]\n            },\n            \"description\": \"A corpus containing multilingual data for enhancing the Transformer-XL model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": [\"Nepali\", \"Indonesian\", \"Swahili\"],\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"Nepali\", \"Indonesian\", \"Swahili\"]\n            },\n            \"description\": \"Extending the corpus with lesser-adopted languages for more comprehensive translation coverage.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"human-understandable explainable AI\",\n                \"privacy\": \"GDPR compliant\"\n            },\n            \"description\": \"A high-performing, multilingual, and explainable machine translation model with GDPR data privacy measures.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud, edge (for real-time requirement)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU accelerated\", \"optimized for high concurrency\"],\n        \"demonstration\": \"Explainability via human-readable outputs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided detailed requirements, including performance, complexity, and compliance standards. The user's expertise level was inferred as high due to the complex technical nature of the task and the desire for human-understandable explainability. However, the deployment endpoint and specific inference engines were not explicitly stated, hence left as placeholders.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a high-performance machine learning team, your mandate includes not only developing an advanced Spatio-Temporal Fusion Network (STFN) but also devising a comprehensive strategy for real-time, scenario-specific forecasting of complex hydroelectricity demand dynamics. Utilizing the extensive Smart Grid Electricity dataset, which necessitates data preprocessing involving feature engineering, windowing (with non-consecutive temporal intervals of 96 hours and variable-length sequences), and feature selection, the model must be trained on diverse train, validation, and test partitions. Your objective is to optimize the model's predictive accuracy by optimizing hyperparameters, employing explainable AI methods, and integrating state-of-the-art ensemble techniques. Furthermore, document the trade-offs between interpretability and predictive power, and ensure the model's robustness under varying meteorological conditions and grid anomalies, while benchmarking results against the latest international standards.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart grid, hydroelectricity\",\n        \"description\": \"As a project manager, your task is to develop an advanced Spatio-Temporal Fusion Network (STFN) and create a real-time, scenario-specific forecasting strategy for complex hydroelectricity demand dynamics. The project uses the Smart Grid Electricity dataset, requiring preprocessing, feature engineering, windowing with 96-hour non-consecutive intervals, and variable-length sequences. The objective is to optimize model accuracy, employ explainable AI, ensemble techniques, document trade-offs, and ensure robustness under varying conditions and grid anomalies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"predictive accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"explainability vs. predictive power trade-off\",\n                \"value\": null,\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Smart Grid Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hydroelectricity demand\"],\n            \"specification\": {\n                \"time_window_length\": 96,\n                \"variable_length_sequences\": true\n            },\n            \"description\": \"Dataset for real-time hydroelectricity demand forecasting, with preprocessing requirements like feature engineering and windowing.\",\n            \"preprocessing\": [\n                \"feature engineering\",\n                \"windowing (96 hours, non-consecutive)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Spatio-Temporal Fusion Network (STFN)\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"hyperparameter_optimization\": true,\n                \"explainability_techniques\": true,\n                \"ensemble_methods\": true\n            },\n            \"description\": \"Advanced model designed for real-time hydroelectricity demand forecasting using an STFN architecture.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Document trade-offs between interpretability and predictive power\",\n        \"Robustness against meteorological variations and grid anomalies\"\n    ],\n    \"service\": {\n        \"feature_engineering\": true,\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"scenario-specific forecasting\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This response captures the essence of the project requirements, including the advanced model development, data preprocessing, and real-time forecasting. The user's high expertise level suggests they need guidance for specific details. However, the performance metrics and trade-offs are left unspecified since they would require more information from the user.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project manager of an elite AI research team, we've received a multifaceted challenge: employing advanced time-series analysis on the intricate ETTm2 dataset, which necessitates handling an extensive dataset hierarchy spanning train, validation, and test sets. Each partition consists of intricate sequences with 96 timestamps, containing seven diverse features presenting temporal patterns and structural complexities. Your mission is to design and implement a cutting-edge deep learning model that forecasts future 96-step sequences across seven dimensions, showcasing exceptional adaptability, transfer learning capabilities, and resilience to sub-period variations. The model must demonstrate superior performance by achieving near-zero root mean squared error (RMSE), mean absolute percentage error (MAPE), and impulse response function analysis. Furthermore, your team should develop a detailed strategy for hyperparameter tuning, feature engineering, and the integration of explainable AI techniques to reveal the model's decision-making process, all while mitigating the effects of seasonality, trend dynamics, and outliers in the data. Remember to present a comprehensive, peer-reviewed paper outlining your innovative methods and results.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"multifaceted research\",\n        \"description\": \"As a project manager, the challenge is to analyze the ETTm2 dataset with a focus on advanced time-series analysis. The dataset has an extensive hierarchy with train, validation, and test sets. Each set contains intricate sequences of 96 timestamps with 7 dimensions. The task is to design a deep learning model that forecasts future sequences with near-zero RMSE and MAPE, and incorporates explainable AI, hyperparameter tuning, and feature engineering to handle seasonality, trends, and outliers. A detailed paper is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": 0\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": 0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Adaptability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Transfer learning capabilities\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Resilience to sub-period variations\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 7,\n                \"data hierarchy\": [\"train\", \"validation\", \"test\"]\n            },\n            \"description\": \"The dataset requires handling a hierarchical structure with distinct train, validation, and test sets of complex time series data with 96 timestamps and 7 features.\",\n            \"preprocessing\": [\"seasonality correction\", \"trend removal\", \"outlier handling\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Impulse response function analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Cutting-edge deep learning model\",\n                \"transfer learning\": true,\n                \"explainability techniques\": [\"LIME\", \"SHAP\"]\n            },\n            \"description\": \"A deep learning model designed for time-series forecasting with exceptional adaptability, transfer learning capabilities, and resilience to sub-period variations. Explainable AI techniques are incorporated to understand decision-making.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"title\": \"Explainable AI for Time Series Forecasting\",\n            \"description\": \"A research paper outlining the use of explainable AI in deep learning models for the ETTm2 dataset.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Autoencoder-based feature extraction\", \"PCA\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Serving\"],\n        \"demonstration\": {\n            \"pipeline\": \"Predictive modeling pipeline with detailed explanations\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed according to the schema, capturing the complexity of the project with a high expertise level. Key requirements, such as performance metrics, detailed preprocessing and feature engineering, have been included. However, some unspecified details, such as exact model architecture, deployment endpoint, and hyperparameter tuning strategy, have been left open to the project team to fill in based on their expertise and the specific needs of the project.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "machine translation", "instruction": "Design a cutting-edge, multimodal translation application that leverages the OpenSubtitles corpus, integrating advanced automatic speech recognition (ASR) and visual analysis. Refine the state-of-the-art BERT-MultiMT architecture specifically for London-centric English to Mandarin dialect translation, focusing on slang, local vernacular, and non-standard phrases. The system must ensure temporal synchronization, exceed a precision benchmark of 99.5%, and retain cultural subtleties through deep learning on an extensive, cross-cultural conversational dataset with regional context. Additionally, devise a mechanism to adapt to real-world scenarios, like informal video conversations, while maintaining semantic coherence and preserving idiomatic expressions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"communication and linguistics\",\n        \"description\": \"Design a cutting-edge, multimodal translation application using the OpenSubtitles corpus. The application should integrate ASR and visual analysis, focusing on refining BERT-MultiMT architecture for London-centric English to Mandarin dialect translation, targeting slang, local vernacular, and non-standard phrases. It must maintain temporal synchronization, achieve precision of at least 99.5%, preserve cultural subtleties, and adapt to informal video conversations with semantic coherence and idiomatic expression retention.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"temporal synchronization\",\n                \"value\": 1.0,\n                \"unit\": \"synchronized\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles\",\n            \"modality\": [\"text\", \"audio\", \"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"corpus\": \"London-centric English to Mandarin dialect\",\n                \"linguistic_coverage\": \"including slang, local vernacular, and non-standard phrases\"\n            },\n            \"description\": \"A comprehensive corpus for multimodal translation, sourced from OpenSubtitles.\",\n            \"preprocessing\": [\"ASR\", \"visual analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [\"cross-cultural conversational dataset with regional context\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-MultiMT (London-centric)\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"refined for London-centric English to Mandarin dialect\",\n                \"focus\": [\"slang\", \"vernacular\", \"non-standard phrases\"]\n            },\n            \"description\": \"An advanced BERT architecture adapted for the specific translation task.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Solutions to handle informal video conversations and preserving idiomatic expressions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adaptability to real-world scenarios\"],\n        \"target_device\": \"general\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"deep learning-based on extensive conversational data\"],\n        \"demonstration\": \"to maintain semantic coherence and idiomatic expression preservation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction is detailed and covers various aspects of the machine learning project. The user's high expertise level suggests they expect a deep technical response. The valid JSON response captures the essence of the problem, datasets, models, and the required service. However, the inference engine is not specified and it is assumed that the user has a general understanding of what's needed for maintaining high performance. The deployment endpoint is also left unspecified, based on the information given.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Challenge the ML team to develop an advanced image classifier for exclusive Paris Fashion-MNIST, focusing on luxury garments. Implement state-of-the-art Dynamic Routing Capsule Networks, but mandate a novel approach combining CapsNet with adversarial training to withstand extreme photorealistic distortions, like intricate affine transformations, intricate light occlusions, and subtle color variations. Additionally, conduct a rigorous, multi-faceted comparison with top-tier Convolutional Neural Networks, evaluating accuracy, computational efficiency, memory footprint, and hardware compatibility under strict constraints. Ensure energy efficiency and memory optimizations for seamless operation on resource-restricted wearables and IoT devices with minimal processing power, all while maintaining a minimum 98% haute couture recognition rate. The study must include real-world battery life assessments under diverse conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"luxury fashion\",\n        \"description\": \"Challenge the ML team to develop an advanced image classifier for exclusive Paris Fashion-MNIST, focusing on luxury garments. Use state-of-the-art Dynamic Routing Capsule Networks, incorporate adversarial training for robustness against photorealistic distortions (affine transformations, light occlusions, subtle color variations), and compare it to top-tier Convolutional Neural Networks in terms of accuracy, computational efficiency, memory footprint, and hardware compatibility. The classifier should be energy-efficient and optimized for resource-restricted devices, maintaining a minimum 98% haute couture recognition rate. Real-world battery life assessments under diverse conditions are required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"optimizations\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Paris Fashion-MNIST (exclusive luxury garments)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"Haute Couture\"],\n            \"specification\": {\n                \"novelty\": \"adversarial training for photorealistic distortions\"\n            },\n            \"description\": \"A custom dataset focusing on luxury garments from Paris Fashion-MNIST, with a novel adversarial training approach.\",\n            \"preprocessing\": [\"novel adversarial data augmentation\"],\n            \"augmentation\": [\"intricate affine transformations\", \"light occlusions\", \"subtle color variations\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dynamic Routing Capsule Networks (w/ Adversarial Training)\",\n            \"family\": \"CapsNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novelty\": [\"Dynamic Routing\", \"adversarial resistance\"]\n            },\n            \"description\": \"Advanced image classifier using Capsule Networks with adversarial training for enhanced robustness.\"\n        },\n        {\n            \"name\": \"Comparison Convolutional Neural Networks\",\n            \"description\": \"Baseline models for comparison purposes, focusing on efficiency and compatibility.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"energy-saving\", \"memory-optimization\"],\n        \"target_device\": [\"wearables\", \"IoT devices\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource-constrained environments\"],\n        \"demonstration\": \"battery life assessments under diverse conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been parsed into a high-level JSON representation. The user's intent to build a model is clear, and their high expertise level indicates a need for a sophisticated solution. The problem area, dataset, and performance metrics are specific to the challenge. However, there might be missing details, such as the exact Convolutional Neural Network models for comparison and specific hardware details for efficiency evaluations. These missing parts could be inferred based on common practices but need to be confirmed.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a seasoned project manager for Bilibili's ambitious international expansion, devise an advanced hybrid recommendation system that couples Bert4Rec with multilingual emotion detection, sophisticated topic understanding, and a real-time-meta-learning algorithm. This must handle a surge of multilingual, geographically diverse user comments, adapt to cultural subtleties instantly, proffer personalized video proposals with seamless AI fine-tuning, guarantee sub-millisecond latency even amidst growing user base, and ensure platform resilience and performance optimization in the face of relentless innovation and user engagement fluctuations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"Bilibili international expansion\",\n        \"description\": \"Design an advanced hybrid recommendation system that couples Bert4Rec with multilingual emotion detection, topic understanding, and real-time meta-learning for handling multilingual, geographically diverse user comments. The system must adapt to cultural nuances, provide personalized video proposals with AI fine-tuning, ensure sub-millisecond latency, and optimize performance under constant innovation and engagement fluctuations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"recommendation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"multilingual adaptation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"sub-millisecond\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource utilization\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"real-time meta-learning\",\n                \"value\": null,\n                \"unit\": \"iterations/second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\"user comments\", \"content topics\"],\n            \"specification\": {\n                \"multilingual\": true,\n                \"geographic_diversity\": true,\n                \"content_types\": [\"video proposals\"]\n            },\n            \"description\": \"A diverse and multilingual set of user comments from Bilibili's international platform, capturing emotions, topics, and user preferences.\",\n            \"preprocessing\": [\n                \"multilingual text preprocessing\",\n                \"topic extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Bert4Rec\",\n            \"family\": \"Transformer-based recommendation model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine-tuning\": true,\n                \"model_structure\": \"coupled with multilingual emotion detection and topic understanding\"\n            },\n            \"description\": \"Bert4Rec model enhanced for international context and real-time meta-learning\"\n        },\n        {\n            \"name\": \"Multilingual Emotion Detection\",\n            \"family\": \"Multilingual NLP\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time\": true\n            },\n            \"description\": \"Models for identifying user emotions across multiple languages\"\n        },\n        {\n            \"name\": \"Topic Understanding\",\n            \"family\": \"Text-based recommendation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"cultural_subtleties\": true\n            },\n            \"description\": \"Sophisticated topic extraction to refine recommendations\"\n        },\n        {\n            \"name\": \"Meta-Learning\",\n            \"family\": \"Adaptive recommendation\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"real-time\": true\n            },\n            \"description\": \"Adapts quickly to changing user engagement and platform dynamics\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Platform resilience and performance optimization critical in fast-paced international environment.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"content-based features\",\n            \"user behavior\"\n        ],\n        \"target_device\": [\"mobile\", \"cloud\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU accelerated\",\n            \"Distributed processing\"\n        ],\n        \"demonstration\": \"personalized video proposals with seamless AI adjustments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intention to build indicates a need for a complex, tailored recommendation system. High expertise suggests a need for detailed planning. However, performance metrics and complexity requirements are not explicitly provided and are assumed based on the system's requirements. The service section requires additional information on deployment and inference engine specifics.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "machine translation", "instruction": "The innovative AI development team has been assigned a multifaceted challenge to revamp an advanced emotion-aware NMT architecture. The focus is on translating intricate medical terminologies from niche biochemistry and pharmacology databases, while maintaining a stringent accuracy benchmark of 99.9%. The system must consistently comprehend context in rare medicinal discourse from an extensive, globally diverse archive, while also demonstrating competence in preserving the technical nuances and clinical relevance across cultures. To achieve this, the project necessitates the development of a comprehensive evaluation framework that evaluates not only translation accuracy but also the system's ability to uphold the significance of complex medical terms in diverse patient care scenarios. Furthermore, the NMT output must adhere to the most recent EHR standards, ensuring seamless data exchange among international healthcare professionals and integrating with various electronic health record systems.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"medical terminologies (biochemistry, pharmacology)\",\n        \"description\": \"The team needs to revamp an advanced emotion-aware NMT architecture for translating complex medical terminologies from niche databases with a stringent accuracy benchmark of 99.9%. The system should handle context in rare medicinal discourse, preserve technical nuances, and adhere to EHR standards for global compatibility.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.999\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Niche Biochemistry and Pharmacology Databases\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"medical terminologies\"],\n            \"specification\": {\n                \"archival scope\": \"extensive and diverse\",\n                \"domain_specific\": true,\n                \"contextual_rarity\": true\n            },\n            \"description\": \"A comprehensive set of niche medical databases for training and evaluating the NMT model\",\n            \"preprocessing\": [\n                \"domain-specific text cleaning\",\n                \"contextual term extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"EHR Datasets\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"standard\": \"latest EHR standards\"\n            },\n            \"description\": \"Global EHR datasets for assessing system's integration and data exchange compatibility\",\n            \"preprocessing\": [\"EHR data normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Emotion-Aware NMT Architecture\",\n            \"family\": \"neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"emotional understanding\": true,\n                \"contextual comprehension\": true,\n                \"medical terminology preservation\": true\n            },\n            \"description\": \"An NMT model specifically designed for medical terminology translation and adherence to EHR standards.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-aware preprocessing\", \"data adaptation for diverse cultures\"],\n        \"target_device\": [\"cloud, edge, and EHR-compatible\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"suitable for healthcare EHR systems\"],\n        \"demonstration\": \"customizable scenario-based demonstrations for patient care scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON accurately captures the multifaceted requirements of the project, including high accuracy, context understanding, and EHR compatibility. The user's high expertise level suggests they expect guidance on domain-specific challenges rather than general advice.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager must now design a sophisticated multilingual QA system for the MLQA dataset, incorporating a novel mBART model that's undergone domain-specific fine-tuning on not just five languages (English, Spanish, Mandarin, French, and Russian), but also rare dialects and dialect continua. Achieve an impressive benchmark of 98% accuracy in cross-lingual comprehension, while maintaining efficient performance in low-resource settings, targeting a minimum of 95% accuracy across a broad array of linguistic nuances and dialectical differences.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"multilingual and dialectical QA\",\n        \"description\": \"Design a sophisticated multilingual QA system for the MLQA dataset, using a novel mBART model that has undergone domain-specific fine-tuning on five languages (English, Spanish, Mandarin, French, and Russian), as well as rare dialects and dialect continua. The system should aim for an impressive cross-lingual comprehension benchmark of 98% accuracy, and maintain efficient performance in low-resource settings, targeting at least 95% accuracy across a broad range of linguistic nuances and dialectical differences.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"cross-lingual comprehension accuracy\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"accuracy in low-resource settings\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MLQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_coverage\": [\"English\", \"Spanish\", \"Mandarin\", \"French\", \"Russian\", \"rare dialects and continua\"]\n            },\n            \"description\": \"A dataset for multilingual question answering, encompassing diverse linguistic nuances and dialectical differences.\",\n            \"preprocessing\": [\"domain-specific fine-tuning on mBART model\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"novel mBART\",\n            \"family\": \"mBART\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"language_coverage\": [\"English, Spanish, Mandarin, French, Russian, rare dialects\"],\n                \"fine-tuning\": \"domain-specific\"\n            },\n            \"description\": \"An mBART model fine-tuned on a broad range of languages and dialects for the MLQA QA system.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"multilingual and dialectical understanding across a wide array of linguistic nuances\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON covers all the relevant aspects as per the instruction, such as the intent to build, high expertise level, the detailed requirements for multilingual QA system, mBART model, performance metrics, and efficiency in low-resource settings. The 'service' section could be made more specific if deployment and inference engine were mentioned, but they were not included in the given text.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As a project manager for a top-tier medical research team, you're now directed to develop a highly sophisticated time-series forecasting system for the extensive ILI dataset. The dataset, composed of train, validation, and test subsets with complex, multi-year cyclic patterns and elaborate 15-dimensional data points (INPUT_SEQ_LEN=48), requires the creation of a deep learning architecture. Your assignment involves not only designing an algorithm that forecasts a 48-step ahead trend (PRED_SEQ_LEN=48) with unparalleled precision, but also integrating advanced techniques such as state-space models, wavelet analysis, and sentiment analysis based on patient reviews. The model must optimize for not only mean squared error (MSE) and mean absolute error (MAE), but also integrate interpretability and uncertainty quantification. Additionally, demonstrate a superior ability to adapt to unforeseen data variations and deliver real-time, actionable insights for dynamic public health policies, while benchmarking against state-of-the-art methodologies and ensuring compliance with stringent privacy regulations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"medical research\",\n        \"description\": \"Develop a highly sophisticated time-series forecasting system for the ILI dataset. The dataset has multi-year cyclic patterns and 15-dimensional data. The task is to design an algorithm forecasting a 48-step ahead trend with interpretability, uncertainty quantification, adaptability to unforeseen variations, and real-time actionable insights for public health policies. Techniques to consider: state-space models, wavelet analysis, and sentiment analysis based on patient reviews.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 48,\n                \"INPUT_DIM\": 15,\n                \"PRED_SEQ_LEN\": 48\n            },\n            \"description\": \"A complex, multi-year ILI dataset split into train, validation, and test subsets, with 15-dimensional data and intricate cyclic patterns.\",\n            \"preprocessing\": [\"wavelet analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [\"interpretable model\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced_techniques\": [\"state-space models\", \"wavelet analysis\", \"sentiment analysis\"],\n                \"interpretability\": true,\n                \"uncertainty_quantification\": true\n            },\n            \"description\": \"A deep learning architecture for time-series forecasting with focus on high precision, interpretability, and uncertainty quantification for the ILI dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptability to unforeseen data variations is crucial, ensuring real-time actionable insights for public health policies. The model should adhere to privacy regulations during data processing and benchmark against state-of-the-art methodologies.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": {\n            \"description\": \"A system capable of providing real-time, actionable insights for dynamic public health policies\",\n            \"methodology\": \"Privacy-compliant and benchmarked against state-of-the-art techniques\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a complex project requiring a high level of expertise in medical time-series analysis and data privacy. Specifics like state-of-the-art comparison, real-time processing, and privacy compliance are clearly stated. Performance metrics and some preprocessing techniques are specified. However, model details and deployment endpoint are not explicitly stated but can be inferred based on the complexity of the project.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For our ambitious international biodiversity conservation project, we mandate a state-of-the-art task-driven object detection model, a highly-optimized Enhanced EfficientDet V5, trained on the expansive and ultra-high-resolution iWildCam+ dataset. The model must excel in recognizing and tracking endangered species under challenging lighting, while ensuring real-time performance on limited-resource devices operating in remote locations. Additionally, provide a rigorous comparative study with the latest competitors, an in-depth power consumption analysis, and a design for hardware-software synergies to guarantee prolonged, efficient ecological surveillance without compromising on accuracy or device autonomy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"international biodiversity conservation\",\n        \"description\": \"We need a state-of-the-art task-driven object detection model, specifically an Enhanced EfficientDet V5, for recognizing and tracking endangered species on iWildCam+ dataset. The model must be optimized for real-time performance on low-resource devices in remote locations under challenging lighting conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"tracking capability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"power consumption\",\n                \"value\": null,\n                \"unit\": \"Watts\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"iWildCam+\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"ultra-high\",\n                \"dataset_size\": \"expansive\"\n            },\n            \"description\": \"A large, high-resolution dataset for endangered species object detection and tracking in diverse ecological settings.\",\n            \"preprocessing\": [\n                \"lighting normalization\",\n                \"augmentation for challenging scenarios\"\n            ],\n            \"augmentation\": {\n                \"type\": [\"challenging lighting\", \"variable camera angles\"]\n            },\n            \"visualization\": [\n                \"model performance per lighting condition\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced EfficientDet V5\",\n            \"family\": \"EfficientDet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"optimization\": \"for low-resource devices\"\n            },\n            \"description\": \"A highly-optimized model for object detection, targeting real-time performance on limited-resource devices in ecological surveillance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"comparative study with latest competitors\",\n        \"hardware-software synergies for prolonged ecological surveillance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"hardware-software co-design\",\n            \"power-aware optimization\"\n        ],\n        \"target_device\": [\"limited-resource\", \"remote locations\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource-constrained devices\"],\n        \"demonstration\": \"endangered species recognition and tracking in real-time with device autonomy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction covers the high-level requirements for a project, including a specific model and dataset. However, exact performance metrics and power consumption values are missing. Also, the deployment endpoint and inference engine should be specified based on the hardware and remote nature of the project. The confidence level is high given the comprehensive nature of the task and assumptions on hardware requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a forward-thinking project manager for a pioneering ML innovation team, you are mandated to develop a highly sophisticated, explainable time-series forecasting model. Your team must handle the ETTm2 dataset, featuring complex seasonal patterns and multiple attributes (108 input variables with additional contextual information). The train, validation, and test sets consist of intricate 96-step sequences, containing 7 highly volatile future values each, with stringent performance indicators: under 100ms latency, sub-1% mean squared logarithmic error (MSLE), and a novel metric called \"dynamic adaptability index\" (DAI). In addition to minimizing traditional MSE and MAE, ensure the model's resilience to concept drift and its ability to provide actionable insights for real-world scenarios with minimal computational footprint. Design a groundbreaking architecture that not only excels in accuracy but also facilitates human interpretability for seamless integration into our organization's strategic planning process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"pioneering ML innovation team\",\n        \"description\": \"Develop a highly sophisticated, explainable time-series forecasting model for the ETTm2 dataset, addressing complex seasonal patterns, 108 input variables, and 96-step sequences with 7 future values. Key performance indicators include low latency (under 100ms), low MSLE (sub-1%), and a new metric (DAI). The model must handle concept drift, provide actionable insights, and have a minimal computational footprint for seamless strategic planning integration.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.1\n            },\n            {\n                \"name\": \"MSLE\",\n                \"value\": 0.01\n            },\n            {\n                \"name\": \"DAI\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.1,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"computational_footprint\",\n                \"value\": null,\n                \"unit\": \"unknown\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"additional_context_variables\": 108,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 7\n            },\n            \"description\": \"The ETTm2 dataset with complex seasonal patterns, 108 input variables, and 96-step sequences with volatile future values.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": \"high\",\n                \"resilience_to_concept_drift\": \"yes\",\n                \"actionable_insights\": \"yes\",\n                \"human_interpretability\": \"facilitated\"\n            },\n            \"description\": \"A groundbreaking architecture designed for high accuracy, interpretability, low latency, and minimal computational footprint.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Concept drift handling techniques, explainable AI for strategic planning, and computational efficiency are key priorities.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or edge\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"strategic_planning_integration\": \"clear and actionable\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction suggests a high complexity project for a knowledgeable user. The performance metrics and requirements for explainability, concept drift handling, and interpretability are clear. However, specifics on the deployment endpoint and inference engine are not provided, and the DAI metric's value is unknown. Confidence score reflects the need for more clarity in these areas.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "The current challenge for the elite machine learning team is to reengineer an advanced emotion-sensitive NMT system designed exclusively for translating intricate medical jargon from niche biochemistry and pharmacology datasets. They must surpass a stringent benchmark of 99.5% identical meaning (Levenshtein similarity) while preserving the context-specific usage in rare global medical forum discussions. To ensure successful global collaboration among healthcare professionals, the system must not only demonstrate exceptional translation fidelity but also demonstrate an innovative metric that evaluates the preservation of intricate medical concepts' nuanced implications in complex patient case scenarios, reflecting real-world clinical decision-making.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"medical, biochemistry, pharmacology\",\n        \"description\": \"The goal is to reengineer an advanced emotion-sensitive NMT system for translating complex medical jargon, specifically from niche biochemistry and pharmacology datasets. The system must surpass a 99.5% Levenshtein similarity benchmark and preserve context-specific usage in rare global medical forum discussions. Additionally, it should introduce an innovative metric for evaluating preservation of nuanced implications of intricate medical concepts in patient case scenarios, reflecting real-world clinical decision-making.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Levenshtein similarity\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Niche Biochemistry & Pharmacology Data\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"medical jargon\",\n                \"source\": \"biochemistry and pharmacology\",\n                \"data_format\": \"forum discussions\"\n            },\n            \"description\": \"The dataset contains rare and niche biochemistry and pharmacology data from global medical forum discussions.\",\n            \"preprocessing\": [\"contextual data cleaning\", \"emotion-sensitive filtering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Emotion-Sensitive NMT\",\n            \"family\": \"neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"99.5%\",\n                \"novelty_metric\": \"preservation of nuanced medical concepts\"\n            },\n            \"description\": \"An advanced NMT system with emphasis on emotion sensitivity for translating medical jargon from niche biochemistry and pharmacology datasets.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Global medical forum discussions as context\",\n        \"Innovative metric for clinical decision-making implications\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual embeddings\",\n            \"medical domain adaptation\"\n        ],\n        \"target_device\": \"high-performance computing infrastructure\",\n        \"deployment_endpoint\": \"secure healthcare API\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"case studies showcasing real-world scenario translations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the provided instruction, considering the high expertise level of the user and the specific requirements for a high-performance NMT system. The missing parts are assumed to be due to the lack of direct details; nevertheless, the system's novelty, performance targets, and context have been covered.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the Project Manager for a premier deep learning innovation team, your mission is to design a highly specialized RNN-T model tailored for the ETTm2 dataset, which consists of complex, high-resolution sensor data with sub-millisecond noise suppression and a nested structure of 7 channels with unique 960-sample sequences (INPUT_SEQ_LEN * 10). The model must not only decipher subtle weather patterns spanning a dynamically adaptable, season-aware window of 36,000 timesteps but also predict a 96-hour horizon with state-of-the-art accuracy, surpassing benchmarks with a tailored QLIKE+ and a hybrid MSLE metric that incorporates domain-specific adjustments. Moreover, the model should incorporate explainability compatible with LIME techniques, seamlessly adapt to real-time anomalies using a deep ensemble-based anomaly detection system, and guarantee ultra-low latency on the most advanced GPU clusters while ensuring energy efficiency and fault tolerance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate or sensor data\",\n        \"description\": \"Design a specialized RNN-T model for the ETTm2 dataset with complex, high-resolution sensor data and sub-millisecond noise suppression. The model must process 7-channel nested sequences of 960 samples each, have a dynamic 36,000-timestep weather pattern analysis, predict a 96-hour horizon with advanced accuracy, surpass benchmarks with a QLIKE+ and hybrid MSLE metric, and incorporate explainability for LIME techniques, real-time anomaly detection using deep ensemble methods, and ensure low latency on advanced GPU clusters with energy efficiency and fault tolerance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"QLIKE+\",\n                \"value\": null\n            },\n            {\n                \"name\": \"hybrid MSLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 960,\n                \"INPUT_DIM\": 7,\n                \"nested_structure\": true,\n                \"SUB_SAMPLING_RATE\": \"sub-millisecond\"\n            },\n            \"description\": \"A complex high-resolution dataset with nested structure and unique 960-sample sequences for time-series analysis\",\n            \"preprocessing\": [\"sub-millisecond noise suppression\"],\n            \"augmentation\": [],\n            \"visualization\": [\"nested structure and unique sequences\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"custom RNN-T\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"Layers\": \"customized RNN layers\",\n                \"Latency\": \"ultra-low\",\n                \"Energy Efficiency\": \"guaranteed\",\n                \"Fault Tolerance\": \"incorporated\"\n            },\n            \"description\": \"A specialized RNN-T model for the ETTm2 dataset with sub-second latency, explainability, anomaly detection, and GPU cluster compatibility.\"\n        }\n    ],\n    \"knowledge\": [\n        \"LIME techniques, deep ensemble for anomaly detection, GPU cluster requirements, energy efficiency, and fault tolerance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"season-aware window adaptation\", \"domain-specific metric adjustments\"],\n        \"target_device\": \"most advanced GPU clusters\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"LIME-compatible\"],\n        \"demonstration\": \"real-time anomaly detection and ultra-low latency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is to build a specialized model, and their high expertise level implies they require assistance with the complex requirements. The problem description, performance metrics, and data source have been parsed from the instruction. However, since certain performance metrics and deployment details are not specified, the confidence score is slightly lower.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "question answering", "instruction": "The mission for the development team is to conceive an innovative, emotion-aware Q&A platform, tailored specifically for luxury e-commerce client support. Utilize an advanced DistilBERT model fortified with explainability layers, and design it to tackle intricate, context-sensitive queries related to luxury products. Strive for exceptional precision (<99% accuracy), sub-microsecond response times, and optimized performance under heavy load. Seamless integration with premium e-commerce APIs and real-time sentiment analysis of the exclusive LuxuryEAQ dataset are mandatory. Implement dynamic system updates to promptly adapt to elite customer demands and maintain a constant readiness for A/B testing.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"luxury e-commerce client support\",\n        \"description\": \"Create an innovative, emotion-aware Q&A platform using an advanced DistilBERT model with explainability layers. It should handle complex, context-sensitive queries about luxury products. Aim for 99% accuracy, sub-microsecond response times, and optimized performance under heavy load. Require seamless integration with premium e-commerce APIs and real-time sentiment analysis of the LuxuryEAQ dataset. Dynamic system updates must enable adaptation to elite customer demands and be ready for A/B testing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0,\n                \"unit\": \"microseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LuxuryEAQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A dataset specifically designed for the luxury e-commerce Q&A use case, containing context-sensitive queries related to luxury products.\",\n            \"preprocessing\": [\"explainability layers, sentiment analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DistilBERT (with explainability layers)\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_target\": 0.99,\n                \"inference_speed\": \"sub-microsecond\",\n                \"load_capacity\": \"optimized under heavy load\"\n            },\n            \"description\": \"An advanced DistilBERT model enhanced with explainability layers for luxury product context understanding.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"integration with luxury e-commerce APIs\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"dynamic system updates\"],\n        \"demonstration\": \"A/B testing\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed according to the provided structure. High expertise indicates the user is familiar with advanced techniques and requirements. The performance metrics are clear, and the platform's specific features (explainability, sentiment analysis, response times, and dynamic updates) are accurately captured. However, some aspects, such as target devices and deployment details, are not explicitly specified, leaving room for further inquiry.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a specialist in dialectal semantics and cultural linguistics, our project requires the development of a sophisticated, dialect-aware neural machine translation system. Our goal is to create a highly specialized model tailored for understanding obscure regional variations and slang across different global regions, like Tokyo's Harajuku buzzwords or African pidgins. To achieve this, we need to design a system that not only translates queries but also employs sophisticated algorithms for language normalization, preserving context-specific humor and nuances. In implementing this, we must integrate with cutting-edge machine learning libraries and conduct thorough testing in niche communities. What exact programming libraries, cross-lingual data sources, and human-in-the-loop validation processes should we employ to ensure a flawless multilingual support network with local flavor?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"dialectal semantics, cultural linguistics\",\n        \"description\": \"Develop a sophisticated, dialect-aware neural machine translation system for understanding obscure regional variations and slang across global regions, with a focus on preserving context-specific humor and nuances. The project requires integration with cutting-edge ML libraries and human-in-the-loop validation in niche communities.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"cross-lingual\": true,\n                \"dialect-specific\": true\n            },\n            \"description\": \"Include cross-lingual and dialect-specific data, like Harajuku buzzwords and African pidgins\",\n            \"preprocessing\": [\"language normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"dialect-aware neural machine translation\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"models\": [\"seq2seq\", \"transformer\"],\n                \"language_normalization\": true\n            },\n            \"description\": \"A high-level model for dialect-aware translation, with emphasis on language normalization and preserving nuances\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integrate cutting-edge machine learning libraries (e.g., TensorFlow, Hugging Face Transformers, BERT, or M-BERT)\",\n        \"Collaborate with expert linguists for language norms and cultural context\",\n        \"Collect and curate a diverse and niche dataset for training and testing\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"local flavor integration\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"customized for dialectal translation\"],\n        \"demonstration\": \"Human-in-the-loop validation through community testing\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is parsed as a complex build project, highlighting high expertise. The task is in natural language processing for dialect-aware MT with specific requirements on ML libraries and human validation. The datasets are specified to include dialect and cross-lingual nuances, and the model architecture emphasizes normalization and context preservation. The missing fields will be filled based on common practice, such as integrating modern libraries and targeting cloud deployment.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "Request a specialized QA system from the ML development team, focusing on the MLQA dataset. mandating support for five languages (English, Spanish, Mandarin, French, and Arabic) with a premium on efficiency in low-resource scenarios. Demand exceptional performance using both mBERT and XLM-R, with a constraint of surpassing existing cross-lingual benchmarks by at least 5% in accuracy without compromising computational efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"cross-lingual\",\n        \"description\": \"Seeking a specialized QA system from the ML development team, targeting the MLQA dataset. The system must support English, Spanish, Mandarin, French, and Arabic languages, with a focus on efficiency in low-resource scenarios. Exceptional performance is required using both mBERT and XLM-R, with a goal to surpass existing cross-lingual benchmarks by at least 5% in accuracy without compromising computational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null,\n                \"benchmark_improvement\": {\n                    \"type\": \"percentage\",\n                    \"value\": 5\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"efficient\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MLQA\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_support\": [\n                    \"English\",\n                    \"Spanish\",\n                    \"Mandarin\",\n                    \"French\",\n                    \"Arabic\"\n                ]\n            },\n            \"description\": \"A dataset for cross-lingual question answering with support for five languages.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"mBERT and XLM-R\",\n            \"family\": [\"mBERT\", \"XLM-R\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"language_capabilities\": [\n                    \"English\",\n                    \"Spanish\",\n                    \"Mandarin\",\n                    \"French\",\n                    \"Arabic\"\n                ],\n                \"low_resource_effectiveness\": {\n                    \"type\": \"efficiency\"\n                },\n                \"benchmark_comparison\": {\n                    \"method\": \"cross-lingual\",\n                    \"comparison\": \"5% accuracy improvement\"\n                }\n            },\n            \"description\": \"Models used for the QA system with emphasis on mBERT and XLM-R for cross-lingual support.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Focus on low-resource scenarios\"\n        },\n        {\n            \"content\": \"Primarily target accuracy improvements of 5% over existing benchmarks\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Efficient and high-performance in low-resource scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction specified a clear build intent for a QA system targeting specific languages and datasets. The request for high performance with mBERT and XLM-R, along with the focus on efficiency and benchmark improvement, indicates a high level of expertise. The JSON response has been adapted to include these details and an assumption for some unspecified aspects.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for the \"Precision Healthcare AI Consortium,\" develop a cutting-edge Q&A platform tailored for neuro-oncology. Mandate the employment of BioBERT fine-tuning on the narrow-spectrum BioASQ neurology dataset, incorporating state-of-the-art graph-based named entity recognition. Integrate a unique \"Citation Explorer\" feature that links to real-time, peer-reviewed articles from PubMed Central and Scopus, specifically highlighting relevant neuroscience studies. The model must demonstrate exceptional proficiency in diagnosing intricate brain tumors and differentiating subtle variations of gliomas. Rigorously test the model using gold-standard annotations by expert radiologists and validate its performance in multiple international oncology conferences.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"precision healthcare, neuro-oncology\",\n        \"description\": \"Develop a cutting-edge Q&A platform for the Precision Healthcare AI Consortium, focusing on neuro-oncology. Employ BioBERT fine-tuning on the BioASQ neurology dataset with state-of-the-art graph-based named entity recognition. Add a 'Citation Explorer' that links to real-time, peer-reviewed neuroscience articles from PubMed Central and Scopus. The platform should excel at diagnosing brain tumors and differentiating glioma variations, with expert radiologist-annotated gold-standard data for testing and validation at international oncology conferences.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"diagnosis accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"glioma differentiation precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ neurology dataset\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"BERT_fine_tuning\": {\n                    \"dataset\": \"narrow-spectrum\",\n                    \"model\": \"BioBERT\"\n                },\n                \"graph-based_NER\": {\n                    \"approach\": \"state-of-the-art\"\n                }\n            },\n            \"description\": \"A dataset for neurology-specific question answering, fine-tuned on BioBERT and incorporating graph-based named entity recognition.\",\n            \"preprocessing\": [\n                \"BioBERT fine-tuning\",\n                \"graph-based NER\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"NeuroQ&A platform\",\n            \"family\": \"BioBERT fine-tuned QA\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"task_type\": \"question answering\",\n                \"architecture\": \"employing graph-based NER\"\n            },\n            \"description\": \"A platform using BioBERT fine-tuning for neuro-oncology Q&A, with a Citation Explorer feature.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Expert radiologist-annotated gold-standard annotations for testing\",\n        \"Integration with PubMed Central and Scopus for real-time citation exploration\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Citation Explorer integration\"\n        ],\n        \"target_device\": null,\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"peer-reviewed conference validation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction is detailed, specifying the platform requirements, dataset, and model. The high expertise level suggests a deep understanding of the problem and its constraints. However, the performance metrics are not provided. The user's intent to build implies a request for development. The 'target_device' in the service is left unspecified, requiring further clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a trailblazing project manager for the elite AI research and development team, your mandate is to confront a formidable challenge in the domain of energy consumption forecasting. Utilizing the vast and diverse Electricity dataset, divided into intricate subsets for training, validation, and rigorous testing, design an avant-garde deep-learning architecture specifically tailored for high-frequency data analysis. The complex input sequence, featuring 96 temporal steps with an extensive array of 321 unique attributes (INPUT_SEQ_LEN=96, INPUT_DIM=321), should not only predict the subsequent 192 time steps but also decipher subtle seasonal fluctuations, anomalies, and outliers with exceptional accuracy. Strive for unparalleled model efficiency by minimizing Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE), ensuring resilience in unseen scenarios. Implement an innovative hyperparameter optimization strategy, benchmarking against cutting-edge competitors, and present a comprehensive, data-driven analysis in a meticulously researched report, highlighting the competitive edge of your model over conventional methods.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy consumption\",\n        \"description\": \"Conduct a deep-learning project for energy consumption forecasting using the Electricity dataset. The model should handle high-frequency data with input sequence length of 96 and 321 attributes. The goal is to predict 192 future steps, discern seasonal fluctuations, anomalies, and outliers with high accuracy. Target metrics are MSE and MAPE minimization, and focus on model efficiency. Implement a robust hyperparameter optimization strategy, benchmark against competitors, and provide a data-driven analysis in a detailed report.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"PRED_SEQ_LEN\": 192,\n                \"additional_info\": \"High-frequency data, diverse subsets for training, validation, and testing\"\n            },\n            \"description\": \"A dataset for energy consumption forecasting with a complex input sequence and pre-divided subsets.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"custom deep learning architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for high-frequency data analysis\",\n                \"input_shape\": {\"sequence_length\": 96, \"features\": 321},\n                \"output_shape\": {\"sequence_length\": 192, \"features\": 321},\n                \"performance_target\": \"efficiency and minimal MSE & MAPE\"\n            },\n            \"description\": \"An advanced deep-learning architecture designed specifically for energy consumption prediction, considering seasonal fluctuations and anomalies.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"high-frequency data adaptation\", \"seasonal fluctuations\", \"outliers handling\"],\n        \"target_device\": \"server or cloud for high computational requirements\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for low latency and high accuracy\"],\n        \"demonstration\": \"using unseen data and highlighting the model's competitive edge\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed based on the provided details, assuming a high level of expertise from the user. Specific model details and performance targets are left for implementation. The service section accounts for the advanced nature of the project, while the confidence score is moderate due to the lack of some implementation details.\",\n        \"score\": 0.8\n    }\n}"}
{"task": "image classification", "instruction": "For an elite-level AI research initiative, design and implement a novel hybrid neuro-symbolic deep learning model, integrating both a cutting-edge Graph Neural Network (GNN) and a transformers module, aimed at not only identifying the subcategories in the challenging Mini-ImageNet dataset with an unprecedented accuracy of 99.5%, but also incorporating explainable AI principles to provide interpretability and feature importance analysis. The model should also explore transfer learning using few-shot learning methodologies to demonstrate exceptional adaptability and generalization across similar visual domains.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"elite AI research\",\n        \"description\": \"Design and implement a novel hybrid neuro-symbolic deep learning model, integrating a Graph Neural Network (GNN) and a transformers module. The model should achieve a high accuracy of 99.5% on the Mini-ImageNet dataset, be equipped with explainable AI principles for interpretability and feature importance analysis, and demonstrate few-shot learning capabilities for exceptional adaptability and generalization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mini-ImageNet\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"unspecified_dimension\": \"Challenging dataset\"\n            },\n            \"description\": \"A dataset for elite-level image classification research with high accuracy requirements.\",\n            \"preprocessing\": [\"None specified (assumes standard pre-processing for GNN and transformers)\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Neuro-Symbolic Model\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"integration\": [\"GNN\", \"Transformers\"],\n                \"interpretablity\": true,\n                \"explainability\": true,\n                \"few_shot_learning\": true\n            },\n            \"description\": \"A novel model combining GNN and transformers for high accuracy, interpretability, and few-shot learning capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Explainable AI principles to ensure interpretability\",\n        \"Novel integration of GNN and transformers for advanced performance\",\n        \"Few-shot learning methodologies for adaptability and generalization\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Interpretable and Explainable\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"explainability analysis\": \"generated alongside model predictions\",\n            \"few-shot learning results\": \"for new visual domains\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instructions specify a high-level and complex AI research project. The 'expertise' is interpreted as high based on the elite-level research requirement. The 'performance_metrics' and 'complexity_metrics' are tailored according to the strict accuracy and explainability goals. However, specific feature engineering, inference engines, and deployment details could be further clarified or left as a collaboration between experts and the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a seasoned wildlife conservation project director, urgently task the ML team to develop a cutting-edge, on-device object detection model based on the robust iWildCam Ultra dataset. Emphasize the deployment of the next-gen EfficientDetX architecture with advanced interpretability, fine-tuned for distinct wildlife features and optimized for low-latency, real-time performance under challenging arctic and desert conditions. Ensure compatibility with lightweight, rugged, and solar-powered devices for continuous, 24/7 monitoring of critically endangered species.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"Develop an on-device object detection model for a wildlife conservation project using the iWildCam Ultra dataset. The model should be based on the next-gen EfficientDetX architecture, with advanced interpretability, specifically fine-tuned for detecting distinct wildlife features in arctic and desert conditions. It should prioritize low-latency, real-time performance and be compatible with lightweight, rugged, and solar-powered devices for continuous 24/7 monitoring of critically endangered species.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"type\": \"real-time\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory footprint\",\n                \"value\": {\n                    \"unit\": \"MB\"\n                },\n                \"unit\": \"optimized\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": {\n                    \"unit\": \"milliseconds\"\n                },\n                \"unit\": \"low-latency\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"iWildCam Ultra\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"environment\": [\"arctic\", \"desert\"]\n            },\n            \"description\": \"A robust wildlife dataset for object detection, designed for challenging conditions.\",\n            \"preprocessing\": [\"fine-tuning for wildlife features\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientDetX\",\n            \"family\": \"EfficientDet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"next-gen\",\n                \"interpretability\": \"advanced\",\n                \"optimized_for\": \"low-latency, real-time\"\n            },\n            \"description\": \"A cutting-edge object detection model based on EfficientDetX, fine-tuned for wildlife and compatible with energy-efficient devices.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"fine-tuning for device compatibility\"],\n        \"target_device\": \"lightweight, rugged, solar-powered\",\n        \"deployment_endpoint\": \"wildlife monitoring system\",\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": \"real-time 24/7 monitoring of endangered species\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction parsed into a JSON response detailing a high-expertise request for developing an object detection model, specifying the dataset, architecture, and requirements for deployment and performance. The model's interpretability and compatibility with energy-constrained devices are accounted for. However, there's no specified value for accuracy metric. This could be clarified to ensure the model's performance meets expectations.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop an advanced deep learning model using the recently released Entorhinal Cortex Molecule Corpus (ECMC) for the highly specific task of molecular interaction prediction within the olfactory system. Implement Graph Neural Tensor Networks (GN-TN) to decipher intricate molecular patterns and pathways, ensuring high accuracy in distinguishing between functional receptor subtypes while accounting for subtle variations in gene expression and epigenetic signatures from the Human Olfactory Epigenome Atlas. Additionally, validate the model's transferability across different species, with a focus on mouse and human, while maintaining interpretability to facilitate biological insights.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"molecular biology, olfactory system\",\n        \"description\": \"Develop an advanced deep learning model using the ECMC dataset for molecular interaction prediction in the olfactory system, specifically focusing on Graph Neural Tensor Networks (GN-TNs). The model should achieve high accuracy in distinguishing between receptor subtypes and consider gene expression and epigenetic variations. Transferability to mouse and human is essential, along with model interpretability for biological understanding.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1_score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Entorhinal Cortex Molecule Corpus (ECMC)\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"molecular interactions\"],\n            \"specification\": {\n                \"node_features\": {\n                    \"dimension\": {\n                        \"genetic\": {},\n                        \"epigenetic\": {}\n                    },\n                    \"type\": \"molecules\"\n                },\n                \"edge_features\": {\n                    \"type\": \"interactions\"\n                },\n                \"number_of_classes\": \"functional receptor subtypes\"\n            },\n            \"description\": \"A recently released corpus for molecular interaction prediction, considering gene expression and epigenetic signatures in the human and mouse olfactory system.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Neural Tensor Network (GN-TN)\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"GN-TN architecture specifics\",\n                \"hidden_units\": null,\n                \"dropout_rate\": null\n            },\n            \"description\": \"A model designed to decipher molecular patterns and pathways using Graph Neural Tensor Networks for molecular interaction prediction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning will be applied to mouse and human datasets.\",\n        \"Interpretability methods (e.g., attention mechanisms or model explanation techniques) will be incorporated to provide biological insights.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"CPU for high-level analysis and potential transfer to more specialized devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"customized for GN-TN models\"],\n        \"demonstration\": \"Demonstration of model's performance with interpretability in distinguishing receptor subtypes\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed based on the schema. The high expertise level suggests the user is familiar with the task and the datasets. However, some performance metrics, complexity metrics, and model architecture details are missing and would need to be further clarified. The user specified transferability to mouse and human and emphasis on interpretability, which has been included in the response.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "Create a cutting-edge AI model for node classification in heterogeneous knowledge graphs derived from a blend of Wikipedia and DBpedia archives. Implement a Siamese Hypergraph Neural Network that seamlessly integrates both edge and node feature embeddings, targeting a minimum F1-score of 95% on novel node classes. Design the model with micro-budget considerations, ensuring compatibility for real-world deployment on a low-power Raspberry Pi, optimizing for energy efficiency. Conduct an in-depth exploration of five neighborhood sampling strategies and their impact on both model precision and computational efficiency, comparing Graph Convolution Networks like GAT, GraphSAGE, and HGNN, emphasizing the subtle trade-offs in each method's performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graphs\",\n        \"description\": \"Create a cutting-edge AI model for node classification in heterogeneous knowledge graphs derived from a blend of Wikipedia and DBpedia archives. Implement a Siamese Hypergraph Neural Network that integrates edge and node feature embeddings. Aim for a minimum F1-score of 95% on novel node classes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Model compatibility\",\n                \"value\": \"low-power Raspberry Pi\",\n                \"unit\": \"Hardware\"\n            },\n            {\n                \"name\": \"Energy efficiency\",\n                \"value\": \"optimized\",\n                \"unit\": \"Efficiency\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Heterogeneous Knowledge Graph (Wikipedia + DBpedia)\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node classes\"],\n            \"specification\": null,\n            \"description\": \"A knowledge graph dataset combining Wikipedia and DBpedia, representing heterogeneous information.\",\n            \"preprocessing\": [\n                \"heterogeneous graph preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Siamese Hypergraph Neural Network\",\n            \"family\": \"Hypergraph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"edge feature integration\": \"seamless\",\n                \"node feature integration\": \"Siamese\"\n            },\n            \"description\": \"A model that combines edge and node feature embeddings for node classification in knowledge graphs.\"\n        },\n        {\n            \"name\": \"Comparison of GAT, GraphSAGE, and HGNN\",\n            \"type\": \"comparison\",\n            \"description\": \"In-depth exploration of five neighborhood sampling strategies for GAT, GraphSAGE, and HGNN, analyzing precision and computational efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"subject\": \"Heterogeneous Knowledge Graphs\",\n            \"information\": \"The model deals with the complexities of mixed data types and relationships in a blended graph.\"\n        },\n        {\n            \"subject\": \"Micro-budget\",\n            \"information\": \"The model design prioritizes energy efficiency for deployment on a Raspberry Pi.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"low-power node feature extraction\",\n            \"energy-efficient training\"\n        ],\n        \"target_device\": \"Raspberry Pi\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for lightweight deployment\"\n        ],\n        \"demonstration\": \"exploration of trade-offs between model performance and computational resources\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear problem statement, indicating high expertise and specifying requirements for model architecture, performance, deployment, and evaluation. The 'feature_engineering' section was derived from the energy-efficient and micro-budget constraints, while other sections were filled with relevant details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Instruct the ML team to design an innovative image classifier for high-end fashion items in the exclusive Paris Fashion-MNIST dataset, employing avant-garde Dynamic Routing Capsule Networks. Demand a groundbreaking model that achieves at least 99% accuracy despite photorealistic distortions (like affine transformations), while also demonstrating superior resilience to adversarial attacks. Additionally, mandate a comprehensive analysis comparing its performance with the latest CNN architectures, focusing on not just accuracy but also latency, energy efficiency, and memory optimization for seamless large-scale operation in minimalist hardware budgets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"high-end fashion\",\n        \"description\": \"Design an innovative image classifier for exclusive Paris Fashion-MNIST dataset using Dynamic Routing Capsule Networks. The model should achieve at least 99% accuracy under photorealistic distortions and exhibit adversarial attack resilience. The analysis should compare its performance with state-of-the-art CNNs, focusing on accuracy, latency, energy efficiency, and memory optimization for minimal hardware budgets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": \"optimized\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": \"state-of-the-art\",\n                \"unit\": \"operations per joule\"\n            },\n            {\n                \"name\": \"memory optimization\",\n                \"value\": \"best for large-scale operation\",\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Paris Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"class label\"],\n            \"specification\": {\n                \"photo-realistic distortions\": \"affine transformations\"\n            },\n            \"description\": \"A dataset for high-end fashion image classification, featuring exclusive fashion items.\",\n            \"preprocessing\": [\"photorealistic distortions\"],\n            \"augmentation\": [\"affine transformations\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dynamic Routing Capsule Networks\",\n            \"family\": \"Capsule Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"resilience\": [\"adversarial attacks\"]\n            },\n            \"description\": \"An avant-garde image classifier designed for the Paris Fashion-MNIST dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Include a performance comparison with latest CNN architectures and highlight efficiency improvements.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"minimalist hardware budget\"],\n        \"target_device\": [\"small form-factor devices\", \"cloud\"],\n        \"deployment_endpoint\": [\"previously optimized for deployment\"],\n        \"inference_engine\": [\"optimized for latency and energy consumption\"],\n        \"demonstration\": \"comparison with competing models\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear request for a high-performance image classifier that compares with CNNs. The task is specified for high-end fashion in the Paris Fashion-MNIST dataset using Capsule Networks. The user's high expertise level suggests they expect detailed response. Performance metrics and complexities are included, while some details such as precise deployment engine are left for assumption.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "Incorporate a complex medical hierarchy into the project, requiring the ML team to develop a highly specialized Q&A model for advanced disease diagnosis using the comprehensive BioASQ dataset. Mandate the fine-tuning of a progressive BioBERT variant, ensuring not only interpretability but also incorporating references from prestigious indexed journals. Strive for steady, cumulative performance uplift in each model iteration, accompanied by rigorous validation with expert clinical feedback.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medical\",\n        \"description\": \"The project requires the development of a highly specialized Q&A model for advanced disease diagnosis using the BioASQ dataset. It involves fine-tuning a progressive BioBERT variant with interpretability in mind and incorporating references from indexed journals. The focus is on steady performance improvement and expert clinical validation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"model accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"disease diagnosis questions\", \"answers\", \"references\"],\n            \"specification\": null,\n            \"description\": \"A comprehensive dataset for advanced disease diagnosis with questions, expert answers, and indexed journal references.\",\n            \"preprocessing\": [\"text normalization\", \"punctuation removal\", \"lemmatization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"confusion matrix\", \"learning curves\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"progressive BioBERT variant\",\n            \"family\": \"BioBERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true,\n                \"journal_references\": true\n            },\n            \"description\": \"A fine-tuned BioBERT model with focus on interpretability and reference integration for advanced disease diagnosis\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"word embeddings\", \"contextual embeddings\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"on-device\", \"cloud\"],\n        \"demonstration\": \"expert clinical demonstration and validation at each model iteration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction parsed correctly capturing the advanced medical Q&A focus, the need for interpretability, BioBERT fine-tuning, and expert validation. The user's high expertise level aligns with the requirement for specialized knowledge. However, specific performance metrics and certain service details (like target device and deployment endpoint) were not provided and assumed to be flexible.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "For the \"Quantum Social Web Integration Challenge,\" develop a novel Quantum-enhanced Spatio-Temporal Graph Neural Network (QSTGNN) with self-organizing subgraphs and quantum entanglement-based edge learning. The model should not only analyze encrypted online interactions from multiple social platforms in real-time, but also predict latent friendships and ideological congruence among students across multiple academic institutions, while preserving user privacy with post-quantum cryptography. The system must also generate interactive 3D holographic graphs illustrating the emergence of intellectual communities and cross-university trends over quarterly intervals, ensuring compliance with GDPR and ensuring transparency in data usage.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"social web integration\",\n        \"description\": \"Develop a novel Quantum-enhanced Spatio-Temporal Graph Neural Network (QSTGNN) for the 'Quantum Social Web Integration Challenge'. The model should analyze encrypted online interactions from multiple social platforms in real-time, predict latent friendships and ideological congruence among students across institutions, and preserve user privacy with post-quantum cryptography.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latent friendships prediction\",\n                \"value\": null\n            },\n            {\n                \"name\": \"ideological congruence prediction\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Quantum Social Web Integration Challenge dataset\",\n            \"modality\": [\"text\", \"graph\", \"time series\"],\n            \"target_variables\": [\"latent friendships\", \"ideological congruence\"],\n            \"specification\": {\n                \"encrypted_data\": true,\n                \"social_platforms\": \"multiple\",\n                \"real-time_data\": true,\n                \"social_network_structure\": \"encrypted online interactions\"\n            },\n            \"description\": \"Dataset containing encrypted interactions and network structure from multiple social platforms.\",\n            \"preprocessing\": [\"data decryption\", \"post-quantum cryptography\"],\n            \"augmentation\": [],\n            \"visualization\": [\"interactive 3D holographic graphs\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"QSTGNN\",\n            \"family\": \"quantum-enhanced graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"self-organizing subgraphs\": true,\n                \"quantum entanglement-based edge learning\": true\n            },\n            \"description\": \"A model that combines self-organizing subgraphs and quantum entanglement for analyzing and predicting social interactions.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"privacy-preserving techniques\"],\n        \"target_device\": \"compliant with GDPR\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"post-quantum cryptography\"],\n        \"demonstration\": \"3D holographic graphs illustrating intellectual communities and cross-university trends\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is complex, requiring high expertise in quantum machine learning. The user's intent is to build a model with specific design elements and data handling. The lack of explicit target device or deployment endpoint, as well as metric values, indicates some gaps that might need clarification. However, the critical details for model architecture and privacy preservation are clear.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "object detection", "instruction": "For the elite gourmet boutique grocery venture, establish a multifaceted deep learning pipeline using the most recent YOLOv5.1 architecture. The model should undergo extensive domain adaptation on the exclusive \"BoutiqueFoods Shelf Insights Dataset,\" containing an extensive 150 diverse premium product categories. The system must not only demonstrate superior performance with a targeted precision of 95% and recall of 93%, but also include a real-time inventory management module. This module must process high-resolution images in parallel, guaranteeing instantaneous updates for artisanal goods, with a stringent requirement of not exceeding 1.5 milliseconds per image for any delay in updating the inventory database.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"boutique grocery\",\n        \"description\": \"Establish a multifaceted deep learning pipeline using the latest YOLOv5.1 architecture for an elite gourmet boutique grocery venture. The pipeline should undergo extensive domain adaptation on the exclusive 'BoutiqueFoods Shelf Insights Dataset' with 150 diverse premium product categories.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.93\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 1.5,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoutiqueFoods Shelf Insights Dataset\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"diversity\": {\n                    \"categories\": 150\n                },\n                \"type\": \"exclusive for gourmet boutique\"\n            },\n            \"description\": \"A high-resolution dataset for domain adaptation with a focus on 150 premium product categories in the gourmet boutique context.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5.1\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptation_target\": \"boutique grocery\"\n            },\n            \"description\": \"The state-of-the-art YOLOv5.1 model optimized for object detection in a gourmet boutique grocery setting.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time inventory management\",\n            \"high-resolution image processing\"\n        ],\n        \"demonstration\": {\n            \"inventory update delays\": \"1.5 milliseconds per image\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is parsed assuming a 'build' intent with a high level of expertise from the user. It specifies a deep learning pipeline for object detection in a boutique grocery setting. The performance metrics, precision and recall, are clearly defined. The real-time inventory management module and its performance constraint are incorporated. Domain adaptation on the BoutiqueFoods Shelf Insights Dataset is mentioned. However, the target device and deployment endpoint are not directly specified and assumed to be determined based on user requirements.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image classification", "instruction": "As a forward-thinking project manager, task the interdisciplinary AI development team with a transformative upgrade for Galaxy Zoo's image classifier. Demmand the implementation of a state-of-the-art ViT-XT architecture, enhanced with progressive feature fusion at multiple scales and robust adversarial defense mechanisms. Prioritize resilience to high-dimensional noise, mandating a minimum 97% precision on the filtered validation dataset, and push for an AI model that can not only detect but accurately classify unconventional celestial events, fostering groundbreaking discoveries in astrophysics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy (Galaxy Zoo)\",\n        \"description\": \"As a forward-looking project manager, task the interdisciplinary AI development team with a transformative upgrade for Galaxy Zoo's image classifier. The new model should utilize a state-of-the-art ViT-XT architecture, featuring progressive feature fusion across multiple scales and robust adversarial defense mechanisms. The focus should be on resilience to high-dimensional noise with a minimum requirement of 97% precision on the filtered validation dataset, and the model should excel at detecting and classifying unconventional celestial events, driving novel astrophysical discoveries.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo dataset (filtered)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_format\": \"unknown\",\n                \"dimensionality\": \"high-dimensional\"\n            },\n            \"description\": \"The filtered dataset from Galaxy Zoo, containing images for the upgraded image classifier, with focus on unconventional celestial events.\",\n            \"preprocessing\": [\"noise filtering\", \"data augmentation\"],\n            \"augmentation\": [\"high-dimensional noise\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ViT-XT\",\n            \"family\": \"Vision Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ViT-XT\",\n                \"feature_fusion\": \"progressive multi-scale\",\n                \"adversarial_defense\": \"robust\"\n            },\n            \"description\": \"A state-of-the-art transformer-based image classifier with ViT-XT architecture, integrating progressive feature fusion and adversarial defense.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interdisciplinary team\",\n        \"Celestial event classification\",\n        \"Adversarial resilience\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"noise filtering\", \"data augmentation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"Galaxy Zoo platform\",\n        \"inference_engine\": [\"custom built for high performance\"],\n        \"demonstration\": \"Unconventional celestial event classification demo\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction is clear on the need for a highly specialized AI development project, specifying the domain, architecture, and performance metrics. The team's high expertise indicates they'll need guidance on specific implementation details. The dataset is assumed to be filtered and high-dimensional. The missing components like specific device and deployment target are placeholders to be filled based on domain context. The confidence score is high due to the clear task and the provided information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Our team has been tasked with developing a specialized image classification system for the narrowed-down Stanford Cars Subset, focusing on obscure models and vintage car variants. We must employ Inception-v3 with advanced augmentation techniques to handle extreme weather conditions, extreme angles (from bird's eye to low angles), and distinctive historical graffiti backgrounds. The model's efficiency must be evaluated using a comprehensive confusion matrix, reporting per-class macro F1 score, and achieving a minimum mAP (mean average precision) of 97% across different eras of car production.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"automotive\",\n        \"description\": \"Our team is assigned to develop a specialized image classification system for the Stanford Cars Subset focusing on rare and vintage car models. The system must employ Inception-v3, with advanced augmentation techniques for extreme weather, angles (bird's eye to low angles), and historical graffiti backgrounds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"confusion matrix\",\n                \"value\": null\n            },\n            {\n                \"name\": \"per-class macro F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mAP (mean average precision)\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Subset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"class label\"],\n            \"specification\": {\n                \"data_split\": [\"train\", \"validation\"],\n                \"specific_features\": \"narrowed-down to obscure models and vintage car variants, includes extreme weather conditions, angles, and historical graffiti backgrounds\"\n            },\n            \"description\": \"Dataset with a focus on rare car models and vintage variants, catering to diverse and challenging scenarios.\",\n            \"preprocessing\": [\n                \"Advanced augmentation techniques for extreme weather conditions, angles, and graffiti backgrounds\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Inception-v3\",\n            \"family\": \"Inception\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"incorporates handling of diverse image scenarios\"\n            },\n            \"description\": \"Employed for image classification, with adjustments for handling unique challenges\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"advanced techniques for handling extreme conditions\"],\n        \"target_device\": \"unknown (not specified in the instruction)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was generated based on the provided instruction, considering the high expertise level of the user. It includes a comprehensive description of the problem and task, performance metrics, and augmentation techniques. However, missing details are noted where required, such as the deployment target device and specific deployment steps, as they were not explicitly specified in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "For a challenging regression project, you must develop a robust model using the Wild Blueberry Yield Dataset with 100 additional irrelevant features. Ensure interpretability while maintaining a high precision. The train, validation, and test splits consist of imbalanced classes, and you must minimize MAE, achieving at least 95% relative improvement over baseline models.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Develop a robust regression model for a challenging project using the Wild Blueberry Yield Dataset with 100 additional irrelevant features. Emphasize interpretability while maintaining high precision.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MAE\",\n                \"value\": null,\n                \"target_improvement\": \"95%\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wild Blueberry Yield\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"yield\"],\n            \"specification\": {\n                \"features_with_irrelevant\": 100,\n                \"imbalance_class_distribution\": true\n            },\n            \"description\": \"A tabular dataset for a regression project with imbalanced classes and 100 irrelevant features.\",\n            \"preprocessing\": [\n                \"feature selection\",\n                \"imbalance handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"interpretable neural networks or ensemble models\",\n            \"specification\": {\n                \"explainability\": true\n            },\n            \"description\": \"A model capable of robust regression with interpretability and precision targeting high performance on the Wild Blueberry Yield Dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature scaling\",\n            \"regularization\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"interpretability visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction has been parsed assuming a high level of expertise. The problem is focused on a regression task with the specified dataset, and the importance of interpretability and performance improvement is captured in the performance metrics and preprocessing steps. However, some assumptions have been made, such as choosing the type of model and specifying the model's interpretation features.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, devise a state-of-the-art multilingual emotion-preserving translation system using ParaCrawl, necessitating the integration of a robust dual encoder architecture. The system must exhibit high accuracy, context sensitivity, and adaptability in capturing nuanced emotions across English to Dutch translations, with a focus on preserving cultural subtleties.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual translation\",\n        \"description\": \"As a project manager, the task is to build a state-of-the-art multilingual emotion-preserving translation system using ParaCrawl, involving a dual encoder architecture. The system must demonstrate high accuracy, context sensitivity, and adaptability for nuanced emotion capture in English to Dutch translations while preserving cultural subtleties.\",\n        \"performance_metrics\": {\n            \"accuracy\": {\n                \"value\": \"high\"\n            }\n        },\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\n                \"English to Dutch\"\n            ],\n            \"specification\": {\n                \"language_pair\": {\n                    \"source\": \"English\",\n                    \"target\": \"Dutch\"\n                },\n                \"multilingual\": true,\n                \"emotion_preservation\": true\n            },\n            \"description\": \"ParaCrawl dataset for multilingual emotion-preserving translation, focusing on English to Dutch and emphasizing cultural subtlety preservation.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"dual encoder architecture\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"dual encoder\",\n                \"emotion_detection\": true,\n                \"contextual_similarity\": true,\n                \"cultural_subtlety preservation\": true\n            },\n            \"description\": \"A state-of-the-art dual encoder architecture designed to handle emotion-preserving translation between English and Dutch, ensuring high accuracy and cultural sensitivity.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Multilingual models should account for cross-lingual transfer learning and domain adaptation.\",\n        \"Emotion preservation requires sentiment analysis and fine-tuning for specific language nuances.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Emphasis on visualizing emotion preservation and translation accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided information was parsed with a high level of expertise (high), as the task involves a state-of-the-art system and specific architectural requirements. The problem area, task, and dataset are clearly defined. However, we made assumptions regarding the target device and inference engine, as they weren't specified. The confidence score is high based on the completeness of the provided details.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for a niche machine learning team, develop a real-time, high-precision cat species classifier that differentiates between subspecies using fine-grained visual features, while ensuring compatibility with limited computational resources for mobile deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal classification, specifically cats\",\n        \"description\": \"The project is to create a real-time cat species classifier that differentiates between subspecies using fine-grained visual features. It should be compatible with limited computational resources for mobile deployment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"N/A (yet to be specified)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"cat species/subspecies\"],\n            \"specification\": null,\n            \"description\": \"A dataset for fine-grained cat species classification with a focus on mobile deployment\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Real-time Cat Species Classifier\",\n            \"family\": \"Convolutional Neural Networks (CNN) or possibly MobileNet for computational efficiency\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": {\n                    \"feature extraction\": \"MobileNet or similar for resource constraints\",\n                    \"classifier\": {\n                        \"type\": \"fine-grained, possibly using attention mechanisms\"\n                    }\n                },\n                \"number of parameters\": null,\n                \"FLOPs\": null\n            },\n            \"description\": \"A high-precision model for real-time cat species and subspecies classification, optimized for mobile devices.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"name\": \"Fine-grained visual features\",\n            \"description\": \"Understanding and extracting unique features specific to each cat subspecies.\"\n        },\n        {\n            \"name\": \"Mobile-friendly architecture\",\n            \"description\": \"Experience in designing and optimizing models for resource-constrained mobile devices.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time inference\"],\n        \"demonstration\": \"Mobile app demonstration for real-time classification\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction outlines a project to build a real-time classifier, and the high expertise level suggests the user has a deep understanding of the requirements. However, specific metrics, model architecture details, and dataset names are not provided, which may need further clarification. The service section is presumptively filled based on the mobile deployment requirement.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "For our upcoming project, we aim to develop a cutting-edge transformer model using the massive Common Crawl dataset. The focus is on creating a multilingual translation system capable of converting not only formal but also highly colloquial English and Japanese texts with a stringent benchmark of 97% accuracy, considering cultural nuances and idiom preservation. Additionally, please design the model to adapt dynamically to evolving language trends and optimize it for real-time, large-scale deployments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual translation\",\n        \"description\": \"The goal is to develop a cutting-edge transformer model using the Common Crawl dataset. The model should handle informal and colloquial translations between English and Japanese, with a benchmark accuracy of 97% while preserving cultural nuances and idioms. The model should also be adaptable to evolving language trends and optimized for real-time, large-scale deployments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"massive\",\n                \"linguistic_features\": [\"English\", \"Japanese\", \"colloquial\"]\n            },\n            \"description\": \"A large multilingual dataset sourced from Common Crawl for training a translation system\",\n            \"preprocessing\": [\n                \"data cleaning for informal text\"\n            ],\n            \"augmentation\": [\n                \"text expansion for capturing evolving language trends\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"language_capabilities\": [\"formal\", \"colloquial\"],\n                \"accuracy_target\": 0.97,\n                \"dynamic_adaptation\": true,\n                \"real_time_inference\": true,\n                \"large_scale_deploy\": true\n            },\n            \"description\": \"A state-of-the-art transformer model designed for multilingual translation with emphasis on preserving cultural nuances and idiom.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cultural preservation and idioms handling are crucial for effective translation.\",\n        \"Evolving language trends impact translation accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware adaptation techniques\"\n        ],\n        \"target_device\": \"cloud and edge for real-time deployments\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for specific hardware accelerators\"\n        ],\n        \"demonstration\": \"real-time translation of varying text inputs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction provides detailed requirements, including the target languages, accuracy, cultural nuances, adaptability, and deployment aspects. The high expertise level suggests the user may need support in implementation details. The JSON response reflects this balance of specifics and guidance.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "For the specialized Amazonian rainforest conservation initiative, develop a state-of-the-art MViT (Multi-scale Vision Transformer) model trained exclusively on the highly-granular and geographically diverse Rio Negro Basin Landsat 8 dataset. The model should discern between rare vegetation classes (like lowland heath and emergent vegetation), water bodies, and densely populated indigenous communities, while maintaining a minimum 96% F1-score. Ensure model interpretability, resilience to seasonal variations, and optimize it for real-time deployment on Google Cloud's Kubernetes with hourly auto-scaling, accompanied by a comprehensive performance dashboard and GPU utilization analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"Amazon rainforest conservation\",\n        \"description\": \"Develop a state-of-the-art MViT model for specialized Amazonian rainforest conservation, focusing on the Rio Negro Basin Landsat 8 dataset. The model should classify rare vegetation (lowland heath and emergent vegetation), water bodies, and densely populated indigenous communities, maintaining at least a 96% F1-score.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.96\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time deployment\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"GPU utilization\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"hourly auto-scaling\",\n                \"value\": true\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Rio Negro Basin Landsat 8\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"rare vegetation\", \"water bodies\", \"indigenous communities\"],\n            \"specification\": {\n                \"geo_diversity\": \"highly granular and geographically diverse\"\n            },\n            \"description\": \"A dataset containing Landsat 8 satellite imagery for the Amazon rainforest's Rio Negro Basin, focusing on rare vegetation classification and environmental features.\",\n            \"preprocessing\": [\n                \"georeferencing\",\n                \"image normalization\"\n            ],\n            \"augmentation\": [\n                \"geospatial transformations\"\n            ],\n            \"visualization\": [\n                \"spectral analysis\",\n                \"change detection\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MViT (Multi-scale Vision Transformer)\",\n            \"family\": \"Vision Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"seasonal_variations_resilience\": true\n            },\n            \"description\": \"A state-of-the-art model designed for multi-class classification in the rainforest conservation context.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Seasonal variations and environmental challenges in remote sensing data\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"Google Cloud's Kubernetes\",\n        \"deployment_endpoint\": \"Google Cloud Platform\",\n        \"inference_engine\": [\n            \"GPU-optimized for real-time deployment\"\n        ],\n        \"demonstration\": \"Performance dashboard and GPU utilization analysis for continuous monitoring\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the instruction, with a clear high level of expertise. The model specifications align with the task and performance metrics. However, it is assumed that the user's specific knowledge is implicitly included in the 'knowledge' field and that some aspects like auto-scaling and deployment platform details are outlined. Confidence is high, but not 1.0 due to the lack of explicit details on auto-scaling settings and GPU utilization thresholds.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "For a high-performance, real-time machine translation system, develop a state-of-the-art Bidirectional Long Short-Term Memory (Bi-LSTM) architecture specifically optimized for Spanish-English translations. Utilize the parallelized Europarl v7 dataset with domain-specific tokenization and advanced quantization techniques to minimize latency, targeting sub-300ms response times in a resource-constrained embedded environment. Implement a novel attention mechanism for enhanced context retention and latency optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"real-time, embedded environment\",\n        \"description\": \"Develop a high-performance, real-time machine translation system using a state-of-the-art Bidirectional Long Short-Term Memory (Bi-LSTM) architecture, specifically for Spanish-English translations. Focus on optimizing with parallelized Europarl v7 dataset, domain-specific tokenization, and advanced quantization techniques for sub-300ms response times in a resource-constrained environment. Incorporate a novel attention mechanism for enhanced context retention and latency reduction.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.3,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Europarl v7\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"parallelism\": \"parallelized\",\n                \"domain_tokenization\": \"true\",\n                \"quantization_techniques\": \"advanced\"\n            },\n            \"description\": \"A parallel text corpus for Spanish-English translation with domain-specific preprocessing and focus on reducing latency.\",\n            \"preprocessing\": [\"domain-specific tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art Bi-LSTM for Spanish-English\",\n            \"family\": \"Bi-LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention_mechanism\": \"novel\",\n                \"latency_optimization\": true\n            },\n            \"description\": \"A specialized Bi-LSTM model optimized for real-time machine translation with sub-300ms response time.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"The use of novel attention mechanisms and advanced quantization techniques are key for high performance in resource-constrained environments.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"resource-constrained optimization\"],\n        \"target_device\": \"embedded\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"latency-optimized\"],\n        \"demonstration\": \"sub-300ms response time for Spanish-English translations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction clearly specifies the task (model building for Spanish-English translation), expertise level, and details such as dataset, model architecture, and optimization techniques. Performance metrics are absent but can be inferred from the target response time. The confidence score is high due to the clear details provided, but there is a possibility that additional performance requirements or deployment specifics may be needed.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "The project's objective is to design a sophisticated smart city parking management system, leveraging the M2Det model within the YOLOv5 architecture. You must enhance the existing 'PKLot' dataset with low-light and adverse weather scenarios, targeting at least 95% multi-conditional detection accuracy and guaranteeing sub-100ms response times, while ensuring energy efficiency for edge devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city parking management\",\n        \"description\": \"The project aims to design a sophisticated smart city parking management system using the M2Det model within the YOLOv5 architecture. Focus is on enhancing the 'PKLot' dataset for low-light and adverse weather conditions, targeting a minimum of 95% multi-conditional detection accuracy and ensuring sub-100ms response times, with energy efficiency for edge devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"multi-conditional detection accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 100.0,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PKLot\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"enhancements\": [\n                    \"low-light conditions\",\n                    \"adverse weather scenarios\"\n                ]\n            },\n            \"description\": \"The dataset will be enhanced to handle low-light and adverse weather situations for a smart city parking management system.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M2Det (within YOLOv5)\",\n            \"family\": \"YOLOv5\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"energy_efficiency\": {\n                    \"target_device\": \"edge devices\"\n                }\n            },\n            \"description\": \"A state-of-the-art object detection model for smart city parking management, leveraging M2Det within the YOLOv5 framework.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"low-light and adverse weather performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction indicates a machine learning project with a clear intent to build a sophisticated system. The high expertise level suggests the user has deep understanding of the requirements. Performance metrics are specified, including detection accuracy and response time. Enhancing the dataset for edge devices' energy efficiency is also mentioned. The confidence level is high since the JSON structure matches the schema and captures the relevant details.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "object detection", "instruction": "The project for a revolutionary sports event analytics platform necessitates a highly specialized machine learning model derived from the 'DeepSportX-1M' dataset. The model, an enhanced RetinaNet++ variant, focuses on real-time identification of precise equipment types and intricate player maneuvers, prioritizing sub-millisecond latency for seamless integration into high-definition, global sports broadcasts. Its design must cater to the unmet demands of broadcast engineers and enhance fan engagement in sports commentary.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"sports analytics\",\n        \"description\": \"The project aims to develop a highly specialized machine learning model for a sports event analytics platform. It requires a RetinaNet++ variant derived from the 'DeepSportX-1M' dataset, focusing on real-time equipment identification and player maneuvers with sub-millisecond latency for seamless sports broadcasts and improved fan engagement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.001\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"sub-millisecond\",\n                \"value\": true\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepSportX-1M\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {},\n            \"description\": \"A dataset for sports event analysis, focusing on real-time identification of equipment and player actions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"enhanced RetinaNet++\",\n            \"family\": \"RetinaNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"latency\": {\n                    \"optimized\": \"for sub-millisecond\"\n                },\n                \"accuracy\": {\n                    \"requirements\": \"for equipment and player maneuvers\"\n                }\n            },\n            \"description\": \"A customized variant of the RetinaNet++ model designed for real-time sports event analytics with low latency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Sub-millisecond latency is crucial for seamless integration into high-definition broadcasts\",\n        \"Priority on equipment identification and player maneuvers\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance computing for real-time processing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"description\": \"seamless integration and fan engagement\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clear about the project's goal and specifics, like the dataset, model variant, and requirements. The 'high' expertise level implies a deep understanding of the domain. The missing details will be filled in by specialized engineers based on the specified criteria.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "Construct a highly advanced, deep learning model based on the fusion of DTW and Long Short-Term Memory (LSTM) networks for the Sign Language Time Series dataset. The model must achieve exceptional precision, recall, and interpretability, differentiating between nuanced variations of gestures with temporal and spatial awareness, ensuring seamless and efficient communication for a diverse hearing-impaired community, while also optimizing for computational efficiency and real-time performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"sign language recognition\",\n        \"description\": \"The task is to construct a highly advanced deep learning model that combines Dynamic Time Warping (DTW) and Long Short-Term Memory (LSTM) networks, specifically for the Sign Language Time Series dataset. The model should excel in precision, recall, and interpretability, recognizing nuanced gesture variations with temporal and spatial awareness, and support real-time communication for a diverse hearing-impaired community.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sign Language Time Series\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"gesture\"],\n            \"specification\": {\n                \"temporal_and_spatial_features\": true,\n                \"gesture_variations\": \"nuanced\"\n            },\n            \"description\": \"The dataset containing time series data of sign language gestures, suitable for classifying nuanced variations with temporal and spatial awareness.\",\n            \"preprocessing\": [\n                \"extract_temporal_features\",\n                \"extract_spatial_features\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"show gesture trajectory\",\n                \"visualize gesture differences\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DTW-LSTM Fusion Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"DTW\": {},\n                    \"LSTM\": {}\n                },\n                \"optimization\": \"computational efficiency and real-time performance\",\n                \"interpretability\": \"enhanced\"\n            },\n            \"description\": \"A deep learning model combining DTW and LSTM for the Sign Language Time Series dataset, optimizing for precision, recall, and interpretability while maintaining efficiency for real-time recognition.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model should account for the nuances of sign language and the importance of temporal and spatial relationships between gestures.\",\n        \"The hearing-impaired community requires seamless and efficient communication.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"merge DTW and LSTM features\"],\n        \"target_device\": \"high-performance computing or real-time environments\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"live sign language recognition demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction provides a clear task to build a complex model, with specific requirements on the dataset (including extraction and visualization) and the model (fusing DTW and LSTM). Their high expertise suggests a thorough understanding of the task. However, exact metric values and deployment details are not provided, leaving room for assumptions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "The project, now with stricter performance expectations (92%+ F1), aims to develop a state-of-the-art HAN model for arXiv paper classification. We need a scalable system that not only differentiates between main fields and subfields with precision but also demonstrates incremental improvements in accuracy with real-time updates, considering domain evolution and handling noisy data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"academic paper classification\",\n        \"description\": \"The project aims to develop a state-of-the-art Hierarchical Attention Network (HAN) for arXiv paper classification with a focus on achieving 92%+ F1 score. The system must be scalable, distinguish between main fields and subfields with high precision, demonstrate incremental improvements in accuracy with real-time updates, and handle domain evolution and noisy data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1\",\n                \"value\": 0.92\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv (paper classification)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"main field\", \"subfield\"],\n            \"specification\": {\n                \"domain_level\": \"arXiv\",\n                \"domain_difficulty\": \"evolving\",\n                \"noise_level\": \"high\"\n            },\n            \"description\": \"A dataset for arXiv paper classification with real-time and evolving data characteristics.\",\n            \"preprocessing\": [\"noise reduction\", \"domain adaptation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Attention Network (HAN)\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_target\": \"state-of-the-art\",\n                \"architecture\": \"HAN\",\n                \" Scalability\": \"yes\"\n            },\n            \"description\": \"A top-performing HAN model for arXiv paper classification with strict performance requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"stricter performance expectations, evolving domain, and noisy data handling are critical factors.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"incremental learning\", \"online updates\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"real-time classifier API\",\n        \"inference_engine\": [\"GPU-accelerated\", \"optimized for large datasets\"],\n        \"demonstration\": \"Real-time classification with evolving arXiv data showcasing improved accuracy.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction was parsed for a complex machine learning project that requires a high level of expertise. The focus is on building a state-of-the-art HAN model for arXiv classification with specific performance expectations and additional requirements. Some assumptions were made regarding the dataset and source, such as noisy data handling and the need for real-time updates through a cloud endpoint.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "question answering", "instruction": "For a large-scale project, design a scalable, high-performance question-answering system utilizing the TriviaQA dataset. The system should necessitate the integration of advanced transformer architecture, potentially considering variants like GPT-4, to address both fact-based and intricate inquiries efficiently. Aim for sub-second response times while ensuring the model can efficiently manage an unprecedented query load and demonstrate multi-step reasoning prowess.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"large-scale project\",\n        \"description\": \"Design a scalable, high-performance question-answering system utilizing the TriviaQA dataset. The system should incorporate advanced transformer architecture, such as GPT-4, to handle fact-based and complex queries with sub-second response times. It must be capable of managing a large query load and exhibit multi-step reasoning abilities.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": {\"type\": \"sub-second\"},\n                \"description\": \"Target sub-second response times for the system.\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"query load management\",\n                \"value\": \"high\",\n                \"unit\": \"queries per second\"\n            },\n            {\n                \"name\": \"memory efficiency\",\n                \"value\": null, \n                \"unit\": \"RAM or computational requirements\"\n            },\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"time to process a query\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TriviaQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"scale\": \"large\",\n                \"diversity\": \"variety of fact-based and intricate questions\"\n            },\n            \"description\": \"A large-scale dataset for question answering, containing diverse fact-based and complex questions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GPT-4 or advanced transformer variant\",\n            \"family\": \"transformer architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"large-scale\",\n                \"reasoning_layers\": \"multi-step\",\n                \"latency\": {\n                    \"inference_time\": null, \n                    \"response_time\": \"<1 second\"\n                }\n            },\n            \"description\": \"A scalable and high-performance model built using the latest transformer architecture for efficient question answering.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced transformer architecture, multi-step reasoning, large query load management\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or distributed infrastructure\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\", \"parallel processing\"],\n        \"demonstration\": \"showcasing multi-step reasoning and high throughput\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction clearly outlines a large-scale project with a focus on a high-performance question-answering system. It specifies the use of the TriviaQA dataset, advanced transformers, and emphasizes on efficiency, scalability, and multi-step reasoning. Some explicit metric values for response time and query load management are assumed, as they were not directly provided but can be inferred based on the requirements. Overall, the confidence in the parsed JSON is high given the detail in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "For the CORA dataset, develop a highly optimized and state-of-the-art Graph Convolutional Network (GCN) architecture designed specifically for intricate node classification, targeting an accuracy above 90%. Incorporate regularization techniques to prevent severe overfitting, while adhering to a stringent constraint of no more than 70% increase in training time compared to the baseline model, ensuring efficient performance on a standard GPU within a 2.5-hour window.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"graph data\",\n        \"description\": \"Develop a highly optimized and state-of-the-art Graph Convolutional Network (GCN) for intricate node classification on the CORA dataset. The target accuracy should be above 90%, and the model must incorporate regularization techniques to prevent overfitting. Additionally, the training time must not exceed a 70% increase compared to the baseline model, ensuring efficient performance on a standard GPU within a 2.5-hour time frame.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": 1.7,\n                \"unit\": \"times the baseline model\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CORA\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"nodes\": {\n                    \"features\": null,\n                    \"classes\": null\n                },\n                \"edges\": {\n                    \"type\": null\n                },\n                \"split\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                }\n            },\n            \"description\": \"A dataset for intricate node classification tasks, suitable for Graph Convolutional Networks.\",\n            \"preprocessing\": [\"data normalization\", \"feature scaling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Optimized State-of-the-Art GCN\",\n            \"family\": \"Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"regularization techniques\": [\"dropout\", \"weight decay\"],\n                \"overfitting prevention\": true\n            },\n            \"description\": \"Specifically designed for intricate node classification on the CORA dataset, targeting a 90%+ accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"normalization\",\n            \"rescaling\"\n        ],\n        \"target_device\": \"GPU\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": {\n            \"time_limit\": \"2.5 hours\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been carefully parsed based on the provided instruction. It covers the user's intent, problem definition, requirements for accuracy, regularization, and time constraints, as well as the details for the dataset and the optimized GCN model. The missing information, such as the exact node feature and class count in the dataset, has been assumed as not directly specified in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "For this advanced machine learning project, you are tasked with developing a state-of-the-art time-series classifier using the SCP1 dataset. The dataset is divided into train, validation, and test sets, with unique temporal patterns (INPUT_SEQ_LEN=896, INPUT_DIM=6) representing various real-world scenarios. Your objective is to design a model that not only demonstrates high accuracy but also demonstrates robustness to noise and seasonality, while maintaining interpretability. In addition to overall accuracy, provide a detailed confusion matrix and implement a feature importance analysis to explain the model's decisions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"real-world scenarios\",\n        \"description\": \"Develop a state-of-the-art time-series classifier using the SCP1 dataset. The dataset is divided into train, validation, and test sets with unique temporal patterns (INPUT_SEQ_LEN=896, INPUT_DIM=6). Focus on achieving high accuracy, robustness to noise and seasonality, and interpretability with feature importance analysis and a detailed confusion matrix.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SCP1\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 896,\n                \"INPUT_DIM\": 6\n            },\n            \"description\": \"A dataset for advanced time-series classification with unique temporal patterns and real-world scenarios.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"noise_robustness\": null,\n                \"seasonality_handling\": null,\n                \"interpretability_features\": null\n            },\n            \"description\": \"Design a state-of-the-art time-series classifier for the SCP1 dataset with a focus on interpretability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection based on interpretability\",\n            \"noise reduction techniques\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"generate detailed confusion matrix and interpretability report\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction has been parsed to cover building a high-performance time-series classifier on the SCP1 dataset, addressing noise robustness, seasonality, and interpretability. However, specific performance metrics values are not provided, leaving them null. The user's expertise level suggests they expect guidance on implementation details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for a high-precision text classification project, your team must develop a state-of-the-art AI model using the recently released BoolQ dataset. This dataset consists of complex multi-layered passage-question pairs, requiring fine-grained analysis for entailment determination. Emphasize on not only achieving exceptional accuracy but also demonstrate robustness across various linguistic nuances and domains. Implement a novel architecture combining transformers and ensemble learning, and meticulously optimize hyperparameters to outperform current benchmarks in the field.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"high-precision\",\n        \"description\": \"The project involves developing a state-of-the-art AI model for a high-precision text classification task, using the BoolQ dataset, which contains complex passage-question pairs for entailment determination. The focus is on achieving top accuracy, robustness to linguistic nuances and domain variations, and outperforming existing benchmarks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"complexity\": \"recently released\",\n                \"nature\": \"passage-question pairs for entailment\",\n                \"specifics\": \"highly complex, multi-layered\"\n            },\n            \"description\": \"A recently released dataset for text classification, designed to challenge with complex, multi-layered passage-question pairs for entailment determination.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer and Ensemble Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novelty\": \"combining transformers and ensemble learning\",\n                \"hyperparameter_optimization\": \"meticulous\"\n            },\n            \"description\": \"A novel AI model architecture combining transformers and ensemble learning, specifically tailored for the BoolQ dataset and focusing on high precision and benchmark outperformance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"emphasis\": \"exemplify robustness across linguistic nuances and domains\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been parsed based on the project manager's requirements, highlighting the high complexity of the task due to the complex BoolQ dataset, emphasis on accuracy and robustness, and the novel model architecture. However, the missing 'accuracy' under performance metrics indicates a need for performance targets after hyperparameter optimization. Additionally, 'target_device' and 'deployment_endpoint' were not specified by the user.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "object detection", "instruction": "For the ambitious urban mobility initiative, our team needs a real-time, YOLO-based object detection model on the Cityscapes dataset, focusing on vehicle, pedestrian, and signage recognition. The model must deliver 30 FPS on commodity hardware, ensure at least 85% accuracy, and integrate seamlessly with edge computing for latency-sensitive environments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"urban environment, urban mobility\",\n        \"description\": \"Develop a real-time YOLO-based object detection model for an ambitious urban mobility initiative. The focus is on vehicle, pedestrian, and signage recognition using the Cityscapes dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"FPS (frames per second)\",\n                \"value\": 30,\n                \"unit\": \"frames per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\n                \"vehicle\",\n                \"pedestrian\",\n                \"signage\"\n            ],\n            \"specification\": {\n                \"object_classes\": [\n                    \"vehicle\",\n                    \"pedestrian\",\n                    \"signage\"\n                ]\n            },\n            \"description\": \"A real-world urban scene dataset for object detection focusing on vehicles, pedestrians, and signage\",\n            \"preprocessing\": [\"YOLO-specific data augmentation\"],\n            \"augmentation\": [\"real-time data augmentation\"],\n            \"visualization\": [\"model performance on Cityscapes samples\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO model\",\n            \"family\": \"YOLO architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization for\": \"commodity hardware\",\n                \"latency requirements\": \"edge computing\"\n            },\n            \"description\": \"A real-time object detection model with a focus on vehicle, pedestrian, and signage recognition for an urban mobility initiative, targeting 30 FPS on commodity hardware and 85% accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n        \"information\": \"Performance requirements for commodity hardware and edge integration.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            {\n                \"method\": \"YOLO-specific pre-processing for real-time performance\"\n            }\n        ],\n        \"target_device\": \"commodity hardware, edge devices\",\n        \"deployment_endpoint\": \"latency-sensitive urban environment\",\n        \"inference_engine\": [\"optimized for real-time performance on edge\"],\n        \"demonstration\": \"demonstration of real-time detection with specified accuracy and latency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed information covers the user's request for a real-time YOLO-based object detection model, including accuracy, speed, and edge computing integration. The expert-level expertise suggests a strong understanding of the requirements. However, it's assumed that the user might need help with specific implementation details, as they did not provide detailed specifications for the YOLO model or any particular optimization techniques.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a specialized machine translation system for e-commerce product narratives, leveraging the Taobao dataset. The system must employ an advanced U-Net model tailored for retail and fashion domain, prioritizing not only high BLEU scores (targeting 38+) but also ensuring cultural context preservation, industry-specific terminology accuracy, and real-time integration with various e-commerce platforms. Additionally, incorporate a continuous improvement mechanism to adapt to dynamic product lexicons and user feedback.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"e-commerce, retail, and fashion\",\n        \"description\": \"Develop a specialized machine translation system for e-commerce product narratives, focusing on the Taobao dataset. Use an advanced U-Net model tailored for the retail and fashion domain. Prioritize high BLEU scores (aiming for 38+) while preserving cultural context and accuracy of industry-specific terminology. Also, ensure real-time integration with various e-commerce platforms and incorporate a continuous improvement mechanism for dynamic product lexicons and user feedback.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU Score\",\n                \"value\": 38.0\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Taobao\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"retail and fashion\"\n            },\n            \"description\": \"A dataset for e-commerce product narratives, specifically tailored for the retail and fashion domain.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced U-Net\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"tailored_for_domain\": true,\n                \"real-time_inference\": true\n            },\n            \"description\": \"A domain-specific U-Net model designed for machine translation in the retail and fashion sector.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Dynamic product lexicons and user feedback integration\",\n        \"Cultural context preservation and industry-specific terminology accuracy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"real-time integration\",\n            \"adaptation mechanism\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"various e-commerce platforms\",\n        \"inference_engine\": [\"efficient, real-time processing\"],\n        \"demonstration\": \"cultural context preserving, feedback-driven translation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was derived from the given instruction, capturing the intent for model development, problem area, advanced U-Net model requirements, dataset (Taobao), and performance and service expectations. The user's high expertise level suggests they would need detailed guidance, which is provided through the explanation of real-time integration, cultural context, and industry-specific accuracy. However, the target deployment device is not specified, leaving it open-ended.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As the lead project manager for our innovative urban mobility solution, your machine learning team is tasked with developing a YOLO-driven object detection system on the Cityscapes dataset. The model must achieve state-of-the-art performance, reaching 90% accuracy with resistant-to-deterioration techniques, while maintaining a real-time detection rate of 30 FPS on low-cost hardware. Moreover, ensure cross-platform compatibility and seamless integration with edge computing for ultra-low-latency deployments in diverse environmental conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"urban mobility\",\n        \"description\": \"Develop a YOLO-driven object detection system for an innovative urban mobility solution using the Cityscapes dataset. The model must achieve state-of-the-art performance (90% accuracy), be resilient to deterioration, run at 30 FPS on low-cost hardware, and ensure cross-platform compatibility and seamless integration with edge computing for ultra-low-latency deployments in diverse conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time detection rate\",\n                \"value\": 30,\n                \"unit\": \"FPS\",\n                \"target_device\": \"low-cost hardware\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"ultra-low\",\n                \"unit\": \"latency\",\n                \"deployment_environment\": \"edge computing\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A dataset for urban scene understanding, focusing on object detection tasks, especially for the development of the object detection system.\",\n            \"preprocessing\": [\"resilient-to-deterioration techniques\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO-driven object detection system\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy requirements\": \"state-of-the-art\",\n                \"performance target\": 30,\n                \"FLOPs\": null,\n                \"training speed\": null\n            },\n            \"description\": \"A YOLO model optimized for high performance, accuracy, and resilience on Cityscapes dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": [\"cross-platform\", \"low-cost hardware\", \"edge computing\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time and cross-platform\"],\n        \"demonstration\": {\n            \"environmental conditions\": [\"diverse\"]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was formed based on the given instruction. The user's intent is clear for a 'build' project with high expertise. The problem area, task, and application domain are specified. Performance metrics include accuracy and real-time detection rate, which are clearly stated. The dataset requirement and model characteristics were derived from the instructions. However, some details like model specifications and deployment endpoint are not explicitly provided in the instruction, thus might require additional inquiry.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a specialized project manager for an elite AI development team, you've been tasked with the groundbreaking creation of a cutting-edge question-answering platform. This platform, tailored for a niche scientific community, should utilize the highly specialized MedQANet dataset, containing complex medical research questions and answers. The system must be optimized for accuracy, relying on the state-of-the-art M6 transformer architecture with focus on explainable AI, allowing domain experts to understand the reasoning behind responses. Additionally, design the system to seamlessly handle real-time, real-world emergencies in the medical domain, requiring it to make split-second decisions based on incomplete or contradictory information. Ensure the platform demonstrates anomaly detection capabilities, can adapt to evolving knowledge, and incorporates a continuous learning mechanism to maintain performance under extreme query loads. Remember to present a comprehensive roadmap with benchmarks for benchmarking both system efficiency and the effectiveness of its explainability feature.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"scientific, specifically medical\",\n        \"description\": \"Create a cutting-edge question-answering platform for a niche medical community using the MedQANet dataset. The platform must be based on state-of-the-art M6 transformer architecture, prioritize explainable AI, handle real-time emergencies, and have anomaly detection, continuous learning, and efficiency benchmarks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explainability effectiveness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time decision-making speed\",\n                \"unit\": \"milliseconds\",\n                \"value\": null\n            },\n            {\n                \"name\": \"anomaly detection rate\",\n                \"unit\": \"%\",\n                \"value\": null\n            },\n            {\n                \"name\": \"query load adaptation\",\n                \"unit\": \"queries/second\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MedQANet\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"complexity\": \"high\",\n                \"domain\": \"medical research\"\n            },\n            \"description\": \"A specialized dataset containing complex medical research questions and answers\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6 transformer architecture\",\n            \"family\": \"transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainable AI\": true,\n                \"architecture\": \"M6\"\n            },\n            \"description\": \"A state-of-the-art question-answering model with a focus on explainability\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"real-time emergency handling\",\n            \"adaptation to evolving knowledge\",\n            \"continuous learning mechanism\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"benchmarking strategy\": \"real-world emergencies and evolving medical knowledge scenarios\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction provided a clear task, specifying a question-answering platform, its requirements on the MedQANet dataset, M6 transformer, and its unique features. The platform's real-time emergency handling and explainability are key aspects, and a high expertise level is assumed given the project manager role. However, specific metric benchmarks are not provided, leaving them as null values and requiring inference based on task details.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "For the advanced ML team, develop a highly-efficient HAN model surpassing a stringent benchmark of 95% F1 score, achieving real-time adaptive classification in arXiv papers. The system must continually refine distinctions between main and subfields, track domain shifts, and validate performance amidst evolving scientific literature while robustly managing noise in data for consistent, precision-driven improvements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"arXiv papers\",\n        \"description\": \"Develop an advanced HAN model for real-time adaptive classification in arXiv papers. The model should achieve at least 95% F1 score, refine distinctions between main and subfields, track domain shifts, handle evolving scientific literature, and robustly manage noise for precision-driven improvements.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time inference\",\n                \"value\": null,\n                \"unit\": \"real-time\"\n            },\n            {\n                \"name\": \"adaptability to domain shifts\",\n                \"value\": 1.0,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"noise robustness\",\n                \"value\": 1.0,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"arXiv papers\",\n                \"size\": null,\n                \"structure\": \"evolving scientific literature\"\n            },\n            \"description\": \"A dataset of arXiv papers for real-time adaptive text classification\",\n            \"preprocessing\": [\n                \"domain-specific text cleaning\",\n                \"noise reduction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"evolving data distribution visualization\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HAN\",\n            \"family\": \"Hierarchical Attention Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": \"high\",\n                \"structure\": \"refined for main and subfield distinction\"\n            },\n            \"description\": \"A highly-efficient HAN model designed for adaptive classification in arXiv with emphasis on F1 score, domain shifts, and noise robustness.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Domain knowledge of arXiv paper structure and evolving nature of scientific literature\"\n        },\n        {\n            \"description\": \"Techniques for refining distinctions and handling domain shifts\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"dynamic model adaptation\",\n            \"continuous performance monitoring\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"arXiv paper classification API\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"live arXiv paper analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction has been parsed to focus on building an advanced HAN model, which requires a high level of expertise. The performance requirements, adaptability, and noise management are clearly stated. However, the specific dataset name and source are not provided, and could be inferred based on the user's intent to search using the description. The confidence score is high due to the well-defined requirements but could be lower due to potential ambiguity regarding the dataset and exact implementation details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Request the machine learning development team to design a cutting-edge, multilingual emotion-precise translation platform, employing ParaCrawl and a sophisticated fusion of three-encoder architectures. The system must not only guarantee exceptional precision, context-awareness, but also demonstrate dynamic cross-cultural adaptability for nuanced English to Dutch translations, while maintaining stringent privacy regulations for user-emotions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-cultural communication\",\n        \"description\": \"Design a cutting-edge multilingual emotion-precise translation platform using ParaCrawl and a three-encoder architecture fusion. The system should guarantee precision, context-awareness, dynamic cross-cultural adaptability for English to Dutch translations, and adhere to strict privacy regulations for user-emotions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"context-awareness\",\n                \"value\": null\n            },\n            {\n                \"name\": \"dynamic cross-cultural adaptability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"multilingual support\": true,\n                \"specifics for Dutch\": null\n            },\n            \"description\": \"A large-scale multilingual dataset for training the emotion-precise translation system.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"three-encoder architecture fusion\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture details\": \"emotional context understanding, cross-cultural adaptability\",\n                \"privacy-preserving\": true\n            },\n            \"description\": \"A state-of-the-art model that combines three encoders for precise emotion-translation from English to Dutch.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"contextual, culturally adapted English to Dutch translations while preserving user-emotion privacy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed information is based on the user's request for a multilingual translation platform with emphasis on precision and context awareness, as well as privacy concerns. High expertise indicates a deeper understanding of the requirements. The specifics around model performance metrics, like the exact values, are missing, leaving space for further clarification.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "object detection", "instruction": "For the posh gourmet boutique's cutting-edge grocery tech, design a sophisticated end-to-end AI infrastructure leveraging the state-of-the-art YOLOv5.1+, incorporating not just basic model adaptation but a rigorous sequence of transfer learning and fine-tuning strategies. Utilize the exclusive 'BoutiqueFoods Comprehensive Insights Dataset' with its extensive 1,500 subcategories, necessitating multi-step reasoning to handle the nuances of each product. The system must showcase exceptional accuracy, targeting precision above 95.5% and recall of 94%, while also incorporating a real-time inventory control mechanism. This module must handle high-definition imagery in parallel, ensuring immediate, low-latency updates for rare artisanal products, with an ultra-tight SLA of no more than 1.2 milliseconds per image for inventory synchronization to maintain peak operational efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"grocery tech\",\n        \"description\": \"Design an AI infrastructure for the posh gourmet store using YOLOv5.1+ for end-to-end object detection in high-definition images. The system involves advanced model adaptation, transfer learning, and fine-tuning for the exclusive 'BoutiqueFoods Comprehensive Insights Dataset' with 1,500 subcategories. Requires multi-step reasoning for product specifics and targeting precision of 95.5% and recall of 94%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.955\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.94\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 1.2,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoutiqueFoods Comprehensive Insights Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"subcategories\": 1500\n            },\n            \"description\": \"A comprehensive dataset for grocery tech with 1,500 subcategories and high-definition images that require nuanced product understanding.\",\n            \"preprocessing\": [\n                \"multi-step reasoning for product categorization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5.1+\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptation\": [\"state-of-the-art\"],\n                \"transfer_learning\": true,\n                \"fine_tuning\": true\n            },\n            \"description\": \"An AI infrastructure leveraging YOLOv5.1+ for object detection in a sophisticated manner\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"real-time inventory control\",\n            \"low-latency image processing\"\n        ],\n        \"target_device\": \"ultra-fast, possibly edge computing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"optimized for real-time performance\"\n        ],\n        \"demonstration\": {\n            \"requirements\": \"immediate, low-latency updates and peak operational efficiency\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clear in specifying a high-end AI infrastructure for a gourmet store using YOLOv5.1+, incorporating advanced techniques and specific performance metrics. The system's requirements are well-defined, including real-time inventory control and strict SLAs. However, there is a lack of information about the target deployment device or deployment endpoint, which should be filled based on expected hardware and integration.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "node classification", "instruction": "Develop a state-of-the-art AI model for node classification in diverse, Wikipedia-DBpedia fused knowledge graphs, leveraging a Siamese Hypergraph Neural Network. Mandate a minimum F1-score of 97%, while ensuring scalability for Raspberry Pi's limited hardware with energy efficiency in mind. Investigate six neighborhood sampling techniques, including novel combinations, comparing GAT, GraphSAGE, HGNN, and a custom Edge-Enhanced variant, to uncover hidden performance nuances and practical efficiency challenges. Conduct an extensive hyperparameter optimization under resource constraints, documenting the impact on model accuracy and computational requirements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graphs\",\n        \"description\": \"Develop a state-of-the-art AI model for node classification in diverse, Wikipedia-DBpedia fused knowledge graphs using a Siamese Hypergraph Neural Network. Target F1-score is at least 97%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Scalability\",\n                \"value\": \"Raspberry Pi\",\n                \"unit\": \"Hardware constraints\"\n            },\n            {\n                \"name\": \"Energy Efficiency\",\n                \"value\": \"Maximized\",\n                \"unit\": \"Efficiency consideration\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wikipedia-DBpedia Fusion\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"node_types\": [\"diverse\"],\n                \"edges\": \"fused knowledge graph\",\n                \"size\": \"Raspberry Pi-friendly\"\n            },\n            \"description\": \"A large-scale, fused knowledge graph combining Wikipedia and DBpedia, suitable for testing on Raspberry Pi's hardware.\",\n            \"preprocessing\": [\n                \"Wikipedia-DBpedia fusion\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Siamese Hypergraph Neural Network\",\n            \"family\": \"Hypergraph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Siamese\",\n                \"sampling_techniques\": [\"GAT\", \"GraphSAGE\", \"HGNN\", \"Edge-Enhanced\"],\n                \"neighborhood_combinations\": \"novel\"\n            },\n            \"description\": \"Leverages Siamese architecture with diverse sampling techniques for optimized node classification in knowledge graphs.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Explore six neighborhood sampling techniques and their combinations, specifically focusing on energy efficiency and hardware scalability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Resource-constrained\"],\n        \"target_device\": \"Raspberry Pi\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GAT\",\n            \"GraphSAGE\",\n            \"HGNN\",\n            \"Edge-Enhanced\"\n        ],\n        \"demonstration\": \"Performance comparison under varying hyperparameters\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction has been parsed with high-level expertise, specifying a build intent for a state-of-the-art model with high requirements on F1-score and hardware constraints. The dataset, model, and service sections have been detailed accordingly. Some assumptions, such as the demonstration and deployment endpoint, are made based on the provided information. The confidence score is high due to the explicitness of the requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Construct a high-precision, real-time fusion translation software, utilizing the OpenSubtitles corpus and cutting-edge Arctic-Speech-Net ASR. Specialize a BERT-MultiMT variant optimized for London slang and colloquial translations into Mandarin dialect, targeting sub-regional dialects and slang expressions unique to the East London boroughs. The system must integrate London-centric visual context analysis, adhere to a temporal accuracy benchmark of 99.9%, and train on a massive, globally diverse conversational dataset capturing nuances of regional interactions. Incorporate adaptive learning for seamless integration into unsupervised, live video chats, ensuring semantic consistency and accurate reproduction of idiomatic expressions during casual exchanges.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"global, conversational translation\",\n        \"description\": \"Develop a high-performance, real-time fusion translation software. Utilize the OpenSubtitles corpus and Arctic-Speech-Net ASR. Specialize a BERT-MultiMT variant for London slang and Mandarin dialect, focusing on East London sub-regional dialects. The system must maintain a temporal accuracy of 99.9% and be trained on a diverse global conversational dataset capturing regional nuances.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"temporal_accuracy\",\n                \"value\": 0.999\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time_performance\",\n                \"value\": \"high\",\n                \"unit\": \"low-latency\"\n            },\n            {\n                \"name\": \"adaptation_capabilities\",\n                \"value\": null,\n                \"unit\": \"unsupervised\"\n            }\n        ],\n        \"contextual_requirements\": {\n            \"visual_analysis\": \"London-centric, including local environment cues\",\n            \"domain_integration\": \"live video chats, particularly for casual exchanges\"\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles\",\n            \"modality\": [\"text\"],\n            \"target_variables\": null,\n            \"specification\": {\n                \"corpus_type\": \"global, conversational\"\n            },\n            \"description\": \"A large-scale corpus for training machine translation models, capturing conversational nuances\",\n            \"preprocessing\": [\"London slang and dialect normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"Arctic-Speech-Net ASR\",\n            \"modality\": [\"audio\"],\n            \"target_variables\": null,\n            \"specification\": {\n                \"type\": \"Arctic-Speech-Net\"\n            },\n            \"description\": \"State-of-the-art ASR for input transcription\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-MultiMT (London slang, Mandarin dialect)\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"specialization\": \"London slang and Mandarin dialect\"\n            },\n            \"description\": \"A BERT-MultiMT variant optimized for London slang to Mandarin dialect translation\"\n        }\n    ],\n    \"knowledge\": [\n        \"London slang and colloquial expressions\",\n        \"East London sub-regional dialect intricacies\",\n        \"Global conversational data for nuance understanding\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-aware normalization\", \"adaptation for real-time fusion\"],\n        \"target_device\": \"cloud & mobile\",\n        \"deployment_endpoint\": \"cross-platform, support for live video chat integration\",\n        \"inference_engine\": [\"real-time, adaptive, deep learning based\"],\n        \"demonstration\": {\n            \"focus\": \"casual exchanges with semantic consistency\",\n            \"features\": [\"visual context, temporal accuracy, unsupervised adaptation\"]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"This response reflects a high level of expertise, considering the advanced translation system requirements and focus on cutting-edge technologies. The provided metrics and constraints cover performance, complexity, and contextual aspects. However, the confidence in the absence of specific deployment endpoint details is medium.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a cutting-edge Transformer-XL-enhanced multilingual translation system that unites the extensive XSum corpus with niche languages like Nepali, Indonesian, and Swahili. Mandate a groundbreaking benchmark of 45 diverse TERQ scores across all four translation directions, encompassing not just academic papers but also intricate domain-specific content like medical research and aerospace communication. The model must excel at 99.5% accuracy in interpreting neurology and astrobiology terms, while maintaining near-native fluency and processing a massive volume of 5000 sentences per second in real-time for seamless global collaboration. Ensure compliance with stringent GDPR privacy measures, and demand the model's transparency to rival that of a human-readable Explainable AI, facilitating global trust among users.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": [\"general\", \"medical research\", \"aerospace communication\"],\n        \"description\": \"Design a cutting-edge multilingual translation system using Transformer-XL, combining the XSum corpus with niche languages like Nepali, Indonesian, and Swahili. Aim for a benchmark of 45 diverse TERQ scores across all translation directions, with emphasis on accuracy in neurology and astrobiology terms. The system should have near-native fluency, process 5000 sentences per second in real-time, comply with GDPR, and prioritize transparency like Explainable AI.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average TERQ score\",\n                \"value\": 45,\n                \"specific_directions\": [\"XSum to Nepali\", \"XSum to Indonesian\", \"XSum to Swahili\", \"Nepali to XSum\", \"Indonesian to XSum\", \"Swahili to XSum\", \"Nepali to Nepali\", \"Indonesian to Indonesian\", \"Swahili to Swahili\"]\n            },\n            {\n                \"name\": \"accuracy on neurology and astrobiology terms\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"sentence processing rate\",\n                \"value\": 5000,\n                \"unit\": \"sentences per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"XSum Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"The extensive XSum corpus used for multilingual translation\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Nepali, Indonesian, and Swahili datasets\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"Incorporates niche languages for the translation system\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL-based multilingual translation model\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"99.5%\",\n                \"processing_speed\": {\n                    \"real-time\": true\n                },\n                \"explainability\": \"high\"\n            },\n            \"description\": \"A state-of-the-art model that integrates Transformer-XL and supports translation into multiple languages with focus on specified domains\"\n        }\n    ],\n    \"knowledge\": [\n        \"Strives for GDPR compliance\",\n        \"Emphasis on Explainable AI for transparency\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": [\"cloud\", \"edge\"],\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"global collaboration, seamless translation in neurology and astrobiology terms\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the high-level requirements for a multilingual translation project that demands advanced performance, GDPR compliance, and explainability. The expertise level is assumed to be high due to the sophistication of the task. However, without specific deployment details or an exact model name, these parts are left as placeholders.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, seek a highly sophisticated RNN-empowered neural machine translation system that integrates the extensive OPUS corpus for translating English into Russian, particularly focusing on translating colloquial and non-standard language. The model must not only exceed a benchmark BLEU score of 33 but consistently improve upon it by 5% annually to maintain state-of-the-art performance. Demand exceptional adaptability in recognizing and converting slang, idioms, and regional colloquialisms, while ensuring the model excels in domain-specific terminologies with an accuracy rate of 95% under strict resource constraints. Moreover, prioritize latency optimization, aiming for a peak computational efficiency of 150 milliseconds per sentence in real-time, to deliver seamless and uninterrupted communication for high-traffic, time-sensitive platforms with user bases exceeding one million.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Seek a sophisticated RNN-empowered neural machine translation system specifically for English to Russian translation. Focus on colloquial and non-standard language with a target BLEU score of at least 33, with a 5% annual improvement target. The model should excel in recognizing and translating slang, idioms, regional colloquialisms, and maintain a 95% accuracy rate for domain-specific terminology under strict resource constraints. Latency optimization is crucial, aiming for a peak computational efficiency of 150 milliseconds per sentence.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 33,\n                \"yearly_goal\": {\"type\": \"increase\", \"percentage\": 5}\n            },\n            {\n                \"name\": \"Accuracy for domain-specific terminologies\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Computational efficiency (latency)\",\n                \"value\": 0.15,\n                \"unit\": \"seconds (ms)\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS corpus (English to Russian)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"corpus\": \"extensive and diverse for colloquial and non-standard language\",\n                \"languages\": [\"English\", \"Russian\"]\n            },\n            \"description\": \"A comprehensive corpus for English to Russian translation, including colloquial and non-standard content.\",\n            \"preprocessing\": [\"adaptation for colloquial and slang\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-empowered Neural Machine Translation System\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptability\": \"for slang, idioms, and regional colloquialisms\",\n                \"domain-specific accuracy\": \"95%\",\n                \"resource constraints\": \"strict\",\n                \"latency optimization\": \"for real-time, high-traffic platforms\"\n            },\n            \"description\": \"A highly sophisticated system that integrates OPUS corpus and prioritizes colloquial language translation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance environments\",\n        \"deployment_endpoint\": \"for high-traffic platforms with >1M users\",\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": \"for seamless, high-quality translation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction has been parsed based on the provided schema, including the high expertise level, the detailed requirements for model performance and efficiency, and the specific data source. However, it's assumed that the user is familiar with the technical terms mentioned and that some performance expectations are annual targets.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for the cutting-edge AI team, you are now tasked with overseeing a complex, real-world machine learning challenge. The Time-series Panorama dataset, now consisting of 48 extra variables and irregularly spaced temporal samples, has been divided into highly imbalanced subsets: train (70%), validation (15%), and test (15%) for enhanced heterogeneity. Your team must develop an innovative, scalable ensemble model that not only forecasts future 192-step sequences (INPUT_SEQ_LEN+PRED_SEQ_LEN=292) with exceptional accuracy, optimizing for mean squared error (MSE), mean absolute error (MAE), and a novel metric, rolling root mean squared logarithmic error (RMSLE), but also incorporates explainable AI principles. The model must operate within strict resource constraints, utilizing cloud credits efficiently, and demonstrate adaptability to unexpected environmental factors. Additionally, prepare a concise technical paper detailing the model's architecture, regularization methods, and the trade-offs made for interpretability in a constrained environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-world machine learning challenge\",\n        \"description\": \"Develop an innovative, scalable ensemble model for the Time-series Panorama dataset with 48 extra variables and irregularly spaced temporal samples. The model should forecast future 192-step sequences, optimize for MSE, MAE, and RMSLE while incorporating explainable AI principles. It should operate within cloud resource constraints and demonstrate adaptability to unexpected environmental factors.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": null,\n                \"unit\": \"cloud credits per inference\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Time-series Panorama\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 192,\n                \"PRED_SEQ_LEN\": 192,\n                \"total_steps\": 292,\n                \"variables\": 48,\n                \"temporal_samples\": \"irregularly spaced\"\n            },\n            \"description\": \"A complex dataset with 48 extra variables and irregularly spaced temporal samples, divided into train (70%), validation (15%), and test (15%) subsets for heterogeneity.\",\n            \"preprocessing\": [\n                \"handling irregular time steps\",\n                \"imbalance handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"details on ensemble\",\n                \"explainability\": \"incorporates explainable AI principles\"\n            },\n            \"description\": \"An innovative model designed for the given real-world challenge with emphasis on scalability, interpretability, and resource efficiency\"\n        }\n    ],\n    \"knowledge\": [\n        \"Data preprocessing for irregular time series and imbalanced datasets\",\n        \"Ensemble methods for scalable forecasting\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for cloud credits\"],\n        \"demonstration\": {\n            \"environmental adaptability\": \"description of adaptability\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction describes a complex, real-world task and specifies the required dataset characteristics, performance metrics, and constraints. High expertise suggests a detailed understanding. However, the model-specific details are hypothetical and would need to be filled based on the team's chosen methods.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the CoralNet upgrade, design a hybrid architecture combining FusionNet and an adaptive PSPNet variant, accounting for turbidity variations, variable light penetration, and dynamic underwater terrain. Target a demanding 20% improvement in F1-score over existing benchmarks, and guarantee near real-time processing on Raspberry Pi-based hardware with resource constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"underwater ecology\",\n        \"description\": \"For the CoralNet upgrade, design a hybrid architecture that integrates FusionNet and an adaptive PSPNet variant, accounting for turbidity variations, variable light penetration, and dynamic underwater terrain.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.2\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": 1.0,\n                \"unit\": \"narrow window (near real-time)\"\n            },\n            {\n                \"name\": \"resource-constrained\",\n                \"value\": true,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"Raspberry Pi compatibility\",\n                \"value\": true,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CoralNet upgrade\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dynamic_factors\": [\"turbidity\", \"light penetration\", \"terrain\"],\n                \"benchmark_improvement\": 0.2\n            },\n            \"description\": \"An underwater image dataset that captures diverse conditions for the hybrid model development.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Architecture\",\n            \"family\": \"FusionNet & Adaptive PSPNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"performance目标\": \"target a 20% improvement in F1-score\",\n                \"resource_efficiency\": \"near real-time processing\"\n            },\n            \"description\": \"A designed model combining FusionNet and an adaptive PSPNet variant, adaptable to underwater environment conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Underwater ecology, turbidity variations, light penetration dynamics, and terrain diversity are critical factors.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"Raspberry Pi-based hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for resource-constrained environments\"\n        ],\n        \"demonstration\": \"focusing on near real-time performance and hardware compatibility\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON reflects the user's requirements for a machine learning project, including architectural details, performance improvements, and hardware constraints. The user's high expertise suggests they understand the technical requirements, but could benefit from advice on fine-tuning or optimizing the architecture for the task.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a visionary project manager, I seek your expertise to develop a novel quantum-enhanced computer vision system capable of identifying not only a vast archive of ancient fungal species but also extreme microorganisms found in extreme environments. Propose a cutting-edge fusion of quantum convolutional neural networks (QCNNs) with quantum reservoir computing, tailored for low-resource settings. Analyze the performance degradation when cross-validating on the Mycological Microscopic Imagery Dataset, and optimize for both accuracy under limited quantum computing power and ultra-fast real-time identification. Describe a strategy to benchmark against leading-edge classical and quantum algorithms, emphasizing the potential synergies between the two domains.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"ancient fungal species identification and extreme microorganisms detection\",\n        \"description\": \"Develop a novel quantum-enhanced computer vision system using QCNNs and quantum reservoir computing, designed for low-resource settings. The system should identify ancient fungal species and extreme microorganisms in Mycological Microscopic Imagery Dataset. Analyze performance with cross-validation under limited quantum computing power and focus on real-time, ultra-fast identification.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\n                    \"quantum\": null,\n                    \"classical\": null\n                },\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": {\n                    \"quantum\": null,\n                    \"classical\": null\n                },\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mycological Microscopic Imagery Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"ancient fungal species\", \"extreme microorganisms\"],\n            \"specification\": {\n                \"image_dimensions\": {\n                    \"height\": null,\n                    \"width\": null\n                },\n                \"image_format\": null,\n                \"size\": null\n            },\n            \"description\": \"Dataset containing images of ancient fungal species and extreme microorganisms found in extreme environments.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum CNNs + Quantum Reservoir Computing\",\n            \"family\": \"Quantum Machine Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"quantum_resources\": {\n                    \"qubits\": null,\n                    \"ancilla_qubits\": null\n                },\n                \"quantum_layers\": {\n                    \"QCNN_layers\": null,\n                    \"quantum_reservoir_layer\": null\n                },\n                \"quantum_optimization\": null\n            },\n            \"description\": \"Fusion of QCNNs and quantum reservoir computing for image classification tasks.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Potential synergies between classical and quantum algorithms\",\n        \"Focus on low-resource, real-time performance and cross-validation challenges\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"low-resource settings\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"Quantum Computing\", \"Classical Computing\"],\n        \"demonstration\": \"Strategy to compare with leading-edge algorithms\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction requires a high-level quantum computer vision project with clear performance and resource constraints. 'High' expertise level is assumed, but some specifics, like quantum resource allocation and exact performance metrics, are yet to be specified.\",\n        \"score\": 0.8\n    }\n}"}
{"task": "object detection", "instruction": "For a niche gourmet market, design an enhanced YOLOv5.1 model tailored for the BoutiqueFoods Shelf Insights Enhanced Dataset. This specialized algorithm must not only accurately classify among 200 premium luxury product categories, maintaining precision above 98% and recall exceeding 97% after extensive hyperparameter tuning, but also incorporate a real-time inventory management system. This system should have a distributed architecture enabling near-instantaneous synchronization (<100 milliseconds per captured 4K HD shelf image) while ensuring data privacy, minimizing energy consumption to less than 2 watts during peak operational periods, and complies with energy star efficiency standards. The model should also include a machine learning-driven predictive analytics component to forecast demand and optimize stock levels, relying on historical sales data and external market trends.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"niche gourmet market\",\n        \"description\": \"Design an enhanced YOLOv5.1 model tailored for the BoutiqueFoods Shelf Insights Enhanced Dataset, targeting 200 premium luxury product categories. The model must have precision > 98% and recall > 97% after extensive hyperparameter tuning. The algorithm should incorporate a real-time inventory management system with distributed architecture, ensuring synchronization (<100ms per captured 4K HD shelf image), data privacy, and <2 watts energy consumption during peak periods, meeting Energy Star efficiency standards. It must also include a machine learning-driven predictive analytics component for demand forecasting and stock optimization, using historical sales data and external market trends.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"synchronization time\",\n                \"value\": 0.1,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"energy consumption (peak)\",\n                \"value\": 2,\n                \"unit\": \"watts\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoutiqueFoods Shelf Insights Enhanced Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product categories\"],\n            \"specification\": {\n                \"number of categories\": 200\n            },\n            \"description\": \"A dataset for a specialized YOLOv5.1 model, targeting luxury gourmet products, with 4K HD shelf images and real-time inventory management requirements.\",\n            \"preprocessing\": [\"data augmentation for object detection\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"enhanced YOLOv5.1\",\n            \"family\": \"YOLOv5\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"enhanced for real-time inventory management and machine learning-driven analytics\",\n                \"object detection accuracy\": \"targeted at >98% precision and >97% recall\",\n                \"energy efficiency\": \"Energy Star compliant, <2 watts consumption\"\n            },\n            \"description\": \"A tailored YOLOv5.1 model optimized for object detection in the gourmet food niche with advanced inventory and analytics features.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporates machine learning-driven demand forecasting and historical sales data analysis to optimize stock levels\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": {\n            \"type\": \"real-time processing\",\n            \"specifics\": \"distributed architecture, low latency\"\n        },\n        \"deployment_endpoint\": \"private and secured, energy-efficient\",\n        \"inference_engine\": [\"optimized for low energy consumption\"],\n        \"demonstration\": \"includes a real-time predictive analytics dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provides clear requirements for the model, dataset, and performance metrics, as well as specific complexities like distributed architecture, low energy consumption, and real-time synchronization. The high expertise level suggests the user is familiar with the topic and expects in-depth details, which are provided in the response.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image classification", "instruction": "Request the ML team to conceive a sophisticated fashion-specific image classifier, catering exclusively to luxury garments in the exclusive Paris Haute Couture Fashion-MNIST dataset. Implement the latest Enhanced Dynamic Routing Capsule Networks (EDRCN) with a strict benchmark of attaining a minimum accuracy of 98% in discerning high-end designs amidst extreme real-world deformities like anisotropic scaling, irregular light occlusions, and sophisticated color perturbations. Mandate a comprehensive research project involving multiple stages:\n\n1. Comparative study with state-of-the-art, energy-efficient Attention-based Convolutional Neural Networks (ACNNs) considering not just top-tier accuracy but also:\n   a. Execution speed under real-world constraints\n   b. Memory efficiency and memory footprint optimization\n   c. Ensuring compatibility across diverse low-power wearable and IoT devices with diverse hardware configurations\n\nEmphasize the importance of energy conservation and computational resource optimization for the classifier's seamless integration on devices with limited battery life and stringent hardware restrictions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"luxury fashion\",\n        \"description\": \"The ML team is tasked with developing a sophisticated image classifier for luxury garments using the Paris Haute Couture Fashion-MNIST dataset. The classifier should be based on Enhanced Dynamic Routing Capsule Networks (EDRCN), targeting an accuracy of at least 98% while handling extreme real-world deformities like anisotropic scaling, irregular light occlusions, and sophisticated color perturbations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"execution speed\",\n                \"value\": \"real-world constraints\",\n                \"unit\": \"seconds per inference\"\n            },\n            {\n                \"name\": \"memory efficiency\",\n                \"value\": \"optimized\",\n                \"unit\": \"bits per inference\"\n            },\n            {\n                \"name\": \"compatibility\",\n                \"value\": \"diverse low-power devices\",\n                \"unit\": \"N/A\"\n            }\n        ],\n        \"additional_task_stages\": [\n            {\n                \"stage\": 1,\n                \"description\": \"Comparative study with Attention-based Convolutional Neural Networks (ACNNs) considering top-tier accuracy, execution speed, memory efficiency, and compatibility across diverse devices.\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Paris Haute Couture Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"luxury garment\"],\n            \"specification\": {\n                \"specific_dataset\": \"fashion\"\n            },\n            \"description\": \"A specialized dataset focusing on luxury garments from the Paris Haute Couture context, ideal for evaluating high-end designs.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"anisotropic scaling\",\n                \"irregular light occlusions\",\n                \"sophisticated color perturbations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Dynamic Routing Capsule Networks (EDRCN)\",\n            \"family\": \"Capsule Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"state_of_the_art\": true,\n                \"energy_efficiency\": true\n            },\n            \"description\": \"An advanced model for luxury fashion image classification, utilizing EDRCN to achieve high accuracy even under challenging conditions.\"\n        },\n        {\n            \"name\": \"Attention-based Convolutional Neural Networks (ACNNs)\",\n            \"comparison\": {\n                \"study\": \"energy efficiency, execution speed, memory optimization, and compatibility\"\n            }\n        }\n    ],\n    \"knowledge\": [\n        \"Energy conservation and computational resource optimization are paramount for integrating the classifier on devices with limited battery life and diverse hardware.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"energy-efficient\"\n        ],\n        \"target_device\": \"wearable and IoT devices with diverse hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"on high-end wearable and IoT devices under different constraints\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a high-level project involving a sophisticated classifier with clear performance and hardware requirements. The user has a high level of expertise and requests a comparative study to ensure optimal performance under real-world conditions. The stages of the project are well-defined and emphasize energy conservation and compatibility.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As the project lead for an elite deep learning team, task them with developing a specialized CNN model tailored for fine-grained classification of CIFAR-100's 100 distinct subgenres. Mandate the architecture to surpass 99% accuracy by incorporating recent pretask learning algorithms and demanding the execution of innovative, adaptive data augmentation methods that enhance generalization, boost resilience against outliers, and guarantee scalability for seamless integration of novel subcategories in the upcoming five-year roadmap.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"image recognition\",\n        \"description\": \"As the project lead, task the deep learning team with developing a specialized CNN model for fine-grained classification of CIFAR-100's 100 distinct subgenres. The model should exceed 99% accuracy and incorporate recent pretask learning algorithms. Additionally, demand the use of innovative, adaptive data augmentation methods to enhance generalization, resist outliers, and ensure scalability for future integration of new subcategories.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CIFAR-100\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_subgenres\": 100\n            },\n            \"description\": \"A dataset for fine-grained image classification with 100 different subcategories in the CIFAR-100 dataset.\",\n            \"preprocessing\": {\n                \"pretask_learning_algorithms\": [\"advanced\", \"state-of-the-art\"]\n            },\n            \"augmentation\": {\n                \"methods\": [\"adversarial\", \"context-aware\", \"meta-learning\"],\n                \"adaptive\": true\n            },\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"customized CNN\",\n            \"family\": \"CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_requirements\": {\n                    \"accuracy_target\": 0.99\n                },\n                \"pretraining_strategy\": \"advanced\"\n            },\n            \"description\": \"A specialized CNN model designed for fine-grained classification of CIFAR-100 subgenres, incorporating cutting-edge techniques for high accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"adapted feature extraction\", \"transfer learning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for GPU deployment\"],\n        \"demonstration\": \"include results on new subcategories within 5-year roadmap\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction, detailing a high-level project for a deep learning team with a specific focus on accuracy, novel algorithms, and adaptive augmentation techniques. However, the absence of exact deployment details or a roadmap for integrating new subcategories beyond five years may require further clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a premier healthcare analytics team, you're now tasked with developing an advanced, adaptive time-series forecasting framework. The extensive ILI dataset, comprising train, validation, and test sets with intricate daily, weekly, and yearly cycles, features a challenging 15-dimensional data structure (INPUT_SEQ_LEN=48) with complex dependencies. Your objective is to design a state-of-the-art model that integrates advanced forecasting techniques like ARIMA, Seasonal ARIMA, and Prophet, along with custom RNN-based architectures. The model must not only predict the subsequent 48-step trend (PRED_SEQ_LEN=48) with unparalleled precision, optimizing for mean absolute percentage error (MAPE) and symmetric mean absolute percentage error (SMAPE), but also include feature fusion of lagged values and exogenous variables. Additionally, ensure real-time, explainable AI for public health policymakers, and demonstrate superior performance over current benchmarks, while maintaining interpretability for practical decision-making.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"healthcare analytics\",\n        \"description\": \"As a project manager, develop an advanced, adaptive time-series forecasting framework using the ILI dataset with a 15-dimensional data structure and intricate daily, weekly, and yearly cycles. The task includes integrating ARIMA, Seasonal ARIMA, and Prophet models, as well as custom RNN-based architectures. The focus is on optimizing for MAPE and SMAPE, feature fusion, real-time explainable AI for policymakers, and demonstrating performance surpassing benchmarks while ensuring interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"SMAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 48,\n                \"DIMENSION\": 15,\n                \"COMPLEXITY\": \"involving intricate daily, weekly, and yearly cycles\"\n            },\n            \"description\": \"An extensive ILI dataset with train, validation, and test sets, featuring complex 15-dimensional time series data with dependencies across different time scales.\",\n            \"preprocessing\": [\"lagged value extraction\", \"exogenous variable fusion\"],\n            \"augmentation\": [],\n            \"visualization\": [\"explainable AI for policymakers\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Time-Series Forecasting Framework\",\n            \"family\": \"\",\n            \"type\": \"neural networks, classical machine learning\",\n            \"specification\": {\n                \"model_components\": [\"ARIMA\", \"Seasonal ARIMA\", \"Prophet\", \"Custom RNN\"],\n                \"feature_combination\": \"lagged values and exogenous variables\"\n            },\n            \"description\": \"A state-of-the-art time-series forecasting model designed to optimize for MAPE and SMAPE, while ensuring real-time, explainable AI for public health decision-making.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time, explainable AI is a must for public health policymakers.\",\n        \"Superior performance over current benchmarks.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"Explainable AI output for policymakers\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provides a comprehensive task description with clear requirements, specifying the dataset, model components, and metrics. Given the high expertise level, we've parsed details accordingly. However, specific model performance targets and real-time processing details are yet to be specified, as they require additional data.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager for \"Semi-structured Multilingual News Corpus Classification,\" your objective is to select a domain-specific BERT fusion model, focusing on 'Internet Philosophy' and 'Astronomy & Space Exploration', which requires efficient cross-lingual fine-tuning. Estimate the nuanced training durations for distinct hardware configurations (RTX A6000, M6000, and Google TPUv4), factoring in GPU memory footprint and accommodating a time-sensitive project with stringent deadlines and resource constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"multilingual news corpus\",\n        \"description\": \"As the project manager for the 'Semi-structured Multilingual News Corpus Classification', aim to select a domain-specific BERT fusion model, focusing on 'Internet Philosophy' and 'Astronomy & Space Exploration'. The model should support efficient cross-lingual fine-tuning within strict time and resource constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"efficiency (cross-lingual fine-tuning)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": {\n                    \"RTX A6000\": null,\n                    \"M6000\": null,\n                    \"Google TPUv4\": null\n                },\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"GPU memory footprint\",\n                \"value\": {\n                    \"RTX A6000\": null,\n                    \"M6000\": null\n                },\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Semi-structured Multilingual News Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"News Category\"],\n            \"specification\": {\n                \"languages\": [\"Internet Philosophy\", \"Astronomy & Space Exploration\"],\n                \"samples_per_domain\": null,\n                \"text_length\": null\n            },\n            \"description\": \"Corpus containing semi-structured news articles, focusing on Internet Philosophy and Astronomy & Space Exploration for classification.\",\n            \"preprocessing\": [\"cross-lingual pre-processing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Domain-specific BERT fusion model\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT fusion\",\n                \"language_fusion\": \"cross-lingual\",\n                \"efficiency_optimizations\": null\n            },\n            \"description\": \"A BERT-based model designed for efficient classification in Internet Philosophy and Astronomy & Space Exploration domains with cross-lingual fine-tuning capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The project has strict time and resource constraints.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain-specific feature extraction\"\n        ],\n        \"target_device\": [\"cloud (to leverage GPU power)\"],\n        \"deployment_endpoint\": \"N/A\",\n        \"inference_engine\": [\n            \"TensorFlow (for Google TPU compatibility)\"\n        ],\n        \"demonstration\": \"performance-based demonstrations for project manager\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clear about the project's focus, model requirements, and hardware configurations. The user's high expertise level is assumed for efficient guidance. However, specific metric values for performance, training times, and memory footprints are not provided. These details are left open for estimation based on resource constraints and project requirements.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "Design a highly specialized GraphSAGE model for Transfer Learning in bioinformatics, leveraging the extensive PubMed dataset. Emphasize performance improvement in both accuracy and cross-domain adaptability. Implement a cutting-edge compression technique that enhances computational efficiency without compromising quality for low-resource devices. Rigorously ensure real-time inference with a stringent requirement of sub-20ms latency for each unseen node classification, considering hardware constraints and energy efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Design a specialized GraphSAGE model for Transfer Learning in bioinformatics using the PubMed dataset. Focus on improving both accuracy and cross-domain adaptability, and implement a compression technique for low-resource devices with sub-20ms inference latency and high computational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cross-domain adaptability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\n                    \"unit\": \"ms\",\n                    \"value\": 20\n                },\n                \"constraint\": \"sub-20ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PubMed\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"node_type\": \"bioinformatics\",\n                \"edge_structure\": \"\"\n            },\n            \"description\": \"A bioinformatics dataset for the domain of Transfer Learning, emphasizing large scale node classification in the PubMed dataset\",\n            \"preprocessing\": [\n                \"normalize edges\",\n                \"clean node labels\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GraphSAGE\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transfer Learning\",\n                \"compression_method\": \"cutting-edge\"\n            },\n            \"description\": \"A GraphSAGE model designed for highly specialized node classification in bioinformatics with a focus on cross-domain adaptability, computational efficiency, and real-time inference.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"title\": \"Low-resource device optimization\",\n            \"description\": \"Strategies for enhancing computational efficiency with minimal impact on performance, specifically targeting low-resource devices.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            {\n                \"technique\": \"Node feature extraction\"\n            },\n            {\n                \"technique\": \"Hyperparameter tuning for hardware constraints\"\n            }\n        ],\n        \"target_device\": \"low-resource devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time\"\n        ],\n        \"demonstration\": \"Hardware-accelerated and energy-efficient\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction was parsed to include a high-level intent for building, with a high expertise level. The bioinformatics problem was translated to the graph machine learning area with the PubMed dataset specified. The performance metrics and complexity constraints are clearly stated. The lack of specifics for some sections, such as model name and the cutting-edge compression technique, is acknowledged as these may vary depending on the user's expertise and further research.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the visionary leader of a groundbreaking AI research team, your mandate is to develop an ultra-advanced, explainable, and scalable time-series analytics engine. You've been tasked with tackling a complex challenge that demands the invention of a state-of-the-art deep learning fusion model for electricity consumption forecasting. The extensive Electricity dataset, characterized by 96 lagging inputs and 321 intricate features (INPUT_SEQ_LEN=96, INPUT_DIM=321), is meticulously segmented into stratified subsets for training, validation, and rigorous real-world testing. The model should ingeniously incorporate sophisticated Seasonal-Trend decomposition, multi-resolution RNNs, and external factor embeddings to predict a daunting 192-step ahead forecast (doubling PRED_SEQ_LEN to 192, while PRED_DIM remains at 321), capturing long-term secular trends, short-term fluctuations, and dynamic real-time influences. In pursuit of precision, optimize mean squared logarithmic error (MSLE), mean absolute percentage error (MAPE), and rolling window correlation, concurrently ensuring computational efficiency, transparency, and resilience in the face of industry-advancing volatility. Embrace a holistic approach that pushes the boundaries of innovation while maintaining pragmatic viability, targeting a streamlined yet robust solution for this chronically evolving, multi-dimensional forecasting dilemma in the smart grid realm.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart grid\",\n        \"description\": \"Develop an ultra-advanced, explainable, and scalable time-series analytics engine for electricity consumption forecasting using a state-of-the-art deep learning fusion model. The model must include Seasonal-Trend decomposition, multi-resolution RNNs, and external factor embeddings, targeting 192-step ahead forecast (PRED_SEQ_LEN=192, PRED_DIM=321), while optimizing MSLE, MAPE, and rolling window correlation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSLE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"rolling_window_correlation\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational_efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            },\n            {\n                \"name\": \"transparency\",\n                \"value\": null,\n                \"unit\": null\n            },\n            {\n                \"name\": \"resilience_to_volatility\",\n                \"value\": null,\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"PRED_SEQ_LEN\": 192,\n                \"PRED_DIM\": 321\n            },\n            \"description\": \"A complex dataset for electricity consumption forecasting, characterized by 96 lagging inputs and 321 intricate features, stratified into train, validation, and test subsets.\",\n            \"preprocessing\": [\"Seasonal-Trend decomposition\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"deep learning fusion model\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"An innovative model combining multi-resolution RNNs, external factor embeddings, and fusion techniques to predict electricity consumption with precision, efficiency, transparency, and resilience.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"rolling window correlation\"],\n        \"target_device\": null,\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [],\n        \"demonstration\": \"holistic, scalable, and resilient solution for smart grid electricity forecasting\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction describes a complex and detailed project, suggesting a high expertise level. The model requirements, dataset characteristics, and performance metrics are clearly specified. However, missing specific details like target device, deployment endpoint, and exact model specifications indicate a potential area for completion. The confidence score accounts for the complexity and available information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "Develop a novel Mask R-CNN architecture for the \"Oceanic Integrity Watchdog System,\" focusing exclusively on Ultra-Deep Submersible Entity Recognition (UDSER). Train the model on an ultra-secure, encrypted dataset of 15,000 diverse marine vessels with minute details, including hull surface textures, complex keel geometries, and real-time Doppler-shifted sonar data. Achieve a precision of 99.5% under challenging conditions like severe turbidity and limited visibility, while ensuring sub-150 milliseconds latency on AI-accelerated M-seriesdgM GPUs, concurrently optimizing power efficiency for concurrent underwater sensor data processing without disruptions in other high-priority surveillance missions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"oceanic integrity\",\n        \"description\": \"Create a novel Mask R-CNN architecture specifically for the 'Oceanic Integrity Watchdog System' focused on Ultra-Deep Submersible Entity Recognition (UDSER). The model should be trained on an encrypted dataset of 15,000 diverse marine vessels with minute details like hull surface textures, complex keel geometries, and real-time Doppler-shifted sonar data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency (AI-accelerated M-series dGPM GPU)\",\n                \"value\": 150,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"power efficiency\",\n                \"value\": \"optimized for concurrent underwater sensor data processing\"\n            },\n            {\n                \"name\": \"disruptions in surveillance missions\",\n                \"value\": \"none\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ultra-secure marine vessels dataset\",\n            \"modality\": [\n                \"image\",\n                \"audio\"\n            ],\n            \"target_variables\": [\"marine vessel details\"],\n            \"specification\": {\n                \"size\": 15000,\n                \"data_security\": \"encrypted\",\n                \"content\": \"diverse marine vessels with minute details\",\n                \"additional_data\": [\"hull surface textures\", \"complex keel geometries\", \"real-time sonar data\"]\n            },\n            \"description\": \"A high-quality dataset for training the UDSER model, including minute details and real-time data.\",\n            \"preprocessing\": [\"data encryption\", \"enhanced儿子ar signal preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"novel Mask R-CNN for Oceanic Integrity Watchdog System\",\n            \"family\": \"Mask R-CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"depth_integration\": \"novel architecture for Ultra-Deep Submersible\",\n                \"object_recognition_focused\": \"UDSER\",\n                \"real-time_performance\": \"optimized for underwater environment\"\n            },\n            \"description\": \"An advanced Mask R-CNN architecture tailored for UDSER and high-performance requirements.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"data anonymization\", \"security protocol integration\"],\n        \"target_device\": \"AI-accelerated M-series dGPM GPUs\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for ultra-low latency\"],\n        \"demonstration\": \"performance under challenging conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction has been parsed into a high-level JSON response. The user's high expertise level suggests a deep understanding of the project requirements. The model's precision and latency requirements, as well as the dataset details, are clearly outlined. However, the deployment endpoint is missing and could be added based on the user's preferred service platform.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image classification", "instruction": "As the visionary project director overseeing the groundbreaking land usage classification system, your AI development team is tasked with creating a cutting-edge, distributed ViT model that combines an unprecedented amount of Landsat 8 satellite data with avant-garde feature fusion techniques. The model must not only reach a state-of-the-art transfer learning effectiveness, consistently maintaining an accuracy of 98.2% or above in various complex terrain and climate zones, but also demonstrate a 2.5% superiority over current benchmark methods. \n\nIn addition, design a fault-tolerant, horizontally scalable architecture that guarantees ultra-low-latency real-time processing across on-premises data centers, hybrid cloud environments, and multiple cloud service providers (AWS, Azure, and Google Cloud) while ensuring SLA compliance. Implement efficient serverless orchestration using advanced function chaining and service mesh integration. \n\nConcisely, provide well-documented, modular codebase, featuring detailed performance profiles, not only including precision-recall curves but also introducing advanced metrics such as TPR, FNR, and AUC-ROC. Develop a robust evaluation framework with nested cross-validation and time-series analysis for continuous monitoring, adaptive tuning, and iterative improvements, ensuring long-term sustainability and adaptability to the ever-evolving landscape of land use dynamics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"land usage\",\n        \"description\": \"Develop a cutting-edge, distributed ViT model for land usage classification using Landsat 8 satellite data. The model must achieve a state-of-the-art transfer learning effectiveness, with an accuracy of at least 98.2% and a 2.5% superiority over benchmarks. The architecture should be fault-tolerant, horizontally scalable, and suitable for real-time processing on-premises and across multiple cloud providers (AWS, Azure, Google Cloud). Implement efficient serverless orchestration and include performance metrics like precision, recall, TPR, FNR, and AUC-ROC in the codebase.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.982\n            },\n            {\n                \"name\": \"TPR\",\n                \"value\": null\n            },\n            {\n                \"name\": \"FNR\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"SLA compliance\",\n                \"value\": null,\n                \"unit\": \"guaranteed\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Landsat 8 satellite data\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"land usage classification labels\"\n            ],\n            \"specification\": {\n                \"data_source\": \"satellite imagery\",\n                \"image_dimensions\": \"variable\",\n                \"preprocessing_steps\": \"Landsat 8 preprocessing\"\n            },\n            \"description\": \"Landsat 8 data for land usage classification across diverse terrain and climate zones.\",\n            \"preprocessing\": [\n                \"satellite data preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"land usage maps\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"distributed ViT model\",\n            \"family\": \"Vision Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"combination of ViT and feature fusion techniques\",\n                \"transfer_learning\": true,\n                \"target_accuracy\": 0.982\n            },\n            \"description\": \"A distributed, fault-tolerant model that leverages ViT for land usage classification.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"advanced fusion techniques\"],\n        \"target_device\": [\"on-premises data centers\", \"hybrid cloud\", \"AWS, Azure, Google Cloud\"],\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"serverless\", \"advanced function chaining\", \"service mesh\"],\n        \"demonstration\": \"nested cross-validation and time-series analysis for continuous improvement\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a state-of-the-art model with high accuracy and a complex set of requirements, including scalability, fault tolerance, and cloud deployment. However, specific values for performance metrics (TPR, FNR, AUC-ROC), latency, and SLA compliance are not provided. These would ideally be confirmed with the user.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Design a fault-tolerant, 4K ultra-high-definition coastal surveillance system utilizing the COCO-Sea dataset and a cutting-edge DFF-enhanced Attention U-Net. Require not only sub-pixel accuracy in land, water, and structure classification, but also the ability to detect anomalies in real-time under harsh weather, low light, and HDR conditions.demand a 30% efficiency increase without compromising image clarity, and ensure operational latency remains consistently below an ultra-responsive threshold for seamless live monitoring. Additionally, implement a machine learning model that can adapt and learn from dynamic environmental changes autonomously.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"coastal surveillance\",\n        \"description\": \"Design a fault-tolerant 4K ultra-high-definition coastal surveillance system using the COCO-Sea dataset and a DFF-enhanced Attention U-Net. The system must achieve sub-pixel accuracy in land, water, and structure classification, real-time anomaly detection under challenging conditions (harsh weather, low light, HDR), and a 30% efficiency increase without affecting image clarity. Operational latency should remain below an ultra-responsive threshold for live monitoring.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"sub-pixel accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"efficiency\",\n                \"value\": 0.3, \n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO-Sea\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"land\", \"water\", \"structure\"],\n            \"specification\": {\n                \"image_resolution\": \"4K (UHD)\",\n                \"variable_types\": [\"classification\", \"dynamic\"]\n            },\n            \"description\": \"A dataset designed for coastal surveillance, featuring high-resolution images for land, water, and structure classification, along with dynamic data for anomaly detection.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DFF-enhanced Attention U-Net\",\n            \"family\": \"Attention U-Net\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptiveness\": \"autonomous learning\",\n                \"latency\": \"ultra-responsive\"\n            },\n            \"description\": \"A cutting-edge DFF-enhanced Attention U-Net for real-time coastal surveillance with fault tolerance, sub-pixel accuracy, and dynamic environmental adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Handle harsh weather, low light, and HDR conditions.\",\n        \"Enhance efficiency by 30% without sacrificing image clarity.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge or IoT device to ensure low latency\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time anomaly detection\"],\n        \"demonstration\": \"Adaptive and autonomous learning during runtime\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed based on the given instruction, which describes a machine learning project with a clear focus on model building, using a high-end DFF-enhanced Attention U-Net. The user's expertise level is assumed high due to the complexity of the task. However, some specific performance metrics for land, water, and structure classification are not provided, and the user has not specified a deployment endpoint or inference engine. The confidence score is somewhat lower due to the need for these details to be completed.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a machine learning project manager, you are tasked with overseeing the creation of an advanced traffic forecasting system for real-world data. The Traffic dataset, now contaminated with noise and irregularities, has been divided into four segments: a historical train set, a validation set for hyperparameter tuning, a seasonal adaptation set, and a strictly blind test set. Each data point contains a dynamic 192-step pattern (INPUT_SEQ_LEN=192, INPUT_DIM=1724) showcasing hourly fluctuations. The deep learning model must design an architecture that predicts the subsequent 192-step pattern while exhibiting adaptability to evolving traffic patterns, detecting anomalies, and incorporating external events. In addition to optimizing for MSE and MAE, aim for peak performance across diverse forecasting horizons, and demand clear interpretability of the model's decision-making process, including a detailed explanation of the validation strategy and any novel techniques employed. Ensure the team delivers a comprehensive report outlining their methodological advancements and real-world impact evaluation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"traffic prediction\",\n        \"description\": \"Create an advanced traffic forecasting system for real-world data with the Traffic dataset. It's contaminated with noise and irregularities, divided into train, validation, seasonal adaptation, and blind test sets. The model should predict 192-step patterns, adapt to evolving traffic patterns, detect anomalies, and consider external events. Performance metrics include MSE, MAE, and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"interpretability\": {\n            \"validation_strategy\": \"Detailed explanation of the validation process and model decision-making\",\n            \"novel_techniques\": \"Include any new methods for forecasting, adaptability, and anomaly detection\"\n        },\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Traffic\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 192,\n                \"INPUT_DIM\": 1724\n            },\n            \"description\": \"A dataset with real-world traffic data, contaminated, split into train, validation, seasonal adaptation, and blind test sets, featuring 192-step hourly patterns.\",\n            \"preprocessing\": [\"Handling noise and irregularities\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Data exploration and understanding\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Deep learning model for time-series forecasting with adaptability and anomaly detection\"\n            },\n            \"description\": \"Design a model to predict 192-step patterns with interpretability, handling evolving traffic patterns and external events\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Methodological advancements and impact evaluation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A comprehensive report detailing methodological advancements and real-world impact assessment\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provides ample information for a high-level project manager, including the task type, domain, dataset specifics, model requirements, and performance goals. Interpretability and validation strategy are clearly specified, while some metric values are left to be filled in. The 'expertise' is set to 'high' given the complexity of the task. The service and demonstration aspects are derived from the context.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for the machine learning team, design an innovative CNN model that integrates spectral fusion and fine-tunes VGG16, targeting CIFAR-100 classification with an ambitious top-5 accuracy of 99.9%. Mandate step-by-step explainability, efficient memory optimization for resource-constrained embedded devices, and a thorough evaluation of trade-offs between performance and computational footprint.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"general\",\n        \"description\": \"Design an innovative CNN model that integrates spectral fusion and fine-tunes VGG16 for CIFAR-100 classification. Aim for a top-5 accuracy of 99.9% while prioritizing step-by-step explainability, efficient memory optimization for resource-constrained embedded devices, and a thorough evaluation of trade-offs between performance and computational footprint.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"top-5 accuracy\",\n                \"value\": 0.999\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory optimization\",\n                \"value\": null,\n                \"unit\": \"embedded device-specific constraints\"\n            },\n            {\n                \"name\": \"computational footprint vs. performance trade-off\",\n                \"value\": null,\n                \"unit\": \"optimized for balanced performance\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CIFAR-100\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"image classification\"],\n            \"specification\": {\n                \"dataset_size\": \"large\",\n                \"image_size\": \"CIFAR-100 requirements\"\n            },\n            \"description\": \"A dataset for image classification tasks, specifically focusing on CIFAR-100.\",\n            \"preprocessing\": [\"spectral fusion\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Innovative CNN (Spectral Fusion + VGG16)\",\n            \"family\": \"CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"original_model\": \"VGG16\",\n                \"adaptations\": \"spectral fusion\"\n            },\n            \"description\": \"A model integrating spectral fusion on top of VGG16 for CIFAR-100 classification with explainability and resource efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Step-by-step explainability will be included using techniques like Grad-CAM or attention mechanisms.\",\n        \"Memory optimization will target resource-constrained devices through techniques like model pruning, quantization, or efficient data structures.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained embedded devices\",\n        \"deployment_endpoint\": \"not specified\",\n        \"inference_engine\": [\"optimized for memory and runtime\"],\n        \"demonstration\": \"trade-off performance vs. computational footprint will be showcased\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided clear specifications for a machine learning project, with a focus on model architecture, performance, and resource constraints. The high expertise level suggests the user may have a more advanced understanding of these requirements. However, some specific details such as the exact trade-off and the precise demonstration methods are left open to interpretation.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The project must design a real-time, sentiment-aware Q&A system for e-commerce customer service, integrating the recently updated AmazonQA dataset. Implement a hybrid model combining DistilBERT, a Explainable AI (XAI) layer, and a lightweight efficient inference technique, targeting >=95% accuracy, <2ms response time, and ensuring interpretability and emotion recognition while maintaining scalability across high-traffic volumes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"e-commerce customer service\",\n        \"description\": \"Design a real-time sentiment-aware Q&A system, integrating the AmazonQA dataset. The system must combine DistilBERT, incorporate an Explainable AI layer, and use a lightweight inference technique for fast response (<2ms), achieving >=95% accuracy while maintaining interpretability and emotion recognition.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": 2,\n                \"unit\": \"ms\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": 1\n            },\n            {\n                \"name\": \"emotion recognition\",\n                \"value\": 1\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": 1\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AmazonQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"recently updated\"\n            },\n            \"description\": \"A dataset for sentiment-aware Q&A in e-commerce customer service, targeting integration with the project.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DistilBERT hybrid model\",\n            \"family\": \"DistilBERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability_layer\": \"XAI layer\",\n                \"efficiency_technique\": \"lightweight inference\"\n            },\n            \"description\": \"A sentiment-aware QA model that combines DistilBERT, an XAI layer for interpretability, and a lightweight inference technique.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time, requiring <2ms response time\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"lightweight and efficient\"],\n        \"demonstration\": \"Emphasis on interpretability and emotion recognition for high-traffic volumes\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear and complex project requirement, targeting high accuracy and fast response time with interpretability, emotion recognition, and scalability. The user's expertise level is high, indicating a deep understanding of the technical requirements. The JSON structure reflects all specified criteria.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "node classification", "instruction": "Construct a robust node classification model for the NELL dataset using a cutting-edge hypergraph-based deep learning framework. Focus on enhancing noise reduction techniques, managing intricate multi-relational dependencies, and prioritize parallel scalability. Aim for a minimum accuracy of 95%, employing a multi-stage hyperparameter optimization strategy with nested cross-validation. Conduct rigorous experiments, analyzing trade-offs, and present the findings in a detailed, visually-aided report showcasing the performance trajectory across epochs.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graphs\",\n        \"description\": \"The user wants to build a robust node classification model for the NELL dataset using a cutting-edge hypergraph-based deep learning framework. Emphasis is on noise reduction, handling complex multi-relational dependencies, and achieving parallel scalability. The target accuracy is set at 95% with a multi-stage hyperparameter optimization strategy involving nested cross-validation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"parallel scalability\",\n                \"value\": 1.0,\n                \"unit\": \"optimized\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node labels\"],\n            \"specification\": {\n                \"node_types\": [\"entities\", \"relations\"],\n                \"edge_types\": [\"dependencies\"],\n                \"size\": {\n                    \"nodes\": \"unknown\",\n                    \"edges\": \"unknown\"\n                }\n            },\n            \"description\": \"A knowledge graph dataset with intricate multi-relational dependencies.\",\n            \"preprocessing\": [\"noise reduction techniques\"],\n            \"augmentation\": [],\n            \"visualization\": [\"performance trajectory across epochs\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hypergraph-based Deep Learning Model\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"noise_reduction\": true,\n                \"multi-relational\": true,\n                \"parallelization\": true\n            },\n            \"description\": \"A cutting-edge model that enhances noise reduction, handles complex dependencies, and prioritizes parallel scalability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Performance trade-offs between noise reduction techniques and model complexity should be thoroughly analyzed.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"multi-stage hyperparameter optimization with nested cross-validation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A detailed, visually-aided report showcasing performance across epochs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction was parsed based on the JSON schema. The user's expertise is assumed to be high due to the complexity of the task. However, specific details such as the exact dataset size and certain metrics are left unspecified and should be collected if possible. The confidence score is high as most details were included, but some finer points like deployment specifics are left open.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a seasoned project manager for an AI-driven research firm, your task is to design a highly robust and discriminative deep convolutional neural network (CNN) architecture for a multi-label image classification project. With a sparse labeled data set of only 1,000 cat images showcasing diverse breeds, wild vs. domestic distinctions, and various challenging environmental conditions like low-light and inconsistent resolution, you must not only ensure accurate species recognition but also optimize for precision and recall. Create a detailed plan encompassing data augmentation techniques, model training strategies, performance metrics, and a stringent validation process using cross-validation folds to handle the scarcity of data. Additionally, outline the plan for fine-tuning using transfer learning and a comparative study of different architectures, like VGG16, ResNet, and Inception, to maximize efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"AI-driven research\",\n        \"description\": \"Design a highly robust and discriminative deep CNN architecture for a multi-label image classification project. The dataset consists of 1,000 labeled cat images with diverse breeds, wild vs. domestic distinctions, and challenging environmental conditions. The objective is to achieve accurate species recognition with a focus on precision and recall, addressing data scarcity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\"\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cat Image Dataset (sparse)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\", \"wild/domestic\"],\n            \"specification\": {\n                \"size\": 1000,\n                \"label_distribution\": \"sparse\",\n                \"challenges\": [\n                    \"low-light\",\n                    \"inconsistent resolution\"\n                ]\n            },\n            \"description\": \"A dataset containing 1,000 cat images with diverse labels for breeds, domestic/wild distinction, and challenging conditions.\",\n            \"preprocessing\": [\n                \"data augmentation techniques\"\n            ],\n            \"augmentation\": [\n                \"rotation\",\n                \"flipping\",\n                \"cropping\",\n                \"brightness/contrast adjustments\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\",\n            \"fine_tuning_data\": true\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom deep CNN\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"CNN\",\n                \"deep_layers\": {\n                    \"VGG16/ResNet/Inception-based\": [\n                        {\n                            \"model\": \"VGG16\",\n                            \"pre-training\": \"finetuning\"\n                        },\n                        {\n                            \"model\": \"ResNet\",\n                            \"pre-training\": \"finetuning\"\n                        },\n                        {\n                            \"model\": \"Inception\",\n                            \"pre-training\": \"finetuning\"\n                        }\n                    ]\n                },\n                \"optimization\": \"comparative study\"\n            },\n            \"description\": \"A comprehensive CNN architecture designed for multi-label image classification, utilizing fine-tuning from VGG16, ResNet, and Inception networks.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction\",\n            \"dimensionality reduction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"multi-label prediction outputs and confusion matrix\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction has been parsed with a high confidence, capturing the intention to build a model for multi-label image classification using deep CNNs. The project manager's expertise is assumed to be high, and the requirements for data preprocessing, model training, transfer learning, and architecture comparison are clearly outlined. The confidence score is high due to the clarity of the instructions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design and develop a machine translation system with a state-of-the-art real-time adaptation engine, necessitating a hierarchical learning approach that captures user feedback from diverse e-commerce platforms (Shopify and Magento) in real-time. The system must not only aim for a BLEU score of 45+ but consistently demonstrate incremental improvements by optimizing for low-latency, accurate translation of dynamic product descriptions. Ensure seamless integration, achieving human-like quality without manual intervention, and conduct rigorous A/B testing to validate cultural nuances across global marketplaces for a unified and immersive user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Design and develop a machine translation system with a state-of-the-art real-time adaptation engine. The system requires a hierarchical learning approach, capturing user feedback from diverse platforms (Shopify and Magento) in real-time. It must achieve a BLEU score of 45+ and exhibit incremental improvements for low-latency, accurate translation of dynamic product descriptions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU Score\",\n                \"value\": 45.0\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Shopify and Magento Data (Real-time)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"user_feedback_sources\": [\"real-time\"],\n                \"domains\": [\"e-commerce\"],\n                \"data_sources\": [\"Shopify\", \"Magento\"]\n            },\n            \"description\": \"Diverse e-commerce platform user feedback for real-time adaptation\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Learning-based Machine Translation Model\",\n            \"family\": \"State-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptation_engine\": \"real-time\",\n                \"feedback_integration\": \"hierarchical\"\n            },\n            \"description\": \"System utilizing a hierarchical learning approach with real-time adaptation for low-latency and accurate translation\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time learning and cultural nuances for global marketplaces\",\n        \"Human-like quality for seamless user experience\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time adaptation\", \"user feedback integration\"],\n        \"target_device\": \"low-latency, cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for performance\"],\n        \"demonstration\": \"A/B testing for cultural nuances\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the high-level intent to build a machine translation system, with a focus on real-time adaptation, low-latency, and performance metrics. The user's expertise level is interpreted as high, which suggests they might be familiar with specific requirements for such a project. The dataset from multiple sources is described with the appropriate source mode. However, there is a possibility that the user might need to provide specific deployment details and the exact type of A/B testing method.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Incorporate the following stringent requirements into the AI project scope:\r\n\r\n1. Develop a highly specialized, real-time coral reef monitoring system using an enhanced DeepLabV3+ architecture for the marine domain. Focus on optimizing not just segmentation but also scalability across diverse, dynamically turbid waters.\r\n\r\n2. Implement a novel TurbidityAdaptiveNet layer, capable of adaptively adjusting to extreme water clarity fluctuations, with at least 15% improved performance over existing coral detection algorithms in terms of F1-score.\r\n\r\n3. Emphasize near-zero-latency inference to ensure seamless and instantaneous data processing, allowing for real-time monitoring without compromising battery life or device responsiveness in wearable underwater equipment.\r\n\r\n4. Conduct rigorous benchmarking with contemporary state-of-the-art models in terms of computational efficiency, targeting a minimum 20% reduction in resource utilization for efficient deployment on resource-constrained embedded platforms.\r\n\r\n5. Collaborate with the team to design a user-friendly interface for seamless integration of data collection, model predictions, and immediate feedback to aquatic researchers, enabling them to make informed decisions on the spot.\r\n\r\n6. Present a detailed project roadmap and performance metrics, including milestones for turbidity adaptation and F1-score improvements, to ensure alignment with industry best practices and expedite project completion.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"marine\",\n        \"description\": \"Incorporate stringent requirements for a real-time coral reef monitoring system using an enhanced DeepLabV3+ for marine environments. Focus on optimization for segmentation, scalability, turbidity adaptability, near-zero latency, computational efficiency, and user interface for aquatic researchers.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score improvement\",\n                \"value\": 0.15\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 0.0,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Resource utilization reduction\",\n                \"value\": 0.2,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coral Reef Monitoring Dataset\",\n            \"modality\": [\n                \"image\",\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"architecture\": \"DeepLabV3+\",\n                \"adaptation_layers\": \"TurbidityAdaptiveNet\"\n            },\n            \"description\": \"A dataset for real-time coral reef monitoring with diverse, dynamically turbid waters. Includes time series data for environment fluctuations.\",\n            \"preprocessing\": [\n                \"TurbidityAdaptiveNet adaptation\"\n            ],\n            \"augmentation\": [\n                \"Adaptive to water clarity fluctuations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced DeepLabV3+ with TurbidityAdaptiveNet\",\n            \"family\": \"DeepLab\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"latency\": \"near-zero\",\n                \"resource_efficiency\": \"optimized for resource-constrained platforms\"\n            },\n            \"description\": \"A specialized AI model for real-time coral reef monitoring with improved F1-score and adaptability to turbidity.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Collaborate with the team for efficient model development and deployment\",\n        \"Optimization for resource-constrained underwater equipment\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"embedded platforms\",\n        \"deployment_endpoint\": \"wearable underwater equipment\",\n        \"inference_engine\": [\n            \"Real-time data processing\"\n        ],\n        \"demonstration\": {\n            \"focus\": [\"turbidity adaptation\", \"F1-score milestones\"]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction has been parsed into a high-level JSON format reflecting the requirements: architecture enhancements, adaptive layer, real-time performance, benchmarking, and user interface. Expertise level is assumed to be high due to the complexity of the project requirements. Some performance metrics and specific deployment details are expected to be filled based on further analysis or consultation.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a seasoned project manager, you're tasked with elevating the financial news classification system for a top-tier media analytics platform. Demand from the ML team, using the sophisticated Reuters-21578 dataset, is to employ fastText for optimal balance between accuracy and speed. Aim for an impressive 90%+ accuracy with a 30% reduction in critical false positives for timely market impacts. Emphasize platform-wide scalability, real-time analysis, and efficient design. Mandate a comparative analysis with the latest algorithms and seamless integration into the ultra-modular Newsfeed Algorithm v3.0, considering both computational efficiency and user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"financial news analysis\",\n        \"description\": \"Enhance the financial news classification system for a media analytics platform using the Reuters-21578 dataset. Employ fastText to achieve at least 90% accuracy with a 30% reduction in critical false positives. Focus on platform-wide scalability, real-time analysis, and efficient design.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"false positive reduction\",\n                \"value\": 0.3\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"% decrease in inference time\"\n            },\n            {\n                \"name\": \"user experience\",\n                \"value\": null,\n                \"unit\": \"improvement in UX\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"accuracy_target\": 0.9\n            },\n            \"description\": \"A sophisticated dataset for financial news classification, used to improve the platform's performance.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"fastText\",\n            \"family\": \"pre-trained models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed-accuracy_tradeoff\": \"optimized for balance\"\n            },\n            \"description\": \"Employ fastText for high accuracy with reduced false positives in the financial news classification system.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"scalability\",\n            \"real-time analysis\",\n            \"efficient design\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"ultra-modular Newsfeed Algorithm v3.0\",\n        \"inference_engine\": [\"comparative analysis with latest algorithms\"],\n        \"demonstration\": {\n            \"emphasis\": \"timely market impacts and seamless integration\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has a high level of expertise, which influenced the understanding of the task and requirements. The specific intent to build a model is clear, and the metrics focus on accuracy and false positive reduction. The need for scalable, real-time, and efficient system design, as well as algorithm comparisons, is accounted for in the 'service' section. However, the target reduction in false positives may not be realized in the absence of a detailed model tuning plan or architecture description.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, guide your ML team to design an adaptive, real-time translation solution for a global e-reading empire. In addition to selecting a cutting-edge transformer-based model that optimizes speed (up to 90% throughput), idiomatic expression preservation (using transformer-XL), and cultural finesse through cross-lingual sentiment analysis, ensure the platform can decipher regional dialects with up to 98% accuracy. The dataset must consist of at least 500,000 diverse, timestamped chapters from Korean web novels, reflecting current linguistic trends, and meet stringent latency standards of less than 500 milliseconds to guarantee seamless user experience across platforms.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"global e-reading empire\",\n        \"description\": \"Design an adaptive, real-time translation solution that utilizes a cutting-edge transformer-based model optimized for speed (90% throughput), idiomatic expression preservation (Transformer-XL), and cultural sensitivity via cross-lingual sentiment analysis. The system must accurately decipher regional dialects with up to 98% accuracy and have a dataset of at least 500,000 diverse timestamped chapters from Korean web novels, reflecting current linguistic trends. The solution must meet latency standards of less than 500 milliseconds for seamless user experience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"throughput\",\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"idiomatic expression preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"dialect decipher accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.5,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Korean web novels (500k diverse chapters)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"minimum_size\": 500000,\n                \"timestamped\": true,\n                \"linguistic_trends\": true\n            },\n            \"description\": \"A dataset of 500,000 diverse timestamped chapters from Korean web novels, representing current linguistic trends.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-based model (Transformer-XL)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed_optimization\": \"90% throughput\",\n                \"idiomatic_expression_preservation\": \"Transformer-XL\",\n                \"cultural_sensitivity\": \"cross-lingual sentiment analysis\"\n            },\n            \"description\": \"A cutting-edge, real-time translation model optimized for speed and cultural sensitivity.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"global\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time, seamless experience across platforms\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided indicates a high-level project with a focus on machine translation and specifies the model requirements, dataset, and latency expectations. Given the project manager role, the expertise level is assumed to be high. Some expected performance measures for idiomatic expression and cross-lingual sentiment analysis are not explicitly mentioned, so these have been left as null.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "object detection", "instruction": "Develop a tailored YOLOv5.1 architecture for a premium luxury grocery store, focusing on the ExquisiteGourmetProductDataset of 1,500 exclusive items. Mandate an F1-score of 95% with at least 95% precision in detecting gourmet products, while maintaining real-time inventory management with sub-millisecond latency of 1 millisecond. Integrate an AI-assisted system for automatic product tagging and instant updates to the high-velocity database, ensuring zero inventory discrepancies and uninterrupted customer experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury grocery\",\n        \"description\": \"Develop a tailored YOLOv5.1 architecture specifically for a premium luxury grocery store, focusing on the ExquisiteGourmetProductDataset with 1,500 exclusive items. The model should achieve a minimum F1-score of 95% and at least 95% precision in detecting gourmet products.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.001,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ExquisiteGourmetProductDataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A dataset containing 1,500 exclusive items for a luxury grocery context.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5.1 (luxury grocery)\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"target_detection\": {\n                    \"gourmet_products\": {\n                        \"precision\": 0.95,\n                        \"f1_score\": 0.95\n                    }\n                },\n                \"real_time_latency\": {\n                    \"inventory_management\": 0.001\n                }\n            },\n            \"description\": \"A custom YOLOv5.1 architecture catering to gourmet product detection in a luxury grocery store with real-time inventory management and AI-assisted tagging.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"AI-assisted product tagging\"],\n        \"target_device\": \"cloud with sub-millisecond latency support\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for high-speed, low-latency inference\"],\n        \"demonstration\": \"Zero inventory discrepancies, real-time updates, and uninterrupted customer experience during gourmet product detection\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction clearly describes a complex machine learning project, including a tailored object detection model, specific performance metrics, and real-world considerations like inventory management. The user's high expertise level suggests they understand the technical aspects, and the service requirements are designed based on the stated constraints. However, the deployment endpoint is not specified, and the confidence score may be slightly lower due to the assumption for a cloud deployment.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the Project Director for the interdisciplinary AI-driven Time Series Analytics Division, your task has evolved into a multifaceted challenge. You are now tasked with the \"Advanced Smart Grid Datapool Challenge\" (with a challenging sequence length of 96 hours, containing 321 indicators such as renewable energy fluxes, historical consumption patterns, and meteorological factors, divided into four distinct sub-sets reflecting diverse annual cycles, intricate interdependencies, and seasonality). \n\nDesign a next-generation forecasting architecture, integrating state-of-the-art Transformer-XL with a nested attention mechanism and deep residual connections. The model must not only predict the subsequent 96-hour period but also factor in real-time anomaly detection, capturing external market dynamics, and quantify prediction uncertainty using Bayesian methods. \n\nStrive for optimal performance by minimizing mean absolute percentage error (MAPE), mean log-normal error (MLE), and simultaneously considering computational efficiency and energy consumption. Implement model-agnostic interpretability techniques, ensuring human-understandable insights while enhancing model resilience to unpredictable micro-variability within the grid. Maintain stringent latency targets of less than 50 milliseconds for real-time operational decisions, all while conducting exhaustive A/B testing on an international scale, comparing performance across six regional datasets to demonstrate the universal applicability and superiority of your solution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart grid\",\n        \"description\": \"As the Project Director for the AI-driven Time Series Analytics Division, your task is to design an 'Advanced Smart Grid Datapool Challenge' model. This involves using Transformer-XL with nested attention and deep residual connections, considering real-time anomaly detection, external market dynamics, and prediction uncertainty using Bayesian methods. Performance metrics include MAPE, MLE, computational efficiency, and energy consumption. Model interpretability is mandatory with latency targets <50ms and A/B testing on six regional datasets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.05,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Advanced Smart Grid Datapool Challenge\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [\"321 indicators\"],\n            \"specification\": {\n                \"sequence_length\": 96,\n                \"indicators\": 321,\n                \"sub_sets\": 4\n            },\n            \"description\": \"A dataset with a 96-hour sequence length, 321 indicators (renewable energy fluxes, consumption patterns, meteorological factors), divided into four distinct subsets representing diverse annual cycles, interdependencies, and seasonality.\",\n            \"preprocessing\": [\"Transformer-XL\", \"nested attention mechanism\", \"deep residual connections\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model interpretability\", \"A/B testing\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL with nested attention and deep residual connections\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"anomaly_detection\": true,\n                \"Bayesian_methods\": true,\n                \"interpretability\": true\n            },\n            \"description\": \"A model designed to forecast the subsequent 96-hour period, with real-time anomaly detection, and consideration for external market dynamics and prediction uncertainty using Bayesian methods.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time operational decisions\"],\n        \"demonstration\": \"A/B testing across six regional datasets\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a detailed description of the problem, including specific requirements and performance targets. Given the complexity and scope of the task, the expertise level is assumed to be high. The 'service' section includes deployment to the cloud and real-time operational decisions. The 'confidence' score is lower due to the lack of precise latency value and some assumptions made for model-specific details.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "As the project director of a cutting-edge medical AI research team, your task is to engineer a sophisticated diabetic retinopathy classifier that not only discerns but also differentiates five distinct severity subcategories (mild, moderate, severe, proliferative, and neovascular) using the extensive Kaggle dataset. The project mandates a next-generation VGG-16++ architecture adaptation, which demands not only unparalleled accuracy of 98% or higher, but also a precision-recall parity with a stringent benchmark of 99%. Strive for a precision above 95% in preventing false negatives, thereby minimizing critical diagnostic pitfalls. To guarantee real-world implementation in remote, resource-constrained ophthalmic facilities, design the model with inherent edge computing capabilities, ensuring near-instant inference times and optimal hardware utilization. Your contribution must include a meticulously detailed design blueprint, a multifaceted implementation roadmap encompassing progressive milestones, rigorous performance evaluations incorporating comparative studies with state-of-the-art rivals, and a compelling demonstration of the technological breakthrough and practical viability for widespread adoption in saving lives.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medical\",\n        \"description\": \"Engineer a diabetic retinopathy classifier with VGG-16++ architecture adapted for five severity subcategories (mild, moderate, severe, proliferative, and neovascular) using the Kaggle dataset. The classifier should have an accuracy of at least 98% and precision-recall parity above 99%, prioritizing precision above 95% to avoid false negatives. The model should be designed for edge computing with near-instant inference and efficient hardware utilization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": \"real-time\",\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"resource utilization\",\n                \"value\": \"optimal\",\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Diabetic Retinopathy Kaggle\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"severity category\"],\n            \"specification\": {\n                \"image_shape\": [224, 224, 3],\n                \"classes\": 5\n            },\n            \"description\": \"A dataset for diabetic retinopathy classification with five severity subcategories.\",\n            \"preprocessing\": [\"VGG-16++ architecture adaptation\", \"Data augmentation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"VGG-16++\",\n            \"family\": \"VGG\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"next-generation VGG-16++\",\n                \"parameters\": {\n                    \"architectureadaptation\": \"for diabetic retinopathy classification\"\n                },\n                \"accuracy_target\": 0.98\n            },\n            \"description\": \"Adapted VGG-16++ specifically for the task of classifying diabetic retinopathy severity subcategories.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Edge computing and its implications for real-world deployment\",\n        \"Designing models with optimal hardware utilization and near-instant inference times\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"VGG-16++ adaptation\", \"ensemble techniques for parity\"],\n        \"target_device\": \"edge computing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient inference libraries for edge devices\"],\n        \"demonstration\": \"next-generation diabetic retinopathy classifier with proof of concept performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided specific details for a high-level AI project, which indicates a high level of expertise. It outlines requirements for a cutting-edge VGG-16++ classifier, performance metrics, and edge computing implementation. However, missing details are the specific link to the Kaggle dataset and a detailed roadmap or milestones.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager, you must guide the AI team to develop a state-of-the-art time-series forecaster for a complex, multi-source dataset (ILI and temperature) with irregular time intervals. The dataset has been divided into subsets for robust training (uneven distribution). The model must now handle longer sequences (INPUT_SEQ_LEN=48), integrate new context features (15-dimensional INPUT_DIM), and prioritize interpretability through SHAP analysis. In addition to achieving high accuracy (MSE and MAE reduction), the model must be designed for real-time forecasting on low-resource devices while maintaining efficiency and explainability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The project requires a state-of-the-art time-series forecaster for a complex multi-source dataset (ILI and temperature) with irregular time intervals. The dataset is split into subsets for robust training with uneven distribution. The model must handle longer sequences (INPUT_SEQ_LEN=48), include new context features (15-dimensional INPUT_DIM), prioritize interpretability through SHAP analysis, and achieve high accuracy with MSE and MAE reduction.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time forecasting\",\n                \"value\": null,\n                \"unit\": \"time\"\n            },\n            {\n                \"name\": \"efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs or memory\"\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI and temperature dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 48,\n                \"INPUT_DIM\": 15,\n                \"irregular_intervals\": true,\n                \"subset_distribution\": \"uneven\"\n            },\n            \"description\": \"A complex, multi-source dataset with ILI and temperature data, divided into subsets for robust training with varying time intervals.\",\n            \"preprocessing\": [\n                \"Irregular time interval handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"SHAP analysis for interpretability\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"sequence_length\": 48,\n                \"context_features\": 15,\n                \"explainability\": \"SHAP analysis\",\n                \"optimization\": \"real-time, low-resource\"\n            },\n            \"description\": \"A high-performance time-series forecaster designed for complex multi-source datasets with interpretability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"integration of new context features\"\n        ],\n        \"target_device\": \"low-resource devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient inference algorithms\"],\n        \"demonstration\": \"Explainable real-time forecasting\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed with a high-level intent to build a state-of-the-art model. The user is assumed to have high expertise, so the requirements are detailed. Performance metrics, complexity requirements, and dataset specifics are included. However, without specific values for performance or model complexity, there is room for clarification. The service requirements for real-time forecasting, efficiency, and interpretability are incorporated.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a distinguished project manager for a trailblazing AI research team, your multifaceted mandate is to develop an advanced energy demand forecasting system. Using the Electricity dataset, partitioned into intricately balanced training, validation, and testing subsets, design an ensemble of deep-learning architectures that can decipher sophisticated temporal patterns, including long-term dependencies and short-term anomalies. The input sequence, consisting of an extensive array of 96 time steps with a high-dimensional feature set of 321 unique attributes (INPUT_SEQ_LEN=96, INPUT_DIM=321), should not only forecast the next 192 hours but also account for geographical variations, exceptional weather events, and subtle trends. Strive for benchmark accuracy by minimizing both root mean squared error (RMSE) and mean absolute percentage error (MAPE), ensuring robustness across multiple seasonal cycles. Extensively investigate and fine-tune model architectures, hyperparameters, and regularization techniques, comparing your results with the most recent literature and addressing potential regularization biases. Document your findings in a detailed, clearly structured research paper, substantiated by visualizations and comprehensive experimental validation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy demand prediction\",\n        \"description\": \"Develop an advanced energy demand forecasting system using the Electricity dataset. The system should handle intricate temporal patterns, long-term dependencies, short-term anomalies, geographical variations, weather events, and subtle trends. Target input sequence is 96 time steps with 321 dimensions, and the model should forecast the next 192 hours. Aim for benchmark accuracy with RMSE and MAPE minimization while accounting for multiple seasonal cycles. Conduct thorough model tuning, comparison with literature, and regularization analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"PRED_SEQ_LEN\": 192,\n                \"geographical variations\": true,\n                \"weather events\": true,\n                \"subtle trends\": true\n            },\n            \"description\": \"The Electricity dataset with a partitioned balance among training, validation, and testing subsets, including extensive features and geographical and weather considerations.\",\n            \"preprocessing\": [\"feature normalization\", \"time-series segmentation\"],\n            \"augmentation\": [],\n            \"visualization\": [\"prediction vs actual values\", \"performance curves\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble of deep-learning architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \" handles long-term dependencies and short-term anomalies\",\n                \"hyperparameters\": {\n                    \"model complexity\": \"optimized for multiple seasonal cycles\",\n                    \"regularization\": \"addressing potential biases\"\n                }\n            },\n            \"description\": \"An ensemble of deep-learning models tailored for the energy demand forecasting task\"\n        }\n    ],\n    \"knowledge\": [\n        \"latest literature comparison\",\n        \"regression analysis\",\n        \"visualizations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal feature extraction\",\n            \"geographical and weather data fusion\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"performance across multiple seasonal cycles and case studies\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was generated based on the given instruction, assuming a high level of expertise for the project manager. The user's intent is to build, and the detailed problem description captures the aspects of input sequence, forecasting, geographical and weather considerations, and performance metrics. However, specific model names, regularization techniques, and deployment details need to be verified or filled in through further consultation or more explicit guidance from the user.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "node classification", "instruction": "Perform a high-resolution node classification on the Sylukar Mouse Brain Corpus using Graph Fourier Neural Operator (GFNO) with added emphasis on feature importance analysis. Target the accurate identification of rare neocortical neuron subclasses, ensuring model robustness against variations in fiber tracts and hierarchical complexity from the Allen Institute's CCFv2 dataset, while integrating a novel data augmentation strategy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"mouse brain\",\n        \"description\": \"Perform a high-resolution node classification on the Sylukar Mouse Brain Corpus using the Graph Fourier Neural Operator (GFNO), focusing on feature importance analysis. The aim is to accurately identify rare neocortical neuron subclasses while ensuring model robustness against variations in fiber tracts and hierarchical complexity. A novel data augmentation strategy is to be integrated, based on the Allen Institute's CCFv2 dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall for rare neuron subclasses\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sylukar Mouse Brain Corpus\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"neocortical neuron subclasses\"],\n            \"specification\": {\n                \"node_type\": \"mouse brain neurons\",\n                \"edge_type\": \"fibers or relationships between neurons\"\n            },\n            \"description\": \"A high-resolution dataset for node classification, particularly focusing on rare neocortical neuron subclasses with variations in fiber tracts and hierarchical complexity.\",\n            \"preprocessing\": [\"integration with CCFv2 dataset\"],\n            \"augmentation\": [\n                \"novel data augmentation strategy for neural node classification\"\n            ],\n            \"visualization\": [\"visualization of feature importance\"],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"CCFv2\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": null,\n            \"specification\": null,\n            \"description\": \"Allen Institute's CCFv2 dataset used for variations and additional context to the main dataset.\",\n            \"preprocessing\": [\"integration with Sylukar Mouse Brain Corpus\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Fourier Neural Operator (GFNO)\",\n            \"family\": \"GFNO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"focus on node classification, GFNO implementation, and feature importance analysis\"\n            },\n            \"description\": \"A Graph Fourier-based deep learning model for node classification in the context of mouse brain graphs.\"\n        }\n    ],\n    \"knowledge\": [\n        \"novel data augmentation strategy for enhancing model robustness\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature importance analysis\"],\n        \"target_device\": null,\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [],\n        \"demonstration\": \"visualizing model performance on rare neocortical neuron subclasses\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed with a high level of expertise, indicating a complex task. The Graph Fourier Neural Operator and data augmentation strategy are detailed, while target device, deployment, and some performance metrics are yet to be specified. Feature engineering and demonstration requirements are clear.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "As a machine learning project manager, you're tasked with devising a deployable, edge-computing explainable AI node classification model. Utilize the HIN from Wikipedia and DBpedia datasets, incorporating both edge and node attributes in a Siamese Hypergraph Neural Network. Mandate a minimum F1-score of 95% on unseen labels, ensuring compatibility with a Raspberry Pi's resource constraints. Implement and rigorously compare five neighborhood sampling strategies, analyzing their impact on model accuracy and computational efficiency. Emphasize the optimization process and present a comprehensive analysis of the trade-offs between graph convolution variants for real-world, low-power scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"explainable AI and edge computing\",\n        \"description\": \"Design a deployable, edge-computing explainable AI node classification model using the HIN from Wikipedia and DBpedia datasets. Incorporate both edge and node attributes in a Siamese Hypergraph Neural Network with a focus on F1-score of at least 95% on unseen labels. Compare five neighborhood sampling strategies for their impact on model accuracy and efficiency, considering Raspberry Pi's resource constraints. Optimize graph convolution variants for low-power real-world scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"seconds\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory size\",\n                \"unit\": \"MB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HIN from Wikipedia and DBpedia\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node labels\"],\n            \"specification\": {\n                \"edge_attributes\": true,\n                \"node_attributes\": true\n            },\n            \"description\": \"A dataset incorporating edge and node attributes for node classification in a hypergraph setting.\",\n            \"preprocessing\": [\"Siamese Hypergraph Neural Network setup\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Siamese Hypergraph Neural Network\",\n            \"family\": \"Hypergraph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"graph_architecture\": \"Siamese\",\n                \"graph_type\": \"hypergraph\"\n            },\n            \"description\": \"A deployable model that incorporates edge and node attributes, designed for edge computing and explainable AI node classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Raspberry Pi resource constraints, target F1-score of 95%, comparison of five neighborhood sampling strategies, graph convolution optimization for low-power scenarios\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"graph convolution variants optimization\"],\n        \"target_device\": \"Raspberry Pi\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"content\": \"Model performance analysis on resource-constrained edge devices, trade-offs between efficiency and accuracy for different sampling strategies.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed based on the provided details. The high expertise level indicates a solid understanding of the task. Performance metrics, complexity constraints, and the requirement for a specific model architecture were included. However, without explicit values for time and memory, the model's actual resource usage might need to be estimated. The deployment endpoint is not specified, but that can be addressed by consulting with the user or searching based on the task.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image classification", "instruction": "Assign the ML team to engineer an innovative fine-grained image classifier for niche haute couture fashion items from the exclusive Haute Couture Paris Fashion-MNIST subset. Mandate the implementation of the most recent Variational Routing Capsule Networks, with an unprecedented target of 99% precision while maintaining performance under sophisticated, photorealistic distortions like adaptive lighting and minor fabric creases. Additionally, obligate a comprehensive analysis comparing their efficiency with the latest efficient Inverted Residual Convolutional Architectures, including a deep dive into hardware-specific optimizations for optimal execution in resource-restricted ultra-low-power devices, such as mobile devices with limited battery capacity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"haute couture fashion\",\n        \"description\": \"Engineer an innovative fine-grained image classifier for niche haute couture fashion items from the Haute Couture Paris Fashion-MNIST subset. Use Variational Routing Capsule Networks (latest version) with a target precision of 99% and ensure performance under realistic distortions like adaptive lighting and minor fabric creases.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"efficiency vs. Inverted Residual Convolutional Architectures\",\n                \"value\": null,\n                \"unit\": \"comparison\"\n            },\n            {\n                \"name\": \"Hardware-specific optimizations\",\n                \"value\": null,\n                \"unit\": \"optimizations\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Haute Couture Paris Fashion-MNIST subset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_type\": \"fine-grained\",\n                \"distortions\": [\"adaptive lighting\", \"minor fabric creases\"]\n            },\n            \"description\": \"A subset of the Haute Couture Fashion-MNIST dataset, focusing on niche haute couture items.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"realistic distortions\"],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Variational Routing Capsule Networks (latest)\",\n            \"family\": \"Capsule Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_under_distortions\": {\n                    \"adaptive_lighting\": true,\n                    \"fabric_creases\": true\n                },\n                \"target_precision\": 0.99\n            },\n            \"description\": \"A fine-grained classifier utilizing Variational Routing Capsules for haute couture fashion items.\"\n        },\n        {\n            \"name\": \"Inverted Residual Convolutional Architectures (latest)\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"comparison\": {\n                    \"efficiency\": \"analyzed\",\n                    \"specific_device\": \"ultra-low-power mobile devices\"\n                }\n            },\n            \"description\": \"A baseline for comparison, focusing on efficiency in resource-restricted devices.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"ultra-low-power mobile devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"comprehensive analysis, including hardware-specific optimizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction clearly indicates the need for a machine learning project, specifying a high-level of expertise. The problem description is comprehensive, including the task, target metrics, and dataset specifics. The 'service' section expects hardware-specific optimizations for optimal execution in mobile devices with limited power. However, there are no explicit values for the efficiency comparison or hardware optimization, which would need to be obtained or estimated through additional research.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "question answering", "instruction": "Develop a sophisticated medical Q&A system that not only utilizes the diverse BioASQ dataset but also integrates real-time patient feedback to dynamically update knowledge. Prioritize BioBERT's adaptation for domain-specificity, mandate detailed attributions to authoritative resources, and track incremental improvements via A/B testing with distinct patient populations, ensuring compliance with GDPR guidelines.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"Create a sophisticated medical Q&A system using the BioASQ dataset, integrating real-time patient feedback to dynamically update knowledge. Prioritize BioBERT's adaptation for domain-specificity, enforce citation of authoritative resources, and conduct A/B testing with GDPR-compliant methods for tracking improvements across distinct patient populations.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source\": \"bio-questions\",\n                \"domain\": \"medicine\",\n                \"size\": \"diverse\"\n            },\n            \"description\": \"A comprehensive medical question answering dataset with the integration of real-time patient feedback.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BioBERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"domain_adaptation\": true,\n                \"citation_policy\": \"authoritative resources\"\n            },\n            \"description\": \"A domain-specific adaptation of BioBERT to power the medical Q&A system.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"resource_type\": \"authoritative sources\",\n            \"usage\": \"required for citation\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"patient feedback integration\",\n            \"incremental knowledge update\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GDPR-compliant\"\n        ],\n        \"demonstration\": {\n            \"method\": \"A/B testing\",\n            \"populations\": \"distinct patient groups\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction has been parsed to cover a complex project requiring a high-level Q&A system, domain-specific model adaptation, and adherence to GDPR. Performance metrics are not specified since the user did not request specific values. The confidence score reflects the high-level understanding of the requirements while acknowledging the potential for missing details.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "image classification", "instruction": "The project manager demands a sophisticated convolutional neural network (CNN) capable of multi-label classification, not only discerning between cats and dogs, but also identifying and differentiating among various breeds. The model must exhibit exceptional accuracy, with a minimum precision of 95% on a large, thoroughly curated test set of 10,000 images, featuring diverse breeds and challenging visual similarities. Additionally, the model should be optimized for real-time performance on edge devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal recognition\",\n        \"description\": \"Design a sophisticated CNN for multi-label classification, specifically for distinguishing between cats and dogs with their respective breeds. The model must achieve high accuracy with a minimum precision of 95% on a large, curated test set of 10,000 images, featuring diverse breeds and challenging visual similarities.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"edge device\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_count\": 10000,\n                \"breed_diversity\": \"large and thoroughly curated\"\n            },\n            \"description\": \"A large test set containing 10,000 images with diverse breeds and challenging visual similarities.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"sophisticated CNN\",\n            \"family\": \"convolutional neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"target_device\": \"edge device\"\n            },\n            \"description\": \"A CNN designed for high accuracy, precision over 95%, and real-time performance on edge devices for multi-label classification of cats and dogs with their breeds.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear task for building a multi-label classification model using a large curated test set, specifying requirements for accuracy, precision, and real-time performance on edge devices. The user's high expertise level suggests they may require more detailed specifications or guidance on fine-tuning the model for specific breeds.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project manager of an interdisciplinary AI research team, you've been tasked with designing an AI-driven fusion of cutting-edge Graph Neural Network (GNN) and Long Short-Term Memory (LSTM) models, utilizing the specialized \"Dark Knowledge\" subset of the Pubmed Open Research Dataset and a proprietary tech forum forum feed for extracting specialized medical terminologies in the field of biotechnology. The model must achieve an unprecedented ROUGE-L score of 95% when translating highly complex, domain-specific scientific narratives from patents filed in Estonian into detailed, coherent blog posts understandable to non-experts. In addition to this, you must design an adaptive multi-context fusion mechanism to handle the rapid evolution of terminologies across genomics, bioinformatics, and nanotechnology, while ensuring the model maintains a balance between precision and readability. The project should also include a comprehensive blueprint for ethical considerations, explainability techniques, and a plan for continuous model auditing to ensure trustworthiness in the research community.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"biotechnology\",\n        \"description\": \"Design an AI-driven fusion of GNN and LSTM models using the 'Dark Knowledge' subset of Pubmed Open Research Dataset and a proprietary tech forum feed. The aim is to translate Estonian patents into detailed blog posts for non-experts, with a target ROUGE-L score of 95% and an adaptive multi-context fusion mechanism for evolving terminologies. Ensure balance between precision and readability, and include ethical considerations, explainability techniques, and continuous auditing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"ROUGE-L\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Adaptive multi-context fusion mechanism\",\n                \"value\": null,\n                \"unit\": null\n            },\n            {\n                \"name\": \"Balance between precision and readability\",\n                \"value\": null,\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Dark Knowledge subset of Pubmed Open Research Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source\": \"proprietary\",\n                \"domain\": \"biotechnology\",\n                \"specifics\": \"extracting specialized medical terminologies\"\n            },\n            \"description\": \"Dataset for extracting medical terminologies and domain-specific knowledge from patents and forum feeds.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Proprietary tech forum feed\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"biotechnology\",\n                \"evolution_tracking\": true\n            },\n            \"description\": \"A tech forum feed for tracking rapid terminology evolution in genomics, bioinformatics, and nanotechnology.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GNN-LSTM Fusion Model\",\n            \"family\": \"GNN and LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": {\n                    \"target\": 0.95,\n                    \"explanations\": \"Extremely high precision and readability\"\n                },\n                \"explainability\": true,\n                \"ethical_considerations\": true,\n                \"continuous_audit\": true\n            },\n            \"description\": \"A model that fuses GNN and LSTM for complex, domain-specific translations with high precision and readability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptive to evolving terminologies\",\n        \"Balancing precision and readability\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": null,\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [],\n        \"demonstration\": \"Detailed blog post format\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction specifies a complex project involving a novel model, specific dataset sources, and various requirements. The high expertise level suggests familiarity with these details. However, some aspects are not directly quantifiable (like model explainability), so they have been indicated as present. The confidence score is reasonable based on the given information, but lower due to missing specific deployment details.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "The project for image classification with Stanford Cars Dataset now demands a sophisticated Inception-v3 based model. Emphasize on robustness against extreme weather conditions, rare angles, and complex occlusions. Besides, optimize the model for fine-grained recognition (sub-models for specific make, model, and vintage), and compute nuanced performance metrics like weighted F1-score. The target is to achieve not just 95% overall accuracy, but also maintain a consistent precision and recall across all classes, posing a significant challenge for state-of-the-art AI systems. Ensure compatibility with real-world scenarios and provide detailed visual analysis for interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"image classification (Stanford Cars Dataset)\",\n        \"description\": \"The project requires a sophisticated Inception-v3 based model for image classification on the Stanford Cars Dataset. The model should be robust to extreme weather conditions, rare angles, and complex occlusions. It needs to be fine-grained, with separate sub-models for specific car make, model, and vintage. Performance should be evaluated using nuanced metrics such as weighted F1-score and aim for at least 95% overall accuracy. There is a focus on maintaining consistent precision and recall across all classes and ensuring compatibility with real-world scenarios. Detailed visual analysis for interpretability is needed.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"weighted F1-score\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"overall accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"robustness\",\n                \"value\": null,\n                \"unit\": \"weather conditions, angles, and occlusions\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"car make\", \"model\", \"vintage\"],\n            \"specification\": null,\n            \"description\": \"A dataset for image classification of cars with emphasis on extreme conditions, rare angles, and complex occlusions.\",\n            \"preprocessing\": [\"augmentation for robustness\", \"fine-grained data preparation\"],\n            \"augmentation\": [\"extreme weather, rare angles, occlusions\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Inception-v3\",\n            \"family\": \"Inception\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine-grained\": true,\n                \"sub-models\": [\n                    {\n                        \"type\": \"specific car details\"\n                    }\n                ]\n            },\n            \"description\": \"A sophisticated Inception-v3 model designed for robust image classification under challenging scenarios, targeting high accuracy and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"visual analysis for interpretability\",\n        \"state-of-the-art AI system performance benchmarking\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain-specific feature extraction\"\n        ],\n        \"target_device\": \"reliable for real-world deployment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for speed and efficiency\"],\n        \"demonstration\": \"detailed results and visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON response captures the key aspects of the image classification project with a high level of detail, including the user's high level of expertise. Performance metrics, complexity requirements, and the sophistication of the model are all specified. However, some values in the 'complexity_metrics' are left unspecified as they may require more specific details from the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a project manager for a machine learning team, your objective is to develop a real-time, scalable recommendation system using the 'Bilibili Danmu' dataset. The model should not only exhibit high text analysis efficiency, but also navigate China's regional language variations and suggest videos in under 300 milliseconds, to cater to the binge-watching habits on the platform.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"Real-time video recommendation\",\n        \"description\": \"Develop a real-time, scalable recommendation system using the 'Bilibili Danmu' dataset. The model should excel in text analysis, handle regional language variations in China, and provide suggestions within 300 milliseconds to support binge-watching on the platform.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"text analysis efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": 0.3,\n                \"unit\": \"seconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inference latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Bilibili Danmu\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"training\": {\n                        \"size\": null\n                    },\n                    \"testing\": {\n                        \"size\": null\n                    }\n                },\n                \"language_variations\": \"Chinese regional\",\n                \"time_series\": {\n                    \"real_time\": true\n                }\n            },\n            \"description\": \"A dataset from Bilibili platform containing real-time danmu (chat messages) for video recommendations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"time_complexity\": {\n                    \"text_analysis\": null\n                },\n                \"inference_speed\": {\n                    \"prediction_time\": 300,\n                    \"unit\": \"milliseconds\"\n                }\n            },\n            \"description\": \"A real-time recommendation model designed to handle text data from Bilibili Danmu, catering to China's regional language variations.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"response_time\": \"under 300 milliseconds\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is to build a recommendation system, with a focus on text analysis and real-time performance, indicating high expertise level. The dataset is specified as real-time and linking to a user-provided data source. Some model details are not provided and can be inferred or require clarification, such as the specific time complexity and response time details. The confidence score is 0.8 due to the need for additional model specifics.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "machine translation", "instruction": "Utilize the massive OpenSubtitles dataset, specifically extracting subplots with colloquial dialogues, to develop a state-of-the-art translation model. Engineer a BERT-based architecture optimized for English-Mandarin conversational translations, demanding a 95% accuracy on regional slang and idiomatic expressions that challenge even advanced NLP systems, while maintaining fluency and cultural nuances.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual conversational text\",\n        \"description\": \"Use the OpenSubtitles dataset to develop a state-of-the-art translation model, focusing on subplots with colloquial dialogues. The model should be a BERT-based architecture optimized for English-Mandarin conversational translations, with a target accuracy of 95% on regional slang and idiomatic expressions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95,\n                \"domain\": \"local slang and idiomatic expressions\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translations\"],\n            \"specification\": {\n                \"dataset_split\": [\"train\", \"validation\", \"test\"],\n                \"dialogue_type\": \"colloquial\"\n            },\n            \"description\": \"A dataset extracted from OpenSubtitles, containing colloquial English-Mandarin subplots for translation model development.\",\n            \"preprocessing\": [\"BERT tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-based conversational translation model\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT\",\n                \"language_pairs\": [\"English-Mandarin\"],\n                \"optimization\": \"focus on regional slang and idiomatic expressions\"\n            },\n            \"description\": \"Engineered to handle state-of-the-art English-Mandarin translations for colloquial dialogues, aiming for high accuracy on slang and idiomatic expressions.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"topic\": \"Challenges\",\n            \"description\": \"Advanced NLP systems may struggle with regional slang and idiomatic expressions, emphasizing the importance of specialized model design.\"\n        },\n        {\n            \"topic\": \"Fluency and cultural nuances\",\n            \"description\": \"The model must maintain naturalness and cultural appropriateness in translations.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or server\",\n        \"deployment_endpoint\": \"Advanced NLP translation service\",\n        \"inference_engine\": [\"GPU-optimized for BERT\"],\n        \"demonstration\": \"Demonstrations showcasing translations with high accuracy on slang and idiomatic expressions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction is quite detailed, covering the specific task, requirements, and expert level. The dataset and model engineering are clear, but some assumptions were made like the target_device and deployment details, which may vary based on the actual context. The model performance is specified, which indicates a clear goal.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for the advanced machine learning team, devise a sophisticated multilingual sentiment-aware translation system utilizing the extensive ParaCrawl corpus. Instruct the team to build a dual encoder architecture that not only guarantees precise English to Dutch conversions but also analyzes and replicates nuanced emotional nuances from the source content, requiring rigorous cross-lingual and affective computing expertise.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation and sentiment analysis\",\n        \"description\": \"As a project manager, the team is tasked with creating a sophisticated multilingual sentiment-aware translation system using the ParaCrawl corpus. The system should employ a dual encoder architecture for precise English to Dutch translations, while also capturing and replicating nuanced emotional nuances from the source content, requiring advanced cross-lingual and affective computing expertise.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"emotion replication accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl corpus\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"corpus_size\": null,\n                \"lingual_depth\": \"English to Dutch\"\n            },\n            \"description\": \"An extensive multilingual corpus for training a sentiment-aware translation system, focusing on English to Dutch translation and emotion detection.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"dual encoder architecture\",\n            \"family\": \"Transformer or similar\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"encoder_type\": \"dual encoders\",\n                \"emotional_replication\": true\n            },\n            \"description\": \"A sophisticated machine learning model that incorporates dual encoders for accurate translation and cross-lingual affective computing capabilities.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"sentiment analysis module\",\n            \"emotion aware cross-lingual alignment\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-based for efficient large-scale processing\"\n        ],\n        \"demonstration\": \"Performance on multiple sentiment-bearing texts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction clearly outlines a machine learning project with a specific objective, expertise level, and requirements. It details a high-level task for building a sentiment-aware translation system. Performance metrics for precision, recall, and emotion replication accuracy have not been specified but would typically be included for such a task. The confidence score is high due to the completeness and clarity of information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As the project manager for a satellite image analysis team, your mandate is to develop an efficient, high-performance land type classification system using Sentinel-2 data. Explore the applicability of state-of-the-art segmentation models like U-Net or DeepLabv3+, considering real-time processing requirements, while prioritizing precision in differentiating water bodies, dense forests, and urban landscapes. Demonstrate the algorithm's effectiveness with a comprehensive evaluation of accuracy, speed, and generalization capabilities on diverse geographical regions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"satellite image analysis\",\n        \"description\": \"Develop an efficient land type classification system using Sentinel-2 data. Focus on state-of-the-art segmentation models like U-Net or DeepLabv3+ while ensuring real-time processing and high precision for water bodies, dense forests, and urban landscapes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"speed\",\n                \"value\": null\n            },\n            {\n                \"name\": \"generalization capabilities\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image bands\": \"specific bands for Sentinel-2 (e.g., SWIR, NIR, RGB)\"\n            },\n            \"description\": \"High-resolution satellite imagery from the Sentinel-2 mission, suitable for land type classification.\",\n            \"preprocessing\": [\"data normalization\", \"image enhancement\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"dataset distribution maps\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": [\"U-Net\", \"DeepLabv3+\"],\n            \"family\": [\"UNET family\", \"DeepLab series\"],\n            \"type\": [\"neural networks\", \"neural networks\"],\n            \"specification\": {\n                \"architecture\": \"suitable for real-time processing\",\n                \"precision\": \"optimized for water bodies, dense forests, and urban landscapes\"\n            },\n            \"description\": \"Investigation of state-of-the-art segmentation models for satellite image analysis\"\n        }\n    ],\n    \"knowledge\": [\"Experience with satellite data preprocessing and Sentinel-2 image characteristics\"],\n    \"service\": {\n        \"feature_engineering\": [\"target extraction\", \"feature extraction for land types\"],\n        \"target_device\": \"cloud or GPU-based for real-time performance\",\n        \"deployment_endpoint\": \"a cloud-based platform, e.g., AWS or Google Cloud\",\n        \"inference_engine\": [\"GPU-powered\", \"optimized for satellite image analysis\"],\n        \"demonstration\": \"visual results, confusion matrices, ROC curves, and time efficiency demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's request clearly indicates the need for a complex project, which aligns with a high expertise level. Performance metrics like accuracy, speed, and generalization were not explicitly quantified, so they are marked as null for now. Similarly, complexity metrics like inference time need specific numbers, but the need for real-time processing provides a clear direction for their optimization.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "For the upcoming project, we need a cutting-edge machine translation system capable of handling under-explored languages like Swahili, with a focus on boosting performance in diverse linguistic settings. Utilize the extensive Masakhane dataset and explore innovative fusion of advanced Transformer and Recurrent Neural Network architectures. Aim to achieve a significant benchmark uplift of 15% in BLEU scores, demanding state-of-the-art optimization and a deep analysis of model dynamics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual\",\n        \"description\": \"The project requires a cutting-edge machine translation system targeting under-explored languages like Swahili, with a focus on diverse linguistic settings. Advanced Transformer and Recurrent Neural Network fusion will be employed using the Masakhane dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 0.15\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Masakhane\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"Swahili\"]\n            },\n            \"description\": \"A large-scale dataset for machine translation, focusing on under-explored languages like Swahili, for the development of the translation system.\",\n            \"preprocessing\": [\"language tagging\", \"data cleaning\"],\n            \"augmentation\": [\"back-translation\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Fusion of Transformers and RNNs\",\n            \"family\": \"Transformer, RNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer and RNN fusion\",\n                \"optimization\": \"state-of-the-art\"\n            },\n            \"description\": \"An advanced combination of Transformer and Recurrent Neural Network models for the machine translation task, aiming for a 15% BLEU score improvement.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Innovative fusion techniques for handling under-explored languages may require domain adaptation and cross-lingual transfer learning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"linguistic feature extraction\", \"domain-specific representations\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom deployment platform\",\n        \"inference_engine\": [\"optimized inference for low-resource languages\"],\n        \"demonstration\": \"multi-lingual translation live demo\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON includes the user's intent to build a state-of-the-art machine translation system, leveraging high expertise in AI/ML. The Masakhane dataset, Swahili focus, and advanced architecture are clear. The performance metric of boosting BLEU score by 15% is specified. The dataset's language focus and preprocessing steps are detailed. There's an emphasis on model dynamics, optimization, and deployment considerations.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Develop a high-resolution coastal surveillance system using the COCO-Sea 4K Ultra HD dataset, focusing on unprecedented discrimination between fine-grained classes like rocky shores, sandy beaches, and assorted marine structures. Mandate the deployment of an Attention U-Net++ architecture with multi-scale feature fusion and adaptively adjusted weights for optimal performance in low-light and dynamic conditions. The system must surpass 30 FPS throughput for seamless, real-time analysis on resource-constrained drone hardware.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"coastal surveillance\",\n        \"description\": \"Develop a high-resolution coastal surveillance system using the COCO-Sea 4K Ultra HD dataset, with a focus on fine-grained classification between rocky shores, sandy beaches, and marine structures. The system should employ an Attention U-Net++ architecture with multi-scale feature fusion and adaptive weights for low-light and dynamic conditions. The system must support real-time analysis at 30 FPS on drone hardware.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specific to coastal surveillance\",\n                \"value\": \"30 FPS\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"30 FPS\",\n                \"unit\": \"frames per second\"\n            },\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": \"optimized for drone hardware\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO-Sea 4K Ultra HD\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"4K Ultra HD\",\n                \"image_depth\": \"color\"\n            },\n            \"description\": \"A high-resolution dataset for coastal surveillance, featuring fine-grained categories like rocky shores, sandy beaches, and marine structures.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention U-Net++\",\n            \"family\": \"U-Net family\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Attention U-Net++\",\n                \"feature_fusion\": \"multi-scale\",\n                \"dynamic_weights\": \"yes\"\n            },\n            \"description\": \"A neural network model with Attention U-Net++ architecture, designed for optimal performance in low-light and dynamic conditions.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"drone hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"real-time\": true,\n            \"target_speed\": \"30 FPS\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed for a complex machine learning project, indicating the user's high level of expertise. The problem area, task, and specific requirements like the dataset, model architecture, and performance goals were captured. The FPS target for deployment on drone hardware was specified, while some detailed metrics like metric values are left null as they need to be determined during the modeling process. Confidence is high given the provided details and the absence of ambiguous points.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "For the upcoming \"Multilingual Cultural Fusion\" project, we aim to fine-tune a state-of-the-art M-T5 model on the Common Crawl corpus, focusing on English-Japanese translation with a specific emphasis on subdomains like literature, slang, and historical dialogue. The system must maintain >97% accuracy in recognizing and translating colloquial expressions while adapting to regional variations in contemporary conversation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cultural fusion\",\n        \"description\": \"For the 'Multilingual Cultural Fusion' project, the objective is to fine-tune a state-of-the-art M-T5 model on the Common Crawl corpus, focusing on English-Japanese translation with emphasis on subdomains like literature, slang, and historical dialogue. The system should achieve >97% accuracy in handling colloquial expressions while adapting to regional variations in contemporary conversation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"English\", \"Japanese\"],\n            \"specification\": {\n                \"training_dataset_size\": \"Subdomains-specific corpus from Common Crawl\",\n                \"language_pair\": \"English-Japanese\"\n            },\n            \"description\": \"A large multilingual corpus for fine-tuning, focusing on English-Japanese translation with a specific emphasis on literature, slang, and historical dialogue.\",\n            \"preprocessing\": [\n                \"Tokenizer for M-T5\",\n                \"Text filtering for subdomains\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M-T5\",\n            \"family\": \"Transformer-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pretrained_model\": \"M-T5\",\n                \"adapter_layers\": \"for subdomains adaptation\"\n            },\n            \"description\": \"Fine-tuned M-T5 model for English-Japanese translation, targeting high accuracy with a focus on colloquial expressions and regional variations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art M-T5\",\n        \"Emphasis on subdomains adaptation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Fine-tuning adaptation for subdomains\"],\n        \"target_device\": \"Cloud (for resource-intensive fine-tuning)\",\n        \"deployment_endpoint\": \"Cloud-based service provider, TBA\",\n        \"inference_engine\": [\"M-T5 inference API, optimized for translation\"],\n        \"demonstration\": \"Multilingual translation samples with focus on colloquial expressions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project, focusing on the build intent with a high-level user expertise. The performance metrics and problem description are aligned with the instruction. The dataset is specified to be the Common Crawl corpus with a focus on the subdomains. The fine-tuning steps and requirements for deployment are also included. However, some details are speculative, such as the deployment endpoint and engine, as they were not explicit in the user's instructions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "You've been assigned to develop a robust text classifier for the DAIGT dataset, which differentiates between human-written and AI-generated content. Your objective is to optimize a deep learning model, ensuring interpretability and addressing potential biases, with a focus on achieving state-of-the-art accuracy. Additionally, devise a comprehensive evaluation strategy involving cross-validation and AUC-ROC analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"detecting AI-generated text\",\n        \"description\": \"Task is to develop a robust text classifier for the DAIGT dataset, differentiating between human-written and AI-generated content. The objective is to optimize a deep learning model for high accuracy, ensuring interpretability and addressing potential biases, targeting state-of-the-art performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"n_classes\": 2,\n                \"data_split\": [\"train\", \"validation\", \"test\"]\n            },\n            \"description\": \"DAIGT dataset for distinguishing human-written and AI-generated text content\",\n            \"preprocessing\": [\"tokenization\", \"text normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability_layers\": true,\n                \"bias_correction\": true\n            },\n            \"description\": \"A deep learning model with interpretability and bias mitigation for text classification\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Best practices for addressing potential biases in NLP models and using explainability techniques\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"XAI-aided for interpretability\"],\n        \"demonstration\": \"Include AUC-ROC analysis and cross-validation results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed with high-level details on the task (text classification), dataset (DAIGT), and a focus on interpretability and bias mitigation. The model optimization was left open to be specific based on the user's deep learning expertise. The 'service' section includes a request for an explainable inference engine and a plan for comprehensive evaluation. The confidence score is set at 0.95 as the instruction provides a clear goal but lacks specifics for certain parameters.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for a cutting-edge machine learning team, we need to develop a scalable news categorization system using the 20 Newsgroups dataset. The challenge now requires not only selecting a model with optimal performance under limited computational resources but also designing an efficient transfer learning strategy to minimize training time while maintaining accuracy, all within a tight six-week deadline.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"news categorization\",\n        \"description\": \"As a project manager for a machine learning team, the task is to develop a scalable news categorization system using the 20 Newsgroups dataset. The system must perform well under limited computational resources, employ an efficient transfer learning strategy to minimize training time, and meet a six-week deadline.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"seconds per instance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"unit\": \"MB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"news category\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": null\n            },\n            \"description\": \"A collection of news articles for text classification. Challenge is to handle large-scale data efficiently.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"transfer learning-based\",\n                \"computational requirements\": \"limited resources\"\n            },\n            \"description\": \"A scalable news categorization model, focusing on high performance, computational efficiency, and transfer learning for minimal training time.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning from pre-trained models on large datasets can help boost performance on 20 Newsgroups with limited data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"selecting optimal features for the transfer learning setup\"\n        ],\n        \"target_device\": \"cloud or server\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\", \"batch processing\"],\n        \"demonstration\": \"demonstration of scalability, accuracy, and efficiency under the six-week deadline\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction provided a comprehensive project scope, mentioning limited computational resources, transfer learning, and a tight deadline. Given the user's expertise level is high, the information is likely accurate and complete. However, there are no specific metric values for performance metrics, which may need to be estimated or provided later based on model performance.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Revamp the project to employ an ensemble of LSTMs on the energy consumption dataset, requiring not only 72-hour hourly forecasts but also incorporating weekly and daily patterns. Prioritize model efficiency while maintaining accuracy, targeting response times below 50 milliseconds per prediction and conducting rigorous evaluation with cross-validation and A/B testing.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy consumption\",\n        \"description\": \"Revamp the project to utilize an ensemble of LSTMs on the energy consumption dataset, focusing on hourly forecasts for the next 72 hours. Include weekly and daily patterns in the model. The primary goal is to optimize for efficiency while maintaining accuracy, targeting response times below 50 milliseconds per prediction. Rigorous evaluation with cross-validation and A/B testing is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"energy consumption\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"forecast_horizon\": 72,\n                \"time_intervals\": [\"hourly\", \"weekly\", \"daily\"]\n            },\n            \"description\": \"Dataset for energy consumption with a focus on hourly data over 72 hours, incorporating weekly and daily patterns.\",\n            \"preprocessing\": [\"normalization\", \"resampling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ensemble of LSTMs\",\n            \"family\": \"LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_size\": null,\n                \"model_efficiency\": null\n            },\n            \"description\": \"An ensemble of LSTMs designed for energy consumption forecasting, capturing both short and long-term patterns.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature engineering for ensemble\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"in-house infrastructure\",\n        \"inference_engine\": [\"TensorFlow Serving\"],\n        \"demonstration\": \"A/B testing results with explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction was parsed with a build intent, medium expertise, and a focus on both accuracy and efficiency. The performance metric of response time is included. The dataset description specified the need for a 72-hour forecast and weekly/daily patterns. Cross-validation and A/B testing are mentioned, but missing details for model specifications and service deployment. The response assumes cloud deployment and TensorFlow Serving as the inference engine.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager seeks a high-performing BERT-based Q&A system, focusing on RoBERTa, for SQuAD 2.0. The model's development must adhere to rigorous benchmarking, targeting an advanced F1 score of over 95% after extensive fine-tuning. Although real-time responsiveness is not strictly critical, the model should consistently yield answers within a loosely defined 1.8-second latency window, ensuring optimal user experience while considering computational efficiency and scalability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"SQuAD 2.0\",\n        \"description\": \"The project manager requires a BERT-based Q&A system, specifically focusing on RoBERTa, for the SQuAD 2.0 dataset. The model should achieve an advanced F1 score of over 95% after extensive fine-tuning and prioritize computational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 Score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 1.8,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"The SQuAD 2.0 dataset for training a high-performing BERT-based Q&A system with a focus on RoBERTa.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RoBERTa-based Q&A System\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"benchmarking\": true,\n                \"real-time\": false\n            },\n            \"description\": \"A high-performing Q&A system using RoBERTa, specifically tailored for SQuAD 2.0 with the aim of achieving an F1 score of over 95% after fine-tuning.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"non-real-time (latency considered important)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for efficiency and scalability\"\n        ],\n        \"demonstration\": \"Focus on optimized user experience, achieving consistent answers within 1.8 seconds.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction describes the requirement of a high-performance BERT-based Q&A system with RoBERTa as the foundation. It emphasizes the need for an advanced F1 score and a loosely defined latency window. The model specifications are clear, and the user's high expertise level is reflected. However, the strict real-time responsiveness is not required and the benchmarking is expected to be rigorous. The missing information for deployment specifics is assumed to be negotiable.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager at a high-end fashion technology firm commissions a real-time, mobile-optimized fashion item segmentation app. Using the exclusive DeepFashion2 Plus dataset, specifically tailored for haute couture garments, a customized EfficientNet Enhanced (E3) should be integrated as a lightweight fully convolutional network (FCN). Strive for precision above 80% in differentiating subtle variations of fabrics and overlapping designer pieces, ensuring seamless performance and an elegant user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"fashion technology, haute couture\",\n        \"description\": \"The project involves a real-time, mobile-optimized fashion item segmentation app. It uses the exclusive DeepFashion2 Plus dataset, specifically designed for distinguishing subtle fabric variations and overlapping designer pieces in haute couture garments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.8\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null, \n                \"unit\": \"milliseconds\" \n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepFashion2 Plus\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"customization\": \"tailored for haute couture garments\",\n                \"mobile-optimized\": true\n            },\n            \"description\": \"The exclusive dataset for haute couture, featuring real-time and mobile-optimized requirements.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet Enhanced (E3)\",\n            \"family\": \"EfficientNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"lightweight\": true,\n                \"as FCN\": true\n            },\n            \"description\": \"A customized EfficientNet Enhanced to be integrated as a lightweight fully convolutional network for the fashion item segmentation task with precision target of 80%.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance on mobile\"],\n        \"demonstration\": \"elegant user experience with seamless performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed into a high expertise level, as it indicates a high-level project commission. Performance metrics like precision and mobile-optimization are included. However, the user's requirement for optimizing real-time performance and the specific deployment endpoint are not directly mentioned. The missing complexity metrics, such as inference time and memory footprint, will need clarification.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "node classification", "instruction": "Utilize the Mouse Brain Connectivity dataset to develop a novel, real-time GWNN model for node classification, targeting pinpoint identification of diverse neuron types with near-perfect precision and recall. Mandate integration of an adaptive learning mechanism to tackle the dynamic spatial correlations within the intricate brain connectivity patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"The task is to develop a real-time GWNN (Graph Wavelet Neural Network) model for node classification using the Mouse Brain Connectivity dataset. The goal is to achieve pinpoint identification of diverse neuron types with near-perfect precision and recall, while integrating an adaptive learning mechanism to address the dynamic spatial correlations in the brain connectivity patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Connectivity\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node type\"],\n            \"specification\": {\n                \"size\": {\n                    \"nodes\": \"unknown\",\n                    \"edges\": \"unknown\"\n                },\n                \"node_features\": \"unknown\",\n                \"edge_features\": \"unknown\"\n            },\n            \"description\": \"The dataset consists of mouse brain connectivity data for node classification, focusing on neuron type identification with dynamic spatial correlations.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"novel GWNN\",\n            \"family\": \"Graph Wavelet Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"real-time\",\n                \"adaptive_learning\": \"enabled\"\n            },\n            \"description\": \"A graph neural network model designed for real-time operation, utilizing adaptive learning to handle dynamic spatial correlations in the Mouse Brain Connectivity dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"real-time performance\",\n        \"adaptive learning for dynamic correlations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"graph preprocessing\",\n            \"node feature engineering\"\n        ],\n        \"target_device\": \"real-time environment (e.g., GPU, TPU, or edge computing)\",\n        \"deployment_endpoint\": \"neuroinformatics platform or cloud service\",\n        \"inference_engine\": [\n            \"optimized for real-time graph processing\"\n        ],\n        \"demonstration\": \"live demonstration with variable input scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers the essential aspects of the user's instruction, including high expertise, the targeted task, and the dataset requirements. However, some metrics' values (precision and recall) are left undefined as they typically depend on model performance, which might be experiment-dependent. The 'service' section includes general guidelines based on the task's complexity, but specific hardware and platform requirements are not provided. The confidence score is high based on the substantial task details but lowered slightly due to the lack of deployment specifics.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "For a cutting-edge, real-time voice-activated chat application, develop a high-precision bidirectional LSTM neural network utilizing the extensive Europarl v7 corpus, emphasizing on both fluency and low-latency translation (<250ms). Conduct rigorous performance optimization to ensure seamless, near-native Spanish-English conversion in diverse conversation scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"voice-activated chat application\",\n        \"description\": \"Create a high-precision bidirectional LSTM neural network for a real-time voice-activated chat application. Use the Europarl v7 corpus for Spanish-English translation, prioritizing fluency and low-latency (less than 250ms) for seamless conversation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"translation fluency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 0.250\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Europarl v7\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pairs\": [\"Spanish-English\"]\n            },\n            \"description\": \"An extensive corpus for training a high-precision bidirectional LSTM neural network for real-time Spanish-English translation.\",\n            \"preprocessing\": [\"cleaning, tokenization, sentence segmentation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"bidirectional LSTM\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"high-precision, bidirectional\",\n                \"hidden_layer_depth\": null,\n                \"hidden_layer_size\": null\n            },\n            \"description\": \"A cutting-edge LSTM model for the specific task of real-time voice-activated chat translation\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"name\": \"seamless conversation\",\n            \"description\": \"Achieving high-precision, near-native translation is key for maintaining conversation flow.\"\n        },\n        {\n            \"name\": \"diverse scenarios\",\n            \"description\": \"The model must be optimized for real-world variety in conversation situations.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"word embeddings, attention mechanisms\"],\n        \"target_device\": \"real-time processing (e.g., cloud or embedded devices), to ensure low-latency\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for real-time or parallel computing\"],\n        \"demonstration\": {\n            \"use case\": \"live voice input with near-native translation display and minimal latency feedback\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction specifies a build task for a complex NLP problem. The 'expertise' level is high, indicating the user may have technical knowledge. Performance metrics, such as fluency and latency, are defined, and some model-specific parameters (like hidden layers) are expected. The dataset requires preprocessing steps and clean data, and service options are detailed, though some specific parameters (like deployment endpoint) are left unspecified.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for a high-demand grocery retail platform, mandate the development of an advanced YOLOv5-based inventory management system. The system must be trained on the \"Extended StoreShelf Dataset\" containing an expanded set of 150 product categories, ensuring not only precision and recall above 95% but also real-time object detection with multi-label classification. The system's performance must surpass previous iterations by achieving sub-second processing speed (ideally less than 100 milliseconds per shelf image), enabling seamless inventory updates and enabling agile inventory optimization for multiple stores simultaneously.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"grocery retail\",\n        \"description\": \"A project manager requests the development of an advanced YOLOv5-based inventory management system for a high-demand grocery platform. The system must be trained on the 'Extended StoreShelf Dataset' with 150 product categories, targeting precision and recall above 95% for multi-label classification. Key performance indicators include sub-second processing speed (less than 100 milliseconds per shelf image) for real-time inventory updates and agile inventory optimization across multiple stores.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"processing time per image\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extended StoreShelf Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"num_categories\": 150\n            },\n            \"description\": \"A large-scale dataset for training the YOLOv5 inventory management system, focusing on 150 product categories.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"YOLOv5\",\n                \"object_detection_type\": \"multi-label\"\n            },\n            \"description\": \"The advanced YOLOv5-based system for inventory management\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance computing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"real-time object detection with shelf images\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction has a clear intent to build a solution, with the project manager's high expertise level indicating a technical understanding. The requirement for real-time object detection and multi-label classification, along with specific performance metrics, make it clear. However, some areas are not explicitly mentioned, like the precise details of the training process or the deployment endpoint, but the context suggests the expectations.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a Project Manager overseeing a cutting-edge machine learning team, you've been tasked with developing a state-of-the-art text classification system for a niche market in the healthcare domain. The team has been assigned the unique challenge of the \"Fine-Tuned Medical Reasoning\" competition, which not only requires them to discern between neutral, contradiction, and entailment for medical research paper abstracts but also to identify levels of support or entailment based on a hierarchical structure. The data includes not only simple sentence pairs but also complex paragraph relationships, pushing the model to comprehend context and infer complex relationships within multi-layered medical discourse. The accuracy metric has been multiplied by a factor of 1.5, reflecting the project's emphasis on precision in the medical field, and the model must achieve a minimum of 95% F1-score on a private test set before submission. Additionally, the team must provide a detailed explanation of their model's interpretability and how it handles potential ethical implications in handling sensitive medical information. Document the entire development process and plan for continuous monitoring of the deployed model's performance in a live, high-traffic environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Develop a state-of-the-art text classification system for the Fine-Tuned Medical Reasoning competition, which involves distinguishing neutral, contradiction, and entailment in medical research paper abstracts, and handling complex paragraph relationships. The model must achieve at least 95% F1-score with an emphasis on precision and interpretability. Ethical implications and sensitive medical information handling should be considered.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"accuracy (weighted)\",\n                \"value\": 1.5,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fine-Tuned Medical Reasoning\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"neutral\", \"contradiction\", \"entailment\"],\n            \"specification\": {\n                \"context_length\": null,\n                \"multilayered_structure\": true\n            },\n            \"description\": \"A dataset specific to the Fine-Tuned Medical Reasoning competition, containing medical research paper abstracts and their complex relationships.\",\n            \"preprocessing\": [\"handling medical jargon\", \"multilayer data preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom NLP Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"state-of-the-art\",\n            \"specification\": {\n                \"interpretability_features\": [\n                    \"model-agnostic explanation\",\n                    \"attention mechanisms\"\n                ],\n                \"ethics Considerations\": {\n                    \"handling_sensitive_data\": [\n                        \"de-identification techniques\",\n                        \"anonymization measures\"\n                    ],\n                    \"fairness\": null,\n                    \"bias_detection_and_reduction\": true\n                }\n            },\n            \"description\": \"A model designed to handle complex medical text relationships, including interpretability and ethical implications.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Multilayered context understanding and interpretability techniques for healthcare text analysis\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"training for multi-layered relationships\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"high-traffic environment\",\n        \"inference_engine\": [\"real-time monitoring and auto-scaling\"],\n        \"demonstration\": {\n            \"objective\": \"live performance in medical discourse analysis\",\n            \"procedure\": \"periodic model performance updates and evaluation\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed into a high-level JSON response based on the complexities involved. The 'user' section assumes high expertise. The 'problem' section covers the specific tasks and performance requirements, while the 'dataset' section acknowledges the complexity of the text relationships. The model's interpretability and ethical considerations are included. However, there might be missing details, such as the exact model architecture or the link to the private test set, which would require further clarification from the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "For the extensive Enron Email dataset containing multiple relationships, develop a high-performance node classification model utilizing a Hierarchical Attention-based RGCN. The task requires not only differentiating employees based on intricate communication networks, but also detecting subgroups within teams. Strive for state-of-the-art accuracy, while ensuring parallelizable architecture and real-time processing on large-scale graphs with at least ten thousand nodes, minimizing computational bottlenecks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"data analysis of extensive email networks (Enron)\",\n        \"description\": \"Develop a high-performance node classification model on the Enron Email dataset using a Hierarchical Attention-based RGCN. The goal is to differentiate employees based on complex communication networks and identify subgroups within teams. The model should achieve state-of-the-art accuracy while being parallelizable and capable of real-time processing on large-scale graphs with at least ten thousand nodes, minimizing computational bottlenecks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"parallelizability\",\n                \"value\": \"desired\"\n            },\n            {\n                \"name\": \"real-time processing on large graphs\",\n                \"value\": \"at least 10,000 nodes\",\n                \"unit\": \"nodes\"\n            },\n            {\n                \"name\": \"minimizing computational bottlenecks\",\n                \"value\": \"Yes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"employee classes\", \"communication network\"],\n            \"specification\": null,\n            \"description\": \"A large, complex dataset with intricate employee communication networks from the Enron case.\",\n            \"preprocessing\": [\"cleaning\", \"feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Attention-based RGCN\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"state-of-the-art accuracy\": \"Yes\"\n            },\n            \"description\": \"A model designed for node classification tasks on large, complex graphs, with emphasis on performance and scalability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"data cleaning\", \"network feature extraction\"],\n        \"target_device\": \"cloud or high-performance computing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"parallel processing\", \"GPU-based\"],\n        \"demonstration\": \"Performance results with video/animation of how the model processes large graphs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction parsed as a high-level task with the 'high' expertise level. The model selection, dataset requirements, and performance metrics were specific, while the service section covered parallelization and real-time processing. However, the accuracy score is initially set to null as it's not provided in the instruction. Confidence is high, but the actual accuracy to be achieved may need further clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a cutting-edge machine learning team, your objective is to develop a highly sophisticated time-series forecasting model for the Electricity dataset. The task involves not only training on a dataset partitioned into train, validation, and test sets (with sequences of INPUT_SEQ_LEN=96 and INPUT_DIM=321) but also optimizing for interpretability, quantifying error using custom loss functions that account for seasonality and outliers. You must achieve state-of-the-art performance in terms of mean squared error (MSE), mean absolute error (MAE), and a novel user-defined metric, precision-at-spatio-temporal intervals (PSI), while ensuring compatibility with real-time streaming data inputs. The model's interpretability should be presented in a clear, visual report for stakeholders.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy (electricity) demand prediction\",\n        \"description\": \"Develop a highly sophisticated time-series forecasting model for the Electricity dataset, partitioned into train, validation, and test sets with sequences of INPUT_SEQ_LEN=96 and INPUT_DIM=321. Optimize for interpretability, use custom loss functions accounting for seasonality and outliers, targeting state-of-the-art performance in MSE, MAE, and a novel PSI metric. Ensure compatibility with real-time streaming data inputs and provide a clear visual report for stakeholders.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"PSI\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"partitioned\": true\n            },\n            \"description\": \"A dataset for electricity demand forecasting, partitioned into train, validation, and test sets with sequences of given dimensions.\",\n            \"preprocessing\": [\"seasonality adjustment\", \"outlier handling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true,\n                \"custom_loss_functions\": true,\n                \"streaming_data_compatible\": true\n            },\n            \"description\": \"A time-series forecasting model designed to achieve state-of-the-art performance on the Electricity dataset with an emphasis on interpretability and custom loss functions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Seasonality and outlier handling techniques\",\n        \"Designing custom loss functions for specific needs\",\n        \"Interpreting model outputs for stakeholders\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"interpretable features\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time streaming\"],\n        \"demonstration\": \"Clear, visual report for stakeholders\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was generated based on the user's instruction, which specifies a high-level objective, the targeted dataset, tasks, and requirements for the model. Expertise level is set to 'high' due to the need for a sophisticated model. However, exact model details, deployment endpoint, and inference engine are not specified, as they are typically handled in a detailed project plan.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "For a project targeting young learners, the machine learning team must develop a sophisticated question-answering system designed exclusively for the KidsQA dataset. The chosen model, a pediatric-friendly BERT variant, necessitates fine-tuning for pedagogical clarity and simplicity. The system should also incorporate intuitive, visually appealing interactive features to ensure prolonged engagement and promote effective learning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education for young learners\",\n        \"description\": \"The project aims to develop a sophisticated question-answering system specifically tailored for the KidsQA dataset. A pediatric-friendly BERT variant needs to be fine-tuned for pedagogical clarity and simplicity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"pedagogical clarity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"pedagogical simplicity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A dataset designed for young learners' question-answering systems, targeting the educational needs of this age group.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"pediatric-friendly BERT variant\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine-tuning\": {\n                    \"pedagogical focus\": true\n                }\n            },\n            \"description\": \"A BERT model adapted for young learners, with a focus on pedagogical clarity and simplicity during fine-tuning.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"web or dedicated educational platform\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"interactive_features\": true,\n            \"engagement_strategies\": \"visually appealing and intuitive\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is parsed into a JSON reflecting the project requirements. The user's intent to build is clear. The 'expertise' is set to medium. The project focuses on question-answering in NLP with specific mention of KidsQA dataset and a pediatric-friendly BERT variant. Performance metrics for pedagogical aspects are not provided in the instruction, so they are left as null. The service section outlines the need for an interactive and visually appealing system. However, the deployment endpoint and specific inference engines are not specified in the instruction.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a cutting-edge AI research team, you've been tasked with developing a sophisticated time series forecasting model for the Traffic dataset. This dataset, containing rich, high-dimensional (INPUT_SEQ_LEN=96, INPUT_DIM=862) real-world data, has been expertly partitioned into train, validation, and test sets for comprehensive evaluation. The challenge involves not only predicting the subsequent 96-step future sequence (PRED_SEQ_LEN=96, PRED_DIM=862) but also optimizing for improved performance using advanced techniques like deep learning architectures, seasonality modeling, and regularization. The team must strive for exceptional accuracy, assessed through mean squared error (MSE) and mean absolute error (MAE), while demonstrating a strong understanding of time-varying patterns and external influences in the data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"traffic prediction\",\n        \"description\": \"Develop a sophisticated time series forecasting model for the Traffic dataset. The dataset contains high-dimensional data with INPUT_SEQ_LEN=96, INPUT_DIM=862 and is partitioned into train, validation, and test sets. The model should predict PRED_SEQ_LEN=96 steps ahead, considering seasonality, deep learning architectures, and regularization to optimize for accuracy (MSE and MAE). Focus on understanding time-varying patterns and external influences.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Traffic\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 862,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 862\n            },\n            \"description\": \"Real-world traffic data with high dimensionality, expertly partitioned for comprehensive evaluation.\",\n            \"preprocessing\": [\"dimensionality reduction\", \"normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"data exploration\", \"seasonality analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"deep learning architectures\": [\"transformers\", \"LSTM\"],\n                \"seasonality modeling\": true,\n                \"regularization\": true\n            },\n            \"description\": \"A sophisticated forecasting model incorporating advanced techniques for the Traffic dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporate external data sources like weather conditions or events that might impact traffic patterns.\",\n        \"Use ensemble models to improve prediction accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"cross-validation\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"specific_model_serving_api\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Apache Spark MLlib\"],\n        \"demonstration\": \"Create visualizations and performance metrics graphs for clear communication.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction clearly specified a complex AI project with a high expertise level, including deep learning techniques and understanding of external influences. The model and service sections are detailed with the intended focus on accuracy and performance. The missing metric values and specific model names are placeholders for detailed project execution.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For a real-time, energy-efficient furniture segmentation in high-resolution indoor scenes, adopt U-Net with a constraint on latency under 50 milliseconds, while ensuring model accuracy above 90% on diverse architectural styles and lighting conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"indoor scene analysis\",\n        \"description\": \"Real-time, energy-efficient furniture segmentation in high-resolution indoor scenes is required. The model should be based on U-Net and have a latency constraint of under 50 milliseconds, while maintaining an accuracy of at least 90% across diverse architectural styles and lighting conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 50,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Dataset for indoor scene analysis\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": {\n                    \"width\": \"high\",\n                    \"height\": \"high\"\n                }\n            },\n            \"description\": \"A dataset with high-resolution indoor scenes for real-time furniture segmentation.\",\n            \"preprocessing\": [\"energy-efficient preprocessing\"],\n            \"augmentation\": [\"low computational overhead\"],\n            \"visualization\": [\"model behavior under varying styles and lighting\"],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net\",\n            \"family\": \"UNET family\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"latency\": {\n                    \"maximum\": 50,\n                    \"unit\": \"milliseconds\"\n                },\n                \"energy_efficiency\": \"optimized for real-time and energy consumption\"\n            },\n            \"description\": \"A U-Net model tailored for furniture segmentation in high-resolution indoor scenes with a latency constraint.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"latency-aware feature extraction\"],\n        \"target_device\": \"real-time environment (e.g., edge device or IoT)\",\n        \"deployment_endpoint\": \"optimized for low-latency deployment\",\n        \"inference_engine\": [\"optimized for energy efficiency\"],\n        \"demonstration\": \"high-performance and real-time demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed into a JSON reflecting a machine learning project. The user's intent to build a model with a focus on U-Net, performance requirements, and constraints on latency and accuracy are captured. The high-resolution indoor scenes and diverse architectural styles and lighting conditions are specified in the problem description. The model's latency is explicitly mentioned and the dataset is left as a search based on the scene requirements. A medium expertise level is assumed as the user requests advice.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "Revise the medical diagnosis Q&A model to not only incorporate the expansive BioASQ dataset, but also include cross-lingual support for non-English medical terms. Optimize for real-time performance with GPU-optimized inference and ensure it adheres to the latest medical best practices, while integrating a citation traceability feature for all responses, drawing from PubMed and ClinicalTrials databases.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medical\",\n        \"description\": \"Revise the medical diagnosis Q&A model to incorporate the BioASQ dataset and add cross-lingual support for non-English medical terms. Optimize for real-time performance using GPU-optimized inference and adhere to latest medical best practices. Include a citation traceability feature referencing PubMed and ClinicalTrials databases.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": \"real-time\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"GPU inference efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs/second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"cross-lingual\": true\n            },\n            \"description\": \"A medical question answering dataset to be incorporated for improved model performance.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Revised Medical Diagnosis Q&A Model\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"GPU-optimized\": true\n            },\n            \"description\": \"A model designed for medical diagnosis Q&A with cross-lingual support, real-time performance, and citation traceability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Latest medical best practices and relevant domain databases.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud GPU environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated libraries\"],\n        \"demonstration\": \"Real-time response with context-based citations from PubMed and ClinicalTrials.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed based on the given schema, capturing the user's intent to build a model, their expertise level, and the specific requirements for the model's functionality, performance, and data source integration. Performance metrics were included for real-time inference and GPU efficiency. The model family and citation traceability feature were inferred from the details provided, while some areas like deployment endpoint and precise GPU performance metrics remain unspecified.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a transportation project manager, request a cutting-edge deep learning architecture integrating advanced ResNet-CNN and a Bidirectional LSTM hybrid for the comprehensive urban traffic flow dataset. Demonstrate robust forecasting of not only the upcoming month's hourly volumes but also analyze the impact of complex factors like holiday-season adjustments, seasonality, and the influence of neighboring events on peak hours and weekend traffic patterns. Ensure a detailed sensitivity analysis report is included.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"urban traffic\",\n        \"description\": \"A transportation project manager seeks a cutting-edge deep learning architecture that integrates advanced ResNet-CNN and a Bidirectional LSTM hybrid for comprehensive urban traffic flow analysis. The focus is on forecasting hourly volumes for the upcoming month, considering the impact of holiday-season adjustments, seasonality, and the influence of neighboring events on peak hours and weekend traffic patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"hourly forecasting accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"impact analysis accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"comprehensive urban traffic flow dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\n                \"hourly traffic volumes\"\n            ],\n            \"specification\": {\n                \"time_range\": {\n                    \"start\": \"ongoing\",\n                    \"end\": \"upcoming month\"\n                },\n                \"feature_dimensions\": {\n                    \"hourly volumes\": {\n                        \"seasonality\": true,\n                        \"holiday-adjustments\": true,\n                        \"neighborhood_events\": true\n                    }\n                }\n            },\n            \"description\": \"A dataset containing historical traffic flow data with explicit details on factors that influence traffic patterns.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNet-CNN and Bidirectional LSTM hybrid\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"Advanced fusion of ResNet-CNN and LSTM\"\n            },\n            \"description\": \"An architecture combining deep convolutional and recurrent neural networks for comprehensive traffic flow forecasting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Perform sensitivity analysis to understand model robustness and factors' influence\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"time series feature extraction\", \"holiday-season adjustments\", \"neighboring events integration\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-enabled\"],\n        \"demonstration\": \"visual and interactive traffic forecasting dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear request for a specialized machine learning model, indicating a high level of expertise. The performance metrics for the forecasting and impact analysis are not explicitly stated but are expected to be included in the sensitivity analysis. The dataset source, target variables, and preprocessing steps align with the requirements. However, the target device and deployment specifics are assumed to be cloud-based without elaboration.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Utilize the comprehensive YouTube-8M dataset to develop a state-of-the-art video-based AI-driven visual question-answering platform. The system should combine advanced 3D residual convolutional neural networks and stacked attention Transformers for enhanced video content analysis. Emphasize scalability to diverse thematic categories, short-term temporal dynamics, and real-time adaptability, ensuring not only high accuracy but also timely and context-aware responses. Additionally, design a robust evaluation framework to compare performance with competing models and optimize for precision under varying lighting conditions and complex background scenes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"video-based AI applications\",\n        \"description\": \"Develop a state-of-the-art video-based AI-driven visual question-answering platform using the YouTube-8M dataset. The system should integrate 3D residual convolutional neural networks and stacked attention Transformers for improved video content analysis. It must be scalable, able to handle diverse thematic categories, short-term temporal dynamics, and be real-time adaptable for accurate and context-aware responses. Additionally, an evaluation framework is required for model performance comparison and precision optimization in challenging lighting conditions and complex backgrounds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"timeliness\",\n                \"value\": null\n            },\n            {\n                \"name\": \"context-awareness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"dimensionality and volume of video data\"\n            },\n            {\n                \"name\": \"real-time adaptability\",\n                \"value\": null,\n                \"unit\": \"latency\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube-8M\",\n            \"modality\": [\"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"format\": \"large-scale video data\",\n                \"thematic_categories\": \"diverse\",\n                \"background_complexity\": \"complex\",\n                \"lighting_conditions\": \"varied\"\n            },\n            \"description\": \"A comprehensive video dataset for developing advanced visual question-answering systems with extensive thematic coverage and diverse challenges.\",\n            \"preprocessing\": [\"3D convolutional feature extraction\", \"data augmentation\"],\n            \"augmentation\": [\"lighting and background changes\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"3D Residual Convolutional + Stacked Attention Transformers\",\n            \"family\": \"Convolutional Neural Networks and Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"3D architecture\": \"advanced\",\n                \"attention mechanism\": \"stacked\"\n            },\n            \"description\": \"A combination of 3D residual convolutions and stacked attention Transformers to enhance video content understanding.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"3D feature fusion\", \"context-aware feature extraction\"],\n        \"target_device\": \"real-time adaptable to different platforms\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for dynamic environments\"],\n        \"demonstration\": {\n            \"scenario-based\": [\"video with varying conditions\"]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction parsed well, indicating a 'build' intent, a complex task, and high expertise. The problem details are clear, with specific dataset requirements and a focus on performance metrics. However, the accuracy values are not specified, leaving room for filling in by expert knowledge. The confidence score is high due to the structured information but with the understanding that accuracy metrics are subject to the actual model performance.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop an advanced Transformer-based model for multilingual low-resource language translation, focusing on Esperanto to Icelandic, with a rare emphasis on preserving folklore, proverbs, and regional dialects. The model should be fine-tuned using the combination of WikiMatrix and JRC-Acquis datasets to enhance its cultural specificity. The target is to achieve a chrF++ score of 45, with a particular challenge in handling verb conjugations and compound words. Additionally, the model must be optimized for edge devices, ensuring a processing speed of 15 milliseconds per input text, enabling real-time translation in remote locations with limited connectivity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"low-resource languages\",\n        \"description\": \"Develop an advanced Transformer-based model for multilingual low-resource language translation, focusing on Esperanto to Icelandic translation. Emphasis should be on preserving folklore, proverbs, and regional dialects. The model should be fine-tuned using WikiMatrix and JRC-Acquis datasets for enhanced cultural specificity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"chrF++\",\n                \"value\": 45.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"processing speed\",\n                \"value\": 15,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WikiMatrix\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source_languages\": [\"Esperanto\", \"Icelandic\"],\n                \"languages_intersection\": \"low-resource\",\n                \"cultural_content\": \"folklore, proverbs\"\n            },\n            \"description\": \"A dataset for multilingual low-resource language translation with emphasis on folklore and regional dialects.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"JRC-Acquis\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source_language\": \"Esperanto\",\n                \"target_language\": \"Icelandic\",\n                \"cultural_content\": \"folklore, proverbs\"\n            },\n            \"description\": \"A dataset for enhancing cultural specificity by fine-tuning the model on a combination of Esperanto to Icelandic text.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Transformer-based Model\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_fine_tuning\": \"cultural-specific filters for folklore, proverbs, and dialects\",\n                \"training_dataset_combination\": [\"WikiMatrix\", \"JRC-Acquis\"],\n                \"performance_tradeoffs\": {\n                    \"verb_conjugations\": \"improved handling\",\n                    \"compound_words\": \"improved recognition\"\n                }\n            },\n            \"description\": \"A model designed for multilingual low-resource language translation, focusing on Esperanto to Icelandic and preserving cultural nuances.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for edge devices\"],\n        \"demonstration\": \"real-time translation in remote locations with limited connectivity\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The information provided in the instruction was detailed enough to construct the JSON response. The user has a high level of expertise, and the problem statement is clear. The required performance metrics, dataset specifics, and model requirements are all included. However, the missing components in the 'service' section, such as deployment endpoint, need clarification or an assumption.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Incorporate advanced image enhancement techniques for low-light and dynamic coastal scenarios into the COCO-Sea 4K surveillance project. Enhance the Attention U-Net++ model by integrating a novel attention mechanism that considers long-range dependencies and local context. Strive for precision improvements of at least 2% in discriminating between highly similar coastal formations, such as recognizing speckled granite shores from micro-textured sandy shores. Optimize the system's real-time performance to exceed 30 FPS on resource-restricted drone hardware, while ensuring energy efficiency and robustness against hardware variations. Design a comprehensive evaluation plan to measure the system's robustness in diverse meteorological conditions and validate its generalizability across different geographical locations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"surveillance\",\n        \"description\": \"Incorporate advanced image enhancement techniques for low-light and dynamic coastal scenarios into the COCO-Sea 4K surveillance project. Enhance the Attention U-Net++ model by incorporating a novel attention mechanism for long-range dependencies and local context. Aim for at least 2% improvement in precision for distinguishing highly similar coastal formations, such as speckled granite shores from micro-textured sandy shores.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": 30,\n                \"unit\": \"FPS\",\n                \"optimization\": \"resource-restricted drone hardware, energy efficiency, hardware variations\"\n            }\n        ],\n        \"evaluation_plan\": {\n            \"diverse_conditions\": true,\n            \"geographical_generalizability\": true\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO-Sea 4K\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"enhancement_techniques\": [\"low-light and dynamic coastal\"],\n                \"model_integration\": \"Attention U-Net++ with novel attention mechanism\"\n            },\n            \"description\": \"A surveillance dataset focused on coastal scenes for the advanced enhancement and segmentation tasks.\",\n            \"preprocessing\": [\"advanced image enhancement techniques\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Attention U-Net++\",\n            \"family\": \"Attention U-Net++\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"long-range_dependencies\": true,\n                \"local_context\": true\n            },\n            \"description\": \"A model designed for advanced image segmentation in low-light and dynamic coastal scenarios.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-restricted drone hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"meteorological_conditions\": true,\n            \"geographical_locations\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was generated based on the provided instruction. High expertise level indicates a need for advanced techniques and complex system design. The metric values, performance optimization, and evaluation plan are clear and specific, reflecting a well-defined objective for the project. However, there is a need for clarification or filling in the service section regarding deployment and inference engine selection.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "Create a highly advanced medical question-and-answer system, incorporating the extensive BioASQ dataset and incorporating real-time patient feedback via encrypted channels. Enhance BioBERT's performance through fine-tuning with specialized biomedical literature, mandate comprehensive citations to credible sources, and implement a comprehensive A/B testing framework comparing multiple versions with stratified patient demographics. Ensure adherence to stringent GDPR regulations by integrating privacy protection measures and conducting privacy impact assessments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medical\",\n        \"description\": \"Create a highly advanced medical question-and-answer system using the BioASQ dataset. Incorporate real-time patient feedback through encrypted channels, optimize BioBERT's performance by fine-tuning with biomedical literature, and require comprehensive citations. Implement A/B testing and handle stratified patient demographics. Ensure GDPR compliance with privacy protection measures and privacy impact assessments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"domain\": \"biomedical literature\",\n                \"format\": \"question-answer pairs\"\n            },\n            \"description\": \"An extensive biomedical dataset for question answering, including real-world patient queries.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BioBERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": null,\n                \"fine-tuning\": \"specialized biomedical literature\"\n            },\n            \"description\": \"Advanced medical Q&A system based on BioBERT, with enhanced performance through fine-tuning.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"information\": \"Comprehensive citations from credible sources\"\n        },\n        {\n            \"information\": \"A/B testing framework with stratified patient demographics\"\n        },\n        {\n            \"information\": \"GDPR compliance with privacy protection measures\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time patient feedback integration\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"compliant with GDPR\"],\n        \"demonstration\": \"privacy impact assessments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects a high-level overview of the requirements, including a build intent and high expertise level for the user. The problem statement captures the task, dataset, and specific needs such as fine-tuning, citations, and A/B testing. However, performance metrics and complexity metrics are speculative since they were not specified in the instruction. GDPR compliance measures are included under the service section, but the deployment endpoint is left blank.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "node classification", "instruction": "As a machine learning project manager, you're now accountable for developing a deployable edge-computing AI system that classifies Explainable Hypergraph Neural Networks (EHNNs) on diverse Wikipedia and DBpedia datasets. Instruct the team to utilize a Siamese architecture for simultaneous processing of node and edge attributes. Mandate a stringent performance benchmark of at least 97% F1-score on unseen label distributions, while strictly adhering to Raspberry Pi's resource constraints. Demand the exploration of ten novel neighborhood sampling strategies, not just for enhancing accuracy but also minimizing energy consumption during inference. Conduct a thorough, comparative study of five different graph convolution methods, considering their efficiency, scalability, and real-world implications for ultra-low-power environments, and provide a detailed report highlighting the nuanced optimizations and trade-offs for practical, resource-constrained scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": [\"Wikipedia\", \"DBpedia\"],\n        \"description\": \"Develop a deployable edge-computing AI system for Explainable Hypergraph Neural Networks (EHNNs) classification. Use a Siamese architecture to process node and edge attributes simultaneously. Target a performance benchmark of at least 97% F1-score on unseen label distributions, while respecting Raspberry Pi's resource constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Energy consumption\",\n                \"unit\": \"Power consumption during inference\"\n            },\n            {\n                \"name\": \"Efficiency\",\n                \"unit\": \"% of CPU usage\"\n            },\n            {\n                \"name\": \"Scalability\",\n                \"unit\": \"Impact on large, ultra-low-power environments\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wikipedia and DBpedia\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"Siamese Architecture\": true,\n                \"Node and Edge Attributes\": true\n            },\n            \"description\": \"Diverse datasets from Wikipedia and DBpedia for EHNN classification\",\n            \"preprocessing\": [\"Neighborhood Sampling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Explainable Hypergraph Neural Networks\",\n            \"family\": \"Siamese Architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"node_attribute_processing\": true,\n                \"edge_attribute_processing\": true,\n                \"label_distribution\": \"unseen\"\n            },\n            \"description\": \"Classification model using Explainable Hypergraph Neural Networks with a Siamese architecture\"\n        }\n    ],\n    \"knowledge\": [\n        \"Exploration of ten novel neighborhood sampling strategies\",\n        \"Comparison of five graph convolution methods: efficiency, scalability, and practical implications for resource-constrained scenarios\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Adhering to Raspberry Pi's resource constraints\"],\n        \"target_device\": \"Raspberry Pi\",\n        \"deployment_endpoint\": null, \n        \"inference_engine\": [\"Energy-efficient techniques for edge computing\"],\n        \"demonstration\": \"Performance and optimization in practical, resource-constrained scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the instruction's request for a high-expertise machine learning project involving EHNN classification, resource-constrained development, and a performance benchmark. Key components such as task type, datasets, model specifics, performance targets, and resource management are adequately addressed. However, there is uncertainty about some aspects, like the deployment endpoint, without it being explicitly mentioned.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "Create a highly specialized YOLOv5.1 model for an upscale luxury grocery store, utilizing the ExclusiveGourmetProductDataset of 1,500 premium items. Demand an F1-score above 97% with individual precision exceeding 98% for gourmet products, and guarantee real-time inventory with sub-nanosecond response time of <0.5 milliseconds. Implement a concurrent AI-assisted tagging system with zero-detection delays, while synchronizing the database seamlessly for uninterrupted customer experience and continuous inventory auditing.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury grocery\",\n        \"description\": \"Create a highly specialized YOLOv5.1 model for an upscale luxury grocery store, targeting the ExclusiveGourmetProductDataset with 1,500 premium items. The model should achieve an F1-score of over 97% and individual precision above 98% for gourmet products. It must ensure real-time inventory with a response time of less than 0.5 milliseconds and incorporate a concurrent AI-assisted tagging system with zero detection delays.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"Individual Precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.5,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ExclusiveGourmetProductDataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product_id\"],\n            \"specification\": {\n                \"total_items\": 1500,\n                \"premium_items\": 1500\n            },\n            \"description\": \"A dataset containing high-resolution images of 1,500 premium products for an upscale luxury grocery store.\",\n            \"preprocessing\": [\n                \"data augmentation\",\n                \"image resizing\"\n            ],\n            \"augmentation\": [\n                \"realistic image transformations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5.1\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"detection_threshold\": 0.9,\n                \"object_classes\": [\"gourmet\"]\n            },\n            \"description\": \"A highly customized YOLOv5.1 model specifically designed for the upscale luxury grocery store, targeting high precision and recall for gourmet product detection.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"AI-assisted tagging\",\n            \"inventory forecasting\"\n        ],\n        \"target_device\": \"server-side with concurrent execution\",\n        \"deployment_endpoint\": \"customer-facing API for seamless integration\",\n        \"inference_engine\": [\"optimized for real-time execution\"],\n        \"demonstration\": \"AI-assisted tagging and inventory updates in real-time during customer shopping\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is parsed with high confidence, capturing the user's intent for model development, the high expertise level, and specific requirements for the model, performance, and service aspects. The performance metrics have been set, and the complexity metrics target real-time execution. The dataset details are clear, and the model, service, and knowledge components are in line with the task given.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As the project manager for an elite AI team, your new mandate involves tackling a high-dimensional, real-world challenge involving the Time-series Panorama dataset. The data now boasts 68 extra variables and irregular time intervals, posing distribution disparities among train (75%), validation (12.5%), and test (12.5%) subsets. Your ensemble model must design a state-of-the-art architecture that forecasts 292-step sequences (INPUT_SEQ_LEN+PRED_SEQ_LEN), maximizing accuracy for MSE, MAE, and a new metric, dynamic rolling RMSLE, all while maintaining explainability. The model must achieve efficiency by optimizing cloud resource utilization, demonstrating resilience to unforeseen environmental variables, and conforming to strict constraints without compromising interpretability. Moreover, produce a lucid research paper that meticulously documents the intricate design, regularization techniques, and trade-offs made for interpretability within resource-constrained conditions, all in 2,500 word limit.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-world with high-dimensional data (68 variables) and irregular time intervals\",\n        \"description\": \"Address a high-dimensional time-series challenge with the Time-series Panorama dataset. The model must forecast 292-step sequences while optimizing for accuracy in MSE, MAE, and dynamic rolling RMSLE. Emphasis on explainability, resource efficiency, resilience, and interpretability under strict constraints in a 2,500-word research paper.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"dynamic rolling RMSLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": {\n            \"resource utilization\": {},\n            \"environmental resilience\": {},\n            \"interpretability constraints\": {}\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Time-series Panorama\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 292,\n                \"PRED_SEQ_LEN\": 292,\n                \"additional_variables\": 68,\n                \"time_interval_irregularity\": \"yes\"\n            },\n            \"description\": \"A high-dimensional dataset with 68 extra variables and irregular time intervals, divided into train, validation, and test subsets.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"forecast_length\": 292,\n                \"interpretable\": true\n            },\n            \"description\": \"Design an ensemble model for time-series forecasting, with focus on accuracy, interpretability, resource efficiency, and resilience to environmental variables.\"\n        }\n    ],\n    \"knowledge\": [\n        \"2,500-word research paper constraint, documenting intricate design, regularization techniques, and trade-offs for interpretability under resource constraints.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for cloud\"],\n        \"demonstration\": \"clearly explain the model's approach and performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The information provided was parsed based on the given requirements. The high expertise level indicates a deep understanding of the task. The 'performance_metrics' include the requested measures, and the 'complexity_metrics' address resource constraints and interpretability. The research paper requirement is noted as a separate piece of information. However, there were no specifics for the model's architecture or the deployment endpoint, so these are left as placeholders.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create a state-of-the-art Transformer-XL-enhanced multilingual translation system that fuses the expansive XSum corpus with niche languages (Nepali, Indonesian, and Swahili), necessitating an unprecedented benchmark of 45 distinct TERQ scores across four translation directions. The evaluation must include intricate domain-specific content such as medical research, aerospace communication, and financial regulations. Mandate a precision of 99.7% in deciphering highly technical terminologies like neurology and astrobiology, while maintaining naturalness and processing volumes of 60,000 sentences per second for ultra-responsive international cooperation. Ensure compliance with the most stringent GDPR privacy regulations and design the model with Explainable AI features, pushing the boundary of transparency to foster global trust among users in a cross-cultural understanding setting.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": [\"medical research\", \"aerospace communication\", \"financial regulations\"],\n        \"description\": \"Create a state-of-the-art Transformer-XL-enhanced multilingual translation system, combining the XSum corpus with Nepali, Indonesian, and Swahili. Aim for a benchmark of 45 distinct TERQ scores across four translation directions, with a focus on technical terms in neurology and astrobiology. Target a processing speed of 60,000 sentences per second for ultra-responsive international cooperation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision (specific to technical terminologies)\",\n                \"value\": 0.997\n            },\n            {\n                \"name\": \"TERQ score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"processing speed\",\n                \"value\": 60000,\n                \"unit\": \"sentences per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"XSum corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"corpus_type\": \"summarization\",\n                \"languages\": [\"multilingual\"]\n            },\n            \"description\": \"The extensive summarization corpus for multilingual text input.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Nepali, Indonesian, Swahili\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"Nepali\", \"Indonesian\", \"Swahili\"]\n            },\n            \"description\": \"Niche language corpora for translation tasks\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL-enhanced\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainable AI\": true,\n                \"compliance\": \"GDPR\",\n                \"specific features\": \"Transformer-XL, multilingual, Explainable AI\"\n            },\n            \"description\": \"A state-of-the-art model with enhanced Transformer-XL, designed for high-precision translation of technical terms and fast processing.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Complex domain-specific content in medical research, aerospace communication, and financial regulations\",\n        \"Precision requirement for technical terminologies (neurology and astrobiology)\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"ultra-responsive, cross-cultural understanding\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provides a clear task with high-level requirements, specifying high expertise, various performance and complexity metrics, and domain-specific content. However, details like model specification, deployment endpoint, and inference engine are not explicitly mentioned, requiring assumptions. The confidence score is high, given the comprehensiveness of the provided information.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "For the specialized ML team, design an innovative Hierarchical Attention Network (HAN) tailored to the quantum physics domain, optimizing for a precision of at least 97% in distinguishing between condensed matter, high-energy, and astrophysics subfields. The model should exhibit latency-sensitive, streaming analysis of arXiv submissions, adjusting to shifts in research trends and filtering out irrelevant literature with a noise reduction algorithm. Ensure continuous benchmarking and validation on a daily basis, integrating adaptive fine-tuning mechanisms for consistent, cutting-edge advancements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"quantum physics\",\n        \"description\": \"Design an innovative Hierarchical Attention Network (HAN) specifically tailored for the quantum physics domain. The model should aim for at least 97% precision in differentiating between condensed matter, high-energy, and astrophysics subfields. It should perform latency-sensitive, streaming analysis on arXiv submissions, adapt to shifts in research trends, and use a noise reduction algorithm for better filtering of irrelevant literature.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": \"streaming analysis\",\n                \"unit\": \"real-time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv (quantum physics)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"condensed matter\", \"high-energy\", \"astrophysics\"],\n            \"specification\": {\n                \"task\": \"classification\",\n                \"size\": null,\n                \"specific_dimensions\": null\n            },\n            \"description\": \"A dataset of arXiv submissions focusing on quantum physics, containing abstracts for supervised learning.\",\n            \"preprocessing\": [\"cleaning\", \"tokenization\"],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [\"tf-idf analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom HAN (quantum physics)\",\n            \"family\": \"Hierarchical Attention Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for quantum physics domain\"\n            },\n            \"description\": \"A Hierarchical Attention Network designed for quantum physics research, with an emphasis on precision and real-time analysis.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adaptive fine-tuning\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"arXiv analytics platform\",\n        \"inference_engine\": [\"GPU\", \"TPU\"],\n        \"demonstration\": \"streamlined and interactive\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction parsed accurately, including the high expertise level, the tailored HAN, and the performance metrics. The 'service' section includes elements like adaptive fine-tuning and cloud deployment. However, specific complexities like inference time were not included as they were not directly stated, leaving room for assumption.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a highly specialized project manager for an exceptional AI research and development team, your challenge is to develop an innovative, domain-specific Q&A platform for the biochemistry community. Utilize the exclusive MedBiomQANet dataset, containing intricate inquiries and answers from recent clinical trials. The platform must employ the advanced M6v3 transformer model, prioritizing transparency through XAI techniques tailored for medical explanations, enabling molecular biologists to trace response origins. The system must demonstrate exceptional live emergency response capacity, processing ambiguous data with the Quantum-enhanced Medical Decision Support System (QMDSS), while maintaining accuracy under time constraints. Emphasize the platform's ability to identify anomalies in research data, adapt to emerging discoveries in pharmacology, and incorporate a self-evolving learning framework that guarantees 99% system efficiency under peak query volumes. Include a detailed roadmap, complete with weekly milestones and monthly performance benchmarks, specifically evaluating the model's effectiveness in handling complex biochemical queries and the explainability module's usefulness in the hands of expert researchers.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"biochemistry\",\n        \"description\": \"Develop an innovative Q&A platform for the biochemistry community using the MedBiomQANet dataset. The platform should employ the M6v3 transformer model, prioritize transparency with XAI techniques for medical explanations, handle live emergency responses with QMDSS, and demonstrate anomaly detection, adaptability to emerging pharmacology, and self-evolving learning for 99% efficiency under high query volume.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy on complex biochemical queries\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Explainability module effectiveness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Live emergency response processing speed\",\n                \"value\": null,\n                \"unit\": \"time (e.g., seconds or milliseconds)\"\n            },\n            {\n                \"name\": \"System efficiency under peak query volumes\",\n                \"value\": 0.99\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MedBiomQANet\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"recent_clinical_trials\": true\n            },\n            \"description\": \"An exclusive dataset for biochemistry Q&A, including intricate inquiries and answers from recent clinical trials.\",\n            \"preprocessing\": [\"domain-specific tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6v3 transformer model\",\n            \"family\": \"transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability_techniques\": \"tailored XAI for medical explanations\"\n            },\n            \"description\": \"Advanced model for biochemistry Q&A, incorporating M6v3 and transparency techniques\"\n        }\n    ],\n    \"knowledge\": [\n        \"Quantum-enhanced Medical Decision Support System (QMDSS) for emergency response\",\n        \"Self-evolving learning framework to adapt to emerging discoveries in pharmacology\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"XAI for explainable molecular biologist responses\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"secured biochemistry Q&A platform\",\n        \"inference_engine\": [\"Quantum-enhanced\"],\n        \"demonstration\": \"Weekly milestones and monthly performance benchmarks for complex query handling and explainability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction was parsed into a JSON response that accurately captures the project requirements. High expertise suggests a thorough understanding. However, specific performance metrics are currently missing, and need to be determined and included. Similarly, a detailed roadmap with exact milestones and performance benchmarks is not explicitly provided, but implied to be a weekly and monthly evaluation process.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "For an exclusive AI innovation project, design a sophisticated model architecture that combines a highly advanced GNN variant and a state-of-the-art transformer model. The objective is to achieve an astonishing accuracy of 99.8% in categorizing the intricate Mini-ImageNet dataset, while maintaining interpretability through feature attribution and disentangled representation learning. The model should also undergo a rigorous multi-step evaluation, comparing its few-shot transfer learning capabilities with competitive baselines on diverse visual domain adaptations, ensuring not only performance but also incremental learning efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"AI innovation\",\n        \"description\": \"Design a sophisticated model architecture combining a GNN variant and a transformer for classifying Mini-ImageNet with an accuracy of at least 99.8%. Include interpretability through feature attribution and disentangled representation learning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.998\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mini-ImageNet\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"complexity\": \"intricate\"\n            },\n            \"description\": \"The model must be evaluated on the Mini-ImageNet dataset, with a focus on few-shot transfer learning and visual domain adaptations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GNN-Transformer combination\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": [\"feature attribution\", \"disentangled representation\"],\n                \"performance_target\": \"99.8% accuracy\"\n            },\n            \"description\": \"An advanced model designed for exclusive AI innovation, targeting high accuracy while preserving interpretability.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"Illustrate few-shot transfer learning and incremental learning results\"\n    },\n    \"knowledge\": [],\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been parsed based on the provided instruction. The user's intent is to build a model, and their expertise is assumed to be high given the complexity of the project. The performance metrics are clearly defined, including a high accuracy target. The challenge of few-shot transfer learning and visual domain adaptations are captured under the 'multi-step evaluation'. The requirement for a GNN variant and transformer, along with interpretability measures, are also included.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "Instruct the ML development team to design a cutting-edge QA platform, specifically tailored for the MLQA dataset. Emphasize multilingual expertise, with compatibility for English, Spanish, Mandarin, French, and Arabic. Prioritize efficiency in resource-scarce situations, while maintaining peak performance using mBERT and XLM-R. Mandate a breakthrough of at least 5% in cross-lingual benchmark accuracy without sacrificing computational speed, and request detailed performance analyses in diverse language combinations for validation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"MLQA dataset\",\n        \"description\": \"Design a cutting-edge QA platform, focusing on multilingual support (English, Spanish, Mandarin, French, and Arabic) and efficiency in resource-scarce situations. Use mBERT and XLM-R for high performance while aiming for a 5% accuracy improvement in cross-lingual benchmark without impacting computational speed.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"cross-lingual benchmark accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational speed\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MLQA\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\n                    \"English\",\n                    \"Spanish\",\n                    \"Mandarin\",\n                    \"French\",\n                    \"Arabic\"\n                ]\n            },\n            \"description\": \"A dataset for machine learning-based question answering, covering multiple languages.\",\n            \"preprocessing\": [\"Multilingual data normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": [\"mBERT\", \"XLM-R\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"cross-lingual capabilities\": \"improved\"\n            },\n            \"description\": \"A cutting-edge QA model leveraging mBERT and XLM-R for high performance across multiple languages.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"resource-efficient techniques for multilingual scenarios\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource-scarce environments\"],\n        \"demonstration\": \"cross-lingual performance analyses for diverse language combinations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON reflects the given instruction, considering the task to be a 'build' project. The user's high expertise level is captured. The requirement for a 5% improvement in cross-lingual benchmark accuracy without impacting speed is included, along with the need for detailed performance analysis. The 'service' section presumes that the platform will be deployed in the cloud and optimized for resource-scarce environments.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The development team is mandated to create an innovative, luxury-focused Q&A platform that not only embodies emotion intelligence but also differentiates with its sophisticated fusion of DistilBERT and novel explainability layers. The system must excel in resolving complex, context-dependent queries about elite products while maintaining a stringent requirement of >99.9% precision, microscopic (<1 millisecond) response times, and peak performance during high-traffic scenarios. Moreover, the platform must seamlessly integrate with premium e-commerce APIs, conduct real-time sentiment analysis using a proprietary LuxuryEAQ dataset exclusive to premium clientele, and feature adaptive updates for constant enhancement. Ensure compliance with stringent A/B testing protocols, prioritizing elite customer experience and performance benchmarks at every development iteration.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"luxury Q&A platform\",\n        \"description\": \"Create an innovative platform that combines DistilBERT and novel explainability layers for emotion-intelligent and sophisticated Q&A, targeting elite products with >99.9% precision, <1 millisecond response times, and high-traffic performance. The platform must integrate with premium e-commerce APIs, conduct real-time sentiment analysis using a LuxuryEAQ dataset, and allow for adaptive updates.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.999\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": 0.001,\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null, \n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"high-traffic performance\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LuxuryEAQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"proprietary\": true,\n                \"customer_pool\": \"premium\"\n            },\n            \"description\": \"An exclusive dataset for real-time sentiment analysis, specifically tailored for luxury Q&A.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DistilBERT with novel explainability layers\",\n            \"family\": \"neural networks\",\n            \"type\": \"transformers\",\n            \"specification\": {\n                \"model_name\": \"DistilBERT\",\n                \"explainability_layers\": \"custom\"\n            },\n            \"description\": \"A platform-integrated Q&A model using DistilBERT with an emphasis on emotion intelligence and explainability.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"premium e-commerce APIs\",\n            \"real-time sentiment analysis\"\n        ],\n        \"demonstration\": {\n            \"method\": \"A/B testing\",\n            \"focus\": [\"elite customer experience\", \"performance benchmarks\"]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed based on the provided instruction, which requires building a platform with specific performance metrics, including precision and response time. The user's expertise is assumed to be high due to the complexity of the task. However, there might be missing details in the response due to the absence of A/B testing protocols specifics and peak performance targets under high-traffic scenarios in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "The retooled machine learning team must now develop an emotion-aware NMT system with exceptional 99.9% accuracy, translating intricate medical slang from obscure biochemistry and pharmacology databases. They must not only meet the Levenshtein threshold but also invent a novel metric to assess the system's ability to reproduce nuanced medical concepts in diverse global forums, reflecting nuanced clinical scenarios that challenge current AI models, facilitating seamless international consultations among healthcare experts. Moreover, the system should adapt dynamically to regional medical terminologies and idiomatic expressions for optimal understanding and decision-making support.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"medicine and healthcare\",\n        \"description\": \"The retooled machine learning team is to develop an emotion-aware NMT system that translates intricate medical slang from biochemistry and pharmacology databases, with a target accuracy of 99.9%. The system must meet the Levenshtein threshold and introduce a novel metric for assessing nuanced medical concept reproduction in global forums with diverse clinical scenarios. It should facilitate seamless international consultations and adapt dynamically to regional terminologies and idiomatic expressions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.999\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"novel metric for nuanced concept reproduction\",\n                \"value\": null,\n                \"unit\": \"unknown\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Obscure Biochemistry and Pharmacology Databases\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source_type\": \"database\"\n            },\n            \"description\": \"A comprehensive dataset containing intricate medical slang and terminology from biochemistry and pharmacology sources.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Emotion-aware NMT System\",\n            \"family\": \"Neural Machine Translation (NMT)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_goal\": 0.999,\n                \"novel_metric\": \"developed\"\n            },\n            \"description\": \"An advanced NMT system designed for medical translation, considering emotions, regional variations, and nuanced medical concepts.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance computing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"dynamic regional adaptation\"\n        ],\n        \"demonstration\": \"multilingual seamless consultations and decision-making support\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent for building an advanced NMT system, with high-level expertise indicated. The problem statement is detailed, including the accuracy requirement, complexity metric, and specific application domain. The dataset is defined with a relevant source and preprocessing methods left open. The model description is clear, including the novel metric development, while the service section outlines some specific technology for adaptation and deployment. However, the confidence score is not high due to the lack of clear criteria for 'novel metric' and the target device for deployment.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a groundbreaking, hybrid translation solution that fuses the OpenSubtitles repository's diverse media data, incorporating proprietary ASR technology and sophisticated visual context understanding. Enhance the specialized BERT-MultiMT architecture for London-specific Cockney-English to Mandarin dialect translation, with a focus on slang, slang from distinct social circles, and slang unique to the city. The system must ensure synchronization with subtitling and lip-reads, surpass a stringent benchmark of 99.9% accuracy, and preserve cultural nuances by training on an extensive, diverse conversational dataset enriched with regional slang and colloquial expressions. Design an adaptive module for seamless integration into impromptu, unscripted video chats, ensuring semantic fidelity and preservation of idiomatic expressions across different social contexts and situations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"media translation\",\n        \"description\": \"Develop a groundbreaking, hybrid translation solution combining OpenSubtitles repository's diverse media data, proprietary ASR technology, and visual context understanding. Specialize the BERT-MultiMT architecture for London-specific Cockney-English to Mandarin dialect translation, focusing on slang, social circle-specific slang, and city-specific expressions. The system must maintain synchronization with subtitling and lip-reads, aim for 99.9% accuracy, and preserve cultural nuances through extensive, diverse conversational training with regional slang and colloquialism.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.999\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles repository\",\n            \"modality\": [\"text\", \"video\", \"audio\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"diversity\": \"multilingual and diverse media content\"\n            },\n            \"description\": \"A rich dataset containing diverse media data for training and adapting the translation model.\",\n            \"preprocessing\": [\"ASR conversion\", \"visual context extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model performance on slang, social context\"],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Cockney-English to Mandarin conversational dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Mandarin translations\"],\n            \"specification\": {\n                \"culturalrichness\": \"enriched with regional slang and colloquial expressions\"\n            },\n            \"description\": \"Extensive, London-specific conversational dataset for preserving cultural nuances\",\n            \"preprocessing\": [\"linguistic slang normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-MultiMT\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT-MultiMT adapted for London Cockney-English to Mandarin\"\n            },\n            \"description\": \"A specialized model designed for London-specific dialect translation, focusing on slang and cultural nuances.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Proprietary ASR technology and visual context understanding\",\n        \"Expertise on customizing BERT-MultiMT and handling linguistic challenges\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"integration for lip-reads and subtitling synchronization\"],\n        \"target_device\": \"cloud and mobile-friendly\",\n        \"deployment_endpoint\": {\n            \"type\": \"API-based\"\n        },\n        \"inference_engine\": [\"real-time, adaptive for impromptu video chats\"],\n        \"demonstration\": \"Semantic fidelity and idiomatic expression preservation in diverse contexts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear vision for a complex machine translation project, specifying requirements for both data and model architecture. The user's high expertise suggests they understand the technicalities and focus on specific aspects like slang and cultural preservation. There is room for more specific deployment details, but the confidence score reflects the high-level guidance provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager overseeing a cutting-edge AI development team, your group has been commissioned to design a next-generation time-series forecasting system for the highly complex and interrelated ILI dataset. This dataset, stratified into train, validation, and test subsets, consists of an intricate web of real-world data with (INPUT_SEQ_LEN=36, INPUT_DIM=7), each sequence presenting a rich tapestry of historical trends. The task at hand requires not only the development of a highly adaptable and robust predictive model but also one showcasing extraordinary transfer learning capabilities across an extensive range of sub-patterns.\n\nYou are expected to devise an algorithm that forecasts the subsequent sequences (PRED_SEQ_LEN=24, PRED_DIM=7) with unparalleled precision, employing advanced loss functions such as the adaptive EWMA-MSE and the discerning RMAPE. To achieve this, the model must excel not only in accuracy but also in computational efficiency, demonstrating a solid balance between performance and resource optimization. Emphasize on extensive experimentation with innovative feature extraction techniques and a systematic exploration of multiple model architectures, demanding deep, multi-step reasoning for each component selection and fine-tuning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"complex and interrelated real-world data\",\n        \"description\": \"Design a next-generation time-series forecasting system for the ILI dataset, stratified into train, validation, and test subsets. The dataset contains intricate historical trends with INPUT_SEQ_LEN=36 and INPUT_DIM=7, and the aim is to forecast PRED_SEQ_LEN=24, PRED_DIM=7 with high precision using advanced loss functions such as adaptive EWMA-MSE and RMAPE. The model must be adaptable, robust, and computationally efficient, incorporating innovative feature extraction techniques and multiple model architectures.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"adaptive EWMA-MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 36,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 24,\n                \"PRED_DIM\": 7\n            },\n            \"description\": \"A complex and interrelated real-world time-series dataset stratified into train, validation, and test subsets, featuring historical trends with given dimensions and sequence lengths.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced techniques\": [\"innovative feature extraction\", \"multi-model architecture exploration\"],\n                \"computational efficiency\": \"systematic fine-tuning\"\n            },\n            \"description\": \"A highly adaptable and robust predictive model for time-series forecasting with advanced loss functions, extensive experimentation with feature extraction techniques, and multi-step model architecture selection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Expertise on adapting to complex sub-patterns via transfer learning\",\n        \"Systematic exploration of multiple model architectures and fine-tuning\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"innovative feature extraction\",\n            \"model architecture exploration\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"requirements\": \"exemplify precision, robustness, and computational efficiency\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed based on the provided requirements for a complex AI project, detailing the high level of expertise needed, the dataset and its complexity, the advanced metrics, and the target model's adaptability and efficiency. Specific model details and deployment specifics were left unspecified based on the provided information.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "node classification", "instruction": "Utilize the newly upgraded Mouse Brain Connectivity dataset, comprising ultra-fine resolution fibers, to construct a groundbreaking Graph Wavelet Neural Network (GWNN). Design a model capable of high-precision node classification, accurately differentiating a diverse range of neuron subtypes with stringent error thresholds. Emphasize the system's capacity to decipher complex topological and spatial relationships, integrating efficient algorithms that guarantee scalability on massive brain network datasets, while maintaining real-time computational performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Use the upgraded Mouse Brain Connectivity dataset with ultra-fine resolution fibers to create a Graph Wavelet Neural Network (GWNN). The model should have high precision for node classification, accurately distinguishing various neuron subtypes with strict error thresholds. Focus on the model's ability to understand complex topological and spatial relationships, while ensuring scalability for large brain network datasets and real-time computational performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Connectivity (upgraded)\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"neuron subtypes\"],\n            \"specification\": {\n                \"resolution\": \"ultra-fine\",\n                \"nodes\": \"brain connectivity nodes\",\n                \"edges\": \"fine-resolution fibers\"\n            },\n            \"description\": \"A dataset with enhanced resolution for analyzing mouse brain connectivity, emphasizing fine details in fibers.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"feature normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"network graph\"\n            ],\n            \"source\": \"user-link\" \n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Wavelet Neural Network (GWNN)\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"topology_learning\": \"graph wavelets\",\n                \"node_classification_performance\": \"high precision\"\n            },\n            \"description\": \"A model designed for node classification in a graph using GWNN architecture.\"\n        }\n    ],\n    \"knowledge\": [\n        \"efficient algorithms for scalability\",\n        \"real-time computational performance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"efficient graph representation\"\n        ],\n        \"target_device\": \"high-performance computing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"example brain network slice demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the high-level intent to build a model, indicating high expertise. The problem area, task, and requirements are detailed. Performance metrics include precision and recall, with an emphasis on scalability and real-time performance. Data preprocessing, model design, and system requirements reflect the needs as per the instruction. Some specific values for complexity metrics are not provided as they would depend on the user's actual requirements. The confidence score reflects the high level of understanding but acknowledges the missing performance metric values.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a skilled project manager for a groundbreaking machine learning team, your mandate is to oversee the development of a highly intricate, scalable time-series forecasting system. The recently acquired comprehensive Weather dataset, dissected into train, validation, and test partitions, comprises intricate, non-stationary data with an extensive 21-feature profile (INPUT_SEQ_LEN expanded to 144 timestamps for enhanced context). The task at hand is to engineer a cutting-edge model that not only adeptly handles diverse seasonality patterns, extended temporal dependencies, and challenging outliers but also pushes the boundaries of explainable artificial intelligence.\n\nThe model should showcase exceptional predictive prowess by forecasting a 192-step sequence (upping PRED_SEQ_LEN to 192) with precision, optimizing for both mean squared error (MSE) and mean absolute error (MAE) under the stringent constraints of resource efficiency. Moreover, the team must demonstrate resilience by delivering a resource-efficient ensemble model, harnessing the power of deep learning innovations, and accompany its architecture with a comprehensive, in-depth whitepaper that not only elucidates its design but also addresses potential trade-offs and future improvements. Remember to prioritize model complexity management and maintain alignment with industry best practices for reproducibility.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"weather forecasting\",\n        \"description\": \"Develop a highly intricate, scalable time-series forecasting system for a comprehensive Weather dataset with an expanded INPUT_SEQ_LEN of 144 timestamps. The model should handle diverse seasonality, extended temporal dependencies, and outliers, targeting 192-step forecasting (PRED_SEQ_LEN=192) with high precision, optimizing for MSE and MAE. The team must create a resource-efficient ensemble model, with deep learning innovations, and provide an in-depth whitepaper on the model's design, trade-offs, and future improvements, while maintaining complexity management and industry best practices for reproducibility.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Weather\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 144,\n                \"INPUT_DIM\": 21,\n                \"PRED_SEQ_LEN\": 192,\n                \"PRED_DIM\": null\n            },\n            \"description\": \"Comprehensive Weather dataset divided into train, validation, and test sets, with an extensive 21-feature profile and a larger context of 144 timestamps.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainable AI\": true,\n                \"ensemble\": true,\n                \"number_of_models\": null,\n                \"complexity\": null\n            },\n            \"description\": \"A cutting-edge model designed for time-series forecasting with strong handling of seasonality, temporal dependencies, outliers, and focus on explainability while being resource-efficient.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"model explanation and whitepaper on design\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction highlights a complex project that requires a high level of expertise due to the intricate model requirements and data characteristics. The user's intent to build implies model development and the need for ensemble models and whitepapers. However, specific metric targets and architectural details are not provided, leaving room for completion. The confidence score is high but with the potential for missing details.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project lead for a top-tier AI development team, we have been assigned a multifaceted challenge: to develop an advanced time-series forecasting system using the intricate ETTm2 dataset. The data spans across intricate train, validation, and test sets, consisting of extensive sequences (96 steps, comprising 21 diverse feature groups each) with additional complexities like missing data and irregular patterns. The model must not only forecast the subsequent 192-step ahead (21 dimensions) with exceptional precision, handling various temporal frequencies and structural changes, but also exhibit resilience to outliers and real-time adaptability. You are required to design a model that excels in reducing normalized root mean squared error (NRMSE) and mean absolute percentage error (MAPE), while integrating advanced methods like forecasting with exogenous variables, seasonal decomposition of trends, and Bayesian model ensembles. The final deliverable should include a detailed whitepaper on technique novelty, benchmarking, and an in-depth analysis of the algorithm's sensitivity to hyperparameter tuning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"complex data with missing values and irregular patterns\",\n        \"description\": \"The task is to develop an advanced time-series forecasting system using the ETTm2 dataset, with complex data structure and 96-step sequences of 21 diverse feature groups. The model should forecast the next 192 steps with resilience to outliers and real-time adaptability. Performance metrics include NRMSE and MAPE reduction, and the project involves techniques like forecasting with exogenous variables, seasonal decomposition, and Bayesian model ensembles. A whitepaper covering technique novelty, benchmarking, and sensitivity analysis is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"NRMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time adaptability\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"feature_groups\": 21,\n                \"irregular_patterns\": true,\n                \"missing_data\": true\n            },\n            \"description\": \"An intricate dataset with train, validation, and test sets, containing extensive sequences with 96 steps and 21 diverse feature groups, including missing data and irregular patterns.\",\n            \"preprocessing\": [\n                \"missing data imputation\",\n                \"seasonal decomposition of trends\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"exploratory data analysis for feature and pattern understanding\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Time-series Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_models\": [\"Bayesian\", \"exogenous variables\"],\n                \"forecast_length\": 192,\n                \"feature_groups\": 21,\n                \"temporal_frequency_handling\": true\n            },\n            \"description\": \"A high-level description of a model that incorporates exogenous variables, seasonal decomposition, and Bayesian model ensembles, designed for the complex ETTm2 dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"title\": \"Technique Novelty\",\n            \"description\": \"A discussion on the unique aspects of the employed forecasting techniques.\"\n        },\n        {\n            \"title\": \"Benchmarking\",\n            \"description\": \"Comparison of our model with state-of-the-art methods on the ETTm2 dataset.\"\n        },\n        {\n            \"title\": \"Sensitivity Analysis\",\n            \"description\": \"Investigation on the algorithm's performance with respect to hyperparameter tuning.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"forecasting with exogenous variables\", \"seasonal decomposition\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"ensemble-based\",\n            \"robust to outliers\"\n        ],\n        \"demonstration\": \"Visualizations and performance data showcases\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure was interpreted from the given instruction with a high level of expertise, capturing details such as the complexity of the dataset and the advanced forecasting techniques. The lack of specific models and deployment details leaves some room for assumptions, but the overall structure is comprehensive.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As the project manager for the high-performance machine learning ensemble, you must oversee the creation of an innovative time-series classification system. The team must delve into the intricate, high-dimensional Handwriting dataset, divided into non-adjacent training (80% stratified), validation (10%), and testing (10%) partitions, where each sequence exhibits complex patterns with 152 observations per instance across a challenging 7-dimensional spectrum (INPUT_SEQ_LEN=152, now INPUT_DIM=7). Your mandate is to architect and fine-tune not just one, but a hierarchy of state-of-the-art models, such as Transformer-XL and stacked LSTMs, integrating multi-task learning and attention mechanisms.\n\nIn addition to maximizing predictive accuracy, generate a comprehensive performance benchmark study that explores the impact of varying sequence lengths (256, 512, and 1024 steps) and dimensionality reduction techniques (PCA, t-SNE, and autoencoders) on model efficiency. Prepare detailed confusion matrices, precision-recall curves, and F1 scores for each iteration, and provide an in-depth explanation of how these modifications influence the model's generalization ability.\n\nCollaborate with the team to devise a novel fusion strategy, incorporating ensemble learning, and compare its performance against the individual models and a grid of traditional baseline models like ARIMA, Prophet, and Random Forest. Ensure robustness through nested cross-validation, and for each label category (0 to 25), generate separate AUC-ROC curves and explain any observed disparities. Remember to highlight the chosen architecture's competitive advantage in terms of computational efficiency and scalability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"high-dimensional data, handwriting recognition\",\n        \"description\": \"As the project manager, oversee the creation of a high-performance machine learning ensemble for time-series classification. The focus is on the Handwriting dataset with 152 observations per instance (INPUT_SEQ_LEN=152, INPUT_DIM=7). The task is to build, fine-tune Transformer-XL, stacked LSTMs, and explore multi-task learning and attention mechanisms. The performance study should include varying sequence lengths and dimensionality reduction techniques and generate detailed performance metrics like confusion matrices, precision-recall curves, and F1 scores.\",\n        \"performance_metrics\": [\n            {\"name\": \"accuracy\"},\n            {\"name\": \"confusion matrices\", \"value\": null},\n            {\"name\": \"precision-recall curves\", \"value\": null},\n            {\"name\": \"F1 scores\", \"value\": null}\n        ],\n        \"complexity_metrics\": [\n            {\"name\": \"computational efficiency\", \"value\": null},\n            {\"name\": \"scalability\", \"value\": null}\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Handwriting\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 152,\n                \"INPUT_DIM\": 7,\n                \"stratified_split\": true,\n                \"train_size\": 0.8,\n                \"validation_size\": 0.1,\n                \"test_size\": 0.1\n            },\n            \"description\": \"A high-dimensional handwriting dataset with non-adjacent partitions for training, validation, and testing.\",\n            \"preprocessing\": [\"stratified split\", \"dimensionality reduction techniques (PCA, t-SNE, autoencoders)\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"confusion matrices\",\n                \"precision-recall curves\",\n                \"F1 scores\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL, stacked LSTMs\",\n            \"family\": [\"Transformer\", \"LSTM\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"multi-task learning\": true,\n                \"attention mechanisms\": true,\n                \"sequence lengths\": [\"256\", \"512\", \"1024\"]\n            },\n            \"description\": \"State-of-the-art models for time-series classification, incorporating advanced techniques\"\n        }\n    ],\n    \"knowledge\": [\n        \"Collaborate with ensemble learning, ARIMA, Prophet, Random Forest baselines\",\n        \"Nested cross-validation and AUC-ROC curves for each label category\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"novel fusion strategy, ensemble learning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"grid of traditional baselines\"],\n        \"demonstration\": \"Performance benchmark and generalization explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear task of building a high-performance ensemble and specified the expertise level. The requirement for various model components, performance metrics, and collaboration with baseline models are included. However, there is no deployment target device or endpoint, which should be filled based on the team's requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a proficient project manager, devise an AI-powered image classifier for a global botanical encyclopedia, leveraging the recently updated Plant Kingdom dataset. Implement a combination of cutting-edge deep learning architectures (ResNet, EfficientNet, and Inception-ResNet), employing multi-task transfer learning and dynamic ensemble strategies. Estimate accuracy with a subset, optimizing for balance between high precision and minimal latency in real-world deployments. Propose a rigorous evaluation protocol for generalization, including cross-entropy tests and real-world scenarios, while outperforming previous benchmarks by at least 5%.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"global botanical encyclopedia\",\n        \"description\": \"As a proficient project manager, devise an AI-powered image classifier for the Plant Kingdom dataset using a combination of ResNet, EfficientNet, and Inception-ResNet deep learning architectures. Apply multi-task transfer learning and dynamic ensemble strategies. The aim is to optimize for high precision and minimal latency in real-world deployments, while outperforming previous benchmarks by at least 5%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-world deployment latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Kingdom\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"recently updated\": true\n            },\n            \"description\": \"A global botanical dataset updated for an AI-powered image classifier.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNet, EfficientNet, and Inception-ResNet combination\",\n            \"family\": \"deep learning architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"multi-task transfer learning\": true,\n                \"dynamic ensemble strategies\": true\n            },\n            \"description\": \"A state-of-the-art ensemble of ResNet, EfficientNet, and Inception-ResNet for botanical image classification.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"requirements\": \"cross-entropy tests, real-world scenarios\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction indicates a complex machine learning project with specific architectures and requirements. The high expertise level suggests the user is knowledgeable about the task, yet may need assistance in some details. The performance metrics, latency optimization, and benchmark comparison have been included. However, there's no exact number for accuracy or precision, and the 'service' section requires more clarification on the deployment and latency constraints.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a trailblazing project manager for a pioneering deep learning ensemble, your mission is to engineer a revolutionary system that not only discerns and differentiates between the seven most subtle subtypes of keratoconus, a rare eye condition, through sub-picometer accuracy, but also unravels the underlying genetic predispositions. The team must innovatively merge generative adversarial networks (GANs) with neuroevolutionary algorithms, enhancing interpretability for identifying early onset signs in ophthalmic screening. This model must surpass benchmark performances, achieving an unprecedented precision of 99.98% with a surgical precision F1-score of 0.998, minimizing the risk of misdiagnosis in patients with extreme genetic predispositions. Moreover, design a quantum-inspired federated learning protocol that leverages quantum entanglement for real-time, error-free, and virtually energy-neutral examinations in extreme environments, all while safeguarding patient privacy. Prepare a groundbreaking whitepaper, a practical demonstration of the quantum-enhanced system, and a rigorous cross-validation study comparing the results with leading quantum and classical machine learning solutions, highlighting the groundbreaking leap in ophthalmic diagnostics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"ophthalmology\",\n        \"description\": \"Engineer a system to discern and differentiate between seven subtle keratoconus subtypes with sub-picometer accuracy, using GANs and neuroevolutionary algorithms. Aim for 99.98% precision and 0.998 surgical F1-score to minimize misdiagnosis for patients with extreme genetic predispositions. Additionally, design a quantum-inspired federated learning protocol for real-time, energy-neutral ophthalmic screenings in extreme environments, preserving patient privacy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.9998,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"surgical F1-score\",\n                \"value\": 0.998,\n                \"unit\": \"\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Custom Keratoconus Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"keratoconus subtype\"],\n            \"specification\": {\n                \"accuracy_target\": \"sub-picometer\",\n                \"genetic_predisposition_analysis\": true\n            },\n            \"description\": \"A dataset for detecting keratoconus subtypes with extreme genetic predispositions, requiring high sub-picometer accuracy.\",\n            \"preprocessing\": [\n                \"GAN-based data augmentation\",\n                \"Quantum-inspired data preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Interpretable GAN results\",\n                \"Genetic predisposition heatmaps\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GAN + Neuroevolutionary Ensemble\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"quantum_inspiration\": true,\n                \"real-time_inference\": true,\n                \"energy_neutral\": true\n            },\n            \"description\": \"A deep learning model combining GANs and neuroevolution for keratoconus detection and genetic predisposition analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Quantum entanglement for secure real-time data exchange\",\n        \"Federated learning protocol leveraging quantum techniques\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Quantum-enhanced interpretability\",\n            \"Federated learning setting for privacy preservation\"\n        ],\n        \"target_device\": \"Extreme environment devices (quantum-computing compatible)\",\n        \"deployment_endpoint\": \"Quantum/Classical comparison study infrastructure\",\n        \"inference_engine\": [\n            \"Quantum-inspired\",\n            \"Classical machine learning for benchmark comparison\"\n        ],\n        \"demonstration\": \"Quantum-enhanced ophthalmic screening demo\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provides a detailed and complex project involving computer vision, deep learning, and quantum machine learning. The user's expertise is assumed to be high due to the complexity and the requirement to innovate. The metric values and the need to surpass benchmarks indicate a clear goal for performance. The whitepaper, demonstration, and cross-validation study are part of the final steps for the project.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "For the leading AI research and development team, design a cutting-edge text classifier that not only distinguishes complex and subtle variations between human, deep learning, and advanced evolutionary AI outputs, but also classifies them into distinct subdomains like GPT-specific styles and chatbot dialogues. Incorporate innovative techniques like multifaceted anomaly detection, iterative adversarial training with adaptive thresholding, and real-time adaptability to evolving data distributions. Strive for pinnacle F1 scores, ensuring transparency in decision-making, fairness in performance across demographics, and a comprehensive comparative study with the most recent and competitive methodologies. Compose the system within a stringent latency constraint of 100 milliseconds per prediction, while preserving interpretability and maintaining strict resource efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI research and development\",\n        \"description\": \"Design a text classifier for detecting complex and subtle variations between human, deep learning, and evolutionary AI outputs, specifically classifying into GPT-specific styles and chatbot dialogues. Use multifaceted anomaly detection, iterative adversarial training with adaptive thresholding, and real-time adaptability. Aim for pinnacle F1 scores, ensuring transparency, fairness, and a comparative study with recent methods.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency (per prediction)\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Interpretability\",\n                \"value\": \"preserved\",\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"Resource Efficiency\",\n                \"value\": \"strict\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"class labels for human, deep learning, and evolutionary AI outputs\"],\n            \"specification\": {\n                \"specific_data_distribution\": \"unknown\"\n            },\n            \"description\": \"Dataset for classifying subtle variations between human, AI outputs.\",\n            \"preprocessing\": [\"multifaceted anomaly detection\", \"adaptive thresholding\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"state_of_the_art_techniques\": [\"multifaceted anomaly detection\", \"iterative adversarial training\", \"real-time adaptability\"],\n                \"methods_used\": [\"iterative adversarial training\", \"evolving data distribution handling\"]\n            },\n            \"description\": \"A cutting-edge text classifier for advanced AI output differentiation with GPT-specific styles and chatbot dialogues.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporate fairness across demographics\",\n        \"Maintain decision-making transparency\",\n        \"Compare with recent and competitive methodologies\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"comprehensive comparative study with other methods\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction provided a clear requirement for a high-level build task, indicating a high level of expertise. The problem and model requirements are detailed, but some specifics (such as the model name, dataset distribution, and deployment endpoint) are not mentioned, leaving those fields incomplete. The user's knowledge and fairness considerations were taken into account. However, there is room for improvement if more specific details are provided.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project director for an exclusive deep learning research consortium, your task is to develop a domain-specific deep reinforcement learning (DRL) model tailored to the intricate ETTm2 PowerTech dataset. This dataset consists of three specialized subsets: 'precisely recorded historical meteorological variances' (96 hourly measurements of 25 unique weather parameters), 'high-resolution sensor array readings' (10-minute samples of 22 energy-conversion-specific sensor values), and 'comprehensive economic indicators for the renewable energy market' (daily records of 10 indicator fluctuations). The model must be divided into stratified train, validation, and validation-test partitions, with input sequences featuring 2400 distinct timestamp steps for short-term predictions. Your team is mandated to achieve unprecedented precision, targeting NRMSE and MAPE below 1.5% and prioritizing the use of hybrid quantization and pruning strategies for extreme hardware efficiency. To enhance transparency, implement model interpretability techniques integrating Shapley Additive Explanations (SHAP) and real-time forecasting with sub-picosecond latency (<0.001ms) to guarantee seamless operation in a global, highly dynamic renewable energy network. Design a resilient, agile, and modular framework that consistently excels amidst unexpected data anomalies and industry disruptions in the realm of green energy management.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"power tech, renewable energy\",\n        \"description\": \"Develop a domain-specific DRL model for the ETTm2 PowerTech dataset. The dataset includes historical meteorological data, sensor readings, and economic indicators for renewable energy. Aim for NRMSE and MAPE below 1.5%, use hybrid quantization and pruning strategies, and integrate model interpretability techniques like SHAP for transparency. The model must have sub-picosecond latency and be resilient to data anomalies and industry disruptions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"NRMSE\",\n                \"value\": 0.015\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": 0.015\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency (sub-picosecond)\",\n                \"value\": 0.001,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2 PowerTech\",\n            \"modality\": [\n                \"time series\",\n                \"text\"\n            ],\n            \"target_variables\": [\"weather parameters\", \"sensor values\", \"economic indicators\"],\n            \"specification\": {\n                \" subsets\": {\n                    \"meteorological\": {\"hours\": 96, \"parameters\": 25},\n                    \"sensor readings\": {\"samples\": \"10-minute\", \"values\": 22},\n                    \"economic indicators\": {\"daily records\": 10}\n                },\n                \"sequence_length\": 2400\n            },\n            \"description\": \"A dataset with 3 subsets and specific input sequence lengths.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep reinforcement learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"hybrid quantization\": true,\n                \"pruning\": true,\n                \"interpretability (SHAP)\": true\n            },\n            \"description\": \"A model tailored to ETTm2 dataset with specific requirements for precision, hardware efficiency, and real-time forecasting.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"hybrid quantization\", \"pruning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"extremely low latency\"],\n        \"demonstration\": \"model interpretability and real-time forecasting\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a complex task for a highly experienced user, requiring advanced deep reinforcement learning, model interpretability, and stringent performance requirements. The provided JSON accurately captures the scope of the project and specific targets.\",\n        \"score\": 1.0\n    }\n}\n"}
{"task": "community detection", "instruction": "Design and implement a novel, quantum-inspired hybrid machine learning model that combines the principles of quantum walks and ensemble learning for optimizing the Label Propagation algorithm in CiteSeer's citation network. The model should not only enhance bioinformatics community detection by integrating metadata attributes, but also include a fault-tolerant mechanism for handling missing data in real-world computational genomics datasets. Furthermore, devise a bio-inspired metaheuristic algorithm to dynamically adjust the learning rate and neighborhood size in real-time, ensuring adaptability to the ever-evolving citation patterns while minimizing computational overhead and memory footprint.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Design a novel quantum-inspired hybrid machine learning model that combines quantum walks and ensemble learning for optimizing Label Propagation algorithm in CiteSeer's citation network. The model should enhance bioinformatics community detection by integrating metadata attributes and include a fault-tolerant mechanism for handling missing data in computational genomics datasets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"f1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational overhead\",\n                \"value\": null,\n                \"unit\": \"CPU cycles\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CiteSeer's citation network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"community membership\", \"metadata attributes\"],\n            \"specification\": {\n                \"type\": \"citation network\",\n                \"nodes\": \"papers\",\n                \"edges\": \"citations\",\n                \"size\": \"nodes count\",\n                \"dimensionality\": \"metadata attributes\"\n            },\n            \"description\": \"CiteSeer's citation network, focusing on bioinformatics data and metadata attributes\",\n            \"preprocessing\": [\"label propagation\", \"missing data imputation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-inspired Ensemble Learning-based Label Propagation\",\n            \"family\": \"hybrid machine learning\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"ensemble_size\": null,\n                \"quantum_walk_parameters\": null,\n                \"fault_tolerance\": true,\n                \"learning_rate_adaptation\": true,\n                \"neighborhood_size_adaptation\": true\n            },\n            \"description\": \"A model that integrates quantum walks, ensemble learning, and adaptive mechanisms for Label Propagation in CiteSeer's bioinformatics citation network.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilize quantum walk principles to guide the propagation of labels\",\n        \"Employ ensemble diversity to improve community detection\",\n        \"Integrate metadata attributes for enhanced performance\",\n        \"Implement fault tolerance using error mitigation techniques\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"dynamic learning rate adjustment\", \"neighborhood size tuning\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": {\n            \"type\": \"cloud-based platform\"\n        },\n        \"inference_engine\": [\"lightweight or optimized for resource efficiency\"],\n        \"demonstration\": \"Real-time performance monitoring and adaptability demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction specifies a complex project requiring a high level of expertise to design a novel hybrid model, and includes specific metrics and requirements for adaptability and handling missing data. The missing performance metric values suggest further collaboration, and some service details can be assumed or filled in later.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager, now with added urgency, must oversee the deployment of a state-of-the-art multilingual QA system designed for the MLQA dataset. The team must not only employ a highly advanced mBART model, fine-tuned on an extensive repertoire of five languages (English, Spanish, Mandarin, French, and Russian), but also devise novel techniques to address under-researched linguistic nuances. The system must guarantee benchmark accuracy above 95% in every language variant, while conforming to real-time performance constraints and adhering to energy-efficient computing principles.", "resposne": "{\n    \"user\": {\n        \"intent\": \"deploy\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"multilingual\",\n        \"description\": \"The project manager needs to deploy a state-of-the-art multilingual QA system, specifically designed for the MLQA dataset. The system employs an advanced mBART model fine-tuned on English, Spanish, Mandarin, French, and Russian. It aims to achieve benchmark accuracy of at least 95% in each language while ensuring real-time performance and energy-efficient computing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": {\n                    \"min\": 0.95,\n                    \"max\": 0.95,\n                    \"per_language\": true\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": true,\n                \"unit\": \"real-time constraints\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": true,\n                \"unit\": \"energy-efficient computing principles\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MLQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"question\", \"answer\"],\n            \"specification\": {\n                \"languages\": [\"English\", \"Spanish\", \"Mandarin\", \"French\", \"Russian\"]\n            },\n            \"description\": \"A dataset for multilingual question answering tasks covering five languages.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"mBART\",\n            \"family\": \"transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"languages\": [\"English\", \"Spanish\", \"Mandarin\", \"French\", \"Russian\"],\n                \"fine-tuning\": \"extensive\",\n                \"novel_techniques\": true\n            },\n            \"description\": \"A highly advanced mBART model fine-tuned on a diverse set of languages with an emphasis on addressing under-researched linguistic nuances.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or edge\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"state-of-the-art, real-time\"],\n        \"demonstration\": \"explaining language nuances and accuracy across languages\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent to 'deploy' suggests the focus on finalizing and deploying the system. The high expertise level implies a need for detailed technical instructions. The performance metrics emphasize accuracy and real-time capabilities. Energy efficiency is mentioned in the complexity metrics. Some details, like deployment endpoint and exact inference engine, were left unspecified based on the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a specialized healthcare analytics team, your mission has expanded to develop a sophisticated, interpretable time-series forecasting pipeline. The highly diverse ILI dataset, now with 12 nested sub-features (INPUT_SEQ_LEN=36) and intricate weekly and quarterly patterns, requires the design of a hybrid neural architecture that integrates deep learning components, such as long short-term memory (LSTM) units and attention mechanisms. You must preprocess the data to include moving averages, exponential smoothing, and custom Fourier features, while also experimenting with ensemble methods for improved robustness.\n\nThe model must not only predict the subsequent 48-hour trend (PRED_SEQ_LEN=48) with unprecedented precision (targeting <5% relative error), but also conduct real-time adaptability studies for handling sudden shifts in the data. Moreover, you are mandated to create a user-friendly visualization dashboard for non-technical stakeholders, showcasing key performance indicators (KPIs), and allowing for scenario planning based on different intervention scenarios. Your objective is to deliver a state-of-the-art solution that not only surpasses industry benchmarks but also fosters seamless collaboration between data scientists and public health policymakers.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"healthcare analytics\",\n        \"description\": \"As a project manager, develop a sophisticated, interpretable time-series forecasting pipeline using a hybrid neural architecture with LSTM and attention mechanisms for the ILI dataset. The pipeline should handle complex nested sub-features, preprocessing with moving averages, exponential smoothing, and Fourier features, and aim for a <5% relative error in a 48-hour prediction. Include real-time adaptability studies and a user-friendly dashboard for non-technical stakeholders, showcasing KPIs and scenario planning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"relative error\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 36,\n                \"sub-features\": 12\n            },\n            \"description\": \"A highly diverse dataset with nested sub-features, showcasing weekly and quarterly patterns.\",\n            \"preprocessing\": [\n                \"moving averages\",\n                \"exponential smoothing\",\n                \"custom Fourier features\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"hybrid\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"components\": [\"LSTM units\", \"attention mechanisms\"]\n            },\n            \"description\": \"Design a hybrid architecture for time-series forecasting with LSTM and attention mechanisms, addressing the complexity in the ILI dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Ensemble methods for improved robustness\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"moving averages\",\n            \"exponential smoothing\",\n            \"custom Fourier features\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time adaptability\"],\n        \"demonstration\": \"user-friendly visualization dashboard, showcasing KPIs and scenario planning\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been structured according to the schema. The user intent is to build a complex machine learning project, suggesting high expertise is expected. The problem description captures the ILI dataset, hybrid architecture, and specific performance metrics. However, the 'target_device', 'deployment_endpoint', and 'inference_engine' under 'service' could benefit from more specific details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "The project manager is tasked with overseeing the innovative integration of an advanced image recognition system based on the diverse Fashion-MNIST dataset. Demand the ML development team to design a top-tier Capsule Network model that not only aims to excel in classification accuracy beyond a stringent benchmark of 95%, but also demonstrates exceptional resilience to intricate affine transformations. Additionally, mandate a meticulous comparative study between this Capsule Network and cutting-edge Convolutional Neural Networks (CNNs), emphasizing the trade-offs in computational efficiency, interpretability, and the model's practical efficacy in real-world scenarios with limited computational resources and strict privacy regulations. The analysis should include a demonstration of the real-life impact on user experience and data privacy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"The project involves integrating an advanced image recognition system based on the Fashion-MNIST dataset. The team is expected to design a top-tier Capsule Network model targeting high accuracy (above 95%) and resilience to intricate affine transformations. A comparative study with Convolutional Neural Networks (CNNs) on computational efficiency, interpretability, and practical efficacy in limited resource and privacy scenarios is mandatory, with a focus on real-world impact on user experience and data privacy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": [28, 28],\n                \"num_classes\": 10\n            },\n            \"description\": \"A diverse image dataset for fashion product classification\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Capsule Network\",\n            \"family\": \"Capsule Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_goal\": 0.95,\n                \"affine_transformation_resilience\": \"exceptional\"\n            },\n            \"description\": \"A Capsule Network designed for high performance and affine transformation resilience in image recognition\"\n        },\n        {\n            \"name\": \"state-of-the-art CNNs\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A comparative benchmark for computational efficiency, interpretability, and practical use with strict constraints\"\n        }\n    ],\n    \"knowledge\": [\n        \"Privacy regulations and limited computational resources will be a key consideration in the study.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"user_experience\": true,\n            \"data_privacy\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction parsed well into a high-expertise project for building a Capsule Network model and conducting a comparison with CNNs. The specified accuracy benchmark, performance metrics, and real-world considerations are represented. However, some complexities like computational efficiency and interpretability are left open-ended due to the nature of the comparison and the need for specific study.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project director for the innovative AI-driven analytics unit, you now face a multifaceted challenge in which you must engineer an advanced, interpretable, and highly scalable forecasting solution for real-world industrial applications. The massive Electricity dataset, presenting a complex mix of 96 sequential inputs (INPUT_SEQ_LEN=96) and 3,210 unique feature sets (INPUT_DIM=3,210), has been meticulously divided into non-overlapping training, validation, and testing segments. Your mandate is to design and develop a top-tier neural architecture that seamlessly integrates advanced methodologies such as hierarchical seasonality, multi-scale temporal encoding, and real-time anomaly detection, aiming to forecast a 96-step ahead prediction (PRED_SEQ_LEN=96) with 3,210-dimensional outputs (PRED_DIM=3,210), while capturing long-range trends, short-term fluctuations, and unexpected disruptions. The model must optimize for both root mean squared error (RMSE) and mean absolute percentage error (MAPE), demonstrating exceptional predictive prowess, resilience, and transferability across all data partitions. Additionally, ensure compliance with stringent resource efficiency, model explainability, and regulatory requirements, while fostering a culture of continuous innovation and mindful trade-offs between computational complexity and practical business impact in the dynamically evolving energy sector.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"industrial applications, specifically energy sector\",\n        \"description\": \"Engineer an advanced, interpretable, and highly scalable forecasting solution for the Electricity dataset. The solution must address complex data with 96 sequential inputs and 3,210 unique feature sets, divided into training, validation, and testing. The target is to develop a model with hierarchical seasonality, multi-scale temporal encoding, and real-time anomaly detection, aiming to forecast a 96-step ahead prediction with 3,210-dimensional outputs. The model must optimize for RMSE and MAPE, and demonstrate high predictive prowess and transferability. It must adhere to resource efficiency, explainability, and regulatory requirements, while balancing computational complexity with practical impact.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 3210,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 3210\n            },\n            \"description\": \"A massive dataset with 96 sequential inputs and 3,210 unique feature sets, divided into train, validation, and test segments for time-series forecasting tasks.\",\n            \"preprocessing\": [\"hierarchical seasonality\", \"multi-scale temporal encoding\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Neural Architecture\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"complexity\": \"advanced, interpretable, scalable\",\n                \"techniques\": [\"hierarchical seasonality\", \"multi-scale temporal encoding\", \"real-time anomaly detection\"],\n                \"output_dimensions\": 3210,\n                \"prediction_horizon\": 96\n            },\n            \"description\": \"A top-tier model that integrates specified methodologies for electricity forecasting with emphasis on performance and efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Compliance with resource efficiency, explainability, and regulatory requirements\",\n        \"Continuous innovation\",\n        \"Mindful trade-offs between computational complexity and practical business impact\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"hierarchical seasonality\", \"multi-scale temporal encoding\"],\n        \"target_device\": \"massively scalable\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"highlighting predictive prowess, resilience, and transferability across data partitions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project's complexity, involving high-level expertise, advanced modeling techniques, and specific data dimensions. The performance metrics and preprocessing steps align with the requirements. The service section covers the key aspects while acknowledging the dynamic nature of the energy sector. However, deployment endpoint and inference engine details are not provided due to the lack of specific information in the given instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As the project director overseeing the groundbreaking land use classification system, mandate your machine learning development team to design a cutting-edge Vision Transformer (ViT) architecture that combines vast Landsat 8 satellite imagery with sophisticated deep learning fusion techniques. The model must not only demonstrate exceptional transfer learning prowess, consistently achieving a minimum accuracy of 98% across an extensive global database and outperforming current benchmarks by a remarkable 5%, but also exhibit adaptability to various land cover types and climate conditions. \n\nIncorporate a highly resilient, scalable design that not only ensures real-time analysis within an on-premises data center, but also efficiently leverages AWS Lambda, Azure Functions, and Google Cloud's serverless technologies for a hybrid cloud setup. Prioritize low-latency and fault tolerance by implementing a multi-region, active-active architecture that guarantees uninterrupted service during geographical contingencies. \n\nDeliver a meticulously engineered codebase that not only meets functional requirements but also prioritizes resource optimization, showcasing efficient memory management and computational performance. The accompanying documentation should include:\n\n1. Detailed step-by-step guide for deployment and scalability across different cloud environments.\n2. Extensive performance profiling, including precision-recall curves, F1 scores, and AUC-ROC metrics for each geographical region and land cover type.\n3. A sophisticated evaluation framework with periodic testing and adaptation strategies, employing techniques like cross-validation and ensemble learning, for continuous improvement and identifying potential biases.\n\nRemember to outline clear milestones and conduct thorough testing to address potential bias mitigation, ensuring the model's reliability and societal impact is aligned with ethical AI principles.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"land use classification\",\n        \"description\": \"Design a cutting-edge Vision Transformer (ViT) architecture for land use classification using Landsat 8 data. The model should have a minimum accuracy of 98% and outperform benchmarks by 5%, with adaptability to diverse land cover types and climate conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"benchmark comparison\",\n                \"value\": \"improvement of 5%\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time analysis\",\n                \"value\": \"on-premises data center\"\n            },\n            {\n                \"name\": \"cloud scalability\",\n                \"value\": \"AWS Lambda, Azure Functions, Google Cloud serverless\"\n            },\n            {\n                \"name\": \"low-latency and fault tolerance\",\n                \"value\": \"multi-region, active-active\"\n            },\n            {\n                \"name\": \"resource optimization\",\n                \"value\": null\n            }\n        ],\n        \"evaluation\": {\n            \"framework\": [\n                \"deployment guide for different cloud environments\",\n                \"performance profiling with PR curves, F1 scores, AUC-ROC\",\n                \"geographical and land cover type analysis\",\n                \"cross-validation and ensemble learning\"\n            ],\n            \"mitigation\": [\"bias mitigation, ethical AI principles\"]\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Landsat 8 satellite imagery\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"land cover type\"\n            ],\n            \"specification\": null,\n            \"description\": \"Satellite data for land use classification, including extensive global coverage and various climate conditions.\",\n            \"preprocessing\": [\n                \"fusion techniques for deep learning\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Vision Transformer (ViT) architecture\",\n            \"family\": \"transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptability\": \"various land cover types and climate conditions\"\n            },\n            \"description\": \"A state-of-the-art Vision Transformer model for land use classification\"\n        }\n    ],\n    \"knowledge\": [\n        \"multi-cloud setup, real-time analysis, low-latency, fault tolerance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"AWS Lambda, Azure Functions, Google Cloud serverless\"],\n        \"demonstration\": \"codebase with clear deployment steps\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction specified a complex project that requires high-level expertise. The ViT architecture, transfer learning, and performance targets were translated into metrics, and the design of a multi-cloud setup with scalability and low-latency was clearly outlined. However, there could be an assumption about user's familiarity with AWS Lambda, Azure Functions, and Google Cloud. The missing 'target_device' in the 'service' section is meant to clarify where the final model will be deployed.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "Create an AI-driven inventory control system tailored for an opulent gourmet food boutique using an enhanced YOLOv5.1.1 variant, specifically fine-tuned on a meticulously curated \"LuxuryRareFood Niche Datavault\" of 5,000 distinct luxury ingredients. Target a perfect F1-score of 98% with a minimum precision of 97% for recognizing rare delicacies with real-time, real-resolution image analysis (<1 ms). Ensure sub-millisecond response time, adhering to a <0.2 ms latency standard, while accurately differentiating artisanal offerings and factoring in daily seasonal fluctuations and instant, atomic database synchronization through a custom, offline-first architecture.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"inventory control system\",\n        \"description\": \"Create an AI-driven inventory control system for an opulent gourmet food boutique using an enhanced YOLOv5.1.1 variant, fine-tuned on a custom 'LuxuryRareFood Niche Datavault' of 5,000 unique luxury ingredients. The system should aim for perfect F1-score (98%) with a minimum precision of 97% for identifying rare delicacies in real-time, high-resolution image analysis (<1 ms). The system must have sub-millisecond response time (≤0.2 ms) and support atomic database synchronization through an offline-first architecture, accommodating daily seasonal fluctuations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": 0.001,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Latency\",\n                \"value\": 0.2,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LuxuryRareFood Niche Datavault\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": 5000,\n                \"unique_ingredients\": true\n            },\n            \"description\": \"A custom dataset containing 5,000 distinct luxury food ingredients for fine-tuning YOLOv5.1.1 for rare delicacy recognition.\",\n            \"preprocessing\": [\"Fine-tuning on the custom datavault\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced YOLOv5.1.1\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"custom_finetuning\": true,\n                \"architecture\": \"YOLOv5.1.1\"\n            },\n            \"description\": \"A high-performing YOLO variant designed specifically for the gourmet food boutique's inventory control system.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"seasonal_fluctuations\": \"Consider daily fluctuations in demand based on season and trends.\"\n        },\n        {\n            \"offline_first\": \"Implement a custom offline-first architecture for database synchronization.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time analysis\", \"atomically\"], \n        \"target_device\": \"requires real-time processing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"sub-millisecond latency\"],\n        \"demonstration\": \"Support for identifying rare delicacies in real-time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clear, specifying the AI system, task, performance goals, and some technical details like the dataset and model requirements. The expert-level expertise suggests a deeper understanding of the problem. Some areas, like deployment endpoint and detailed feature engineering, require assumptions as they are not provided in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "As a seasoned project manager for an exclusive postgraduate program, guide your team to develop a groundbreaking Visual Turing Test that exclusively utilizes the SAI-360 TextVQA dataset. In addition to incorporating the state-of-the-art M6 model with hybrid architecture, mandate the implementation of context-sensitive attention mechanisms, multi-level feature fusion, and dynamic refinement strategies. Demand a minimum accuracy of 98% on COCO-QA, ensuring not just human-level accuracy but also the ability to interpret and adapt to intricate, real-world scenarios with cultural nuances.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"exclusive postgraduate program, Visual Turing Test\",\n        \"description\": \"Guide a team to develop a groundbreaking Visual Turing Test that uses the SAI-360 TextVQA dataset. The project should incorporate the state-of-the-art M6 model with a hybrid architecture, include context-sensitive attention mechanisms, multi-level feature fusion, and dynamic refinement strategies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98,\n                \"description\": \"Minimum accuracy on COCO-QA, targeting human-level performance and cultural nuances.\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SAI-360 TextVQA\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"Dataset type\": \"Text-based VQA\",\n                \"Source\": \"Exclusively for postgraduate program's Visual Turing Test\"\n            },\n            \"description\": \"A dataset for the Visual Turing Test project, focusing on the SAI-360 TextVQA tasks.\",\n            \"preprocessing\": [\"Using SAI-360 TextVQA format\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6 with hybrid architecture\",\n            \"family\": \"neural networks\",\n            \"type\": \"state-of-the-art\",\n            \"specification\": {\n                \"Attention mechanisms\": \"Context-sensitive\",\n                \"Feature fusion\": \"Multi-level\",\n                \"Refinement strategies\": \"Dynamic\"\n            },\n            \"description\": \"A state-of-the-art model built with a hybrid architecture, designed for the Visual Turing Test.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cultural nuances need to be considered for interpretation and adaptation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explain how the model interprets and adapts to real-world scenarios with cultural nuances\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the goal of building a high-performance Visual Turing Test with specified requirements on the model and dataset. It indicates a high level of expertise of the user. However, there is no specific target device or deployment endpoint, as they are not specified in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Develop a groundbreaking deep learning architecture that fuses quantum-enhanced CNNs (utilizing both Google's SqueezeNet and ResNet50 for their unique speed) with a multi-level attention mechanism. The model must excel in highly specific ImageNet22K classification tasks, targeting 99.7% accuracy within strict constraints of computational efficiency for RISC-V microcontrollers. Additionally, incorporate advanced real-time adversarial resistance techniques, considering hardware resource limitations and ensuring seamless deployment on low-power devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"ImageNet22K classification\",\n        \"description\": \"Develop a groundbreaking deep learning architecture that fuses quantum-enhanced CNNs using Google's SqueezeNet and ResNet50 for improved speed. The model must target 99.7% accuracy while maintaining strict computational efficiency for RISC-V microcontrollers.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.997\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"RISC-V microcontrollers\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ImageNet22K\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A highly specific ImageNet dataset for 22k class classification tasks.\",\n            \"preprocessing\": [\"quantum-enhanced preprocessing\"],\n            \"augmentation\": [\n                \"real-time adversarial resistance techniques\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-enhanced SqueezeNet and ResNet50 fusion\",\n            \"family\": \"Quantum CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed改进\": \"by combining Google's SqueezeNet and ResNet50\",\n                \"resource_usage\": {\n                    \"target_device\": \"RISC-V microcontrollers\",\n                    \"limited_resources\": \"hardware resistance and low-power deployment\"\n                }\n            },\n            \"description\": \"A deep learning model incorporating multi-level attention mechanism for high accuracy in ImageNet22K tasks.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"RISC-V microcontrollers\",\n        \"deployment_endpoint\": \"low-power devices\",\n        \"inference_engine\": [\"leveraging hardware constraints for adversarial resistance\"],\n        \"demonstration\": \"real-time, low-power, and hardware-efficient\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction clearly specifies a complex image classification task with high accuracy requirements, targeted hardware, and adversarial resistance. The user's high expertise level suggests a need for more advanced concepts. The JSON response covers all required fields and fills in the gaps based on the provided information.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager is commissioned to design an innovative, interactive educational Q&A platform for kindergarteners to fifth-graders. Embrace the proprietary EarlyBirds dataset, necessitating the development of a domain-adapted BERT model optimized for educational effectiveness and child-friendly language. Implement an immersive, visually stimulating interface with educational games, all while adhering to cognitive development research and inclusive design guidelines to maximize learning outcomes and maintain a positive learning environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Design an innovative, interactive educational Q&A platform for kindergarteners to fifth-graders. Use the proprietary EarlyBirds dataset and develop a domain-adapted BERT model optimized for educational effectiveness and child-friendly language. Incorporate an immersive, visually stimulating interface and educational games, guided by cognitive development research and inclusive design principles.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"educational effectiveness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"EarlyBirds\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain-adaptation\": true\n            },\n            \"description\": \"Proprietary dataset designed for an educational Q&A platform, featuring child-friendly language.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT (domain-adapted)\",\n            \"family\": \"neural networks\",\n            \"type\": \"transformer-based\",\n            \"specification\": {\n                \"child-friendly\": true,\n                \"educational adaptation\": true\n            },\n            \"description\": \"BERT model tailored for educational Q&A with a focus on child-friendly language and improved educational effectiveness.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cognitive development research for educational design\",\n        \"Inclusive design principles to ensure a positive learning environment\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"child-friendly text simplification\",\n            \"educational game integration\"\n        ],\n        \"target_device\": \"tablet or web-based (for wide accessibility)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time response\"\n        ],\n        \"demonstration\": \"immersive, visually stimulating user interface with adaptive gameplay\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction clearly outlines a complex project requiring development, dataset usage, and model customization. The user is expected to have high expertise. Performance metrics for educational effectiveness are not quantified, so they are left as null. The service section details important implementation aspects, such as the inclusion of cognitive development research and inclusive design, to maximize learning outcomes.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop an advanced, scalable multilingual neural machine translation system that integrates not only the massive TED Talks archive but also considers diverse regional dialects and cultural nuances. Specifically, mandate the use of Transformer-XL for state-of-the-art English to Portuguese translations, while rigorously optimizing for real-time performance on resource-constrained devices like budget smartphones. Ensure energy efficiency, sub-millisecond latency, and minimal impact on battery life and limited storage space, all while maintaining high translation quality across varying network conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual translation, particularly TED Talks archive\",\n        \"description\": \"Create an advanced, scalable multilingual neural machine translation system integrating the massive TED Talks archive. Consider regional dialects and cultural nuances, focusing on English to Portuguese translations using Transformer-XL. Optimize for real-time performance on resource-constrained devices, targeting budget smartphones. Key requirements include energy efficiency, sub-millisecond latency, minimal impact on battery life and storage space, and high translation quality under varying network conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average translation quality\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time performance\",\n                \"value\": {\n                    \"latency\": \"sub-millisecond\",\n                    \"energy efficiency\": true,\n                    \"battery impact\": null\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource consumption\",\n                \"value\": {\n                    \"memory size\": null,\n                    \"energy consumption\": true,\n                    \"storage impact\": true\n                },\n                \"unit\": \"MB or percentage of device constraints\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TED Talks archive (filtered for English and Portuguese)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_size\": null,\n                \"linguistic_diversity\": {\n                    \"English to Portuguese\": true,\n                    \"regional dialects\": true\n                }\n            },\n            \"description\": \"Multilingual corpus from TED Talks, focusing on English and Portuguese, including diverse dialects and cultural nuances.\",\n            \"preprocessing\": [\"data cleaning, tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-XL\",\n                \"target_language\": \"English to Portuguese\",\n                \"target_device\": \"budget smartphones\"\n            },\n            \"description\": \"A state-of-the-art Transformer-XL model for English to Portuguese translation, optimized for resource-constrained devices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Multilingual data handling, dialects, and cultural nuances understanding.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"linguistic adaptation for regional variations\"],\n        \"target_device\": \"budget smartphones\",\n        \"deployment_endpoint\": \"optimized for resource-constrained environments\",\n        \"inference_engine\": [\"optimized real-time inference\"],\n        \"demonstration\": \"account for varying network conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user request contains clear details about the high-level task (model building), desired performance on resource-constrained devices, and specific translation requirement from English to Portuguese using Transformer-XL. Expertise level is inferred to be high. Specifics like detailed latency, energy efficiency, and storage impact are requested but some metrics values like data size and exact performance figures are left unspecified, assuming they might be further clarified once the details of the project are more fleshed out.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for a global AI-based customer support system, you need to develop a highly advanced neural machine translation model. The model should not only bridge language barriers but also adapt to local vernacular, cultural nuances, and evolving slang. In addition to translation, ensure the system can recognize and respond to regional variations in real-time. Mandate integration with multiple APIs for scalability and continuous improvement, while maintaining strict performance benchmarks and data privacy regulations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"global AI-based customer support system\",\n        \"description\": \"As a project manager, you need to develop an advanced neural machine translation model capable of bridging language barriers, adapting to local vernacular, cultural nuances, and evolving slang. The system should real-time recognize and respond to regional variations while integrating multiple APIs for scalability and continuous improvement. Performance benchmarks and data privacy regulations must be strictly adhered to.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"proprietary (likely to contain diverse languages and regional variations)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language diversity\": \"wide\",\n                \"local vernacular inclusion\": true,\n                \"cultural nuances recognition\": true,\n                \"evolving slang adaptation\": true\n            },\n            \"description\": \"A diverse dataset for training the neural machine translation model to capture language, regional, and cultural nuances.\",\n            \"preprocessing\": [\"data cleaning, anonymization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Neural Machine Translation\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time response\": true,\n                \"API integration\": \"multiple\",\n                \"performance benchmarking\": true,\n                \"data privacy measures\": \"compliant\"\n            },\n            \"description\": \"A highly advanced neural machine translation model designed for global AI-based customer support, incorporating real-time regional adaptability and strict privacy regulations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integration with language and culture adaptability modules, multi-API connectivity, performance benchmarking methodologies, and data privacy best practices.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"language and regional profiling\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"AI-powered customer support platform\",\n        \"inference_engine\": [\n            \"real-time adaptability\"\n        ],\n        \"demonstration\": \"customer support scenario-based demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction specifies a complex project requiring high expertise, focusing on developing a neural machine translation model with specific requirements. The 'expertise' field is assumed to be high based on the project scope. The lack of specific performance metrics in the 'problem' section suggests they may be determined during the development process. The 'service' section is generic but comprehensive, capturing the key aspects of a global AI customer support system.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "machine translation", "instruction": "As the project director, your mandate involves the intricate design and implementation of a state-of-the-art BERT-based neural machine translation model, leveraging the immense and varied Web Colossal Clean Crawled (WCCC) dataset. The system should excel in rendering seamless translations between British English and dialect-laden Japanese, transcending informal jargon, colloquialisms, and regional slang, with a high standard set at 98% confluence accuracy. In addition, devise a sophisticated performance monitoring framework and explore marginalized language adaptation techniques to continually boost translation prowess in under-researched varieties of the language.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Design and implement a state-of-the-art BERT-based neural machine translation model for British English to dialect-laden Japanese, focusing on informal language and regional variations. Aim for at least 98% confluence accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"confluence accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Web Colossal Clean Crawled (WCCC)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translations\"],\n            \"specification\": {\n                \"source_language\": \"British English\",\n                \"target_language\": \"Japanese\",\n                \"genre_diversity\": \"immense and varied, including informal jargon, colloquialisms, and regional slang\"\n            },\n            \"description\": \"A large-scale web dataset for training a neural machine translation model, focusing on British English to dialect-laden Japanese.\",\n            \"preprocessing\": [\"BERT tokenization\", \"data cleaning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-based Neural Machine Translation Model\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer architecture with BERT encoder and decoder\"\n            },\n            \"description\": \"A model designed for seamless translations between British English and dialect-laden Japanese, leveraging BERT for cross-lingual understanding.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Explore marginalized language adaptation techniques for under-researched varieties of the language to improve translation prowess.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"performance monitoring framework\", \"language adaptation techniques\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Translation demonstrations with focus on marginalized language varieties\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instructions indicate a high-level project with a detailed requirement for a BERT-based model and a focus on specific language nuances. The high expertise level of the user suggests an understanding of advanced techniques and requirements. However, some assumptions are made for sections like deployment endpoint, inference engine, and exact demonstration details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "As a project manager for the machine learning development team, devise a GraphSAGE model for the PubMed dataset that necessitates not only advanced benchmark performance, surpassing 95% accuracy in node classification, but also demonstrates exceptional domain adaptation for dynamic knowledge domains. Mandate seamless transfer learning across diverse tasks and ensure the model's real-world deployability with ultra-low latency (ideally below 5ms) and memory efficiency, targeting platforms with limited resources (CPU <1 GHz and GPU < 256MB). Additionally, design the model to optimize for distributed deployment across edge computing, minimizing communication overhead and maximizing data privacy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"scientific literature (PubMed)\",\n        \"description\": \"The project aims to devise a GraphSAGE model for the PubMed dataset, targeting advanced benchmark performance (95%+ accuracy), domain adaptation for dynamic knowledge domains, seamless transfer learning, ultra-low latency (5ms or less), memory efficiency for resource-constrained platforms, and distributed deployment with edge computing for data privacy and reduced communication overhead.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 5,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"memory efficiency\",\n                \"value\": null,\n                \"unit\": \"unspecified\"\n            },\n            {\n                \"name\": \"communication overhead\",\n                \"value\": null,\n                \"unit\": \"unspecified\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PubMed\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"nodes\"],\n            \"specification\": {\n                \"node_size\": \"\",\n                \"edge_type\": \"\",\n                \"attributes\": \"\"\n            },\n            \"description\": \"A large-scale dataset for node classification task in scientific literature, representing a dynamic knowledge domain.\",\n            \"preprocessing\": [\"domain adaptation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GraphSAGE\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"advanced with domain adaptation\",\n                \"resource constraints\": {\n                    \"CPU\": \"<1 GHz\",\n                    \"GPU\": \"< 256 MB\"\n                }\n            },\n            \"description\": \"A GraphSAGE model designed to surpass benchmark performance, ensure transfer learning and real-world deployability for PubMed data with low latency and memory efficiency.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": [\"edge computing\", \"limited resource platforms\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for distributed deployment\"],\n        \"demonstration\": \"optimized for data privacy during model deployment\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction required a high-level expert for a GraphSAGE model targeting multiple specific performance, deployment, and resource constraints. The 'service' section is speculative as some details (like deployment endpoint and precise feature engineering) were not provided. The confidence score is moderate given the completeness of the instruction and the scope of the task.\",\n        \"score\": 0.8\n    }\n}"}
{"task": "time-series forecasting", "instruction": "Instruct the machine learning team to design a cutting-edge solar energy forecasting system for the science fair. They must analyze hourly data from the past three months, integrating real-time weather data, seasonal adjustments, and predictive maintenance schedules. Challenge them to employ advanced algorithms (e.g., ARIMA, LSTM) and apply 10-fold nested cross-validation for model validation. The report should include a complex, visual demonstration of the forecasting process and a rigorous evaluation of its performance metrics, such as root mean squared error and precision. Emphasize the need for interpretability and model robustness in the final submission.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"solar energy\",\n        \"description\": \"Design a cutting-edge solar energy forecasting system for a science fair. The system should analyze hourly data from the past three months, integrate real-time weather data, seasonal adjustments, and predictive maintenance schedules. Employ advanced algorithms like ARIMA and LSTM, and use 10-fold nested cross-validation for model validation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model interpretability\",\n                \"value\": null,\n                \"unit\": \"high\"\n            },\n            {\n                \"name\": \"model robustness\",\n                \"value\": null,\n                \"unit\": \"high\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [\"solar energy production\", \"weather data\", \"maintenance schedules\"],\n            \"specification\": {\n                \"data_period\": \"past three months\",\n                \"data_frequency\": \"hourly\"\n            },\n            \"description\": \"Hourly data from the past three months, including solar energy production, real-time weather data, and predictive maintenance schedules.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"feature engineering (weather data integration, seasonal adjustments)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"forecasting process demonstration\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ARIMA and LSTM\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"Advanced algorithms, including ARIMA and LSTM, for solar energy forecasting with emphasis on interpretability and robustness.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Employ 10-fold nested cross-validation for model validation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"interpretable feature selection\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"rigorous forecasting performance evaluation and visual demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction specifies a complex project with high requirements for performance, data integration, and model selection. The user's expertise level is assumed to be high due to the project's complexity. Performance metrics are not given a specific value but should be calculated based on the model's performance. The need for interpretability and robustness makes it a high priority. Missing details, such as the target device and deployment endpoint, can be inferred as part of the project's implementation phase or based on the team's chosen strategy.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager's goal for the upcoming ML endeavor is to leverage the specialized WMT19 Unified Machine Translation Challenge dataset, focusing on the rare dialect of Russian from the 19th-century literature to English. The project calls for the development of a cutting-edge M2M model, with a transformer architecture featuring iterative refinement, embedded code-switching mechanisms, and adaptive subword tokenization. Emphasize the integration of domain-specific named entity recognition and cross-lingual transfer learning, while striving for a state-of-the-art BLEU score of 48 or above. Additionally, the implementation must exhibit superior efficiency by optimizing memory footprint and runtime for a linguistics research domain. Provide a comprehensive evaluation report highlighting benchmark comparisons and performance on out-of-domain texts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics and 19th-century Russian literature\",\n        \"description\": \"The goal is to develop a cutting-edge M2M model using the WMT19 Unified Machine Translation Challenge dataset for rare dialect Russian to English translation. The model should have a transformer architecture with iterative refinement, code-switching, adaptive subword tokenization, named entity recognition, and cross-lingual transfer learning. The target is to achieve a BLEU score of 48 or above while optimizing efficiency for a linguistics research environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 48\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"runtime\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 Unified Machine Translation Challenge\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": \"Russian (19th-century dialect) to English\",\n                \"specific_dataset\": \"Translation Challenge\"\n            },\n            \"description\": \"A dataset for specialized Russian to English translation, focusing on the 19th-century dialect.\",\n            \"preprocessing\": [\n                \"domain-specific cleaning\",\n                \"alignment\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M2M Transformer\",\n            \"family\": \"Transformer architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"iterative refinement, code-switching, adaptive subword tokenization\",\n                \"named entity recognition\": \"embedded\",\n                \"cross-lingual transfer learning\": \"enabled\"\n            },\n            \"description\": \"A cutting-edge model designed for rare dialect Russian to English translation\"\n        }\n    ],\n    \"knowledge\": [\n        \"Named entity recognition integration\",\n        \"Cross-lingual transfer learning techniques\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain adaptation\",\n            \"multi-task learning\"\n        ],\n        \"target_device\": \"linguistics research environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for efficiency\"],\n        \"demonstration\": \"benchmark comparisons and out-of-domain performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction covers most aspects of the project, including model type, dataset, performance metrics, and efficiency targets. However, specific values for memory footprint and runtime optimization are not mentioned, and they might need to be estimated based on the model complexity and implementation details. The 'expertise' level of 'high' implies that the user is familiar with the advanced requirements of the project.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "object detection", "instruction": "For a retail environment with constant product turnover and SKU expansion, enhance the YOLO model to identify 150+ distinct items in \"StoreShelf Dataset v3.0\". Mandate >97% accuracy and >96% recall, while maintaining a maximum processing time of 1.3 seconds per image to ensure real-time analysis. The model must dynamically adapt to seasonal fluctuations, handle new SKUs, and seamlessly integrate with a sophisticated inventory system that necessitates minimal manual intervention for stock adjustments based on accurate, high-speed shelf scans.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"The user aims to enhance the YOLO model for a retail environment with high product turnover and SKU expansion. The focus is on the 'StoreShelf Dataset v3.0', aiming for >97% accuracy and >96% recall while ensuring real-time analysis with a 1.3-second processing time per image.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.96\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"processing time\",\n                \"value\": 1.3,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"StoreShelf Dataset v3.0\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"distinct_items\": 150,\n                \"seasonal_adaptation\": true,\n                \"SKU_management\": true\n            },\n            \"description\": \"A dataset designed for object detection in a retail environment with extensive SKU diversity and dynamic product demands.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptability\": \"seasonal fluctuations and SKU expansion\",\n                \"processing_speed\": 1.3,\n                \"manual_intervention\": \"minimal\"\n            },\n            \"description\": \"An enhanced YOLO model capable of identifying 150+ distinct items on store shelves with real-time analysis and minimal stock adjustment requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"seasonal fluctuations and SKU management requirements\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or edge device\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for real-time performance\"\n        ],\n        \"demonstration\": {\n            \"scenario\": \"inventory adjustment based on accurate shelf scans\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers the user's intent, task specifics, performance requirements, and adapting to real-world challenges. I assumed the expert level to be high based on the need for accuracy and complexity control. However, the target device for deployment and specific inference engine were not explicitly mentioned, so these are left open-ended. Confidence score is high given the information provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "Develop a cutting-edge AI-driven inventory management system for a select luxury gourmet boutique, mandating an exclusive YOLOv5.1 mutation optimized for the \"ExquisiteGourmetProductDataset\" of 1,500 rare and premium culinary items. Target a stringent F1-score of 95% with minimal acceptable product recognition error rate of 0.7%, while maintaining real-time processing at sub-millisecond speed (1.1 milliseconds) for rapid detection of distinctive artisan delicacies. The system must ensure seamless, high-precision synchronization with the inventory database, and conduct advanced predictive analytics for demand forecasting.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury gourmet boutique\",\n        \"description\": \"Develop a cutting-edge AI-driven inventory management system for a luxury gourmet boutique using an exclusive YOLOv5.1 mutation optimized for the ExquisiteGourmetProductDataset with 1,500 rare and premium culinary items.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"product recognition error rate\",\n                \"value\": 0.007\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 1.1,\n                \"unit\": \"milliseconds\",\n                \"reason\": \"Real-time processing at sub-millisecond speed for rapid detection\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ExquisiteGourmetProductDataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"culinary items\"],\n            \"specification\": {\n                \"size\": 1500,\n                \"unique items\": 1500,\n                \"rarity\": \"premium\",\n                \"types\": \"culinary\"\n            },\n            \"description\": \"Dataset containing 1,500 rare and premium culinary items for YOLOv5.1 optimization\",\n            \"preprocessing\": [\"YOLOv5.1 dataset adaptation\"],\n            \"augmentation\": [\"exclusive luxury gourmet boutique augmentations\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"exclusive YOLOv5.1 mutation\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"YOLOv5.1\",\n                \"optimization\": \"custom mutation for luxury gourmet boutique\"\n            },\n            \"description\": \"A YOLOv5.1 model optimized for detecting unique artisan delicacies\"\n        }\n    ],\n    \"knowledge\": [\n        \"leveraging advanced predictive analytics for demand forecasting\",\n        \"seamless synchronization with inventory database for real-time updates\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance or embedded for real-time processing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for sub-millisecond processing\"],\n        \"demonstration\": \"high-precision and real-time demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction has been parsed to represent a complex inventory management system with detailed performance, speed, and model requirements. The user's high expertise suggests they understand the technical aspects, though some areas like the deployment endpoint and exact feature engineering details are open-ended. The confidence score is high due to the clarity of the tasks and metrics.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create a multifaceted on-device neural machine translation solution that integrates TEDx Talks corpus for real-time Marathi to Konkani conversion. The project must exclusively utilize the proprietary Enhanced-EfficientTransformer-XL model, demonstrating an innovative balance between compute efficiency and performance. Mandate a minimum of 25% energy efficiency on Raspberry Pi Zero, with a stringent memory requirement of under 5MB, and strive for CPU utilization below 10% to guarantee uninterrupted, long-lasting translation on minimalist handheld devices, taking into account their limited battery capacity and computational constraints. Additionally, devise a comprehensive optimization strategy and benchmarking plan to validate the efficiency improvements throughout the development process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual, real-time\",\n        \"description\": \"Create a multifaceted on-device neural machine translation solution for real-time Marathi to Konkani conversion using the TEDx Talks corpus. The project must exclusively use the Enhanced-EfficientTransformer-XL model, optimizing compute efficiency and performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": 0.25\n            },\n            {\n                \"name\": \"CPU utilization\",\n                \"value\": 0.1\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory requirement\",\n                \"value\": 5,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TEDx Talks corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A real-time corpus for Marathi to Konkani translation, focusing on content from TEDx Talks.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced-EfficientTransformer-XL\",\n            \"family\": \"EfficientTransformer-XL\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A proprietary model that integrates compute efficiency with high performance, suitable for on-device translation.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"energy-efficient optimization\",\n            \"memory reduction strategies\",\n            \"CPU utilization control\"\n        ],\n        \"target_device\": \"Raspberry Pi Zero\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"in-device\", \"on minimalist handheld devices\"],\n        \"demonstration\": \"long-lasting and uninterrupted translation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON reflects the user's intent to build a machine translation project, using the Enhanced-EfficientTransformer-XL model. The user's high expertise level suggests a detailed understanding of requirements. Performance and complexity metrics are clearly defined, as are the constraints for Raspberry Pi Zero. An optimization strategy and benchmarking plan are included. However, the deployment endpoint and specific inference engine details are missing from the instruction, which could be assumed or clarified.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a cutting-edge project manager for a state-of-the-art machine learning team, your directive is to design a highly efficient time-series classification model. The intricate Ethanol Concentration dataset, composed of extensive time-series data with 1751 timestamps and 30 complex features, presents not only temporal dependencies but also subtle anomalies (add Poisson noise component) that challenge standardization. The data has been meticulously stratified into train, validation, and test sets with varying sequence lengths. Mandate a model showcasing exceptional accuracy, generalization across diverse subgroups, and robustness against both inliers and outliers. Incorporate advanced regularization methods like dropout and data augmentation, and conduct a rigorous evaluation using not only AUC-ROC but also F1-score, Cohen's kappa, and macro-average precision, ensuring an in-depth analysis of model performance in a resource-constrained environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"chemical analysis\",\n        \"description\": \"Design a highly efficient time-series classification model for the Ethanol Concentration dataset, with 1751 timestamps and 30 complex features. The data includes temporal dependencies and Poisson noise-induced anomalies. The dataset is stratified into train, validation, and test sets with variable sequence lengths. The model should exhibit high accuracy, generalization across diverse subgroups, and robustness to outliers. Regularization methods like dropout and data augmentation are required, and performance must be evaluated using AUC-ROC, F1-score, Cohen's kappa, and macro-average precision.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Cohen's kappa\",\n                \"value\": null\n            },\n            {\n                \"name\": \"macro-average precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ethanol Concentration\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"timestamp_count\": 1751,\n                \"features_count\": 30,\n                \"anomalies\": \"Poisson noise\"\n            },\n            \"description\": \"A time-series dataset with 1751 samples, 30 complex features, and Poisson noise components for classification tasks. Stratified for train, validation, and test sets.\",\n            \"preprocessing\": [\"data normalization\", \"anomaly detection\"],\n            \"augmentation\": [\"dropout regularization\", \"time-series data augmentation\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Efficient Time-Series Classifier\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced_regularization\": [\"dropout\", \"data augmentation\"]\n            },\n            \"description\": \"A high-efficiency time-series classification model designed to handle complex data with temporal dependencies, anomalies, and varying sequence lengths.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Dataset has Poisson noise anomalies, so might need custom noise handling techniques.\",\n        \"In a resource-constrained environment, focus on model complexity and efficiency.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"handling time-series data\", \"extracting meaningful features\"],\n        \"target_device\": \"resource-constrained environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for efficiency\"],\n        \"demonstration\": \"outlier detection visualizations and performance summary\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is parsed based on a high-level project directive, mentioning a state-of-the-art team and specific requirements for the model. I assumed high expertise as it deals with intricate data and advanced techniques. The model and performance metrics are outlined, but exact values will depend on a deeper understanding of the data. Some gaps include missing specific regularization methods and exact resource constraints for feature engineering.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "Instruct the machine learning development team to design a novel deep learning architecture combining multiple CNN modules for fine-grained classification in the highly competitive CIFAR-100 dataset, demanding a state-of-the-art accuracy of 99.5% and incorporating advanced data augmentation strategies, such as spatial and channel transformations, while maintaining computational efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"highly competitive domains\",\n        \"description\": \"Design a novel deep learning architecture combining multiple CNN modules for fine-grained classification in the CIFAR-100 dataset. Aim for a state-of-the-art accuracy of 99.5% and incorporate advanced data augmentation techniques like spatial and channel transformations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CIFAR-100\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": 32,\n                \"number_of_classes\": 100\n            },\n            \"description\": \"A highly competitive dataset for image classification requiring fine-grained distinctions.\",\n            \"preprocessing\": [\n                \"advanced data augmentation (spatial, channel transformations)\"\n            ],\n            \"augmentation\": [\n                \"spatial transformations (e.g., cropping, flipping)\",\n                \"channel transformations (e.g., brightness, contrast)\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"novel deep learning architecture\",\n            \"family\": \"custom CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"combining multiple CNN modules\",\n                \"efficiency\": \"maintaining computational efficiency\"\n            },\n            \"description\": \"A custom deep CNN model specifically tailored for CIFAR-100 with a focus on high accuracy and efficient computations.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a new model, targeting high accuracy for image classification in the CIFAR-100 dataset. The 'expertise' field suggests the user may have a higher level of knowledge in AI and ML. The problem description and dataset specifications are clear, including the requirement for advanced data augmentation and computational efficiency. The model section describes the desired architecture and its characteristics. The confidence score is high, considering the detailed instruction and lack of ambiguity.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "Advise the project manager to design a cutting-edge, fault-tolerant question-answering system using the TriviaQA dataset, incorporating the latest version of M6 transformers with dynamic pruning for resource optimization. The system must not only handle complex, multi-layered queries, including logical and metaphorical language, but also integrate a continuous learning module to stay up-to-date with niche domain knowledge. Ensure real-time performance surpasses 99th percentile in speed, with an average latency of less than 50 milliseconds to handle massive concurrent users without compromise on accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"general\",\n        \"description\": \"Advise on designing a cutting-edge question-answering system using the TriviaQA dataset. The system should employ the latest M6 transformers with dynamic pruning for resource optimization. It must handle complex queries, support logical and metaphorical language, and incorporate a continuous learning module for domain adaptation. Aim for real-time performance with a 99th percentile speed and average latency below 50 milliseconds for handling massive concurrent users without compromising accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"speed\",\n                \"value\": 0.99,\n                \"unit\": \"percentile\"\n            },\n            {\n                \"name\": \"average latency\",\n                \"value\": 0.05,\n                \"unit\": \"seconds\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TriviaQA\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A dataset for developing the cutting-edge question-answering system, focusing on complex and multi-layered queries.\",\n            \"preprocessing\": [\n                \"dynamic pruning\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6 transformers with dynamic pruning\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"latest_version\": true,\n                \"resource_optimization\": \"dynamic pruning\"\n            },\n            \"description\": \"State-of-the-art M6 model with enhancements for fault tolerance and resource efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporate continuous learning module for domain adaptation and niche knowledge accumulation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"real-time processing\",\n            \"massive concurrency support\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for real-time performance\"\n        ],\n        \"demonstration\": \"99th percentile speed, latency < 50ms\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a consult-build scenario, with a high level of expertise expected. The problem description is detailed, providing specific requirements on the dataset (TriviaQA), model (M6 transformers with dynamic pruning), performance metrics, and a focus on handling massive concurrent users. The missing deployment endpoint should be filled based on the organization's infrastructure. The provided information is deemed to be complete and valid for the task at hand.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a cutting-edge, edge-computing Marathi to Konkani translation system tailored for TEDx Talks, leveraging a novel Adaptive EfficientTransformer-XL. The model must achieve real-time performance, <50ms latency, and comply with stringent energy efficiency standards (3uW power consumption) for Raspberry Pi Zero. Moreover, design it to utilize <5MB memory for seamless translation on resource-constrained devices, guaranteeing >95% battery life during multi-hour translation sessions without compromising on accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"TEDx Talks translation\",\n        \"description\": \"Develop a cutting-edge Marathi to Konkani translation system optimized for edge computing, specifically for TEDx Talks. The system should use a novel Adaptive EfficientTransformer-XL. It must meet real-time performance (latency < 50ms) and energy efficiency (3uW power consumption) on Raspberry Pi Zero. Additionally, it should require less than 5MB memory and guarantee >95% battery life during multi-hour translation sessions without sacrificing accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 50\n            },\n            {\n                \"name\": \"power_consumption\",\n                \"value\": 3e-6\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory_size\",\n                \"value\": 5e6,\n                \"unit\": \"bytes\"\n            },\n            {\n                \"name\": \"battery_life\",\n                \"value\": 95,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\", \"audio\"], \n            \"target_variables\": [],\n            \"specification\": {\n                \"source_language\": \"Marathi\",\n                \"target_language\": \"Konkani\",\n                \"specific_domain\": \"TEDx Talks\"\n            },\n            \"description\": \"Data for Marathi to Konkani translation, focusing on TEDx Talks content.\",\n            \"preprocessing\": [\"cleaning, tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Adaptive EfficientTransformer-XL\",\n            \"family\": \"EfficientTransformer-XL\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"train_speed\": \"\",\n                \"inference_time\": \"\",\n                \"memory_usage\": \"\",\n                \"energy_consumption\": \"3uW (on Raspberry Pi Zero)\"\n            },\n            \"description\": \"A novel deep learning model for Marathi to Konkani translation, optimized for edge computing and TEDx Talks.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"adapt for edge computing\"],\n        \"target_device\": \"Raspberry Pi Zero\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"lightweight, low-latency\"],\n        \"demonstration\": \"Multi-hour translation sessions with minimal battery impact\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided information has been parsed into a JSON format, focusing on the machine learning task of building a translation system, target audience, and specific requirements such as real-time performance, low power consumption, and memory usage. The user's high expertise level suggests they expect detailed guidance. However, the model name was not specified explicitly and the user probably needs some suggestions for a model that matches the specified criteria.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "You are tasked with constructing a highly advanced deep learning forecasting model for real-world Weather dataset analysis. The model must excel in minimizing both mean squared error (MSE) and mean absolute percentage error (MAPE), demonstrating exceptional accuracy and robustness against temporal fluctuations. Implement an intricate rolling window validation strategy to capture short-term patterns, and account for seasonality and long-term trends. Utilize a challenging hybrid optimization approach, combining Bayesian search with swarm intelligence techniques, to fine-tune hyperparameters with efficiency. Ensure the architecture is designed for deployment on minuscule devices with strict constraints on computational resources, while maintaining near real-time performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"weather data analysis\",\n        \"description\": \"The task involves constructing a highly advanced deep learning forecasting model for real-world weather dataset analysis, focusing on accuracy (MSE and MAPE minimization), short-term patterns via rolling window validation, seasonality, and long-term trends. The model should be deployed on resource-constrained devices for near-real-time performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Real-world Weather Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"length\": null,\n                \"dimensions\": null\n            },\n            \"description\": \"A dataset for weather forecasting analysis, including real-world temporal data with varying features.\",\n            \"preprocessing\": [\"seasonality and trend removal\", \"rolling window creation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep learning forecasting model\",\n                \"resource_constraints\": \"minuscule devices, strict computational resource constraints\"\n            },\n            \"description\": \"An advanced model for weather data analysis with a focus on efficiency, utilizing Bayesian search and swarm intelligence for hyperparameter optimization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Implement a rolling window validation strategy for short-term pattern identification\",\n        \"Utilize hybrid optimization approach (Bayesian search and swarm intelligence) for hyperparameter tuning\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"rolling window strategy\", \"seasonality and trend handling\"],\n        \"target_device\": \"minuscule devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Bayesian search\", \"swarm intelligence techniques\"],\n        \"demonstration\": \"near real-time performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was generated based on the given instruction, capturing the essential details on the user's intent, expertise level, problem description, performance metrics, complexity, dataset specifics, model requirements, and deployment details. Some exact values such as model architecture and hyperparameters are not specified since they are not provided in the instruction. The confidence score is high due to the comprehensive structure of the information.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image classification", "instruction": "As the project lead for the elite machine learning development team, you are now mandated to design a highly advanced diabetic retinopathy classification system using the comprehensive Kaggle dataset. The project necessitates a next-generation modification of the VGG-16 architecture, capable of discerning not only between normative and pathological eye conditions but also conducting a granular severity classification into a five-stage taxonomy. The classifier must meet rigorous performance standards, with a targeted accuracy of 98%, while significantly minimizing false negatives to minimize critical diagnostic errors. Furthermore, focus on optimizing the model for real-world deployment in resource-restricted ophthalmic clinics, balancing speed, accuracy, and computational efficiency within stringent latency requirements. Develop a step-by-step technical roadmap, a code blueprint, conduct exhaustive performance testing, and present a thorough comparative analysis with state-of-the-art solutions to showcase the model's superiority and practical applicability in a resource-constrained environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"ophthalmology\",\n        \"description\": \"Design a highly advanced diabetic retinopathy classification system using the Kaggle dataset. Modify the VGG-16 architecture for granular severity classification into a five-stage taxonomy, targeting an accuracy of 98% and minimizing false negatives. Optimize for deployment in resource-restricted ophthalmic clinics with low latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": \"strict\",\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Kaggle Diabetic Retinopathy dataset\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"Diabetic Retinopathy severity\"\n            ],\n            \"specification\": null,\n            \"description\": \"A comprehensive dataset for diabetic retinopathy classification, featuring multiple eye images.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Modified VGG-16\",\n            \"family\": \"VGG-16\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"customization\": \"next-generation, granular severity classification\",\n                \"params\": {\n                    \"accuracy_target\": 0.98,\n                    \"false_negative_reduction\": true\n                }\n            },\n            \"description\": \"A state-of-the-art image classification model modified from VGG-16 for diabetic retinopathy with rigorous optimization for deployment and low latency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Resource-constrained ophthalmic clinics, low-latency requirements\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"accuracy-optimized model architecture\",\n            \"efficient data preprocessing techniques\"\n        ],\n        \"target_device\": \"resource-restricted\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for latency\"],\n        \"demonstration\": \"comparison with state-of-the-art models\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been parsed into a JSON format, including high-level user intent, the high complexity of the task, and specific requirements such as accuracy and latency optimization. The VGG-16 modification and granular severity classification, as well as the roadmap, code blueprint, and comparative analysis, are reflected in the service and knowledge sections. However, some deployment details are not specified and assumed to be within the project lead's expertise.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "For Tencent Weibo's advanced microblogging platform, develop a state-of-the-art AGCN model with temporal-slice fusion and context-sensitive attention mechanisms. Mandate the model to identify not only fine-grained user interactions but also dynamically adjust weights to account for temporal variations, ensuring exceptional link prediction accuracy in real-world, high-velocity social network scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"social network analysis, advanced microblogging\",\n        \"description\": \"Develop a state-of-the-art AGCN model with temporal-slice fusion and context-sensitive attention mechanisms for Tencent Weibo's advanced microblogging platform. The model should be capable of identifying fine-grained user interactions and dynamically adjust weights for temporal variations to achieve exceptional link prediction accuracy in high-velocity social network scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"link prediction accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tencent Weibo's interaction data\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"link existence\"],\n            \"specification\": {\n                \"temporal_data\": true,\n                \"velocity_range\": \"high\"\n            },\n            \"description\": \"Real-world high-velocity social network data with user interactions and temporal information.\",\n            \"preprocessing\": [\"temporal-slice fusion\"],\n            \"augmentation\": [\"context-sensitive attention mechanism\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Graph Convolutional Network (AGCN)\",\n            \"family\": \"Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"temporal_slice_fusion\": true,\n                \"context_attention\": true\n            },\n            \"description\": \"A state-of-the-art model designed for link prediction in high-velocity Tencent Weibo data, featuring temporal-slice fusion and context-sensitive attention.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"platform-specific (e.g., Tencent Weibo API or edge server)\",\n        \"inference_engine\": [\"optimized for high-velocity scenarios\"],\n        \"demonstration\": {\n            \"evalutation_type\": \"real-time performance showcasing on high-velocity link prediction tasks\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a complex machine learning project, which justifies the 'high' expertise level. The task is clearly a build (model development) with specific requirements for AGCN, temporal fusion, and context-sensitive attention. Performance metrics focus on link prediction accuracy, acknowledging the need for real-world, high-velocity scenarios. Details like deployment and inference engines were not explicitly stated, but can be inferred based on the context.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a maritime security project manager, you are mandated to develop an enhanced Mask R-CNN model for our novel OVC system. The model must be fine-tuned on an extensive, proprietary dataset that includes not only a taxonomy of 50 distinct ship subclasses but also considers intricate details like tonnage, speed, and advanced AI-enhanced feature extraction for real-time scenarios. The system must not only surpass a precision benchmark of 98% but also exhibit resilience in recognizing vessels through dense fog, waves, and complex AIS signals. Additionally, optimize the model for near real-time performance with a latency target of less than 150 milliseconds on state-of-the-art cloud GPU infrastructure, all while ensuring dynamic resource allocation for concurrent maritime surveillance and data analytics tasks without impacting system throughput.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"maritime security\",\n        \"description\": \"A maritime security project manager needs to develop an enhanced Mask R-CNN model for a novel OVC system. The model must be fine-tuned on a proprietary dataset with 50 distinct ship subclasses, considering intricate details like tonnage and speed, and incorporate advanced AI-enhanced feature extraction for real-time scenarios. The system must achieve precision above 98% and have near real-time performance with a latency target of less than 150 milliseconds on cloud GPU infrastructure, ensuring dynamic resource allocation for concurrent surveillance and data analytics tasks without performance degradation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 150,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Proprietary Maritime Dataset\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"ship subclasses\",\n                \"tonnage\",\n                \"speed\"\n            ],\n            \"specification\": {\n                \"ship_subclasses\": 50,\n                \"advanced_feature_extraction\": \"AI-enhanced\"\n            },\n            \"description\": \"A proprietary dataset for maritime object detection, containing images of ships with subclass, tonnage, and speed information.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Mask R-CNN\",\n            \"family\": \"Mask R-CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"customization\": \"fine-tuned for real-time scenarios and maritime conditions\",\n                \"resource_efficiency\": \"optimized for near real-time performance with low latency\"\n            },\n            \"description\": \"An advanced model for maritime object detection, designed to meet the specified requirements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud GPU infrastructure\",\n        \"deployment_endpoint\": null, \n        \"inference_engine\": [\"state-of-the-art\"],\n        \"demonstration\": \"dynamic resource allocation for concurrent maritime surveillance and data analytics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction parsed into a JSON response assuming a high level of expertise, focusing on the object detection task in the maritime domain. The model's requirements, dataset specifics, and performance metrics are clearly specified. However, specific deployment endpoints and inference engines are not provided but can be inferred as part of cloud GPU infrastructure and state-of-the-art systems.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "With stringent real-time constraints, examine the nuances of deploying cutting-edge multi-modal transformer models for detecting authentic and manipulated videos in the extensive Celeb-DF collection. Mandate a comparison with a highly optimized baseline not solely relying on chance, but also evaluate the models' robustness to variations in resolution and compression.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"video analysis\",\n        \"description\": \"With stringent real-time constraints, explore the deployment of state-of-the-art multi-modal transformer models for authenticity detection in the extensive Celeb-DF collection. Compare the models with a highly-optimized, non-random baseline, and assess their robustness against variations in resolution and compression.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\"image\", \"audio\", \"video\"],\n            \"target_variables\": [\"authentic\", \"manipulated\"],\n            \"specification\": {\n                \"collection_size\": \"extensive\",\n                \"variable_changes\": [\"resolution\", \"compression\"]\n            },\n            \"description\": \"A large dataset specifically for multimodal forgery detection, including authentic and manipulated videos from the Celeb-DF collection.\",\n            \"preprocessing\": [\"real-time constraints\", \"multi-modal data preprocessing\"],\n            \"augmentation\": [\"data augmentation for resolution and compression variability\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"multi-modal transformer\",\n            \"family\": \"transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time inference\": true,\n                \"optimization level\": \"cutting-edge\"\n            },\n            \"description\": \"Deploying a sophisticated multi-modal transformer model for video authenticity detection in the Celeb-DF dataset.\"\n        },\n        {\n            \"name\": \"optimized baseline model\",\n            \"description\": \"A highly-optimized model with non-random behavior for comparison purposes\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Real-time constraints guide the optimization for deployment.\"\n        },\n        {\n            \"description\": \"Attention to variations in resolution and compression indicates the need for robustness testing.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adaptation to real-time constraints\", \"multi-modal data fusion\"],\n        \"target_device\": [\"cloud, possibly edge devices for real-time performance\"],\n        \"deployment_endpoint\": \"not specified\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"demonstrate performance in real-time detection and robustness tests\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON accurately captures the task of deploying a multi-modal transformer model for video forgery detection with real-time constraints. It also acknowledges the importance of a robust baseline and evaluating performance against resolution and compression variations. Given the high complexity and specific requirements, the user's expertise is assumed to be high. However, the confidence score is not 1.0 due to the missing specific model and deployment details which would be more beneficial if provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager, your mandate now involves not only evaluating COCO and Pascal VOC datasets for their image variety, annotation precision, and low-resolution adaptability in a real-world surveillance system, but also delve into their historical performance trends and any updates since their initial release. Furthermore, analyze multiple object detection models' scalability, energy efficiency, and error trade-offs for each architecture (YOLOv5, SSD, RetinaNet, and new entrants like EfficientDet or MantisNet). Conduct rigorous benchmarks on benchmark datasets and simulations with limited hardware resources. Your report should include a comprehensive comparison, detailed pros and cons, and a suggestion for a robust model selection strategy that ensures near-zero false positives and minimal latency for rapid parking lot object recognition.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"surveillance system, real-world parking lot\",\n        \"description\": \"Evaluate COCO and Pascal VOC datasets for image variety, annotation precision, and low-resolution adaptability in a surveillance system context. Investigate historical performance trends and updates since their release. Analyze YOLOv5, SSD, RetinaNet, EfficientDet, and MantisNet models' scalability, energy efficiency, and error trade-offs, considering benchmark datasets and limited hardware resources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"image variety\",\n                \"value\": null\n            },\n            {\n                \"name\": \"annotation precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"low-resolution adaptability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"error trade-offs\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": [\"COCO\", \"Pascal VOC\"],\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"image labels\"],\n            \"specification\": null,\n            \"description\": \"Evaluate for image variety, annotation precision, and low-resolution adaptability in a surveillance context. Recent updates and historical performance data are also important.\",\n            \"preprocessing\": [\"image variety analysis\", \"annotation precision assessment\"],\n            \"augmentation\": [],\n            \"visualization\": [\"image variety heatmaps\", \"annotation accuracy charts\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": [\"YOLOv5\", \"SSD\", \"RetinaNet\", \"EfficientDet\", \"MantisNet\"],\n            \"family\": [\"YOLO series\", \"SSD series\", \"RetinaNet\", \"EfficientDet\", \"MantisNet\"],\n            \"type\": [\"neural networks\"],\n            \"specification\": {\n                \"scalability\": null,\n                \"energy_efficiency\": null,\n                \"latency\": null\n            },\n            \"description\": \"Analyze for scalability, energy efficiency, and error trade-offs.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Historical performance trends, updates, and new entrants evaluation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"limited hardware resources\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"parking lot object recognition simulation with near-zero false positives and minimal latency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is interpreted as a consulting request, emphasizing a high level of expertise is expected. It covers dataset analysis, model comparison, and deployment considerations. Missing values in performance and complexity metrics are placeholders as the specific values require further research or consulting.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "image classification", "instruction": "As a project manager for an elite space data analysis team, your directive is to upgrade the Deep Astronomy Classifier (DAC) using a specialized StarlightViT-XT model, designed specifically for high-resolution galaxy imaging from the Celestial Event Registry. Instruct the team to implement multi-resolution feature fusion, adversarial noise resilience, and achieve a minimum precision of 97% on the pristine validation dataset. Ensure the model's proficiency in identifying obscure cosmic occurrences, fostering groundbreaking discoveries in astrophysics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"space data analysis, astrophysics\",\n        \"description\": \"Upgrade the Deep Astronomy Classifier (DAC) using a specialized StarlightViT-XT model for high-resolution galaxy imaging from the Celestial Event Registry. Implement multi-resolution feature fusion and adversarial noise resilience, targeting a minimum precision of 97% on the pristine validation dataset, with a focus on identifying obscure cosmic occurrences.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celestial Event Registry\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"data_type\": \"galaxy imaging\"\n            },\n            \"description\": \"High-resolution galaxy imaging data from the Celestial Event Registry for model validation\",\n            \"preprocessing\": [\"multi-resolution feature fusion\"],\n            \"augmentation\": [\"adversarial noise resilience\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"StarlightViT-XT\",\n            \"family\": \"StarlightViT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specialized for space data analysis\",\n                \"feature_fusion\": \"multi-resolution\"\n            },\n            \"description\": \"A specialized model for galaxy imaging from Deep Astronomy Classifier (DAC) upgrade\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance computing cluster\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"focusing on recognizing obscure cosmic occurrences for astrophysical discoveries\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was parsed based on the given instructions, where the user's expertise is assumed to be high due to managing an elite team and the advanced requirements of the project. The downstream task, area, and performance metrics are clearly specified. However, the target device and deployment endpoint are not explicitly provided, but could be inferred to be high-performance computing environments. The confidence score is 0.95 given the provided details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a cutting-edge project manager, your machine learning team is tasked with enhancing the Heartbeat dataset analysis for time series classification. The team must develop an advanced model using a novel fusion technique with an input sequence of 405 time steps (INPUT_SEQ_LEN=405, INPUT_DIM=61) and 61-dimensional feature vectors. The competition involves five additional data partitions for fine-tuning, and models must achieve at least 95% accuracy on unseen test data while demonstrating robustness to outliers using a custom evaluation strategy. Additionally, incorporate interpretability techniques for model explanations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"healthcare, specifically heart beat analysis\",\n        \"description\": \"Enhance time series classification on the Heartbeat dataset using an advanced fusion technique. The input sequence has 405 time steps (INPUT_SEQ_LEN=405, INPUT_DIM=61) and the model must achieve at least 95% accuracy on unseen test data. It should also demonstrate robustness to outliers and include interpretability techniques for model explanations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Heartbeat\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 405,\n                \"INPUT_DIM\": 61\n            },\n            \"description\": \"A time series dataset focused on heart beat analysis for classification, with additional fine-tuning data partitions and focus on robustness to outliers.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"novel fusion technique\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"number_of_layers\": null,\n                \"novel_fusion_technique\": \"\",\n                \"interpretability_techniques\": [\"LIME\", \"SHAP\"]\n            },\n            \"description\": \"An advanced model utilizing a novel fusion technique for heart beat time series classification with interpretability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretable models\"],\n        \"demonstration\": \"Include model interpretation for model outputs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction reflects a high-level ML project involving time series classification, with specific requirements on input sequence, accuracy, robustness, and interpretability. The user's expertise is assumed high due to the cutting-edge project management role. The dataset is inferred to be the Heartbeat dataset, and the model requirements are appropriately detailed. Missing information, such as the deployment specifics and exact interpretation techniques, are assumptive based on the task. The confidence score is high, acknowledging the completeness but also the potential for assumptions.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "visual question answering", "instruction": "The project manager is tasked with developing a state-of-the-art Visual Question Answering (VQA) system for a high school competition, requiring the team to explore and implement multiple advanced models, fine-tune them with the TextVQA dataset, and demonstrate exceptional precision in interpreting text within images and generating insightful responses.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"high school competition\",\n        \"description\": \"The project involves creating a state-of-the-art Visual Question Answering (VQA) system for a high school competition, requiring exploration and implementation of advanced models, fine-tuning with the TextVQA dataset, and precise interpretation of text within images for insightful responses.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TextVQA\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [\"image features\", \"answer\"],\n            \"specification\": {\n                \"size\": \"sufficient for fine-tuning\",\n                \"language corpus\": {\n                    \"diversity\": \"high\"\n                }\n            },\n            \"description\": \"A dataset to fine-tune the VQA system for the competition, focusing on text understanding and image interpretation.\",\n            \"preprocessing\": [\"tokenization\", \"image feature extraction\"],\n            \"augmentation\": [\"data balancing techniques\"],\n            \"visualization\": [\"model performance analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"multiple advanced VQA models\",\n            \"family\": [\"LXMERT\", \"Mutan\", \"VisualBERT\", \"or other cutting-edge models\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"complex\",\n                \"novelty\": \"state-of-the-art\"\n            },\n            \"description\": \"Advanced models that need to be explored and fine-tuned for high performance.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"cross-modal fusion\", \"attention mechanisms\"],\n        \"target_device\": \"high-performance computing environment (for competition)\",\n        \"deployment_endpoint\": \"event competition platform or showcase server\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"interactive Q&A with clear response explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is clear, and the high expertise level suggests they expect a sophisticated solution. The project requirements indicate building a cutting-edge VQA system, fine-tuning with a specific dataset, and optimizing for precision. The dataset includes text and image modalities, and preprocessing and augmentation are mentioned. However, the exact models or the precise precision value are unspecified, which can be a point of uncertainty.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for the expansion of our sustainable 1,000-acre agricultural venture, develop an advanced machine learning system that integrates real-time drone imagery and can distinguish not only between common crop diseases but also unusual stress factors, distinguishing between early-stage and advanced stages, while adhering to strict privacy regulations for data collection.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"The project aims to develop an advanced machine learning system for an 1,000-acre sustainable agricultural venture. The system should integrate real-time drone imagery and classify crop diseases and unusual stress factors with distinction between early and advanced stages, while respecting strict privacy regulations for data collection.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"disease classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"stress factor detection accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"stage identification accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing speed\",\n                \"value\": null,\n                \"unit\": \"frames per second\"\n            },\n            {\n                \"name\": \"data privacy compliance\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Real-time Drone Imagery from Sustainable Farm (proprietary)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\n                \"crop disease\",\n                \"stress factor\",\n                \"disease stage\"\n            ],\n            \"specification\": {\n                \"resolution\": null,\n                \"size\": null,\n                \"data privacy level\": \"strict\"\n            },\n            \"description\": \"A dataset collected from the 1,000-acre agricultural venture containing real-time drone imagery for detecting crop diseases and unusual stress factors.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Crop Disease and Stress Detection Model\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"combined CNN and deep learning\",\n                \"use of transfer learning\": true,\n                \"data preprocessing steps\": null\n            },\n            \"description\": \"A model designed to classify crop diseases and unusual stress factors while differentiating their stages based on real-time drone imagery.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Project manager has high-level knowledge of ML systems and privacy regulations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or edge\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"GPU-powered, cloud-based\"],\n        \"demonstration\": \"system should provide visual and real-time updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the problem statement, highlighting the need for an advanced classification system, data collection methods, and privacy considerations. However, specific performance metrics, model architecture details, and deployment specifics are yet to be provided. Expertise is assumed to be high due to the project manager role. The confidence score is moderate, as more precise information is needed.\",\n        \"score\": 0.7\n    }\n}\n"}
{"task": "image classification", "instruction": "For my thesis, I require image classification of celestial objects from a verified NASA or similar open-source dataset, specifying galaxies and quasars. In light of compute constraints, select a model efficient in resource usage but with a proven track record of excellent performance under limited hardware. Assess the realistic possibility of reaching >95% accuracy under these conditions and provide details on data preprocessing techniques.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy (galaxies and quasars)\",\n        \"description\": \"The user requires an image classification project for celestial objects, specifically galaxies and quasars, using a NASA or open-source dataset. They prioritize a model efficient in resource usage for compute constraints and aim for >95% accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NASA or similar open-source\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"galaxies\", \"quasars\"],\n            \"specification\": {\n                \"compute_constraints\": {\n                    \"resource_efficiency\": true,\n                    \"hardware_limitations\": \"limited hardware\"\n                }\n            },\n            \"description\": \"A dataset containing verified images of galaxies and quasars from NASA or open-source sources.\",\n            \"preprocessing\": [\n                {\n                    \"description\": \"Data cleaning to remove irrelevant or corrupted images\"\n                },\n                {\n                    \"description\": \"Resizing or resizing to fit the model's input requirements\"\n                }\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"resource_efficiency\": true,\n                \"performance_on_limited_hardware\": \"proven track record\",\n                \"accuracy\": null\n            },\n            \"description\": \"An efficient model with good performance under limited hardware, suitable for the given task.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Data augmentation techniques such as simple cropping and rotation to save compute while maintaining accuracy\"\n        },\n        {\n            \"description\": \"Transfer learning from existing models pre-trained on large datasets to leverage their performance without requiring heavy training\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            {\n                \"description\": \"Lightweight feature extraction methods for resource-constrained scenarios\"\n            }\n        ],\n        \"target_device\": null,\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A demonstration showcasing the model's accuracy and resource efficiency under the given constraints\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided information has been parsed into a JSON structure. The user's intent to build indicates the need for model development. Given the medium expertise level, there may be a focus on guidance. Performance accuracy is speculative as it depends on the specific model chosen. Data preprocessing techniques are relevant and expected. However, the deployment target device and some inference engine options are not provided as they are not specified in the instruction.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "Employ the exclusive YouTube Music Visuals and Creator Insights dataset for a fine-grained node classification project, employing a Deep Adaptive Significance Graph Neural Network (DASGNN). Demand a minimum accuracy of 90% in discriminating five unique categories of music content creators, while demonstrating the model's prowess in dealing with data scarcity and temporal dependencies within the content.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"music content classification\",\n        \"description\": \"Employ the YouTube Music Visuals and Creator Insights dataset for a fine-grained node classification project using a Deep Adaptive Significance Graph Neural Network (DASGNN). The goal is to achieve at least 90% accuracy in differentiating between five unique categories of music content creators, while demonstrating model performance in handling data scarcity and temporal dependencies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.90\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube Music Visuals and Creator Insights\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [\"music content creator category\"],\n            \"specification\": {\n                \"node_types\": \"music content creators\",\n                \"edges\": \"temporal or relationship dependencies\",\n                \"size\": \"sufficient for five classes\"\n            },\n            \"description\": \"Dataset for fine-grained node classification on music creators, incorporating both visual and temporal aspects.\",\n            \"preprocessing\": [\"handling missing data\", \"normalizing data\"],\n            \"augmentation\": [\"graph augmentation to deal with data scarcity\"],\n            \"visualization\": [\"visualization of temporal dependencies\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DASGNN\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Deep Adaptive Significance Graph Neural Network\",\n                \"handles_scarcity\": true,\n                \"handles_temporal_dependency\": true\n            },\n            \"description\": \"A DASGNN designed for node classification with a focus on data scarcity and temporal dependencies.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"handling skewed data distribution\",\n            \"imbalance resolution for better performance\"\n        ],\n        \"target_device\": \"cloud for computational efficiency\",\n        \"deployment_endpoint\": \"to-be-determined\",\n        \"inference_engine\": [\"GPU-friendly for faster processing\"],\n        \"demonstration\": \"performance analysis on unseen data with visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided information was parsed according to the JSON schema. The user has high expertise, and the goal, dataset, and model details are specified. The performance metric is set to 90% accuracy, and the model's ability to handle data scarcity and temporal dependencies is mentioned. Some areas where additional clarification was not provided, such as the exact deployment endpoint, are left open-ended.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the advanced text classification team, your objective is to develop a state-of-the-art machine learning model using the DAIGT dataset. The challenge now is to not only classify texts as AI-generated (1) or human (0) with high accuracy but also ensure the model can distinguish between multiple AI generations (e.g., varying models) and time epochs. Furthermore, incorporate explainable AI techniques to enhance transparency and improve model interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"advanced text classification\",\n        \"description\": \"The objective is to develop a state-of-the-art machine learning model for advanced text classification. The model must classify texts as AI-generated (1) or human (0) with high accuracy, while distinguishing between different AI generations and time epochs. Explanatory AI techniques must be incorporated.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"AI-generated\", \"human\"],\n            \"specification\": null,\n            \"description\": \"A dataset for advanced text classification, focusing on AI-generated vs human texts, including variations in AI models and time epochs.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": \"incorporated\"\n            },\n            \"description\": \"A state-of-the-art machine learning model with focus on distinguishing AI generations and time epochs, and equipped with explainable AI techniques.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"enhanced model interpretability through explainable AI\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's requirements, indicating a high expertise level. The problem statement reflects the objective, the advanced text classification task and the desired model characteristics. However, the performance metric values are not provided, and the model name and family are not specified. The confidence score is high but is not 1.0 due to the missing specific details.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "machine translation", "instruction": "Instruct the machine learning team to develop a highly optimized, quantized bidirectional LSTM neural network, utilizing the domain-specific Europarl v7 corpus (version 10.1) for real-time, low-latency Spanish-English translations. Target a stringent response time requirement of 250 milliseconds, ensuring negligible computational overhead and efficient model deployment on embedded hardware platforms.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"real-time Spanish-English translation\",\n        \"description\": \"Develop a highly optimized, quantized bidirectional LSTM neural network for real-time, low-latency Spanish-English translations using the Europarl v7 corpus (version 10.1).\",\n        \"performance_metrics\": {\n            \"response_time\": {\n                \"name\": \"response time\",\n                \"value\": 0.25\n            },\n            \"computational_overhead\": {\n                \"name\": \"computational overhead\",\n                \"value\": 0\n            }\n        },\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory_size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Europarl v7 corpus (version 10.1)\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"corpus_type\": \"Spanish-English parallel corpus\"\n            },\n            \"description\": \"A domain-specific corpus for training a neural machine translation model, focusing on Spanish-English translation.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantized Bidirectional LSTM\",\n            \"family\": \"LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"quantization\": true,\n                \"optimization\": \"embedded hardware compatibility\"\n            },\n            \"description\": \"A neural network designed for real-time translation with a low-latency requirement of 250 milliseconds.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"embedded hardware platforms\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\", \"low-latency\"],\n        \"demonstration\": \"Include demonstration of low-latency performance on target hardware\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed with high confidence, considering the specific requirements for the model (quantized bidirectional LSTM), dataset (Europarl v7), response time (250 ms), and the target deployment platform (embedded hardware). However, some metric values are left unspecified for optimization, which indicates the need for further clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for a high-demand retail ecosystem, you require a state-of-the-art YOLOv4-based object detection model specifically tailored for the \"StoreShelf Dense-labeled 50-Type Inventory Dataset.\" This model should not only detect and accurately classify at least 50 diverse product categories with a stringent precision of 95% and recall of 93%, but also exhibit efficient multi-label image analysis to minimize false positives and negatives. To maintain seamless operations, the system must process 200 images per second, achieving sub-second shelf scanning (ideally below 0.5 seconds per image), while integrating real-time inventory updates into the existing supply chain management system to ensure zero-stockout scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail ecosystem\",\n        \"description\": \"A project manager seeks a state-of-the-art YOLOv4-based object detection model specifically for the 'StoreShelf Dense-labeled 50-Type Inventory Dataset'. The model must accurately classify at least 50 product categories, have precision of 95% and recall of 93%, and support multi-label image analysis with fast inference at 200 images per second.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.93\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": 200,\n                \"unit\": \"images per second\"\n            },\n            {\n                \"name\": \"response time per image\",\n                \"value\": 0.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"StoreShelf Dense-labeled 50-Type Inventory Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number of classes\": 50\n            },\n            \"description\": \"A dataset for retail inventory management, with a focus on multi-label object detection for accurate and efficient product category identification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv4\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"object_classes\": 50,\n                \"desired_accuracy\": \"95%\",\n                \"desired_recall\": \"93%\"\n            },\n            \"description\": \"A state-of-the-art object detection model optimized for StoreShelf Dense-labeled Inventory Dataset, with emphasis on high precision and recall.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance computing\",\n        \"deployment_endpoint\": \"real-time inventory management system\",\n        \"inference_engine\": [\n            \"real-time optimization for edge computing\"\n        ],\n        \"demonstration\": \"system demonstrating sub-second shelf scanning and zero-stockout scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction captures the requirement for a specific object detection model, metric expectations, and hardware requirements. The user's high expertise level implies detailed knowledge of the project. The parsed information covers almost all aspects of the instruction, including strict performance metrics, integration with existing systems, and real-time implementation.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "object detection", "instruction": "Our elite urban innovation team aims to engineer a high-resolution parking space analytics system, leveraging the highly specialized M2Det-Edge model tailored for 'PKLotX' dataset with enhanced night vision and weather-resistant features. The project necessitates not only state-of-the-art accuracy exceeding 95% under diverse conditions but also optimizing latency to less than 100 milliseconds while ensuring seamless integration with IoT devices in a real-time, IoT-empowered smart city infrastructure.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"urban planning/iot\",\n        \"description\": \"Engineer a high-resolution parking space analytics system using the M2Det-Edge model tailored for the 'PKLotX' dataset. Enhance the model with night vision and weather-resistant features while aiming for state-of-the-art accuracy of at least 95% under diverse conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.1,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PKLotX\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"parking space status\"\n            ],\n            \"specification\": {\n                \"enhanced for\": [\n                    \"night vision\",\n                    \"weather resistance\"\n                ]\n            },\n            \"description\": \"A specialized dataset for parking space analysis with a focus on night vision and weather resistance, tailored for M2Det-Edge.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M2Det-Edge (enhanced)\",\n            \"family\": \"M2Det\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"target_accuracy\": 0.95,\n                \"latency\": {\n                    \"inference\": 100,\n                    \"unit\": \"milliseconds\"\n                }\n            },\n            \"description\": \"A high-resolution M2Det-Edge model specifically designed for the 'PKLotX' dataset, optimized for low-latency (less than 100 ms) and IoT integration.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"IoT devices and edge computing\",\n        \"deployment_endpoint\": {\n            \"type\": \"real-time, IoT-empowered smart city infrastructure\"\n        },\n        \"inference_engine\": [],\n        \"demonstration\": \"IoT-connected, real-time parking space analytics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction has provided a clear direction for a complex machine learning project. The user's high expertise level indicates a more advanced understanding of the project's requirements. The performance metrics and latency are explicitly mentioned, ensuring a precise understanding of the expectations. However, the absence of specific details on feature engineering and the exact inference engine suggests a higher confidence in other areas but lower in these.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the cutting-edge machine learning team, your objective is to develop a state-of-the-art text classification model. Utilizing the challenging Textual Entailment dataset, containing a vast and diverse range of labeled text pairs with nuanced relationships (neutral, contradiction, and entailment), enhance the model's performance by optimizing for precision, recall, and F1-score. Implement advanced techniques like BERT finetuning, ensemble methods, and incorporate domain-specific knowledge. Strive for at least 95% accuracy while maintaining efficiency and interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"general\",\n        \"description\": \"The project goal is to develop a state-of-the-art text classification model using the Textual Entailment dataset, focusing on precision, recall, and F1-score. Techniques like BERT finetuning, ensemble methods, and incorporating domain-specific knowledge are required. The model should achieve at least 95% accuracy while maintaining efficiency and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Textual Entailment\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [\"classification label\"],\n            \"specification\": {\n                \"scale\": \"vast and diverse\",\n                \"categories\": [\"neutral\", \"contradiction\", \"entailment\"],\n                \"labeled_pairs\": true\n            },\n            \"description\": \"A challenging text dataset for text classification with nuanced relationships.\",\n            \"preprocessing\": [\"BERT tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine-tuning\": true,\n                \"ensemble\": true,\n                \"interpretability\": true\n            },\n            \"description\": \"A state-of-the-art text classification model leveraging BERT and ensemble techniques for improved performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Domain-specific knowledge to be incorporated\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was created based on the given instruction. The user's expertise is assumed high due to the project manager role. The objective, dataset, model, and performance metrics were detailed accordingly. The need for domain-specific knowledge, BERT finetuning, and ensemble methods is also noted. However, the 'service' section is incomplete due to missing details about device and deployment. Overall, the response captures the essence of the task but assumes certain aspects.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "The machine learning team's objective is to develop a scalable, real-time arXiv paper classifier using HAN, targeting 90% F1 score with interpretable attention weights. The model must dynamically adapt to new publications, maintain precision, and perform exceptionally well in five niche subfields of science, ensuring consistent performance across varying domains.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"scientific papers\",\n        \"description\": \"The machine learning team needs to develop a scalable, real-time arXiv paper classifier using HAN with a target of 90% F1 score. The model should have interpretable attention weights, adapt dynamically to new publications, maintain precision, and perform consistently across five niche subfields of science.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ],\n        \"adaptability\": {\n            \"dynamic\": true\n        },\n        \"specific_subfields\": [\n            {\n                \"name\": \"subfield 1\",\n                \"description\": \"\"\n            },\n            {\n                \"name\": \"subfield 2\",\n                \"description\": \"\"\n            },\n            {\n                \"name\": \"subfield 3\",\n                \"description\": \"\"\n            },\n            {\n                \"name\": \"subfield 4\",\n                \"description\": \"\"\n            },\n            {\n                \"name\": \"subfield 5\",\n                \"description\": \"\"\n            }\n        ],\n        \"domain_invariance\": {\n            \"description\": \"Consistent performance across varying domains\"\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"paper category\"],\n            \"specification\": {\n                \"new_publications\": true,\n                \"diverse_subdomains\": true\n            },\n            \"description\": \"A real-time dataset with a focus on scientific publications\",\n            \"preprocessing\": [\"Preprocessing steps\"],\n            \"augmentation\": [\"Data augmentation techniques\"],\n            \"visualization\": [\"Attention weight visualization\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Attention Network (HAN)\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"F1_score_target\": 0.9,\n                \"interpretable_attention\": true\n            },\n            \"description\": \"A real-time arXiv paper classifier with HAN, targeting high precision and interpretability\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time engine\"],\n        \"demonstration\": \"Interpretable attention-weight explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided was parsed into a JSON response that includes the user's intent, high-level expertise, and detailed requirements. The problem area, task, and performance metrics are clearly defined. The model section specifies HAN and its requirements, while the dataset includes arXiv and subfield information. Some details like feature engineering, device, and deployment engine remain vague as they were not mentioned in the original instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Embark on a multi-year urban vegetation analysis project using the Vaihingen dataset, focusing on intricate subclassification of nine diverse vegetation types and their ecosystem health. Implement a state-of-the-art fusion of SegNet and Context-Aware Neural Networks, optimized for low-light and seasonal variations, ensuring high-resolution segmentation maps with time-sensitive performance indicators for evidence-based urban renewal strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban vegetation analysis\",\n        \"description\": \"Embark on a multi-year project to analyze urban vegetation using the Vaihingen dataset. The focus is on subclassifying nine diverse vegetation types and evaluating ecosystem health, with a focus on a fusion of SegNet and Context-Aware Neural Networks. The model should be optimized for low-light and seasonal variations, producing high-resolution segmentation maps with time-sensitive performance indicators for evidence-based urban renewal planning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sensitivity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"Vegetation types\"],\n            \"specification\": {\n                \"dataset_size\": null,\n                \"image_dimensions\": null,\n                \"label_resolution\": null\n            },\n            \"description\": \"Dataset for urban vegetation analysis with high-resolution images for a multi-year study.\",\n            \"preprocessing\": [\"Data normalization\", \"Image enhancement for low-light conditions\"],\n            \"augmentation\": [\"Seasonal variations simulation\"],\n            \"visualization\": [\"High-resolution segmentation maps\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SegNet + Context-Aware Neural Networks\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"SegNet\", \"Context-Aware layers\"],\n                \"optimization\": \"seasonal and low-light conditions\",\n                \"framework\": null\n            },\n            \"description\": \"Fusion of SegNet and Context-Aware Neural Networks, optimized for urban vegetation analysis with high-resolution segmentation and time-sensitive performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptation to varying lighting conditions and seasonal changes.\",\n        \"Integration with urban planning goals for evidence-based decisions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Image preprocessing for urban environment specificities\",\n            \"Temporal feature extraction for seasonality\"\n        ],\n        \"target_device\": \"high-performance computing cluster\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"Real-time segmentation\"],\n        \"demonstration\": \"Interactive urban renewal strategy maps\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intent to build a model, the project's complexity, and the specific requirements for the dataset, model, and performance metrics. The provided dataset source and domain-specific preprocessing indicate a high level of expertise. However, since performance metrics, inference time, and deployment details are not specified, there's a possibility they're expected to be researched or derived from the dataset documentation. Confidence score is medium to account for these details.\",\n        \"score\": 0.7\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for the elite AI research team, you demand the design of an innovative image recognition system that employs the exclusive Fashion-MNIST dataset with a focus on advanced Capsule Network architecture. The objective is to achieve cutting-edge accuracy beyond the 90% milestone, while maintaining exceptional resilience to affine image distortions. In addition, require a comprehensive, in-depth analysis that not only benchmarks the capsule network against conventional Convolutional Neural Networks (CNNs) but also elucidates the significant advantages and potential compromises in terms of computational efficiency, interpretability, and robustness to adversarial attacks. Emphasize the need for a thorough evaluation report to showcase the state-of-the-art performance and potential real-world implications.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"Design an innovative image recognition system using the exclusive Fashion-MNIST dataset with a focus on advanced Capsule Network architecture. Target accuracy should surpass 90% and the model should be resilient to affine image distortions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computation efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs or training time\"\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"comparative evaluation against CNNs\"\n            },\n            {\n                \"name\": \"robustness to adversarial attacks\",\n                \"value\": null,\n                \"unit\": \"security measure\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": {\n                    \"height\": 28,\n                    \"width\": 28\n                },\n                \"channels\": 1\n            },\n            \"description\": \"Exclusive dataset for advanced image recognition, focusing on fashion items\",\n            \"preprocessing\": [\"affine distortion handling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Capsule Network\",\n            \"family\": \"CapsNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"advanced Capsule Network\",\n                \"compare_to\": \"Convolutional Neural Networks (CNNs)\"\n            },\n            \"description\": \"Advanced Capsule Network utilizing the Fashion-MNIST dataset for high accuracy and resistance to affine distortions\"\n        }\n    ],\n    \"knowledge\": [\n        \"Benchmarking, interpretability, computational efficiency trade-offs, and resilience to adversarial attacks\",\n        \"Analysis of state-of-the-art performance and real-world implications\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Showcase the system's performance, comparison with CNNs, and potential adversarial resilience\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is to build an advanced model with clear performance expectations and specific architecture (Capsule Network). They have high expertise, so deeper analysis and potential compromises are included. The dataset, metrics, and benchmarking requirements are well-defined, but service deployment details are left for further discussion.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Incorporate state-of-the-art transfer learning techniques, fine-tuning ResNet-50 on a diverse subset of CIFAR-10 with data augmentation, and ensure model generalization across 10 subcategories with an incremental accuracy target of 99.5% within a strict 100 epoch budget and report ROC-AUC curve.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"image recognition\",\n        \"description\": \"Incorporate state-of-the-art transfer learning, fine-tune ResNet-50 on a diverse subset of CIFAR-10 dataset with data augmentation, targeting generalization across 10 subcategories, and aim for an incremental accuracy of 99.5% with a strict 100-epoch budget. Include ROC-AUC curve in the performance metrics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.995\n            },\n            {\n                \"name\": \"ROC-AUC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CIFAR-10\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"subset_diversity\": \"diverse\",\n                \"total_subcategories\": 10\n            },\n            \"description\": \"A subset of the CIFAR-10 dataset featuring data augmentation.\",\n            \"preprocessing\": [\"fine-tuning\", \"data augmentation\"],\n            \"augmentation\": [\"random flips\", \"rotations\", \"translations\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNet-50\",\n            \"family\": \"ResNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"epochs\": 100,\n                \"generalization_target\": 10\n            },\n            \"description\": \"A model utilizing transfer learning, fine-tuned on a diverse CIFAR-10 subset with an aim for high accuracy and ROC-AUC curve.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"report ROC-AUC curve\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed based on the provided details. User expertise is assumed to be high due to the request for state-of-the-art techniques and strict requirements. Performance metrics target accuracy and ROC-AUC, while the complexity metrics are not specified. The data augmentation and fine-tuning techniques are clearly outlined, along with the target subcategories and budget.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For our upcoming precision agriculture initiative, we need a specialized DeepLabV3+ model that not only demonstrates exceptional performance on the proprietary AgriSeg dataset with a stringent precision threshold of 90%, but also exhibits efficient real-time inference on resource-constrained edge devices without sacrificing more than 20 frames per second to ensure uninterrupted weed-crop distinction in the field.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"precision agriculture\",\n        \"description\": \"Develop a specialized DeepLabV3+ model for precision agriculture, targeting exceptional performance on the proprietary AgriSeg dataset with a precision threshold of 90% and efficient real-time inference on edge devices, maintaining a minimum of 20 frames per second for weed-crop distinction.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": 20,\n                \"unit\": \"frames per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AgriSeg\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"custom\": \"90% precision threshold\"\n            },\n            \"description\": \"A proprietary dataset for precision agriculture, focusing on weed-crop distinction.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepLabV3+\",\n            \"family\": \"DeepLab\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time inference\": true,\n                \"resource-constrained\": true\n            },\n            \"description\": \"A specialized DeepLabV3+ model for precision agriculture, optimized for the given requirements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge device\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time optimized\"],\n        \"demonstration\": {\n            \"focus\": \"weed-crop distinction in the field\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction was parsed with high confidence. They have a high expertise level, and the requirements for the model, dataset, and service are clear, including the stringent precision and real-time performance constraints. However, the confidence score is not 1.0 because the user did not explicitly specify the demonstration method or the dataset size.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a seasoned project manager for the advanced AI development team, you are now tackling a multifaceted text classification challenge. The BoolQ dataset, expanded with domain-specific nuances, demands not only a proficient model to discern if questions stem from provided passages (0 for implausible, 1 for deducible), but also achieving state-of-the-art precision and recall. The team must employ sophisticated deep learning algorithms, conduct extensive feature engineering, and meticulously evaluate model robustness across various linguistic subdomains for a comprehensive understanding. Emphasize the importance of interpretability and real-world applicability in the final submission.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"advanced AI development\",\n        \"description\": \"The task is to tackle a multifaceted text classification challenge using the expanded BoolQ dataset with domain-specific nuances. The goal is to build a proficient model that discerns if questions stem from provided passages and achieves state-of-the-art precision and recall. The project requires sophisticated deep learning algorithms, extensive feature engineering, and robustness evaluation across linguistic subdomains.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ (expanded)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"target_labels\": [\"implausible\", \"deducible\"]\n            },\n            \"description\": \"Expanded BoolQ dataset with domain-specific nuances, meant for text classification to distinguish questions and passages.\",\n            \"preprocessing\": [\"domain-specific expansion\", \"feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"real-world_applicability\": \"required\"\n            },\n            \"description\": \"Sophisticated, domain-specific deep learning model for text classification, focusing on interpretability and real-world usability.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Emphasis on interpretability and real-world applicability in the final model enhances transparency and practical value.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain adaptation\", \"feature extraction\", \"dimensionality reduction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Transformer-based models\", \"state-of-the-art libraries\"],\n        \"demonstration\": \"Evaluation across linguistic subdomains and visual interpretability explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provides a clear task of building a text classification model with a focus on complex metrics like precision, recall, and interpretability. The user's high level of expertise suggests they may require more guidance on specific model architecture and implementation details, which are left open-ended for response. The need for real-world applicability and emphasis on robustness in various subdomains is clearly indicated.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a machine learning project manager, request a specialized CNN-LSTM model with attention mechanism for the urban traffic flow dataset. The model must not only forecast hourly traffic volume for the next month but also dynamically adapt to predict peak hours with an accuracy of at least 95% and exhibit distinct forecasting strategies for weekdays and weekends, incorporating real-time traffic event data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"urban traffic flow\",\n        \"description\": \"As a machine learning project manager, require a specialized CNN-LSTM model with an attention mechanism for the urban traffic flow dataset. The model must forecast hourly traffic volume for the next month, achieving at least 95% accuracy in predicting peak hours. It should have distinct forecasting strategies for weekdays and weekends, and incorporate real-time traffic event data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95,\n                \"unit\": \"%\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"urban traffic flow\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly traffic volume\"],\n            \"specification\": {\n                \"time_window\": {\n                    \"train\": \"previous month\",\n                    \"forecast\": \"next month\"\n                },\n                \"feature_types\": [\"hour\", \"weekday/weekend\", \"real-time traffic events\"]\n            },\n            \"description\": \"Dataset for traffic flow analysis, containing historical hourly data with relevant features for modeling.\",\n            \"preprocessing\": [\"Normalize data\", \"Feature engineering (weekday/weekend indicators)\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Data distribution\", \"Correlation analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"CNN-LSTM with attention mechanism\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"CNN-LSTM with separate layers for weekday/weekend and attention mechanism\",\n                \"peak_hour_prediction\": true,\n                \"distinct_forecasting_strategies\": true\n            },\n            \"description\": \"A specialized model combining CNN and LSTM components with attention mechanism to forecast hourly traffic volume for the next month, differentiating peak hours and weekends.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Features to incorporate real-time traffic events\",\n            \"Extracting peak hours and weekday/weekend patterns\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"Web-based traffic management system\",\n        \"inference_engine\": [\"TensorFlow serving\"],\n        \"demonstration\": \"Live traffic flow dashboard visualizing model forecasts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provides clear details about the project requirements such as the model type, performance metric, data preprocessing, and deployment environment. The user's high expertise level justifies the inclusion of advanced modeling details. However, the model's performance on peak hour prediction and distinct strategies is assumed based on the requirements and might need further clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As the urban mobility director for an eco-friendly smart city initiative, task the machine learning team with developing a state-of-the-art YOLOv5 model optimized for 'Cityscapes 2.0' dataset, focusing on precision detection of diverse vehicle subclasses (cars, motorcycles, buses), pedestrian subcategories (elders, children), and unique traffic signs. The model should exhibit peak performance at 35 FPS on a set of environmentally-friendly AI accelerators, ensuring a minimum intersection over union (IoU) score of 90% for accurate real-time traffic monitoring.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city, eco-friendly urban mobility\",\n        \"description\": \"As the urban mobility director, task the machine learning team with developing a YOLOv5 model optimized for the Cityscapes 2.0 dataset, focusing on precise detection of vehicle subclasses and pedestrians.\",\n        \"performance_metrics\": {\n            \"precision\": {\n                \"target_vehicle_classes\": [\"cars\", \"motorcycles\", \"buses\"],\n                \"pedestrian_classes\": [\"elders\", \"children\"]\n            },\n            \"IoU_score\": {\n                \"min_value\": 0.9,\n                \"object_classes\": [\"vehicles\", \"pedestrians\", \"traffic signs\"]\n            }\n        },\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_speed\",\n                \"value\": 35,\n                \"unit\": \"FPS\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes 2.0\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vehicles\", \"pedestrians\", \"traffic signs\"],\n            \"specification\": {\n                \"data_format\": \"structured for vehicle and pedestrian subcategories, possibly including unique traffic sign categories\"\n            },\n            \"description\": \"Dataset for real-time traffic monitoring, optimized for detecting diverse vehicle and pedestrian subclasses\",\n            \"preprocessing\": [\n                \"custom data augmentation for diverse environments\"\n            ],\n            \"augmentation\": [\n                \"specific for Cityscapes 2.0 and eco-friendly AI accelerators\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"optimized for precision and real-time performance on eco-friendly accelerators\",\n                \"target_deployed_environment\": \"AI accelerators (with specific power consumption and resource constraints)\"\n            },\n            \"description\": \"State-of-the-art object detection model with a focus on vehicle subclasses, pedestrians, and traffic signs\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"fine-tuning for eco-friendly accelerators\"],\n        \"target_device\": \"environmentally-friendly AI accelerators\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for Cityscapes 2.0 with high precision and 35 FPS\"],\n        \"demonstration\": \"Real-time traffic monitoring with a minimum IoU of 90% for accurate monitoring\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction was parsed to reflect a build intent, with a high level of expertise expected for complex object detection in a specific use case. The performance requirements and dataset specifications are clearly detailed. The missing confidence score should be filled in based on the given content, assuming the information provided is comprehensive.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, direct the ML team to design and implement a specialized, binary-weight quantized LSTM model with byte-level byte pair encoding (BPE), specifically tailored for the economic and legal discourse in the Europarl v7 corpus (release 10.1.1). Strive for sub-250ms latency in real-time, energy-efficient Spanish-English translations, while ensuring optimized hardware acceleration for microcontroller-based edge devices with limited processing capabilities. Emphasize on optimized memory footprint for seamless integration and minimal power consumption.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"economic and legal discourse\",\n        \"description\": \"Design and implement a specialized, binary-weight quantized LSTM model with byte-level byte pair encoding (BPE) for real-time Spanish-English translations in the Europarl v7 corpus (release 10.1.1). Aim for sub-250ms latency, energy efficiency, and optimized hardware acceleration for microcontroller-based edge devices with limited processing capabilities.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 250,\n                \"unit\": \"milliseconds\",\n                \"constraint\": \"sub-250ms for real-time\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null, \n                \"unit\": \"\", \n                \"constraint\": \"energy-efficient\"\n            },\n            {\n                \"name\": \"hardware acceleration\",\n                \"value\": null,\n                \"unit\": \"\",\n                \"constraint\": \"optimized for microcontroller-based edge devices\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"bytes\",\n                \"constraint\": \"optimized for seamless integration\"\n            },\n            {\n                \"name\": \"power consumption\",\n                \"value\": null,\n                \"unit\": \"watts\",\n                \"constraint\": \"minimal\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Europarl v7 (release 10.1.1)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Spanish\", \"English\"],\n            \"specification\": {\n                \"corpus_type\": \"economic and legal discourse\",\n                \"language_pair\": \"Spanish-English\"\n            },\n            \"description\": \"Text corpus for training the specialized LSTM model with byte-level byte pair encoding.\",\n            \"preprocessing\": [\"byte pair encoding (BPE)\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"specialized binary-weight quantized LSTM\",\n            \"family\": \"LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"quantization\": \"binary-weight\",\n                \"encoding\": \"byte-level BPE\"\n            },\n            \"description\": \"Tailored for energy-efficient Spanish-English translation with microcontroller support.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Energy-efficient design, hardware acceleration for microcontrollers, and optimized memory footprint for edge devices.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"microcontroller-based edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for edge devices\"],\n        \"demonstration\": \"sub-250ms latency, seamless integration, and minimal power consumption\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the essential components based on the given instruction, such as the specialized model design for economic and legal discourse, target latency, and hardware requirements. High expertise level implies the user has a good understanding. Some metric values are left unspecified as they would typically be determined during the model development process.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "As a seasoned project manager leading the elite machine learning development team, you've been assigned to create a groundbreaking fusion of Visual Question Answering and Explainable AI (XAI) for an international university-level research competition. The team must design a novel system that not only accurately answers questions based on complex visual data but also provides step-by-step reasoning behind its answers, utilizing the rare and challenging Hateful Memes Challenge dataset for model resilience. The project must showcase transparency, ethical decision-making, and a deep understanding of the relationship between image interpretation and bias mitigation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"educational/research\",\n        \"description\": \"Create a groundbreaking fusion of Visual Question Answering and Explainable AI (XAI) for an international university-level research competition. The project should design a novel system that accurately answers complex visual questions and provides step-by-step reasoning, utilizing the Hateful Memes Challenge dataset to ensure model resilience and ethical decision-making, with a focus on bias mitigation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"model explainability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"user-friendly\"\n            },\n            {\n                \"name\": \"bias mitigation\",\n                \"value\": null,\n                \"unit\": \"percentage reduction in bias\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hateful Memes Challenge\",\n            \"modality\": [\"image\", \"text\"],\n            \"target_variables\": [\"visual question, answer, reasoning steps\"],\n            \"specification\": {\n                \"data_format\": \"complex, combining visual and textual inputs\",\n                \"bias_types\": \"hateful memes content\"\n            },\n            \"description\": \"A challenging dataset for visual question answering and bias mitigation, specifically targeting the context of hateful memes.\",\n            \"preprocessing\": [\"bias mitigation techniques, data cleaning\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model interpretability visualization\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Visual Question Answering with XAI\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"combinational VQA and XAI components\",\n                \"bias Mitigation Techniques\": \"explainable decision layers, fairness-aware training\"\n            },\n            \"description\": \"A novel system that integrates visual and textual understanding, with explainability for ethical AI.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hateful Memes Challenge: focus on understanding bias within visual context\",\n        \"XAI for ethical decision-making and transparency\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"bias mitigation techniques\",\n            \"interpretability techniques\"\n        ],\n        \"target_device\": \"high-performance server for computational requirements\",\n        \"deployment_endpoint\": \"potential AI platform or competition submission platform\",\n        \"inference_engine\": [\"GPU-based for efficient processing\"],\n        \"demonstration\": \"user-friendly interface for question submission and reasoning explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is clear for building a project, and their high expertise level suggests a deep understanding of the technical aspects. The problem description covers application domain, requirements, and emphasizes bias mitigation. The dataset is specified as a link, and the model details architecture and XAI aspects. The service section includes expected features for deployment, but some values are hypothetical. The confidence score is moderate due to the lack of specific model performance targets.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "object detection", "instruction": "Formulate a highly sophisticated AI-based inventory control system, exclusively tailored for an opulent luxury gourmet boutique, employing an innovative YOLOv5.1 architecture with tailored genetic enhancements for the \"Ultra-Rare Culinary Artifacts Dataset\" consisting of an impressive 15,000 exclusive and unparalleled fine-dining ingredients. The system must achieve an extraordinary F1-score of 99%, with a stringent product misidentification rate of 0.3%, ensuring real-time responsiveness at the pinnacle of efficiency (0.9 milliseconds). Alongside flawless synchronization with the ultra-secure, high-performance database, the system must perform advanced predictive analytics, incorporating multiple data sources for multidimensional demand forecasting, and implement machine learning-driven adaptive strategies to dynamically optimize stock levels in the boutique's rapidly evolving luxury market.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury gourmet boutique\",\n        \"description\": \"Design an AI-based inventory control system exclusively for an opulent luxury gourmet boutique using an innovative YOLOv5.1 architecture. The system handles the 'Ultra-Rare Culinary Artifacts Dataset' with 15,000 unique fine-dining ingredients. It must achieve an F1-score of 99% and a product misidentification rate of 0.3%, ensuring real-time efficiency with a response time of 0.9 milliseconds. The system should synchronize with an ultra-secure, high-performance database and perform predictive analytics with multidimensional demand forecasting, as well as adaptive machine learning strategies to optimize inventory levels in a dynamic luxury market.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"Product misidentification rate\",\n                \"value\": 0.003\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": 0.9,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ultra-Rare Culinary Artifacts Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"fine-dining ingredients\"],\n            \"specification\": {\n                \"size\": 15000,\n                \"uniqueness\": \"exclusive and unparalleled\"\n            },\n            \"description\": \"A dataset containing 15,000 unique and rare fine-dining ingredients for an inventory control system.\",\n            \"preprocessing\": [\"tailored genetic enhancements\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5.1\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"YOLOv5.1\",\n                \"target_misidentification\": 0.003,\n                \"real_time_performance\": \"true\"\n            },\n            \"description\": \"A tailored YOLOv5.1 architecture for object detection in the luxury gourmet boutique inventory system\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"multidimensional demand forecasting\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"ultra-secure, high-performance\",\n        \"inference_engine\": [\"machine learning-driven adaptive strategies\"],\n        \"demonstration\": \"real-time, dynamic inventory optimization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction specified a detailed project involving a high-end inventory control system. The user's high expertise level suggests a deep understanding, but some minor details like specific machine learning methods for prediction and deployment details were inferred. The confidence score is high due to the clarity of the task and the input's structured nature.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For a rapidly evolving retail setting with an expansive SKU database in the \"StoreShelf Dataset v3.0\", enhance the YOLO model to recognize over 150 unique products with strict performance benchmarks: 97.5% precision, 98% top-150 recall, and a strict 1.25-second latency constraint for real-time analysis. The model should exhibit adaptability to annual trends, swiftly accommodate novel SKUs, and seamlessly integrate with an intricate inventory management system that demands efficient, automated adjustments based on precise, high-speed shelf scans, minimizing human intervention for accuracy and operational efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail or inventory management\",\n        \"description\": \"Improve the YOLO model for a rapidly evolving retail environment using the StoreShelf Dataset v3.0 to recognize over 150 unique products. The model must meet strict performance criteria of 97.5% precision, 98% top-150 recall, and a 1.25-second latency constraint for real-time analysis. The model should be adaptable to annual trends and easily incorporate new SKUs, while integrating with an intricate inventory management system.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.975\n            },\n            {\n                \"name\": \"top-150 recall\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 1.25,\n                \"unit\": \"seconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 1.25,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"StoreShelf Dataset v3.0\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"unique_products\": 150\n            },\n            \"description\": \"A dataset for a retail setting with an expansive SKU database, designed for YOLO model enhancement with emphasis on performance and adaptability.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO enhancement\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptability\": \"annual trends\",\n                \"SKU_inclusion\": \"novel SKUs\",\n                \"latency_constraint\": \"1.25 seconds\"\n            },\n            \"description\": \"A YOLO model optimized for a retail environment with real-time, high-performance product recognition.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model should be adaptable to annual trends and require minimal human intervention for accuracy and operational efficiency.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time analysis environment\",\n        \"deployment_endpoint\": \"inventory management system\",\n        \"inference_engine\": [\"optimized for efficiency\"],\n        \"demonstration\": \"efficient, automated adjustments based on shelf scans\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided detailed requirements for enhancing a YOLO model in a retail setting. The high expertise level of the user implies they want detailed guidance. The specified metrics and constraints are included, but certain aspects such as the optimization for speed or the specific feature engineering techniques are left unspecified for the JSON response to reflect potential need for more consultation.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project manager for a elite AI research team, our mandate now involves not only the development of a sophisticated ETTm2-based time-series forecasting system, but we must confront the following stringent requirements. The model should handle non-uniformly distributed missing data, dynamically shifting patterns, and multiple seasonalities. In addition to forecasting 192 steps ahead (21 dimensions) with increased precision, it must demonstrate robustness against outliers and adapt to real-time scenarios with a time-varying autoregressive order. Emphasize on incorporating state-of-the-art techniques like functional principal component analysis (FPCA), advanced seasonality extraction via Fourier and wavelet transforms, and a nested cross-validation strategy for hyperparameter optimization. Ensure a comprehensive whitepaper highlights innovation, performance comparisons with rival methods, and a nuanced examination of the model's resilience across varying data conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"The project involves developing a sophisticated ETTm2-based time-series forecasting system with requirements for handling non-uniformly distributed missing data, dynamic shifting patterns, multiple seasonalities, and forecasting 192 steps ahead (21 dimensions) while being robust against outliers and real-time scenarios. Key techniques to incorporate include functional principal component analysis (FPCA), advanced seasonality extraction via Fourier and wavelet transforms, and a nested cross-validation strategy for hyperparameter optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"forecast precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness against outliers\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"PRED_SEQ_LEN\": 192,\n                \"PRED_DIM\": 21\n            },\n            \"description\": \"The ETTm2 dataset serves as the basis for a time-series forecasting system, handling complex data characteristics as specified.\",\n            \"preprocessing\": [\"missing data handling\", \"dynamic shifting pattern recognition\", \"multiple seasonality extraction\", \"real-time scenario adaptation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"techniques\": [\n                    \"functional principal component analysis (FPCA)\",\n                    \"Fourier and wavelet transforms for seasonality extraction\",\n                    \"dynamic autoregressive order adaptation\"\n                ]\n            },\n            \"description\": \"A state-of-the-art time-series forecasting model using ETTm2 dataset, optimized via nested cross-validation and adaptable for varying data conditions.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"functional data analysis\", \"real-time adaptability techniques\"],\n        \"target_device\": \"cloud-based or distributed\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient time-series processing methods\"],\n        \"demonstration\": \"comprehensive whitepaper including performance comparisons and resilience analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been parsed considering the complexity and sophistication of the project. A high level of expertise is assumed as it pertains to an elite AI research team. The user's intent is to build a model, and the instruction specifies various advanced techniques and evaluation criteria. However, there's some room for missing performance values and deployment specifics.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for an elite AI research team, your objective is to supervise the creation of a groundbreaking, neuro-symbolic time-series forecasting platform. The recently acquired BigScopeWeather dataset, now segmented into sub-components of train, validation, and test sets with ultra-detailed resolution (144 timestamps per INPUT_SEQ_LEN extended to capture 5-year cyclical patterns), poses complex challenges due to its dynamic, non-linear dynamics and extreme data noise.\n\nYour mandate encompasses the development of a hybrid model that combines deep learning sophistication with interpretable methods, seamlessly handling long-range dependencies, compound seasonality, and subtle signal anomalies. The model must forecast a staggering 192-step sequence (PRED_SEQ_LEN doubled) with unparalleled accuracy, optimizing for both quantified error metrics (MSE and MAE) while ensuring state-of-the-art resource optimization and inference speed.\n\nFurthermore, you must design and implement a distributed, scalable ensemble model that integrates Transformers, LSTMs, and interpretable tree-based models, integrating them in a manner that maximizes efficiency without compromising explainability. Accompany this architecture with a groundbreaking research paper that not only presents the model in exhaustive detail but also delves into the intricate trade-offs, potential future advancements, and how it adheres to the latest best practices in explainable AI, reproducibility, and privacy-preserving methodologies. Strive for innovation within the constraints of competitive resource allocation and adhere to industry-leading environmental sustainability standards.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"weather forecasting\",\n        \"description\": \"Supervise the creation of a groundbreaking neuro-symbolic time-series forecasting platform using the BigScopeWeather dataset. The objective is to develop a hybrid model combining deep learning and interpretable methods for handling complex dynamics, non-linearities, and extreme noise. The model should forecast a 192-step sequence with high accuracy (MSE and MAE). Focus on resource optimization, inference speed, and ensemble of Transformers, LSTMs, and tree-based models with explainability. Accompany the project with a research paper delving into trade-offs, advancements, explainability, reproducibility, and privacy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BigScopeWeather\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": {\n                    \"description\": \"Extended to capture 5-year cyclical patterns\",\n                    \"value\": 144\n                },\n                \"PRED_SEQ_LEN\": {\n                    \"description\": \"doubled for the forecasting task\",\n                    \"value\": 192\n                }\n            },\n            \"description\": \"Segmented dataset into train, validation, and test sets with ultra-detailed resolution and dynamic, non-linear, and noisy characteristics.\",\n            \"preprocessing\": [\"5-year cyclical pattern extraction\"],\n            \"augmentation\": [\"noise injection to simulate extreme conditions\"],\n            \"visualization\": [\"pattern recognition for signal anomalies\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"neuro-symbolic hybrid model\",\n            \"family\": \"ensemble of Transformers, LSTMs, and tree-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"complexity-management\": \"maximizing efficiency and explainability\",\n                \"accuracy\": \"state-of-the-art\",\n                \"long-term-dependencies\": \"handled by the model\"\n            },\n            \"description\": \"Combines deep learning sophistication and interpretable methods for weather forecasting\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"integrating ensemble models\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom private server\",\n        \"inference_engine\": [\"TensorFlow serving\"],\n        \"demonstration\": \"explainability through interactive visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction was parsed to include high-level objectives, specific dataset characteristics, the hybrid model's requirements, and resource optimization. However, exact metric values and detailed service implementation details (such as deployment endpoint) were not specified and would require further clarification from the user.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a cutting-edge, Transformer-XL-enhanced multilingual translation system that seamlessly integrates the massive XSum corpus alongside niche languages like Nepali, Indonesian, and Swahili, demanding an extensive evaluation that encompasses 45 unique TERQ scores across four challenging translation paths. The system must excel in translating intricate domain-specific content, such as medical breakthroughs in oncology, aerospace industry protocols, and intricate fiscal regulations. Achieve a remarkable precision of 99.95% in decoding terminologies specific to fields like astrobiology and neurology, while maintaining a super-fast response rate of 65,000 sentences per second for international collaboration with ultra-low latency. Stringent adherence to the latest GDPR regulations and the implementation of Explainable AI features, complete with interactive visualizations, are mandatory to boost global trust and foster comprehensive cultural comprehension in a secure digital environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual\",\n        \"description\": \"Create a cutting-edge, Transformer-XL-enhanced multilingual translation system integrating the XSum corpus and niche languages like Nepali, Indonesian, and Swahili. It should support extensive evaluation with 45 unique TERQ scores across diverse paths, focusing on intricate domain-specific content and achieving 99.95% precision in specialized fields (astrobiology, neurology) with a speed of 65,000 sentences per second. Compliance with GDPR and Explainable AI with interactive visualizations are mandatory.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"TERQ Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Precision (astrobiology & neurology)\",\n                \"value\": 0.9995\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Response Time\",\n                \"value\": 65000,\n                \"unit\": \"sentences per second\"\n            },\n            {\n                \"name\": \"Latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"XSum\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"The XSum corpus for integrating into the multilingual translation system\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Nepali, Indonesian, & Swahili corpora\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"Niche language corpora for enhancing multilingual support\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL-enhanced\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explanation\": \"Enhanced for multilingual and domain-specific content\"\n            },\n            \"description\": \"A state-of-the-art machine translation model with Transformer-XL for extensive language support\"\n        }\n    ],\n    \"knowledge\": [\n        \"Latest GDPR regulations, Explainable AI with interactive visualizations\",\n        \"Intricate content in oncology, aerospace, and fiscal regulations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"ultra-low latency\"],\n        \"demonstration\": \"Interactive visualizations for understanding translation process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction parsed the user intent as high expertise, defining a complex project with specific requirements, diversity of languages, and performance metrics. However, some details such as specific TERQ scores and deployment endpoint are not provided, which may lower the confidence score.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "As a forward-thinking machine learning project manager, your mandate extends to not only engineer a deployable edge-computing AI system that harnesses Explainable Hypergraph Neural Networks (EHNNs) on a diverse array of Wikipedia and DBpedia datasets, but also to pioneer an innovative Siamese architecture that effectively processes both node and edge attributes in parallel. In addition to striving for a groundbreaking benchmark of at least 99% F1-score on unseen, dynamically evolving label distributions, the project must demonstrate exceptional resilience under strict Raspberry Pi hardware limitations, optimizing for real-world scenarios. Your team is tasked with devising and evaluating ten distinct neighborhood sampling strategies, with a dual objective of maximizing accuracy and minimizing energy expenditure during inference. Moreover, delve into the intricate nuances of five cutting-edge graph convolution techniques, examining their efficiency, scalability, and impact on ultra-low-power environments. Assemble a comprehensive, comparative analysis that pinpoints the optimal optimizations and balances for practical resource-constrained deployments, necessitating a deep understanding of trade-offs in real-world implementation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graphs\",\n        \"description\": \"Engineer a deployable edge-computing AI system using Explainable Hypergraph Neural Networks (EHNNs) on Wikipedia and DBpedia datasets. Implement a Siamese architecture to process node and edge attributes in parallel. Strive for at least 99% F1-score under dynamically evolving label distributions with Raspberry Pi hardware constraints. Conduct an analysis of ten neighborhood sampling strategies for accuracy and energy optimization, and investigate five cutting-edge graph convolution techniques under ultra-low-power conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Energy consumption during inference\",\n                \"value\": null,\n                \"unit\": \"Wh\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wikipedia and DBpedia\",\n            \"modality\": [\n                \"graph\",\n                \"text\"\n            ],\n            \"target_variables\": [\"node labels\", \"dynamic labels\"],\n            \"specification\": {\n                \"dynamic_label_distribution\": true\n            },\n            \"description\": \"A diverse set of Wikipedia and DBpedia data for the project, including both node and edge attributes.\",\n            \"preprocessing\": [\" EHNN feature extraction\", \"Siamese architecture\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Explainable Hypergraph Neural Networks (EHNNs)\",\n            \"family\": \"Hypergraph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": true,\n                \"Siamese architecture\": true\n            },\n            \"description\": \"Utilizes EHNNs for node classification with a Siamese design on Wikipedia and DBpedia datasets.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hypergraph convolutions, Siamese architecture nuances, and graph sampling strategies are key aspects to consider.\",\n        \"Understanding energy-efficient optimizations and trade-offs for ultra-low-power environments.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Neighborhood sampling techniques evaluation\",\n            \"Graph convolution technique benchmarking\"\n        ],\n        \"target_device\": \"Raspberry Pi\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Edge computing\"],\n        \"demonstration\": \"Analysis of optimal resource constraints for practical deployments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the high-level intent (build), the expert-level requirement (high), and the specific details about the machine learning project: area, task, performance metric, dataset properties, and the model. It includes the need for explainable models and ultra-low-power optimizations. However, missing information includes specific neighborhood sampling strategies and graph convolution techniques, as well as deployment endpoint and inference engine details. The confidence score is moderately high due to these assumptions.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager at a premium luxury fashion tech startup embarks on a multifaceted endeavor. They require a cutting-edge, adaptive mobile app, optimized for haute couture, that instantly segments garments in real-time. The app necessitates the integration of a novel, Enhanced EfficientNet (E3+) variant, specifically architected as an ultra-lightweight FCN, tailored to handle the proprietary DeepFashion2 Plus dataset. Key performance indicators include a precision benchmark of over 85% for discerning intricate fabric textures and delicate designer overlays, while maintaining smooth operation, optimized graphics, and a refined, intuitive user interface designed to uphold the brand's image of sophistication and elegance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"luxury fashion tech\",\n        \"description\": \"A project manager at a premium luxury fashion tech startup needs a real-time garment segmentation mobile app for haute couture. The app must integrate an Enhanced EfficientNet (E3+) variant, specifically an ultra-lightweight FCN, for the proprietary DeepFashion2 Plus dataset. Key performance metrics are a precision benchmark over 85% for detecting fabric textures and designer overlays while ensuring smooth operation, optimized graphics, and a user interface reflecting the brand's sophistication.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepFashion2 Plus\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"fabric textures\", \"designer overlays\"],\n            \"specification\": {\n                \"proprietary\": true\n            },\n            \"description\": \"A high-end dataset for haute couture garment segmentation, optimized for the Enhanced EfficientNet (E3+)\",\n            \"preprocessing\": [\"ultra-lightweight preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [\"dataset insights for texture and overlay detection\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced EfficientNet (E3+)\",\n            \"family\": \"EfficientNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ultra-lightweight\": true,\n                \"architected as\": \"ultra-lightweight FCN\"\n            },\n            \"description\": \"An adaptive and lightweight FCN variant for real-time segmentation of garments in the DeepFashion2 Plus dataset\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating E3+ to handle high-resolution details and maintaining real-time processing is crucial.\",\n        \"Smooth operation and visual aesthetics align with luxury brand standards.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"lightweight feature extraction for mobile app\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"optimized for haute couture app store distribution\",\n        \"inference_engine\": [\"GPU-efficient for real-time performance\"],\n        \"demonstration\": \"Smooth and intuitive user experience with precision demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been parsed into a high-level JSON structure reflecting the user's intent for building a complex mobile app and model integration. Expertise is assumed to be high given the complexity of the project. However, some additional specific features or requirements could be fleshed out, such as the exact deployment endpoint, for a full confidence score of 1.0.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager now demands a next-generation BERT-enhanced Q&A platform, exclusively utilizing RoBERTa with Hugging Face Transformers, for the challenging SQuAD 2.0 dataset. The development team must design not just a state-of-the-art model, but one that consistently surpasses the 98% benchmark in F1 score after profound, cross-domain fine-tuning on multiple publicly available and proprietary datasets. This advanced system should not only guarantee near-instantaneous response times (ideally <1.5 seconds), but also exhibit dynamic latency optimization, efficient parallel processing, and be scalable to handle millions of queries per day, all while ensuring minimal latency spikes under varying load conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"Q&A platform\",\n        \"description\": \"The project manager requires a BERT-enhanced Q&A platform using RoBERTa and Hugging Face Transformers. The model should surpass a 98% F1 score on SQuAD 2.0 after cross-domain fine-tuning. It should achieve near-instantaneous response times (<1.5 seconds), dynamic latency optimization, efficient parallel processing, and scalability for millions of daily queries without latency spikes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Response Time\",\n                \"value\": 1.5,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Latency Optimization\",\n                \"value\": \"Dynamic\",\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"Efficiency\",\n                \"value\": \"Parallel processing\",\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"Scalability\",\n                \"value\": \"Millions of queries/day\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"format\": \"JSON-like\",\n                \"dataset_format_version\": \"v2\"\n            },\n            \"description\": \"The platform should be developed for the challenging SQuAD 2.0 dataset with a focus on question answering.\",\n            \"preprocessing\": [\"Cross-domain fine-tuning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-enhanced RoBERTa\",\n            \"family\": \"Hugging Face Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT-RoBERTa\",\n                \"training_algorithm\": \"Fine-tuning\",\n                \"model_state\": \"Cross-domain\"\n            },\n            \"description\": \"A state-of-the-art Q&A model using RoBERTa, aiming to surpass a 98% F1 score after fine-tuning on SQuAD 2.0 and additional datasets.\"\n        }\n    ],\n    \"knowledge\": [\n        \"It is specified to be built using Hugging Face Transformers library.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Profund, cross-domain fine-tuning\"],\n        \"target_device\": \"Cloud or server\",\n        \"deployment_endpoint\": \"N/A - internal API\",\n        \"inference_engine\": [\"Hugging Face Transformers' inference\"],\n        \"demonstration\": \"Dynamic latency optimization and response time under varying loads\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction is clear about the requirements for the advanced Q&A platform, model performance, speed, and scalability. Expertise level is assumed to be high due to the technical nature of the project. The Hugging Face Transformers library and 'deployment endpoint' assumption are made based on the context, but not explicitly stated. Confidence score reflects the certainty of interpreting the user's intent.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for the AI development team, devise an innovative Transformer-XL-enhanced neural machine translation system that integrates the extensive XSum dataset with lesser-known languages like Nepali, Indonesian, and Swahili. Mandate a pinnacle benchmark of 45 distinct TERQ scores, covering a wide range of content, from advanced academic journals to highly specialized domains like nanotechnology patents and climate change policy. The model must exhibit exceptional proficiency, achieving 99.9% precision in deciphering complex scientific jargon within cognitive science and extraterrestrial exploration, while maintaining natural language fluency and processing an impressive 10,000 concurrent sentences per second for instant, global knowledge collaboration. Ensure compliance with the most stringent privacy regulations under GDPR, and design the model to rival human-level transparency, providing users with AI-generated explanations in clear, step-by-step logic to foster international trust.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": [\n            \"advanced academic journals\",\n            \"nanotechnology patents\",\n            \"climate change policy\"\n        ],\n        \"description\": \"Design an innovative Transformer-XL-enhanced neural machine translation system integrating the XSum dataset with Nepali, Indonesian, and Swahili languages. Aim for a benchmark of 45 distinct TERQ scores across various content domains, with a focus on scientific jargon (99.9% precision in cognitive science and extraterrestrial exploration). The model must process 10,000 concurrent sentences per second, comply with GDPR, and exhibit human-level transparency with AI-generated explanations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"TERQ scores\",\n                \"value\": 45\n            },\n            {\n                \"name\": \"Precision (cognitive science and extraterrestrial exploration)\",\n                \"value\": 0.999\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Concurrent sentence processing rate\",\n                \"value\": 10000,\n                \"unit\": \"sentences per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"XSum\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"An extensive dataset for enhancing the machine translation system with the Transformer-XL architecture.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Nepali, Indonesian, and Swahili translations of XSum\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"Translations of the XSum dataset for integration with lesser-known languages.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL-enhanced Neural Machine Translation System\",\n            \"family\": \"Transformer-XL\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"complexity\": {\n                    \"latency\": \"optimized for real-time performance\",\n                    \"transparency\": \"high\"\n                },\n                \"model_components\": [\"Transformer-XL\", \"cross-lingual capabilities\"]\n            },\n            \"description\": \"A state-of-the-art machine translation system with superior performance in deciphering complex jargon and high concurrency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"privacy-preserving techniques (GDPR compliant)\",\n            \"AI-generated explanations\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"AI-generated explanations with real-time translation examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided detailed requirements for a complex machine learning project, including a high-level intent, clear specifications, and target domains. The high expertise level suggests that the user understands the intricacies of the project. However, the deployment endpoint and some performance metrics (such as TERQ scores) were not provided explicitly, so those areas are left open to assumptions based on context.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Devise an advanced, precision-driven translation software incorporating the OpenSubtitles dataset and Arctic-Speech-Net's cutting-edge ASR. Design a bespoke BERT-MultiMT variant specifically tailored for translating London's colloquial slang into Mandarin, accounting for sub-regional variations in East London. Implement a system that integrates comprehensive visual context analysis, demands a temporal accuracy benchmark of 99.99% and surpasses it with real-time translation. Require the system to be trained on a massive, globally diverse conversation corpus capturing intricate regional nuances and colloquial interactions, with an emphasis on preserving dialect authenticity. Additionally, incorporate adaptive learning to facilitate seamless, unsupervised integration into live video chats, ensuring semantic fidelity and precise reproduction of complex idiomatic expressions in spontaneous conversations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"translation software\",\n        \"description\": \"Create an advanced translation software using OpenSubtitles dataset and Arctic-Speech-Net's ASR. Develop a custom BERT-MultiMT variant for translating London's colloquial slang into Mandarin, considering sub-regional variations in East London. The system must have a temporal accuracy of at least 99.99%, real-time translation, and use comprehensive visual context analysis. Training should be on a large, globally diverse conversation corpus capturing regional nuances and colloquial interactions, preserving dialect authenticity. Incorporate adaptive learning for seamless integration into live video chats with semantic fidelity and precise rendering of complex idiomatic expressions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"temporal accuracy\",\n                \"value\": 0.9999\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time translation\",\n                \"value\": null,\n                \"unit\": \"real-time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translations\"],\n            \"specification\": {\n                \"domain\": \"conversational corpus\"\n            },\n            \"description\": \"A large corpus of subtitles for training translation model, capturing global conversations.\",\n            \"preprocessing\": [\"contextual data cleaning\", \"tokenizer\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"sentence distribution across regions\"\n            ],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"Arctic-Speech-Net\",\n            \"modality\": [\"audio\"],\n            \"target_variables\": [\"transcriptions\"],\n            \"specification\": {\n                \"ASR quality\": \"state-of-the-art\"\n            },\n            \"description\": \"Advanced ASR for extracting speech-to-text data in London's colloquial slang\",\n            \"preprocessing\": [\"speech-to-text conversion\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-MultiMT (London colloquial slang to Mandarin)\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"customization\": \"tailored for London slang and Mandarin translation, accounting for regional variations\",\n                \"architecture\": \"Bespoke BERT-MultiMT\"\n            },\n            \"description\": \"A specialized model trained to handle London colloquial slang and preserve dialect authenticity.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Emphasis on preserving dialect authenticity and semantic fidelity in real-time conversations\"\n        },\n        {\n            \"content\": \"Adaptive learning for unsupervised integration into live video chats\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual analysis\",\n            \"dialect preservation\"\n        ],\n        \"target_device\": \"cloud-based for real-time performance\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"interactive, real-time translation during live video chats\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction was clear on the advanced translation system requirements, with a focus on high accuracy, real-time, dialect preservation, and adaptable learning. However, missing details are the target device, deployment endpoint, and a definitive score for the temporal accuracy. The high expertise level suggests the system's complexity, but without more specific information, the confidence score reflects a need for clarification.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a cutting-edge, cross-lingual affect-aware translation solution that unites ParaCrawl with a sophisticated fusion of transformer-based encoders, optimizing for English-to-Dutch conversions. Demand not just precision, but granular emotional consistency, and cultural fidelity, ensuring seamless adaptation to colloquial expressions and idiomatic nuances. The system must also incorporate a real-time learning mechanism and scalable architecture to handle growing datasets in multiple domains.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual affect-aware translation\",\n        \"description\": \"Develop a cutting-edge solution that combines ParaCrawl with transformer-based encoders for English-to-Dutch translations, prioritizing precision, emotional consistency, cultural fidelity, and handling colloquial expressions and idiomatic nuances. The system must include real-time learning and a scalable architecture for growing datasets across multiple domains.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"emotional consistency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cultural fidelity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language\": {\n                    \"source\": \"English\",\n                    \"target\": \"Dutch\"\n                },\n                \"size\": {\n                    \"raw\": null,\n                    \"preprocessed\": null\n                }\n            },\n            \"description\": \"A large-scale, cross-lingual parallel corpus for training the machine translation system.\",\n            \"preprocessing\": [\n                \"emotion detection\",\n                \"linguistic analysis\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based encoder fusion\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": {\n                    \"type\": \"sequence-to-sequence\",\n                    \"encoder\": \"multi-encoder fusion\",\n                    \"decoder\": \"attention-based\"\n                },\n                \"optimization\": {\n                    \"task\": \"cross-lingual affective translation\",\n                    \"loss function\": \"combined (e.g., BCE, MSELoss, etc.)\"\n                }\n            },\n            \"description\": \"A cutting-edge translation model that fuses transformer encoders for English-to-Dutch translation, targeting emotional consistency and cultural fidelity.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time learning: use reinforcement learning for continuous adaptation\",\n        \"Scalable architecture: leverage distributed training and dynamic resource allocation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"cross-lingual feature embeddings\",\n            \"affect-aware loss functions\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom translation service API\",\n        \"inference_engine\": [\"TPU\", \"GPU\"],\n        \"demonstration\": \"Interactive demo showcasing emotional consistency and real-world examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON includes the user's intent to build a high-level model, specific problem details for a cross-lingual affect-aware translation task, performance metrics for precision and cultural fidelity, and requirements for a scalable architecture and real-time learning. The model architecture and datasets were derived from the instruction. However, without concrete metric targets for precision or cultural fidelity, some fields are left as null. Also, the deployment endpoint and inference engine options are hypothetical based on the context.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For the luxury gourmet marketplace, develop a highly efficient and scalable deep learning pipeline utilizing the state-of-the-art YOLOv5.1+, incorporating a novel constraint of unsupervised anomaly detection. The system must undergo extensive fine-tuning on the exclusive \"BoutiqueFoods Premium Diversity Dataset,\" encompassing an unprecedented 200 sub-genre categories, ensuring a consistent precision of 97% and recall of 95% or above. Furthermore, integrate a real-time inventory management system capable of processing high-definition images in parallel, delivering near-instantaneous updates (ideally within 1 millisecond) for each artisanal item, while maintaining a fault-tolerant database to handle peak traffic during food festivals.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury gourmet marketplace\",\n        \"description\": \"Develop a highly efficient and scalable deep learning pipeline using YOLOv5.1+ for object detection, with a focus on unsupervised anomaly detection. The system must be fine-tuned on the BoutiqueFoods Premium Diversity Dataset with 200 sub-genre categories, targeting consistent precision and recall of at least 97% and 95%, respectively.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.001,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoutiqueFoods Premium Diversity Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"categories\": 200,\n                \"sub-genre categories\": \"exclusive\"\n            },\n            \"description\": \"An extensive dataset for luxury gourmet marketplace, focusing on 200 sub-genre categories.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5.1+\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine-tuned for\": \"unsupervised anomaly detection\",\n                \"dataset\": \"BoutiqueFoods Premium Diversity Dataset\"\n            },\n            \"description\": \"State-of-the-art deep learning pipeline for object detection with emphasis on anomaly detection\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time, high-performance\",\n        \"deployment_endpoint\": {\n            \"type\": \"cloud-based\",\n            \"performance\": \"high-traffic, fault-tolerant\"\n        },\n        \"inference_engine\": [\"high-definition image processing\", \"parallel processing\"],\n        \"demonstration\": \"near-instantaneous updates (1 ms) for artisanal items\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the provided instruction, inferring a high expertise level for the user. It includes a detailed 'problem' section with fine-tuning requirements and desired performance metrics, and a 'service' section addressing real-time inventory management and system scalability. However, the absence of specific performance metrics for anomaly detection and the assumption of fault-tolerant database is based on the given context.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For the advanced urban transportation management initiative, the machine learning team must develop a novel YOLO model, trained exclusively on the Cityscapes++ dataset, which attains >95% precision in simultaneous detection of vehicles, pedestrians, and traffic signs, while integrating a real-time adaptive neural network architecture that enhances energy efficiency by optimizing for low-power devices. This model must demonstrate a consistent 30 FPS runtime under extreme weather conditions and dynamic lighting, ensuring optimal performance in a constantly evolving adversarial urban environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"urban transportation management\",\n        \"description\": \"The machine learning team must develop a novel YOLO model for advanced urban transportation management. The model must be trained exclusively on the Cityscapes++ dataset, targeting vehicles, pedestrians, and traffic signs with a precision target of >95%. It should incorporate a real-time adaptive neural network architecture, optimizing for low-power devices, while maintaining a consistent 30 FPS runtime under extreme weather conditions and dynamic lighting.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95,\n                \"target\": \"vehicles, pedestrians, traffic signs\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 30.0,\n                \"unit\": \"FPS\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"optimized for low-power devices\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes++\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"vehicles\",\n                \"pedestrians\",\n                \"traffic signs\"\n            ],\n            \"specification\": {\n                \"customization\": \"exclusive training for urban transportation management\"\n            },\n            \"description\": \"A dataset used for training the YOLO model, focusing on vehicles, pedestrians, and traffic signs in urban environments.\",\n            \"preprocessing\": [\"adaptation for real-time and low-power requirements\"],\n            \"augmentation\": [\"adaptive to dynamic lighting and weather conditions\"],\n            \"visualization\": [\"model performance under extreme scenarios\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Novel YOLO model with real-time adaptive architecture\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"real-time adaptive\",\n                \"energy_efficiency\": \"optimized for low-power devices\"\n            },\n            \"description\": \"A state-of-the-art YOLO model specifically designed for urban transportation management, with targeted precision and real-time performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"real-time and low-power adaptation\"],\n        \"target_device\": \"low-power devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time scenarios\"],\n        \"demonstration\": \"model performance under extreme weather and lighting conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the intent to build a model, the user's high expertise level, the details of the problem and performance requirements, the Cityscapes++ dataset with its specified tasks and metrics, and requirements for real-time performance, low-power optimization, and robustness under extreme conditions. However, a specific target device is not provided, nor is a deployment endpoint or inference engine mentioned, which might vary based on the user's infrastructure or the project's requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a forward-thinking project manager, your interdisciplinary ML team has been commissioned to develop a state-of-the-art, deep-learning ensemble model for the complex ILI dataset, comprising a vast, temporally intricate network of real-world data partitions (train, validation, and testing with varying granularities). Each input sequence, characterized by 36 historical points and 7 diverse dimensions (INPUT_SEQ_LEN=36, INPUT_DIM=7), necessitates the creation of a highly adaptable and interpretable forecasting mechanism capable of seamlessly capturing multi-scale dependencies and seasonality.\n\nYour mandate expands to not only engineer a predictive model predicting subsequent sequences of 24 time steps and 7 elements (PRED_SEQ_LEN=24, PRED_DIM=7) with extraordinary precision, but also to devise a hybrid model integrating LSTM, Transformer, and attention mechanisms. In addition to delivering exceptional accuracy, ensure the model demonstrates robustness under concept drift and is benchmarked against advanced metrics, including EWMA-MSE with a rolling window adaptation and customized RMAPE variants for imbalanced data.\n\nMoreover, the project must prioritize computational efficiency by optimizing parallel processing, model pruning, and quantization techniques, while comprehensive exploratory data analysis and a comparative study of feature selection methods are mandatory. The team must submit a detailed report outlining the experimentation process and justify the chosen model architecture and hyperparameters, all while maintaining a focus on explainability and interpretability for practical business insights.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-world, temporal data\",\n        \"description\": \"Develop a state-of-the-art deep-learning ensemble model for the ILI dataset, focusing on complex, temporally intricate data with varying granularities. The model should handle input sequences with 36 historical points and 7 dimensions, aiming for 24-step ahead forecasting, capturing multi-scale dependencies and seasonality.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"EWMA-MSE\",\n                \"value\": null,\n                \"rolling_window\": true\n            },\n            {\n                \"name\": \"RMAPE (customized for imbalanced data)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Computational Efficiency\",\n                \"value\": {\n                    \"Parallel Processing\": true,\n                    \"Model Pruning\": true,\n                    \"Quantization\": true\n                }\n            }\n        ],\n        \"feature_selection\": true,\n        \"exploratory_data_analysis\": true\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 36,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 24,\n                \"PRED_DIM\": 7\n            },\n            \"description\": \"A large, temporally complex dataset with varying granularities for train, validation, and testing partitions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ensemble model\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"components\": [\n                    {\n                        \"model\": \"LSTM\",\n                        \"attention\": true\n                    },\n                    {\n                        \"model\": \"Transformer\",\n                        \"attention\": true\n                    }\n                ],\n                \"hyperparameters\": {\n                    \"LSTM\": {\n                        \"units\": \"custom\",\n                        \"dropout\": null\n                    },\n                    \"Transformer\": {\n                        \"d_model\": null,\n                        \"heads\": null\n                    }\n                }\n            },\n            \"description\": \"A hybrid model integrating LSTM, Transformer, and attention mechanisms for forecasting complex sequences.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"explainability\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the provided project requirements, capturing the details about the ensemble model, ILI dataset, task, and complexity optimizations. It accounts for feature selection, EDA, custom performance metrics, and explainability. The user's high expertise level suggests a strong understanding, though some values for metrics and hyperparameters are currently placeholders.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "You're now in charge of a specialized machine learning project that entails developing a cutting-edge time-series forecasting model for a niche dataset - \"Oceanographic Turbulence Data,\" segregated into train, validation, and test subsets. This unique dataset contains intricate, non-stationary patterns with 45 specialized oceanographic indicators (INPUT_SEQ_LEN=144), requiring exceptional handling of sub-daily variations, interannual trends, and extreme anomalies. The task is to engineer a hybrid model blending transformer architectures, LSTMs, and attention mechanisms, ensuring explainability through SHAP feature importance, while optimizing for both weighted root mean squared error (WRMSE) and F1 score for rare event prediction. The model must be deployed efficiently within tight memory constraints, and its detailed design, including memory-efficient optimization strategies, should be presented in a comprehensive technical report alongside the codebase and visual explainability demos.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"oceanography\",\n        \"description\": \"Develop a cutting-edge time-series forecasting model for the 'Oceanographic Turbulence Data' with a segregated train, validation, and test subset. The model should handle non-stationary patterns, sub-daily variations, interannual trends, and extreme anomalies. Targeted architecture is a hybrid of transformer, LSTMs, and attention mechanisms with explainability through SHAP feature importance. Optimization should focus on WRMSE and F1 score for rare event prediction, while maintaining efficient deployment within tight memory constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"WRMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory efficiency\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Oceanographic Turbulence Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"45 specialized oceanographic indicators\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 144,\n                \"data characteristics\": {\n                    \"non-stationary\": true,\n                    \"sub-daily variations\": true,\n                    \"interannual trends\": true,\n                    \"extreme anomalies\": true,\n                    \"indicators\": 45\n                }\n            },\n            \"description\": \"Intricate dataset containing oceanographic turbulence data split into train, validation, and test subsets.\",\n            \"preprocessing\": [\n                \"non-stationarity handling\",\n                \"extreme anomaly detection\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"data pattern analysis\",\n                \"exploratory data visualization\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Time-Series Forecasting Model\",\n            \"family\": \"Transformers, LSTMs, and Attention Mechanisms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": \"SHAP feature importance\",\n                \"target metrics\": [\"WRMSE\", \"F1 score\"],\n                \"memory-efficient optimization\": true\n            },\n            \"description\": \"A cutting-edge model blending transformer, LSTM, and attention mechanisms for explainable forecasting with oceanographic turbulence data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Oceanographic peculiarities: sub-daily patterns, interannual trends, extreme anomalies\",\n        \"SHAP for feature importance in explainability\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"memory optimization strategies\",\n            \"model deployment\"\n        ],\n        \"target_device\": \"efficient deployment within tight memory constraints\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"visual explainability demos\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the provided instructions, detailing the high level of expertise, the complexities in handling oceanographic data, the specified model components and performance optimization, and the requirement for a comprehensive report and visual explainability. Missing specifics like WRMSE, F1 score targets, memory-efficient strategies, and deployment endpoint have been left out as they would typically be filled based on the model development process and specific implementation details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project director, your task is to develop a sophisticated neural machine translation system leveraging the cutting-edge M2M-100 architecture. Utilize a custom dataset, 'Global Conversational English-Japanese Corpus v2.0', which demands not only precise formal translation (98%+ precision in Gengo Business Style and Local Slang categories) but also capturing cultural idiosyncrasies and sub regional variations. Mandate the system to dynamically adapt based on user interactions, with a real-time learning mechanism, and design an intuitive, touch-enabled interface catering to diverse user demographics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"global conversational translation\",\n        \"description\": \"Develop a sophisticated neural machine translation system using the M2M-100 architecture. The dataset 'Global Conversational English-Japanese Corpus v2.0' should yield at least 98% precision in formal translation (Gengo Business Style and Local Slang). The system must dynamically adapt based on user interactions with a real-time learning mechanism. An intuitive, touch-enabled interface for diverse user demographics is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision (Gengo Business Style)\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"precision (Local Slang)\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Global Conversational English-Japanese Corpus v2.0\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"customized\": true,\n                \"demands\": [\n                    \"precise formal translation\",\n                    \"cultural idiosyncrasies\",\n                    \"sub-regional variations\"\n                ]\n            },\n            \"description\": \"A custom dataset for machine translation, focusing on capturing a broad range of English-Japanese conversation with high precision requirements.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M2M-100 architecture\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"M2M-100\",\n                \"adaptability\": \"real-time learning\",\n                \"interface_type\": \"touch-enabled\"\n            },\n            \"description\": \"A sophisticated, real-time learning neural machine translation system based on the M2M-100 architecture.\"\n        }\n    ],\n    \"knowledge\": [\n        \"System should dynamically adapt to user interactions\",\n        \"Intuitive interface catering to diverse demographics\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Real-time learning mechanism\"\n        ],\n        \"demonstration\": \"touch-enabled, user-adaptive interface\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction clearly defines a build task, with the user having high expertise in the field. The problem description covers the target dataset, performance metrics, and real-time learning requirements. The 'service' section includes touch-enabled interface, target device is not specified, and highlights the importance of real-time learning. Some details may need additional clarification, like the target device for deployment.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, commission an innovative AI-based image recognition system for an expansive botanical archive, requiring not only the deployment of ResNet or EfficientNet with advanced transfer learning and ensemble strategies, but also the integration of progressive neural network fusion. Elaborate on the sequential steps for fine-tuning using a meticulously stratified subset of the Plant Seedlings Dataset, while analyzing the impact on both precision and latency. Devise a comprehensive evaluation plan that includes cross-validation techniques, transfer learning robustness tests, and a comparison of performance against the latest academic benchmarks in real-world scenarios, ensuring generalization and scalability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"botanical archive\",\n        \"description\": \"Develop an innovative AI-based image recognition system for an expansive botanical archive. Use ResNet or EfficientNet with advanced transfer learning and ensemble strategies, integrating progressive neural network fusion. Fine-tune with a stratified subset of the Plant Seedlings Dataset, while assessing the impact on precision and latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Seedlings Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"stratification\": \"meticulous\",\n                \"subset_description\": \"stratified for fine-tuning\"\n            },\n            \"description\": \"A dataset for botanical image recognition, to be fine-tuned with a stratified subset for in-depth analysis.\",\n            \"preprocessing\": [\n                \"advanced data cleaning\",\n                \"image resizing\",\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\"random crop\", \"horizontal flip\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNet or EfficientNet\",\n            \"family\": \"deep convolutional neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": \"advanced\",\n                \"ensemble_strategies\": [\"ensembling models\", \"weighted voting\"],\n                \"fusion_techniques\": \"progressive\"\n            },\n            \"description\": \"Innovative AI image recognition system using ResNet or EfficientNet with advanced features and ensemble methods.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"adaptation to botanical domain\"],\n        \"target_device\": \"potentially, cloud for scalability\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated\", \"optimized for real-time performance\"],\n        \"demonstration\": \"evaluation against real-world benchmarks and scalability tests\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the core requirements of the AI project, including specific model architecture and strategies, dataset usage, and evaluation methods. The expert level expertise suggests a high understanding of the requirements. However, missing specific metrics for precision and latency (indicated by null values), as well as detailed deployment endpoint and inference engine choices, could be filled based on assumptions or further clarification from the user.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "As a Project Manager for the AI research team, task the development of a specialized CNN architecture, specifically tailored for distinguishing between 100 distinct and complex subgenres within the extremely detailed CIFAR-100 dataset. Mandate the incorporation of the latest Tiny ImageNet pre-trained models, precision-oriented hyperparameter tuning, and innovative batch-by-batch mixup augmentation. In addition, demand a minimal error rate below 0.1%, ensuring not only dominance over existing benchmarks but also seamless integration with existing infrastructures for future expansion and real-world deployment in volatile market conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"artificial intelligence research\",\n        \"description\": \"As a Project Manager, task the development of a specialized CNN architecture for distinguishing among 100 complex subgenres in the CIFAR-100 dataset. Incorporate Tiny ImageNet pre-trained models, perform precision-focused hyperparameter tuning, and use innovative batch-by-batch mixup augmentation. Demand an error rate below 0.1% for superior performance and seamless integration with future infrastructures for real-world deployment in volatile market conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"error rate\",\n                \"value\": 0.001\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CIFAR-100\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_classes\": 100,\n                \"complexity\": \"detailed\"\n            },\n            \"description\": \"CIFAR-100 dataset for image classification task, containing 100 distinct and complex subgenres.\",\n            \"preprocessing\": [\"Tiny ImageNet pre-trained models\"],\n            \"augmentation\": [\"batch-by-batch mixup\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Specialized CNN\",\n            \"family\": \"CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pre-trained\": \"Tiny ImageNet\",\n                \"hyperparameter_tuning\": \"precision-oriented\"\n            },\n            \"description\": \"A CNN architecture designed for 100 complex subgenres in CIFAR-100, with advanced techniques for improved accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"seamless integration with existing infrastructures, for future expansion and real-world deployment in volatile market conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction indicates a clear project task, specifying a specialized CNN model for a challenging classification problem, advanced techniques, and strict performance requirements. The user's high expertise suggests they expect detailed and accurate implementation. The only missing information might be the target device and deployment details, which are usually not provided at the planning stage.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project lead for a trailblazing AI research team, you've been tasked with developing an innovative, zero-carbon smart grid management system. Leveraging the recently obtained Oceanographic Dynamics dataset, which is an immense, ultra-detailed archive (spanning decades) with an intricate mix of real-time oceanographic patterns, rare meteorological events, and encrypted sequences (INPUT_SEQ_LEN=512, INPUT_DIM=70,000), the challenge demands the integration of state-of-the-art, explainable AI algorithms.\n\nYour objective is to architect a multi-layered, secure forecasting framework that combines the RNN-XOR technique with cutting-edge quantum-inspired neural networks, seamlessly integrating edge and cloud computing for scalability. This architecture must handle the complex energy dynamics of marine sources (hydro, wave, and tidal), forecasting both the next 512-step production profiles (PRED_SEQ_LEN=512, PRED_DIM=70,000) and the potential bottlenecks that could disrupt energy flow and stability across the grid.\n\nTo ensure robustness, not only evaluate the model using rigorous Wasserstein distance and asymmetric uncertainty metrics, but also demonstrate resilience in the face of unforeseen environmental challenges, such as extreme weather events. In your deliverables, provide a step-by-step explanation of your algorithm's decision-making rationale, including the trade-offs made for precision and adaptability. Furthermore, highlight the environmental benefits of your model, and explore its practical applications in marine resource management for sustainable urban planning and energy security.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart grid management\",\n        \"description\": \"Develop a zero-carbon smart grid management system using the Oceanographic Dynamics dataset. The system should combine RNN-XOR technique with quantum-inspired neural networks and integrate edge and cloud computing. Forecast 512-step production profiles and identify potential bottlenecks while handling marine energy sources and extreme weather events.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Wasserstein distance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Asymmetric uncertainty metrics\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Oceanographic Dynamics\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 512,\n                \"INPUT_DIM\": 70000,\n                \"PRED_SEQ_LEN\": 512,\n                \"PRED_DIM\": 70000\n            },\n            \"description\": \"An immense, ultra-detailed dataset with real-time oceanographic patterns, rare meteorological events, and encrypted sequences.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-XOR with quantum-inspired neural networks\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": true,\n                \"quantum_inspired_features\": null,\n                \"edge_and_cloud_integration\": true\n            },\n            \"description\": \"A multi-layered forecasting framework integrating RNN-XOR and quantum-inspired neural networks for secure grid management.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Trade-offs made for precision and adaptability will be clearly explained.\"\n        },\n        {\n            \"description\": \"Environmental benefits and practical applications in marine resource management and sustainable urban planning will be explored.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge-cloud hybrid\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"quantum-inspired\"],\n        \"demonstration\": {\n            \"explanatory_steps\": \"Demonstrating decision-making and adaptability\",\n            \"unforeseen_cases\": \"Resilience to extreme weather events\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided detailed details for a complex AI project in smart grid management. The high expertise level indicates a deep understanding of the task. However, specific metrics and deployment details are not included. A high confidence can be assumed based on the scope and requirements outlined.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "For the high-caliber machine learning team, develop a cutting-edge text classifier specialized in differentiating subtle nuances of human and AI-generated content, particularly isolating works from advanced chatbots and GPT iterations. Incorporate innovative methods like anomaly detection based on deep learning, relentless adversarial training through differential privacy, and real-time evaluation of concept drift using driftwood algorithm. Strive for impressive precision and recall, ensuring model interpretability, fairness in bias mitigation, and rigorous benchmarking against the bleeding-edge competitors while maintaining a stringent latency requirement of 100 milliseconds per prediction, all within the context of an energy-efficient, scalable architecture.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"detecting human vs AI-generated content\",\n        \"description\": \"Develop a cutting-edge text classifier for the high-caliber ML team to distinguish subtle nuances in human and AI-generated content, focusing on works from advanced chatbots and GPT iterations. Use advanced methods like anomaly detection, adversarial training with differential privacy, and real-time concept drift evaluation with driftwood algorithm.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"metrics/energy\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"custom\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"AI-generated content\", \"human-generated content\"],\n            \"specification\": {\n                \"data_distribution\": \"imbalance\",\n                \"chatbot品种\": [\"advanced\", \"GPT iterations\"]\n            },\n            \"description\": \"A dataset consisting of human and AI-generated text, with a focus on advanced chatbots and GPT iterations, to train and evaluate the model.\",\n            \"preprocessing\": [\n                \"lemmatization\",\n                \"stopword removal\",\n                \"anomaly detection\"\n            ],\n            \"augmentation\": [\"adversarial examples\"],\n            \"visualization\": [\"concept drift analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep Learning (e.g., Transformers or BERT)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"method\": [\"anomaly detection\", \"adversarial training\"],\n                \"privacy_protection\": \"differential privacy\",\n                \"concept_drift_detection\": \"driftwood algorithm\"\n            },\n            \"description\": \"An innovative text classifier using advanced machine learning techniques and high interpretability standards.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Anomaly detection: detect unusual patterns in AI-generated content\",\n        \"Adversarial training: enhances robustness against adversarial examples\",\n        \"Differential privacy: ensures model fairness while protecting individual privacy\",\n        \"Driftwood algorithm: real-time concept drift monitoring\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for latency\"],\n        \"demonstration\": \"Explanation of model interpretability and bias mitigation techniques\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed to reflect the high-level requirements, including advanced techniques and performance metrics. The 'expertise' is set to 'high' due to the complexity of the tasks, and the 'service' section highlights the requirement for energy efficiency, scalability, and strict latency. However, some missing aspects are the specific model name, benchmarking details, and an exact deployment endpoint, which would be fillable based on more detailed instructions or collaboration with the user.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "As a seasoned project director for the elite AI research division, your mandate is to develop a TextAuthenticity Classifier utilizing the intricate DAIGT-DeepFusion dataset. Emphasize on advanced pipeline, including data preprocessing, diverse augmentation strategies, concurrent fine-tuning on multiple architectures, and intricate performance metrics (F1-score, AUC-PR, and balanced accuracy). Mandate involves iterative validation, rigorous comparative analysis with cutting-edge baselines, and continuous optimization for real-world robustness. Each step must adhere to Explainable AI principles and cater to low-resource domain adaptability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"Develop a TextAuthenticity Classifier using the DAIGT-DeepFusion dataset. The project requires an advanced pipeline with data preprocessing, diverse augmentation strategies, concurrent fine-tuning on multiple architectures, and sophisticated performance metrics (F1-score, AUC-PR, balanced accuracy). Mandates include iterative validation, comparative analysis with cutting-edge baselines, and continuous optimization for real-world robustness. The solution should adhere to Explainable AI principles and cater to low-resource domain adaptation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-PR\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Balanced accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT-DeepFusion\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Text authenticity\"],\n            \"specification\": null,\n            \"description\": \"An intricate dataset for TextAuthenticity classification, requiring preprocessing and advanced augmentation strategies.\",\n            \"preprocessing\": [\"advanced data preprocessing techniques\"],\n            \"augmentation\": [\n                \"diverse augmentation strategies\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Text Classifier Pipeline\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"fine-tuning on multiple architectures\"],\n                \"explainability\": \"Explainable AI principles\"\n            },\n            \"description\": \"A classifier designed for TextAuthenticity with a focus on low-resource domain adaptability and real-world robustness.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Iterative validation, low-resource domain adaptability, and adherence to Explainable AI principles.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Advanced data preprocessing and augmentation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Catered to low-resource domain adaptation and real-world robustness\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed with a high confidence level as it specifies a complex project involving multiple aspects of a machine learning pipeline. The 'expertise' level is assumed to be high based on the request for advanced techniques and low-resource adaptability. Missing metric values would be obtained through model training, but that part is not included in the JSON. Other assumptions include the need for practical application details that might not be covered in the JSON due to the lack of specific information.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "machine translation", "instruction": "Construct an innovative neural machine translation system using the massive OpenSubtitles dataset, focusing on real-time, spontaneous English-Mandarin dialogue. The model, built upon a cutting-edge Transformer architecture incorporating M-BERT, must not only handle casual speech, slang, and colloquial expressions but also decode and produce accurate translations that reflect local idioms and colloquialisms. To ensure peak performance, propose a rigorous, iterative training protocol involving domain adaptation, back-translation, and continuous evaluation with culturally diverse, in-the-wild data, while accounting for low-resource Mandarin dialects. Additionally, design a user-friendly interface that streamlines error detection and correction for user feedback.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"real-time, spontaneous English-Mandarin dialogue\",\n        \"description\": \"Construct an innovative neural machine translation system using the OpenSubtitles dataset. The model should base on an advanced Transformer architecture with M-BERT, capable of translating casual speech, slang, and colloquial expressions, reflecting local idioms and colloquialisms. The training process should involve domain adaptation, back-translation, and continuous evaluation with diverse data, considering low-resource Mandarin dialects.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"BLEU score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"English text\", \"Mandarin text\"],\n            \"specification\": {\n                \"languages\": [\"English\", \"Mandarin\"],\n                \"dialogue_type\": \"real-time, spontaneous\"\n            },\n            \"description\": \"A large-scale dataset for English-Mandarin machine translation, focusing on conversational content.\",\n            \"preprocessing\": [\n                \"Tokenization\",\n                \"Lowercasing\",\n                \"Domain-specific cleaning\"\n            ],\n            \"augmentation\": [\n                \"Back-translation\",\n                \"Data diversification\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Innovative Transformer with M-BERT\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer with M-BERT\",\n                \"specifics\": \"Covers casual speech, slang, and colloquial expressions\"\n            },\n            \"description\": \"A state-of-the-art model for real-time, spontaneous English-Mandarin translation using M-BERT.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Domain adaptation\",\n            \"Error detection and correction\"\n        ],\n        \"target_device\": \"real-time environment\",\n        \"deployment_endpoint\": \"API-based, user-friendly\",\n        \"inference_engine\": [\"Optimized for performance and real-time translation\"],\n        \"demonstration\": {\n            \"user feedback mechanism\": \"Streamlines error correction through user interface\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed for a building task, assuming a high level of expertise due to the complexity and requirements. Performance metrics and data preprocessing steps were included. The cutting-edge Transformer with M-BERT was appropriately matched to the task. However, specific performance expectations and some service details are missing, as they are not provided in the instruction.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project director of a specialized AI-driven team working on temporal sequence analysis, your mandate is to conceive an innovative, comprehensible, and scalable forecasting model for the complex Energy Market Dynamics dataset. This dataset, consisting of 96 sequential inputs parsed through 900 unique attributes (INPUT_SEQ_LEN=96, INPUT_DIM=900, with 5 newly incorporated external factors), has been meticulously divided into fine-grained training, validation, and testing segments to ensure real-world applicability. The task is to design a cutting-edge deep learning architecture that harmoniously integrates intricate annual and weekly cycles, multiple intertwined time scales, and real-time meteorological influences, specifically forecasting a 108-step-ahead prediction (PRED_SEQ_LEN=108, PRED_DIM=900) while accounting for long-term fluctuations and sudden variations. The model must excel in minimizing root mean squared error (RMSE) and mean absolute percentage error (MAPE), demonstrating robustness, resilience, and adaptability in handling high-resolution, swiftly changing data across all partitions, all while optimizing for computational efficiency, model transparency, and maintaining a balance between novel techniques and practical implementation for this competitive industry.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy market dynamics\",\n        \"description\": \"The project aims to conceive an innovative, interpretable, and scalable forecasting model for the Energy Market Dynamics dataset. The dataset has 96 sequential inputs with 900 unique attributes, including 5 external factors, and is split into training, validation, and testing sets. The goal is to design a deep learning architecture that incorporates annual and weekly cycles, multiple time scales, and real-time meteorological influences, with a 108-step-ahead prediction task. Key performance metrics are RMSE and MAPE, while computational efficiency, model transparency, and practical implementation balance are important.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            },\n            {\n                \"name\": \"model transparency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"model scalability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Energy Market Dynamics\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 900,\n                \"new external factors\": 5\n            },\n            \"description\": \"A dataset for time-series forecasting in the energy market, with detailed seasonal and meteorological factors, and fine-grained data partitions.\",\n            \"preprocessing\": [\"handling external factors\", \" seasonality adjustments\"],\n            \"augmentation\": [\"temporal noise injection\"],\n            \"visualization\": [\"input and output sequence patterns\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Innovative Deep Learning Architecture\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture details\": \"harmoniously integrates annual and weekly cycles, multiple time scales, meteorological influences, and novel techniques for high-resolution forecasting\"\n            },\n            \"description\": \"A state-of-the-art model designed for energy market forecasting, targeting long-term stability, quick adaptation, and practical implementation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider industry competition and regulatory requirements for explainability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"integration of external factors\", \"time scale feature extraction\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"API server for real-time predictions\",\n        \"inference_engine\": [\"GPU-optimized inference\"],\n        \"demonstration\": \"Interactive visualizations of forecasts and key performance metrics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has detailed requirements for a complex project, indicating a high level of expertise. All sections were accurately filled, from dataset characteristics to model design and deployment considerations. Missing metric values will be obtained after model training or specified by the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop an AI system for the machine learning team, focusing on a niche domain of Heterogeneous Information Network (HIN) analysis derived from a specialized scholarly subset of Wikipedia and DBpedia. Mandate the integration of multi-faceted node and edge properties in a Siamese Hypergraph Neural Network (SHNN) design, with a requirement to achieve a minimum precision of 97% for novel label predictions. Prioritize efficiency by optimizing the model for real-time deployment on resource-constrained Raspberry Pi devices, considering edge computation on the go. Conduct a meticulous investigation into the impact of novel neighborhood sampling algorithms and a comparative analysis of distinct graph convolution strategies, highlighting the fine balance between accuracy improvements and hardware constraints. Ensure the research output includes empirical evidence and actionable recommendations for continuous advancements in the field, using advanced visualization techniques to enhance interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"scholarly research\",\n        \"description\": \"Develop an AI system for Heterogeneous Information Network (HIN) analysis from a specialized scholarly subset of Wikipedia and DBpedia. The system should utilize a Siamese Hypergraph Neural Network (SHNN) with multi-faceted node and edge properties. It must achieve a minimum precision of 97% for novel label predictions and be optimized for real-time deployment on Raspberry Pi devices, considering edge computation. Investigate novel neighborhood sampling algorithms and graph convolution strategies while ensuring interpretability through advanced visualization techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"seconds\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory size\",\n                \"unit\": \"MB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Specialized scholarly subset of Wikipedia and DBpedia\",\n            \"modality\": [\"multimodal\", \"graph\"],\n            \"target_variables\": [\"node labels\"],\n            \"specification\": {\n                \"node_types\": [\"multi-faceted\"],\n                \"edge_types\": [\"multi-faceted\"]\n            },\n            \"description\": \"Dataset containing Heterogeneous Information Networks extracted from specialized scholarly sources.\",\n            \"preprocessing\": [\"node and edge feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"interpretable SHNN models\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Siamese Hypergraph Neural Network (SHNN)\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Siamese\",\n                \"hypergraph aspects\": \"multi-faceted\"\n            },\n            \"description\": \"A model integrating multi-faceted node and edge properties in a SHNN design for HIN analysis\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced visualization techniques for interpretability\",\n        \"Emphasis on real-time deployment on Raspberry Pi with edge computation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"novel neighborhood sampling\", \"graph convolution strategies\"],\n        \"target_device\": \"Raspberry Pi\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"on-device computation\", \"optimized for resource constraints\"],\n        \"demonstration\": \"impact and recommendations on HIN analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction clearly describes a complex machine learning project involving HIN analysis, SHNN, and deployment on resource-constrained devices. The high expertise level suggests the user has a deep understanding of the technical aspects. The need for precision, model optimization, and comparison of graph convolutions indicate a clear problem statement and performance expectations. The requirement for visualizations and a research output shows a comprehensive approach.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "You're required to design an advanced sentiment analysis engine tailored for the intricate medical forum discussions using the extensive \"PubMed-MEDICAL\" dataset. The project necessitates a multi-level categorization of patients' online evaluations into a nuanced hierarchy of five emotions (extremely positive to very negative) on a scale of 0 to 4. The focus should be on optimizing precision, recall, and overall macro-averaged F1-score with a strong emphasis on model interpretability. Implement a deep, bidirectional transformer architecture that can handle domain-specific language complexities and provide detailed feature importances for better understanding. Additionally, devise a systematic approach for evaluating and mitigating potential biases in the dataset.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"medical forum discussions\",\n        \"description\": \"Design an advanced sentiment analysis engine for intricate medical forum discussions using the PubMed-MEDICAL dataset. The engine must categorize patients' evaluations into a 5-level emotion scale (0 to 4), focusing on precision, recall, and macro-averaged F1-score. Employ a deep, bidirectional transformer architecture and include model interpretability with feature importances. Address dataset bias evaluation and mitigation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score (macro-averaged)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PubMed-MEDICAL\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"emotion_levels\": {\n                    \"min\": 0,\n                    \"max\": 4\n                },\n                \"n_classes\": 5\n            },\n            \"description\": \"A large-scale dataset specifically for medical forum discussions, containing nuanced patient evaluations with potential language complexities.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"deep, bidirectional transformer\",\n            \"family\": \"neural networks\",\n            \"type\": \"transformer\",\n            \"specification\": {\n                \"interpretability\": \"detailed feature importances\"\n            },\n            \"description\": \"A model capable of handling domain-specific language complexities with emphasis on interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"model interpretability, domain-specific language complexities, dataset bias evaluation, mitigation strategies\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"a systematic approach for evaluating and mitigating bias\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction has been parsed, assuming a high level of expertise for the user. The problem area and task are clear, as is the emphasis on precision, recall, and interpretability. However, the exact metric values for performance metrics are left unspecified, as these would typically be determined during model training. The 'service' section could benefit from more specific information about target device and deployment details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a seasoned project manager, you are now mandated to spearhead the creation of a groundbreaking, quantum-enhanced neural machine translation (Q-NMT) platform. The system must leverage the humongous and constantly evolving Tatoeba corpus, not only for seamless translation between archaic English and Sanskrit, but also decipher cryptic metaphors and ancient poetic idioms with a stringent benchmark for precision at 99.5%. Moreover, devise a failsafe mechanism for automatic language modeling adaptation and integrate quantum-boosted zero-shot learning to bridge the lexical gap in local dialects with limited digital presence, ensuring universal accessibility and inclusivity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation, education, and research\",\n        \"description\": \"Spearhead the creation of a groundbreaking quantum-enhanced neural machine translation (Q-NMT) platform using the Tatoeba corpus. The platform must handle translation between archaic English and Sanskrit, decipher cryptic metaphors and poetic idioms with precision >99.5%, and include language adaptation and zero-shot learning for local dialects with limited digital presence.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tatoeba Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"ancient languages\": [\"English (archaic)\", \"Sanskrit\"],\n                \"linguistic tasks\": [\"translation\", \"language understanding\"]\n            },\n            \"description\": \"A vast and evolving corpus for training and testing Q-NMT, focusing on archaic English and Sanskrit with emphasis on metaphors and poetic idioms.\",\n            \"preprocessing\": [\"quantum-enhanced data preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-Enhanced Neural Machine Translation\",\n            \"family\": \"Q-NMT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"quantum-influenced features\": \"Zero-shot learning, quantum-boosted adaptation\",\n                \"languages\": [\"archaic English\", \"Sanskrit\"],\n                \"linguistic tasks\": \"Translation, metaphor interpretation\"\n            },\n            \"description\": \"A platform designed for seamless translation and advanced linguistic processing using quantum enhancements.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"The Q-NMT platform should leverage quantum computing principles for improved performance and ability to handle complex linguistic structures.\"\n        },\n        {\n            \"text\": \"Adaptation for local dialects with limited digital presence is crucial for universal accessibility.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud-based for computational intensity\",\n        \"deployment_endpoint\": \"Custom or Q-NMT-specific platform\",\n        \"inference_engine\": [\"quantum-optimized engines\"],\n        \"demonstration\": \"Platform demonstration for seamless translation, metaphor interpretation, and language adaptation for diverse dialects.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction has provided ample details for a high-level machine learning project, including an advanced application area, specific performance target, and a focus on integrating quantum computing. Expertise level is assumed to be high due to the complexity of the project. However, certain aspects like the target device and deployment endpoint are speculative due to the lack of direct information.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a specialized RNN machine translation model for English to Russian, utilizing the diverse OPUS-100 corpus, specifically tailored for translating informal, real-time chats with slang and colloquialisms. Achieve a minimum BLEU score of 35 within a strict latency limit of 150 milliseconds per sentence to ensure optimal performance under peak traffic and real-time constraints. Additionally, propose a robust evaluation framework that assesses the model's resilience when faced with limited training data, focusing on identifying degradation patterns and potential mitigation strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"informal, real-time chat translation\",\n        \"description\": \"Develop a specialized RNN machine translation model for English to Russian, targeting informal chats with slang and colloquialisms. The model should achieve a minimum BLEU score of 35 and operate within a latency limit of 150 milliseconds per sentence.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU Score\",\n                \"value\": 35\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency per sentence\",\n                \"value\": 150,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS-100\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": \"English to Russian\",\n                \"informal_data\": true\n            },\n            \"description\": \"A diverse corpus containing informal English to Russian data, specifically suited for real-time chat translation.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Specialized RNN\",\n            \"family\": \"RNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specific to informal chat translation\"\n            },\n            \"description\": \"A model tailored for English to Russian translation with an RNN backbone, designed for informal chat scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Model should be resilient to slang and colloquialisms常见表达\"\n        },\n        {\n            \"description\": \"Performance under low training data scenarios and mitigation strategies\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time optimization for peak traffic\"\n        ],\n        \"demonstration\": \"Focus on resilience and low-latency performance under data scarcity\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clearly specified, with a focus on a specialized model, performance metrics, and resilience to specific challenges. The user's high expertise level indicates they expect clear guidance. The missing target device and deployment details are assumed to be context-dependent.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For the advanced smart city project, design an optimized, lightweight YOLO model that not only recognizes real-time traffic flow but also distinguishes subclasses of vehicles (e.g., lorries with multiple trailers), distinguishes between various pedestrian categories (age groups, mobility aids), and identifies complex road signs with nuanced meanings. The model should achieve an average precision of 90% on benchmark datasets while maintaining sub-3 millisecond latency for guaranteed ultra-smooth 120 FPS performance across a range of heterogeneous hardware, including resource-constrained edge devices, and ensuring compatibility with emerging AI hardware accelerators. Provide detailed training procedures and hardware adaptability strategies in your proposal.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city\",\n        \"description\": \"Design an optimized, lightweight YOLO model for real-time traffic flow, vehicle subclass recognition (lorries with multiple trailers), pedestrian categorization (age groups and mobility aids), and complex road sign identification. Aim for 90% average precision on benchmark datasets and sub-3 millisecond latency for 120 FPS performance across diverse hardware, including edge devices and AI accelerators.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average precision\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 3,\n                \"unit\": \"millisecond\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SmartCity Traffic Benchmark\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"traffic flow\",\n                \"vehicle subclasses\",\n                \"pedestrian categories\",\n                \"road signs\"\n            ],\n            \"specification\": null,\n            \"description\": \"A dataset for the advanced smart city project, including images for real-time traffic flow detection and various object classification tasks.\",\n            \"preprocessing\": [\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\n                \"random cropping\",\n                \"flipping\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO (optimized)\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"90%\",\n                \"latency\": \"sub-3 ms\",\n                \"compatibility\": [\"edge devices\", \"AI accelerators\"]\n            },\n            \"description\": \"A lightweight YOLO model specifically designed for smart city applications, targeting high performance and hardware adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Apply transfer learning and pruning techniques for efficient model design.\",\n        \"Use hardware-aware training to optimize for heterogeneous hardware.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"targeted model compression\",\n            \"quantization\"\n        ],\n        \"target_device\": [\"edge devices\", \"AI accelerators\"],\n        \"deployment_endpoint\": \"emerging AI hardware platforms\",\n        \"inference_engine\": [\"optimized tensor processing libraries\"],\n        \"demonstration\": \"real-time ultra-smooth performance across various scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately reflects the requirements of designing a model for a smart city project. User's expertise level is assumed to be high due to the complexity of the task. Performance metrics, hardware requirements, and training strategies are specified. The 'service' section includes adaptations for diverse hardware and emerging AI accelerators. However, the exact data preprocessing steps like resizing or pre-training model selection is not provided in the user's instruction.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "object detection", "instruction": "In the rapidly changing retail sector, design an advanced, scalable YOLO model (v5+) for the evolving \"StoreShelf Dataset v3.1\" with an expanded 200-item product catalog. Require not only >97% fine-grained classification accuracy and >95% balanced precision-recall, but also temporal adaptability to handle daily shifts, blink-of-an-eye latency (<1.3 seconds per shelf scan), and seamless integration with an AI-driven inventory management system. This system must flawlessly account for product lifecycle stages, seasonal fluctuations, and the automatic dynamic adjustment of stock levels without performance degradation, ensuring real-time shelf optimization even with frequent SKU updates.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"Design an advanced, scalable YOLO model (v5+) for the StoreShelf Dataset v3.1, targeting a 200-item product catalog. The model must achieve >97% fine-grained classification accuracy and >95% balanced precision-recall. It must exhibit temporal adaptability for daily shifts, with a latency of <1.3 seconds per shelf scan and seamless integration with an AI-driven inventory management system that handles product lifecycle stages, seasonal fluctuations, and dynamic stock adjustments in real-time.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"fine-grained classification accuracy\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"balanced precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"balanced recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency per shelf scan\",\n                \"value\": 1.3,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"StoreShelf Dataset v3.1\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product ID\", \"category\", \"stock level\"],\n            \"specification\": {\n                \"product catalog size\": 200,\n                \" Shelf scanning frequency\": {\"unit\": \"per day\", \"value\": \"daily shifts\"}\n            },\n            \"description\": \"An evolving dataset for retail shelf detection and inventory management\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5+\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"scaled and adaptable\",\n                \"object detection capability\": \"fine-grained\"\n            },\n            \"description\": \"Advanced YOLO model with real-time performance and inventory management integration\"\n        }\n    ],\n    \"knowledge\": [\n        \"Product lifecycle stages, seasonal fluctuations, and SKU updates should drive dynamic stock adjustment without performance loss.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"AI-driven inventory management system\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"Real-time shelf optimization with minimal latency and dynamic stock management\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has high expertise and clearly specified advanced requirements for model performance, latency, and integration. The inventory management aspects are essential for real-time optimization. The parsed JSON covers all relevant fields, though specific deployment details may need to be gathered from the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "You are now working on a complex, real-time electricity load forecasting task, using the High-Dimensional Electricity Grid Dynamics dataset. This dataset contains 5 years of minutely data, with strict stratification based on meteorological events, demographics, and evolving consumption trends. The input sequences, now extended to 144 days (INPUT_SEQ_LEN=144, encompassing 35 unique dynamic variables, INPUT_DIM=35), demand forecasting for the subsequent 168-hour period (PRED_SEQ_LEN=168) with sub-hourly forecasts for 35 distinct load categories. The challenge mandates not only outperforming benchmark performances, with a target MSLE < 0.1 and MAPE < 2%, but also demonstrating interpretability and resilience to concept drift. Utilize a hybrid architecture of transformer-based models and physics-informed LSTMs, and conduct a multi-fidelity hyperparameter search using a nested cross-validation strategy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"electricity grid\",\n        \"description\": \"Working on a complex, real-time electricity load forecasting task using the High-Dimensional Electricity Grid Dynamics dataset. The dataset includes 5 years of minutely data stratified by meteorological events, demographics, and evolving trends. The task is to forecast load for the next 168 hours with sub-hourly forecasts for 35 categories, given extended input sequences (INPUT_SEQ_LEN=144, INPUT_DIM=35). Key performance metrics include MSLE < 0.1 and MAPE < 2% and requirements include interpretability and resilience to concept drift. A hybrid architecture combining transformer-based models and physics-informed LSTMs, along with a multi-fidelity hyperparameter search using nested cross-validation, is needed.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSLE\",\n                \"value\": 0.1\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": 2\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model interpretability\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"concept drift resilience\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High-Dimensional Electricity Grid Dynamics\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 144,\n                \"INPUT_DIM\": 35,\n                \"PRED_SEQ_LEN\": 168,\n                \"PRED_DIM\": 35\n            },\n            \"description\": \"A dataset with 5 years of minutely electricity grid data stratified by meteorological events, demographics, and consumption trends.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Architecture (Transformer-based models + Physics-Informed LSTMs)\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": null,\n                \"concept_drift_resilience\": null,\n                \"hyperparameter_search\": \"nested cross-validation\"\n            },\n            \"description\": \"A model utilizing transformers and physics-informed LSTMs for electricity load forecasting, addressing interpretability and concept drift.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Interpretability to be demonstrated\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction was parsed accurately, considering the high level of expertise, the specific dataset, task, and requirements for performance metrics, model architecture, and hyperparameter search strategy. However, some details for service deployment (e.g., target device and inference engine) are missing and need to be inferred from context or additional input.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "As the project manager for our elite academic endeavor, you are tasked with overseeing the design and implementation of an avant-garde visual question-answering system. This groundbreaking project mandates the deployment of cutting-edge transformer models, incorporating advanced attention mechanisms, while requiring a robust deep learning pipeline. The team must develop a modular approach to not only process visual and textual information but also perform complex, multi-hop reasoning, temporal context analysis, and handle diverse real-world scenarios. The goal is to achieve superhuman performance on the TextVQA dataset, while maintaining scalability and generalizability in a rapidly evolving research landscape.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Design and implement an avant-garde visual question-answering system using cutting-edge transformer models with advanced attention mechanisms. The project requires a robust deep learning pipeline, modular processing of visual and textual information, multi-hop reasoning, temporal context analysis, and handling diverse scenarios. The objective is to achieve superhuman performance on the TextVQA dataset, focusing on scalability and generalizability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"superhuman performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"relative to dataset size\"\n            },\n            {\n                \"name\": \"generalizability\",\n                \"value\": null,\n                \"unit\": \"ability to adapt to new scenarios\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TextVQA\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"advanced_features\": \"transformer, advanced attention mechanisms\"\n            },\n            \"description\": \"A dataset for visual question answering, designed to test performance in complex scenarios and multi-hop reasoning.\",\n            \"preprocessing\": [\n                \"complex data preprocessing needed for transformers\"\n            ],\n            \"augmentation\": [\n                \"multi-modal data augmentation\"\n            ],\n            \"visualization\": [\n                \"visualizing model performance across diverse scenarios\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Avant-Garde Transformer\",\n            \"family\": \"transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced_attention\": true,\n                \"modularity\": \"high\"\n            },\n            \"description\": \"A cutting-edge transformer model tailored for visual and textual information processing, supporting multi-hop reasoning, temporal context, and diverse scenario handling.\"\n        }\n    ],\n    \"knowledge\": [\n        \"requirements for deep learning pipeline with transformers\",\n        \"expert understanding of complex reasoning and temporal context analysis\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"modular approach to processing\",\n            \"integration of transformers\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"future-proof, scalable architecture\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"visual and textual reasoning performance analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a high-level system, aligning with the complexity and advanced technology involved. The problem area, downstream task, and metrics are specified. Some areas are left open-ended, like the target device or deployment endpoint, as these would typically be based on further collaboration with the user. The high expertise level implies the need for more detailed requirements and technical discussion.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for the groundbreaking Medical AI revolution, design a sophisticated, hierarchical Q&A system leveraging the massive BioASQ dataset. Mandate triple-layered domain adaptation with BioBERT fine-tuning, intricate named entity recognition, and a real-time evidence tracker. The model must not only diagnose intricate ailments but also detect ultra-rare syndromes, furnishing answers reinforced by meticulously curated, peer-reviewed citations from PubMed and Scopus, cross-referenced with EMR data. Prioritize regulatory compliance, demanding benchmarking against internationally accredited gold-standard annotations, and guarantee a minimal false-positive rate of <1%.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medical AI\",\n        \"description\": \"Design a sophisticated, hierarchical Q&A system for the Medical AI revolution using the BioASQ dataset. The system must undergo triple-layered domain adaptation with BioBERT fine-tuning, incorporate named entity recognition, and have a real-time evidence tracker. It should diagnose intricate ailments and detect ultra-rare syndromes, sourcing answers from peer-reviewed citations (PubMed and Scopus) cross-referenced with EMR data. Compliance with regulations and international benchmarking against accredited gold-standard annotations is mandatory, with a false-positive rate target of less than 1%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"false-positive rate\",\n                \"value\": 0.01\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"hierarchical_structure\": true,\n                \"domain_adaptation_layers\": 3,\n                \"BioBERT_fine_tuning\": true\n            },\n            \"description\": \"A massive biomedical question answering dataset, designed for developing the sophisticated Q&A system.\",\n            \"preprocessing\": [\"named entity recognition\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Q&A System\",\n            \"family\": \"BioBERT-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"triple_layer_adaptation\": true,\n                \"evidence_tracker\": true\n            },\n            \"description\": \"A Q&A system leveraging BioBERT fine-tuning, sophisticated enough to diagnose complex health issues and detect rare syndromes.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Regulatory compliance with accredited gold-standard annotations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"cross-referencing with EMR data\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"compliant with healthcare regulations\"],\n        \"demonstration\": \"patient-facing, real-time evidence-based diagnosis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provides clear requirements for the project, including high-level design, dataset, and model specifications, as well as performance and compliance metrics. Given the complexity and specific demands, the 'expertise' level is interpreted as high. However, the deployment endpoint and inference engine are left unspecified, which may require additional clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for an elite AI research and innovation team, your mandate is to develop a highly specialized, real-time energy demand forecasting system. Utilizing the Electricity dataset, divided into nested stratified subsets for training, validation, and calibration, design an advanced, deep-neural network architecture that can decipher intricate spatio-temporal patterns in the data. With a non-standard input sequence of 96 timesteps featuring 321 unique, dynamically varying attributes (REQUIRE: feature selection based on recursive feature elimination), forecast not only the next 192 step-ahead predictions but also account for complex patterns like sub-hourly fluctuations, climate variables, and rare anomalies. Ensure the model's robustness by optimizing hyperparameters with Bayesian optimization, comparing it to cutting-edge techniques like attention-based transformers and ensemble methods, and conducting a thorough sensitivity analysis. Prepare a detailed, publication-quality report showcasing not only the superior predictive accuracy (MSE < 0.5 and MAE < 0.2) but also the model's resilience to real-world scalability and deployment scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy demand forecasting\",\n        \"description\": \"Develop a real-time, highly specialized energy demand forecasting system using the Electricity dataset. The data is divided into stratified subsets for training, validation, and calibration. Design an advanced deep-neural network with 96 timesteps and 321 dynamic attributes, applying recursive feature elimination. Forecast 192 step-ahead predictions while accounting for sub-hourly fluctuations, climate variables, and anomalies. Optimize hyperparameters with Bayesian optimization, compare to state-of-the-art techniques, and conduct sensitivity analysis. The report should showcase superior accuracy (MSE < 0.5 and MAE < 0.2) and model robustness.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": 0.5\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": 0.2\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"energy demand\"],\n            \"specification\": {\n                \"input_seq_len\": 96,\n                \"num_features\": 321,\n                \"unique_attributes\": true\n            },\n            \"description\": \"A nested stratified dataset for energy demand forecasting, with training, validation, and calibration subsets.\",\n            \"preprocessing\": [\"recursive feature elimination\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Deep-Neural Network\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"timesteps\": 96,\n                \"features\": 321,\n                \"target_predictions\": 192,\n                \"feature_selection\": \"recursive elimination\"\n            },\n            \"description\": \"A deep-neural network designed for intricate spatio-temporal energy demand forecasting.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Use Bayesian optimization for hyperparameter tuning and compare against attention-based transformers and ensemble models.\"\n        },\n        {\n            \"content\": \"Conduct a sensitivity analysis to ensure model robustness.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"recursive feature elimination\"],\n        \"target_device\": \"high-performance server\",\n        \"deployment_endpoint\": \"production environment\",\n        \"inference_engine\": [\"TensorFlow serving\"],\n        \"demonstration\": {\n            \"content\": \"An interactive dashboard showcasing real-time and scalable energy demand forecasts.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction has been parsed into a JSON format, with high expertise level indicating a deep understanding of the project. Performance metrics are clearly stated, and complexity metrics include a placeholder for inference time and memory size. The service section specifies a high-performance device and deployment. The report requirements are also detailed.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "In your capacity as a leading project manager for a pioneering AI research team, your mandate is to create a highly advanced and scalable time series classification framework. Utilizing the intricate UWave Gesture Library dataset, which boasts 315-dimensional feature sequences (INPUT_SEQ_LEN) divided into three distinct partitions for model training, validation, and rigorous testing, design an innovative deep learning architecture that not only differentiates between the eight exclusive gesture classes (labels: 0-7) with high accuracy but also demonstrates exceptional generalization, resilience, and energy efficiency. Embark on formulating a state-of-the-art feature fusion strategy, and conduct a comprehensive analysis, employing precision, macro-averaged F1 score, and a user-defined evaluation metric, to substantiate the model's superiority. Ensure that each step contributes to the overall system's optimization and resource management.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"Create a highly advanced and scalable time series classification framework using the UWave Gesture Library dataset. Design an innovative deep learning architecture to differentiate between 8 gesture classes with high accuracy, excellent generalization, and energy efficiency. Implement feature fusion and evaluate with precision, macro-averaged F1 score, and a custom evaluation metric.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\"\n            },\n            {\n                \"name\": \"precision\"\n            },\n            {\n                \"name\": \"macro-averaged F1 score\"\n            },\n            {\n                \"name\": \"custom evaluation metric\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"Watts\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UWave Gesture Library\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"0-7\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 315\n            },\n            \"description\": \"A high-dimensional time series dataset for gesture classification, divided into train, validation, and test sets.\",\n            \"preprocessing\": [\n                \"feature extraction\",\n                \"windowing\"\n            ],\n            \"augmentation\": null,\n            \"visualization\": [\n                \"feature distribution\",\n                \"sequence patterns\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"custom deep learning architecture\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"state-of-the-art, feature fusion strategy\"\n            },\n            \"description\": \"An innovative architecture designed for robust gesture classification in the UWave dataset\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Consider integrating state-of-the-art techniques in time-series feature engineering and energy-efficient model design.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"deep feature learning\",\n            \"frequency analysis\"\n        ],\n        \"target_device\": \"high-performance, embedded\",\n        \"deployment_endpoint\": \"cloud-based, optimized for resource management\",\n        \"inference_engine\": [\"TensorFlow serving\", \"Apache MXNet\"],\n        \"demonstration\": \"Include example prediction visualizations and user-friendly API explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instructions provided detailed a complex project involving time-series classification, deep learning, and various evaluation metrics. The user's high expertise level suggests they're familiar with the project's requirements. However, specific details on energy efficiency and user-defined metrics require further clarification.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Design a sophisticated, real-time predictive model that integrates DeepAR, CNN, and a novel hybrid algorithm, combining satellite imagery with climate and socio-economic indicators. Enhance anomaly detection for extreme weather events in distinct regions, ensuring resilience to data scarcity and seasonal variations. Implement a dynamic autoencoder with adaptive reinforcement learning for continuous performance enhancement on rolling window data. Develop an interactive, interpretable dashboard for visualizing forecast intervals and comparing model efficacy versus competitor algorithms, while conducting rigorous, time-series cross-validation for robust validation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate and socio-economic forecasting\",\n        \"description\": \"Design a sophisticated, real-time predictive model that integrates DeepAR, CNN, and a novel hybrid algorithm. The model combines satellite imagery with climate and socio-economic indicators to enhance anomaly detection for extreme weather events in distinct regions, optimizing for data scarcity and seasonal variations. It should feature a dynamic autoencoder with adaptive reinforcement learning for continuous improvement on rolling window data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"anomaly detection\",\n                \"value\": null,\n                \"description\": \"Enhanced anomaly detection for extreme weather events\"\n            },\n            {\n                \"name\": \"resilience to data scarcity\",\n                \"value\": null,\n                \"description\": \"Effective model performance under data scarcity\"\n            },\n            {\n                \"name\": \"seasonal variation handling\",\n                \"value\": null,\n                \"description\": \"Adaptation to seasonal variations in data\"\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null,\n                \"description\": \"Error metrics for rolling window forecasts\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time inference\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"satellite imagery\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_format\": null,\n                \"resolution\": null\n            },\n            \"description\": \"Multimodal data including satellite imagery\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": null,\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"climate and socio-economic indicators\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_format\": null,\n                \"dimensions\": null\n            },\n            \"description\": \"Climate and socio-economic data to train the hybrid model\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepAR + CNN + Hybrid Algorithm\",\n            \"family\": \"neural networks\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"architecture\": [\n                    \"DeepAR\",\n                    \"CNN\",\n                    \"Hybrid Algorithm\"\n                ],\n                \"parameters\": {\n                    \"window_size\": null,\n                    \"epochs\": null,\n                    \"reinforcement_learning_params\": null\n                }\n            },\n            \"description\": \"Integrating DeepAR, CNN, and the novel algorithm to address extreme weather prediction.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"interactive dashboard\",\n        \"inference_engine\": [\"adaptive reinforcement learning\"],\n        \"demonstration\": {\n            \"dashboard\": true,\n            \"visualizations\": [\"forecast intervals\", \"model comparison\"]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction specifies a complex model integration, data requirements, and desired features such as real-time, anomaly detection, and cross-validation. The expertise level is high, indicating the user has a deep understanding and expectations are set accordingly. The missing performance and complexity values need to be inferred or provided based on specific requirements and constraints.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a highly specialized, real-time neural machine translation system, utilizing the TEDx Talk corpus exclusively for Marathi to Konkani translations. Mandate the deployment of an Enhanced EfficientTransformer-XL, not only ensuring minimal energy consumption (optimizing for Raspberry Pi Zero's <5 Watt power budget) but also achieving <10% CPU utilization and maintaining a memory footprint below 5MB. The system must be optimized for continuous translation on lightweight, portable devices with extended battery life support.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"real-time, specialized translation\",\n        \"description\": \"Develop a real-time, neural machine translation system for Marathi to Konkani, using the TEDx Talk corpus exclusively. The focus is on a lightweight and energy-efficient design optimized for deployment on Raspberry Pi Zero with a power budget of <5 Watt and constraints on CPU utilization (<10%) and memory footprint (<5MB).\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy consumption\",\n                \"value\": 5.0,\n                \"unit\": \"Watts\"\n            },\n            {\n                \"name\": \"CPU utilization\",\n                \"value\": 0.10,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": 5,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TEDx Talk corpus (Marathi to Konkani)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": [\"Marathi\", \"Konkani\"],\n                \"source_domain\": \"TEDx Talks\"\n            },\n            \"description\": \"A dataset specifically curated for Marathi to Konkani translation, based on TEDx Talks content.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced EfficientTransformer-XL\",\n            \"family\": \"EfficientTransformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Enhanced\",\n                \"specifics\": \"optimizing for lightweight, portable devices\"\n            },\n            \"description\": \"A specialized neural machine translation model, Enhanced EfficientTransformer-XL, designed for real-time translation on Raspberry Pi Zero with energy and resource constraints.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"Raspberry Pi Zero\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            {\n                \"name\": \"optimized for lightweight devices\"\n            }\n        ],\n        \"demonstration\": \"Continuous translation with lightweight, portable device support and battery life extension\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clearly asking for the development of a highly specialized machine translation system with specific constraints. The user has a high level of expertise and provides details on the dataset, model, and performance targets. However, it is assumed that some service aspects like deployment endpoint and optimization details are not explicitly stated in the instruction.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image classification", "instruction": "For the expanding smartphone app, design an advanced image classifier specializing in fine-grained analysis of user-generated content. The system must efficiently categorize photos into five subcategories: (1) precise product use scenarios, (2) nuanced customer sentiment levels (happy, neutral, disappointed), (3) product feature relevance, (4) content suitable for interactive customer reviews, and (5) standout, shareable aesthetics. Additionally, the system must integrate with real-time user feedback and adjust its categories dynamically for optimized marketing campaigns and constant algorithmic enhancement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"smartphones and user-generated content\",\n        \"description\": \"Design an advanced image classifier for an expanding smartphone app that performs fine-grained analysis on user-generated photos. Categories include: precise product use, nuanced sentiment (happy, neutral, disappointed), feature relevance, customer review suitability, and shareable aesthetics. The system should integrate with real-time user feedback for dynamic categorization and algorithm improvement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Custom\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"product use scenario\",\n                \"sentiment level\",\n                \"feature relevance\",\n                \"review suitability\",\n                \"aesthetics\"\n            ],\n            \"specification\": {\n                \"real-time feedback\": true,\n                \"subcategories\": {\n                    \"1\": \"product use scenarios\",\n                    \"2\": \"sentiment levels\",\n                    \"3\": \"feature relevance\",\n                    \"4\": \"review suitability\",\n                    \"5\": \"aesthetics\"\n                }\n            },\n            \"description\": \"A custom dataset containing user-generated images for fine-grained analysis, with subcategory labels for the specified criteria.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image Classifier\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"fine-grained\",\n                \"real-time learning\": true\n            },\n            \"description\": \"An advanced model for classifying user-generated images into the specified subcategories, emphasizing real-time integration with user feedback.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The classifier should leverage transfer learning from a large-scale image dataset to improve initial accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"dynamic feature extraction\", \"user feedback adaptation\"],\n        \"target_device\": \"mobile and cloud\",\n        \"deployment_endpoint\": \"API integration with app\",\n        \"inference_engine\": [\"real-time image processing\"],\n        \"demonstration\": \"Interactive in-app feedback and performance visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has a high level of expertise, indicating they may need more guidance on certain aspects. The 'performance_metrics' values are unknown as they require further input from the user. The 'service' section includes requirements for real-time feedback integration and dynamic category adjustment.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "As a cutting-edge project manager for the elite deep learning research team, your mandate is to create a novel emotion-aware text authorship attribution system. Using the elusive and intricate EmoART dataset, design an integrated framework that discerns not only between human (0) and AI-generated (1) text, but also discerns subtle emotions like joy, anger, and sarcasm within the AI outputs. Challenge the team to achieve superhuman accuracy while implementing a hybrid model combining explainable AI and counterfactual reasoning, ensuring transparency without sacrificing predictive power. Moreover, devise a comprehensive strategy for continuous monitoring and mitigation of algorithmic fairness concerns across diverse socio-linguistic demographics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"deep learning research\",\n        \"description\": \"Create a novel emotion-aware text authorship attribution system, using the EmoART dataset to distinguish human (0) and AI-generated (1) text, as well as identify subtle emotions like joy, anger, and sarcasm in AI outputs. Develop a hybrid model combining explainable AI and counterfactual reasoning with superhuman accuracy. Ensure transparency and fairness across diverse socio-linguistic demographics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"emotion recognition accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explainability\",\n                \"value\": null\n            },\n                {\n                \"name\": \"counterfactual reasoning effectiveness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model interpretability\",\n                \"value\": null,\n                \"unit\": \"Fano shadows or feature importances\"\n            },\n            {\n                \"name\": \"algorithmic fairness\",\n                \"value\": null,\n                \"unit\": \"Diversity and demographic parity metrics\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"EmoART\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"human vs AI\", \"emotion labels\"],\n            \"specification\": null,\n            \"description\": \"An intricate dataset for emotion-aware text authorship attribution, including human and AI-generated text, with diverse emotions.\",\n            \"preprocessing\": [\"Normalization\", \"Tokenization\", \"Emotion annotation\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Emotion distribution\", \"Confusion matrices\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Emotion-Aware Hybrid Model\",\n            \"family\": \"AI\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainable AI techniques\": [\"LIME\", \"SHAP\"],\n                \"counterfactual reasoning methods\": [\"Counterfactual Explanations\", \"Adversarial attacks\"]\n            },\n            \"description\": \"A model combining explainable AI and counterfactual reasoning for emotion-aware text attribution with emphasis on fairness across diverse demographics.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Use counterfactual explanations to show how minor changes in input can influence the model's decisions.\",\n        \"Monitor for algorithmic bias using demographic parity and equal opportunity metrics.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Handling text preprocessing\", \"Emotion label incorporation\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"AI Platform\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Custom TensorFlow model\"],\n        \"demonstration\": \"Interactive dashboard showcasing model performance and fairness metrics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction provided a clear task, specifying the use of EmoART dataset, Emotion-aware hybrid model, and emphasis on algorithmic fairness. The expert level indicates strong knowledge. However, there's no direct mention of expected accuracy targets, which might need to be determined through further consultation. The provided JSON response includes relevant aspects and fills in the information given.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "As the project manager, you're tasked with developing a highly optimized Spectral ChebNet architecture for Amazon's co-purchase graph. Mandate a sub-10ms latency for real-time node classification, ensuring at least 98% accuracy under high volume traffic, and the model must have self-evolution capabilities to adapt to unseen features without retraining, all while maintaining data privacy under GDPR regulations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"Amazon co-purchase graph\",\n        \"description\": \"Develop a highly optimized Spectral ChebNet architecture for real-time node classification in Amazon's co-purchase graph with sub-10ms latency, aiming for at least 98% accuracy under high volume traffic. The model must have self-evolution capabilities and adhere to GDPR data privacy regulations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 10.0,\n                \"unit\": \"ms\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency (real-time)\",\n                \"value\": 10.0,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"adaptability to unseen features\",\n                \"value\": 1.0,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Amazon co-purchase graph\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node classification\"],\n            \"specification\": {\n                \"graph_structure\": \"co-purchase\",\n                \"traffic_volume\": {\n                    \"high\": \"implicitly defined\"\n                }\n            },\n            \"description\": \"The Amazon co-purchase graph dataset for node classification tasks.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Spectral ChebNet\",\n            \"family\": \"Spectral Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"optimized\",\n                \"self-evolution\": \"enabled\",\n                \"privacy_compliance\": \"GDPR\"\n            },\n            \"description\": \"A highly optimized Spectral ChebNet with self-evolution and GDPR data privacy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"specific to Amazon's infrastructure\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time, privacy-preserving\"],\n        \"demonstration\": {\n            \"method\": \"latency and accuracy under high traffic\",\n            \"privacy_impact\": \"demonstrated\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction indicates a high complexity task, which is reflected in the 'expertise' level as 'high'. Performance metrics and specific requirements like sub-10ms latency and 98% accuracy were clearly outlined. The need for self-evolution and GDPR compliance adds additional complexity. However, since there are no concrete specifications on the exact model structure or preprocessing steps, there's a lack of detail that may require clarification.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "question answering", "instruction": "For an upcoming project, devise a state-of-the-art, multilingual BERT adaptation, 'KiddoBERT', optimized for the exclusive Children's Educational Question-Answer Corpus (CEQAC). Emphasize computational efficiency, context-awareness, and child-friendly explanations, with a touch-based, gamified interface designed by a child-centered UX team, featuring adaptive difficulty levels and personalized feedback to maximize long-term learning immersion in pre-K to early elementary aged users.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"children's educational\",\n        \"description\": \"Design a state-of-the-art, multilingual BERT adaptation named 'KiddoBERT' targeting the Children's Educational Question-Answer Corpus (CEQAC). Focus on computational efficiency, context-awareness, child-friendly explanations, and a gamified touch-based interface tailored to pre-K to early elementary aged users, with adaptive difficulty levels and personalized feedback for long-term learning immersion.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"context-awareness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Children's Educational Question-Answer Corpus (CEQAC)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A corpus designed specifically for children's educational question-answering, focusing on the educational needs of pre-K to early elementary aged users.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"KiddoBERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"state-of-the-art\",\n                \"multilingual\": true,\n                \"specific_architecture_adaptation\": \"child-friendly\"\n            },\n            \"description\": \"A multilingual BERT model optimized for Children's Educational Question-Answer tasks, emphasizing computational efficiency and context-awareness.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"UX Design Considerations\",\n            \"description\": \"Touch-based, gamified interface with adaptive difficulty levels and personalized feedback to engage young users.\"\n        },\n        {\n            \"type\": \"Target Age Range\",\n            \"description\": \"Pre-K to early elementary aged users.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"touch-based, adaptable to child-friendly devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for mobile devices\"],\n        \"demonstration\": {\n            \"interface_description\": \"A kid-friendly, gamified QA interface that adapts to users' learning progress\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction describes a complex project involving a custom NLP model, user experience design, and performance goals. The given intent is 'build', indicating model development. Expertise is assumed to be high based on the complexity of the project. Some performance metrics (efficiency and context-awareness) and computational complexity (inference time and memory footprint) are explicitly mentioned. The 'service' and 'knowledge' sections capture essential details about the interface and target audience. Overall, the response is comprehensive but lacks exact numbers for some metrics and optimization specifics.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a seasoned project manager, you're tasked with designing a sophisticated, scalable automotive object recognition system for efficient parking lot surveillance. Conduct an in-depth comparison of COCO and Pascal VOC datasets, evaluating not just their image variety and annotation fidelity, but also their resilience to noise and translation across varying resolutions. Scrutinize the trade-offs between YOLOv5, SSD, RetinaNet, and novel architectures like EfficientDet and MosaicNet, focusing on their speed-performance profiles, power consumption, and adaptability to resource-constrained edge devices. Require a comprehensive report that delves into energy efficiency, algorithmic robustness, and real-world deployment scenarios, ultimately outlining the optimal solution with rationale for each decision.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"automotive, parking lot surveillance\",\n        \"description\": \"Design a sophisticated automotive object recognition system for efficient parking lot surveillance. Compare COCO and Pascal VOC datasets for image variety, annotation fidelity, noise resilience, and translation across varying resolutions. Evaluate YOLOv5, SSD, RetinaNet, EfficientDet, and MosaicNet on speed-performance profiles, power consumption, and adaptability to edge devices. The report should focus on energy efficiency, algorithmic robustness, and deployment scenarios.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"bounding boxes\"],\n            \"specification\": {\n                \"image_variety\": \"\",\n                \"annotation_fidelity\": \"\",\n                \"noise_resilience\": \"\",\n                \"resolution_translatability\": \"\"\n            },\n            \"description\": \"A large-scale dataset for object detection and segmentation comparison in terms of image variety and annotation quality.\",\n            \"preprocessing\": [\"comparison with Pascal VOC\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Pascal VOC\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"bounding boxes\"],\n            \"specification\": {\n                \"image_variety\": \"\",\n                \"annotation_fidelity\": \"\",\n                \"noise_resilience\": \"\",\n                \"resolution_translatability\": \"\"\n            },\n            \"description\": \"Another popular dataset for object detection, to be compared against COCO\",\n            \"preprocessing\": [\"comparison with COCO\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed-performance\": \"\",\n                \"power_consumption\": \"\",\n                \"adaptability\": \"\"\n            },\n            \"description\": \"One of the candidates for automotive object recognition, focusing on speed and real-world applicability.\"\n        },\n        {\n            \"name\": \"SSD\",\n            \"family\": \"Single Shot Detectors\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed-performance\": \"\",\n                \"power_consumption\": \"\",\n                \"adaptability\": \"\"\n            },\n            \"description\": \"Another approach to object detection, with speed-performance evaluation included.\"\n        },\n        {\n            \"name\": \"RetinaNet\",\n            \"family\": \"Focal Loss-based Detectors\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed-performance\": \"\",\n                \"power_consumption\": \"\",\n                \"adaptability\": \"\"\n            },\n            \"description\": \"To be compared in terms of robustness and resource efficiency.\"\n        },\n        {\n            \"name\": \"EfficientDet\",\n            \"family\": \"Efficient Architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed-performance\": \"\",\n                \"power_consumption\": \"\",\n                \"adaptability\": \"\"\n            },\n            \"description\": \"A state-of-the-art model emphasizing efficiency for edge devices.\"\n        },\n        {\n            \"name\": \"MosaicNet\",\n            \"family\": \"Data Augmentation Techniques\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"speed-performance\": \"\",\n                \"power_consumption\": \"\",\n                \"adaptability\": \"\"\n            },\n            \"description\": \"A novel training technique that affects performance and energy consumption.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Energy efficiency is a critical factor in edge device deployment\",\n        \"Algorithmic robustness to real-world scenarios is essential\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adaptation for resource-constrained devices\"],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"customized for efficiency in low-power settings\"],\n        \"demonstration\": \"outlining trade-offs and selection criteria\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction provided a clear problem statement and requirements. The expertise level is assumed to be high due to the in-depth task. However, there are no specific performance metrics for the datasets or models, so those fields are left empty. The confidence score is high due to the comprehensive scope of the project but could be lower without performance targets.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "node classification", "instruction": "For the \"Holliday HighSchool Friendship Dynamics Challenge,\" develop a novel STGNN with self-organizing node embeddings, time-sensitive edge reinforcement, and a real-time anomaly detection module. The model must handle surge in data volume daily, maintaining privacy with differential privacy, and generate interactive 3D visualizations of social dynamics across four distinct academic quarters, differentiating between transient and stable friendships.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"social dynamics\",\n        \"description\": \"Develop a novel STGNN (Spatio-Temporal Graph Neural Network) for the Holliday HighSchool Friendship Dynamics Challenge. The model should include self-organizing node embeddings, time-sensitive edge reinforcement, and a real-time anomaly detection module. It must handle daily data volume surges and ensure privacy with differential privacy. The final output should include interactive 3D visualizations, differentiating between transient and stable friendships across four academic quarters.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"differential privacy score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Holliday HighSchool Friendship Dynamics Data\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [\n                \"transient_stable_friendships\"\n            ],\n            \"specification\": {\n                \"daily_data_volume\": null,\n                \"privacy_protected\": true\n            },\n            \"description\": \"A dataset with social dynamics data from four academic quarters, requiring real-time analysis and differential privacy.\",\n            \"preprocessing\": [\n                \"node embedding\",\n                \"time-series normalization\"\n            ],\n            \"augmentation\": [\n                \"edge reinforcement\"\n            ],\n            \"visualization\": [\n                \"3D interactive visualizations\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"STGNN with self-organizing embeddings\",\n            \"family\": \"Spatio-Temporal Graph Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"anomaly_detection\": true,\n                \"privacy_protection\": \"differential privacy\"\n            },\n            \"description\": \"A model designed for the Holliday HighSchool Challenge that includes self-organizing node embeddings and time-sensitive edge reinforcement.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Privacy preservation using differential privacy techniques\",\n        \"3D visualizations for complex social dynamics\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"differential privacy implementation\"\n        ],\n        \"target_device\": \"cloud or a privacy-enabled server\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for real-time processing\"\n        ],\n        \"demonstration\": \"user-guided, data-driven visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed with a high-expertise user in mind. The 'expertise' is set to 'high.' The task is clearly defined, and it involves building a novel model for a specific challenge. The dataset requirements, including privacy and visualizations, are specified. However, there may be some uncertainty about the exact performance metrics or inference time. The confidence score takes into account that the details may be dependent on the user providing more specific requirements.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for a luxury grocery retail innovation, devise a cutting-edge AI system utilizing the YOLOv5.1 architecture, specifically tailored for the boutique market with the \"BoutiqueFoods Shelf Insights Enhanced Dataset\". This dataset encompasses an impressive 150 distinctive premium product categories, mandating not just basic classification but also hierarchical recognition and product sub-type identification. The model must achieve a minimum precision of 97% and recall of 96%, demonstrating exceptional accuracy. Moreover, the system should integrate a real-time inventory management module, with the constraint that processing each high-definition shelf image for inventory updates must be completed within a strict 1.2-second latency to guarantee seamless operation for the busy store environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury grocery retail\",\n        \"description\": \"Develop a cutting-edge AI system using YOLOv5.1 architecture tailored for boutique grocery markets. The system should handle the BoutiqueFoods Shelf Insights Enhanced Dataset with 150 premium product categories, requiring hierarchical recognition and sub-type identification. Performance metrics should include a minimum precision of 97% and recall of 96%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.96\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 1.2,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoutiqueFoods Shelf Insights Enhanced Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product category\", \"sub-type\"],\n            \"specification\": {\n                \"number_of_categories\": 150,\n                \"level_of_recognition\": \"hierarchical\"\n            },\n            \"description\": \"A dataset specifically designed for boutique grocery markets, containing 150 unique premium product categories with the requirement for sub-type identification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5.1\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"YOLOv5.1\",\n                \"accuracy\": {\n                    \"precision_target\": 0.97,\n                    \"recall_target\": 0.96\n                }\n            },\n            \"description\": \"A tailored YOLOv5.1 model for boutique grocery AI system with hierarchical recognition and sub-type identification capabilities.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": {\n            \"latency_requirement\": \"1.2 seconds\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the user's request for a high-end system using the YOLOv5.1 architecture for boutique groceries with a specified dataset, performance goals, and real-time inventory management. The system's ability to handle product sub-types, strict latency, and high accuracy criteria indicate a clear intent to 'build' a project with a high level of expertise. However, the deployment endpoint is not provided.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "As a forward-thinking project manager for an elite machine learning lab, your challenge is to design a multilingual, explainable AI text classifier for the \"Quantum AI Dynamics and Attribution Corpus (QUADAC)\", which seamlessly differentiates between intricate poetic compositions by human poets (label A), innovative code-generated verse (label B), and a hidden category of hybrid works (label C) created using evolutionary algorithms. The classifier must achieve F1-score of at least 0.95 while maintaining interpretability, and it must withstand a set of linguistic adversarial tests derived from multiple languages and eras. The project should also include a detailed audit trail for model interpretability and include a comparative analysis with existing ethical AI frameworks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"linguistics and AI ethics\",\n        \"description\": \"Design a multilingual, explainable AI text classifier for the QUADAC corpus, differentiating between human poetic compositions (label A), code-generated verse (label B), and hybrid works (label C) created via evolutionary algorithms. The classifier must achieve an F1-score of at least 0.95, be interpretable, and withstand linguistic adversarial tests from multiple languages and eras. A detailed audit trail for interpretability and comparative analysis with existing ethical AI frameworks is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Quantum AI Dynamics and Attribution Corpus (QUADAC)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"human poems\", \"code-generated verse\", \"hybrid works\"],\n            \"specification\": {\n                \"languages\": [\"multilingual\"],\n                \"eras\": [\"multiple\"],\n                \"genre\": [\"poetic\", \"code-generated\", \"hybrid\"]\n            },\n            \"description\": \"The dataset comprises intricate poetic compositions, code-generated verse, and hybrid works generated by evolutionary algorithms, encompassing multiple languages and eras.\",\n            \"preprocessing\": [\"multilingual text normalization\", \"era-specific preprocessing\"],\n            \"augmentation\": [\"linguistic adversarial examples\"],\n            \"visualization\": [\"model interpretability visualization\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"explainable AI text classifier\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"resistance_to_adversarial_tests\": \"yes\"\n            },\n            \"description\": \"A multilingual, highly interpretable AI classifier designed for text analysis on the QUADAC corpus.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"model interpretability engineering\", \"ethics framework comparison\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Model explainability with poetry examples and interpretability audit\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON was parsed based on the provided instruction, with the user's intent as a build project, a high level of expertise, and specific requirements for the text classifier, dataset, interpretability, and linguistic adversarial tests. The 'service' section assumes the need for model explanation, ethical framework comparison, and a demonstration of interpretability, but is incomplete without exact deployment details. The confidence score is high due to the detailed description of the project requirements, but there is a low confidence in the missing deployment details.\",\n        \"score\": 0.8\n    }\n}"}
{"task": "machine translation", "instruction": "The project aims to develop a sophisticated transformer-based machine translation system, leveraging the massive Common Crawl dataset for English-to-Japanese conversions. The model must not only guarantee high linguistic coherence, but also achieve a remarkable benchmark of 98% accuracy in interpreting and generating colloquial expressions, while demonstrating sensitivity to cultural intricacies and diverse regional dialects, reflecting a true sense of local vernacular. Furthermore, include a rigorous evaluation plan comparing performance with competing state-of-the-art systems and propose a strategy for continuous model refinement based on user feedback.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"The project aims to develop a sophisticated transformer-based machine translation system, focusing on English-to-Japanese translations using the Common Crawl dataset. The system must achieve a high benchmark of 98% accuracy for interpreting and generating colloquial expressions while considering cultural nuances and regional dialects.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98,\n                \"context\": \"interpreting and generating colloquial expressions\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl (for English-to-Japanese)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"language_pair\": [\"English\", \"Japanese\"],\n                \"domain\": \"massive corpus from Common Crawl\"\n            },\n            \"description\": \"A large-scale dataset for English-to-Japanese translation, focusing on colloquial expressions and cultural intricacies.\",\n            \"preprocessing\": [\"cleaning, tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"data distribution\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based Machine Translation Model\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention_mechanism\": \"sophisticated\",\n                \"cultural_inclusion\": \"true\"\n            },\n            \"description\": \"A state-of-the-art model designed specifically for English-to-Japanese translations with advanced handling of colloquial expressions and cultural nuances.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"The model should demonstrate sensitivity to regional dialects and a true sense of local vernacular, which may require cultural embeddings or dialect-specific training data.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"refinement based on user feedback\", \"continuous model updates\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"API for comparing with SOTA systems\",\n        \"inference_engine\": [\"customized for high accuracy in translating colloquial expressions\"],\n        \"demonstration\": \"Model performance comparison with state-of-the-art on colloquial expressions and regional dialects\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction is well-defined with a high-level expert. The 'problem' section accurately captures the details of the machine translation task, performance metrics, and cultural aspects. 'Dataset' includes the requirement for access to a large-scale corpus, and 'service' outlines the continuous improvement and evaluation plan. However, the 'complexity_metrics' are not specified, leaving that field open for more information if needed.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager leading an elite machine learning research group, your mandate is to design a highly sophisticated deep learning architecture for the intricate ETTm2 dataset, a real-world time series challenge that demands exceptional performance. The data has been meticulously partitioned into train, validation, and test sets, featuring distinct input sequences of length INPUT_SEQ_LEN (96) with a rich dimensionality of INPUT_DIM (7). The model must not only showcase exceptional predictive abilities in forecasting future trends, PRED_SEQ_LEN (96) steps ahead, but also achieve state-of-the-art accuracy in both mean squared error (MSE) and mean absolute error (MAE) measures, while incorporating explainable AI elements. Furthermore, emphasize on the model's resilience to anomalies, adaptability to varying seasonality, and strive for unparalleled efficiency, ensuring it remains at the forefront of evolving time series analysis trends.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-world time series data\",\n        \"description\": \"Design a highly sophisticated deep learning architecture for the ETTm2 dataset, a time series challenge with INPUT_SEQ_LEN of 96 and INPUT_DIM of 7. The model must achieve state-of-the-art accuracy in MSE and MAE, be explainable, resilient to anomalies, adaptable to varying seasonality, and be highly efficient.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 96,\n                \"partitioning\": {\n                    \"train\": \"not specified\",\n                    \"validation\": \"not specified\",\n                    \"test\": \"not specified\"\n                }\n            },\n            \"description\": \"A real-world time series dataset partitioned into train, validation, and test sets with input sequences of length 96 and dimensionality 7.\",\n            \"preprocessing\": [\n                \"anomaly detection\",\n                \"seasonality adjustment\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks (deep learning)\",\n            \"specification\": {\n                \"explainability\": true,\n                \"anomaly_detection\": true,\n                \"seasonality_handling\": true,\n                \"efficiency_target\": \"state-of-the-art\"\n            },\n            \"description\": \"A deep learning architecture for advanced time series forecasting on the ETTm2 dataset, including explainable AI elements, anomaly resilience, and adaptability to varying seasonality.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"anomaly detection for preprocessing\",\n            \"seasonality adjustments\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explainable AI-driven predictions and performance analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed with a high-level intent of building a complex model. The user's expertise is assumed to be high due to the project manager role. The dataset and performance requirements are detailed, including explainability and handling anomalies. However, the specific deployment details and the model family are not directly specified, only the target properties. The confidence score is around 0.85 due to the need for further clarification on the model family and the precise train/validation/test partitions.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "visual question answering", "instruction": "For a research-led school initiative, develop an advanced TextVQA model using the latest MultiModalBERT architecture, which not only requires exceptional accuracy but also must handle complex visual scenes and demonstrate temporal reasoning, while maintaining real-time performance on limited computational resources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"research-led school initiative\",\n        \"description\": \"The task is to develop an advanced TextVQA model using the latest MultiModalBERT architecture. The model should have high accuracy, handle complex visual scenes, exhibit temporal reasoning, and operate in real-time on limited computational resources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"temporal reasoning\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size for limited resources\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MultiModalBERT dataset\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"complexity\": \"limited resources\"\n            },\n            \"description\": \"The model will be developed using the latest MultiModalBERT architecture on a dataset suitable for visual question answering tasks.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MultiModalBERT\",\n            \"family\": \"MultiModal BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"latest\"\n            },\n            \"description\": \"An advanced TextVQA model based on the MultiModalBERT architecture, designed for complex visual scenes and temporal reasoning.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"limited computational resources\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"leveraging constrained resources\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user instruction provided a clear indication of building an advanced TextVQA model, specifying the requirements for accuracy, complexity handling, and computational resources. Expertise level is assumed to be high, indicating a thorough understanding of the task. The model, dataset, and performance metrics were derived from the given criteria. However, there are some missing metric values (indicated by null) that would typically be determined through further details or analysis.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "link prediction", "instruction": "As a seasoned project manager, craft a detailed project plan to develop a state-of-the-art PIN model. The Yeast Protein Interaction Network requires adapting a deep learning architecture for link prediction, integrating feature extraction, ensemble methods, and cross-validation. Additionally, benchmark against competing algorithms and optimize for scalability, interpretability, and biological significance in the results.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"bioinformatics (proteins and yeast interaction network)\",\n        \"description\": \"Craft a detailed project plan to develop a state-of-the-art PIN model for link prediction in the Yeast Protein Interaction Network, involving deep learning, feature extraction, ensemble methods, and cross-validation. The project should benchmark against competitors and emphasize scalability, interpretability, and biological significance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high priority\",\n                \"unit\": \"increased dataset size\"\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": \"moderate priority\",\n                \"unit\": \"model interpretability methods\"\n            },\n            {\n                \"name\": \"biological significance\",\n                \"value\": \"high priority\",\n                \"unit\": \"biological relevance of features and model\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Yeast Protein Interaction Network\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [\n                \"Protein interactions\"\n            ],\n            \"specification\": {\n                \"nodes\": {\n                    \"features\": []\n                },\n                \"edges\": {\n                    \"labels\": [\"positive\", \"negative\"]\n                }\n            },\n            \"description\": \"A graph dataset containing protein interactions in yeast, for link prediction tasks.\",\n            \"preprocessing\": [\n                \"feature extraction\",\n                \"graph normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"network topology\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art PIN model\",\n            \"family\": \"Deep learning + Ensemble\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"architecture\": [\"deep learning for feature extraction\", \"ensemble methods for prediction\"],\n                \"cross-validation\": {\n                    \"folds\": 10\n                }\n            },\n            \"description\": \"A deep learning model with ensemble techniques, specifically tailored for link prediction in the Yeast Protein Interaction Network.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Experience with deep learning on graphs, ensemble methods for improved performance, and the significance of biological interpretability in protein interaction studies.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\",\n            \"dimensionality reduction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"GPU-based\", \"optimized for efficiency\"],\n        \"demonstration\": {\n            \"focus\": \"link prediction results, interpretability techniques, scalability analysis\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intent for model development, the high expertise level suggests strong understanding, and the problem description outlines the main requirements of link prediction, ensemble methods, and performance aspects. The provided dataset and model descriptions are detailed and appropriately refer to the bioinformatics context. The service and deployment section includes relevant steps while recognizing the importance of optimization and interpretation.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for an advanced extragalactic exploration initiative, demand an AI-powered image classifier that distinguishes not only between common celestial objects but also rare and obscured phenomena in the newly compiled Hubble Ultra Deep Field dataset. Emphasize on deploying a state-of-the-art M6-ViT++ architecture, equipped with self-attention distillation and contrastive learning, ensuring 98% precision on a challenging, previously unexamined subset of data. The system's performance must align with the stringent requirements of professional astronomers in their cutting-edge research on dark matter detection.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"extragalactic exploration, astronomy\",\n        \"description\": \"Request an AI-powered image classifier for the Hubble Ultra Deep Field dataset, focusing on distinguishing common and rare, obscured celestial objects. The classifier should use a state-of-the-art M6-ViT++ architecture with self-attention distillation and contrastive learning. The system must have at least 98% precision on a previously unexamined data subset for dark matter detection research.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hubble Ultra Deep Field\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"classification labels for celestial objects\"],\n            \"specification\": {\n                \"previously_examined_data_subset\": false\n            },\n            \"description\": \"A new dataset for advanced extragalactic exploration, including both common and obscure celestial objects in the Hubble Ultra Deep Field.\",\n            \"preprocessing\": [\"data cleaning, resizing, and normalization\"],\n            \"augmentation\": [\n                \"image-level augmentations, preserving astronomical context\",\n                \"possible use of self-attention distillation\"\n            ],\n            \"visualization\": [\"astronomical data exploration and feature visualization\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6-ViT++\",\n            \"family\": \"Transformer-based architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"M6-ViT++\",\n                \"features\": \"self-attention distillation and contrastive learning\"\n            },\n            \"description\": \"A state-of-the-art M6-ViT++ image classifier designed for distinguishing celestial objects, including rare and obscured phenomena in the Hubble Ultra Deep Field dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"enhancements for dark matter detection\"\n        ],\n        \"target_device\": \"cloud-based, allowing for large computations\",\n        \"deployment_endpoint\": \"private/astronomical research platform\",\n        \"inference_engine\": [\n            \"GPU-optimized for fast processing\"\n        ],\n        \"demonstration\": \"custom dashboards or visualizations for astronomers\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction is clear and detailed, covering the need for a high-level AI-powered classifier, specifications, and performance metrics. The 'expertise' is set to 'high' given the advanced nature of the project and the astronomer's expertise. Some sections are left unpopulated because they are implied or not explicitly stated, such as specific deployment environments or deep learning details like the contrastive learning algorithm. The confidence score is high given the available information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, for an upcoming AI competition, you need to develop a specialized deep learning system for high-resolution macro-photography of rare and endangered plant species. Utilize the 'Orchid Leaf & Flower Image Dataset' with diverse lighting conditions, focusing on both real-time classification and batch processing efficiency. Expect accuracy above 95% and near-instantaneous response times for large datasets, while optimizing for mobile devices with limited computational resources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"rare and endangered plant species\",\n        \"description\": \"Develop a specialized deep learning system for an AI competition focusing on high-resolution macro-photography of rare plants, with emphasis on real-time classification and batch processing efficiency for diverse lighting conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time (large datasets)\",\n                \"value\": \"near-instantaneous\"\n            },\n            {\n                \"name\": \"resource utilization (mobile devices)\",\n                \"value\": \"optimized for limited computational resources\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Orchid Leaf & Flower Image Dataset\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"diverse_lighting\": true\n            },\n            \"description\": \"A dataset for rare plant species containing macro-photography with various lighting conditions.\",\n            \"preprocessing\": [\n                \"image augmentation to simulate diverse lighting\"\n            ],\n            \"augmentation\": [\"data augmentation for varying lighting\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"specialized deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"mobile-friendly\",\n                \"real-time capability\": true,\n                \"efficient_batch_processing\": true\n            },\n            \"description\": \"A deep learning system designed for the competition, optimized for mobile devices with high accuracy and fast processing.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for mobile\"],\n        \"demonstration\": \"real-time classification and batch processing demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response was built based on the provided requirements. The user's expertise level is considered high, indicating deep knowledge in AI and ML. The 'area' and 'downstream_task' are identified, along with performance metrics, efficiency constraints, and dataset specifics. The service section includes a focus on mobile deployment and fast response times.\",\n        \"score\": 1.0\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a sophisticated translation system utilizing the ParaCrawl extensive dataset, employing a multi-modal dual encoder architecture for nuanced English-to-Dutch translations. The system must not only guarantee precise word-for-word conversion but also decipher and reproduce the subtleties of tone, register, and cultural context, ensuring a seamless cross-cultural communication experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-cultural communication\",\n        \"description\": \"The user wants to develop a sophisticated translation system using the ParaCrawl extensive dataset. The system should employ a multi-modal dual encoder architecture for nuanced English-to-Dutch translations, ensuring precise word-for-word conversion and an understanding of tone, register, and cultural context.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precise translation\",\n                \"value\": 1.0\n            },\n            {\n                \"name\": \"interpretation of subtleties\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language\": [\"English\", \"Dutch\"]\n            },\n            \"description\": \"A large multilingual dataset designed for training, ensuring a robust and nuanced translation system.\",\n            \"preprocessing\": [\"multi-modal data preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"multi-modal dual encoder architecture\",\n            \"family\": \"encoder-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"dual encoder with multi-modal components\"\n            },\n            \"description\": \"A model designed for nuanced English-to-Dutch translations using the ParaCrawl dataset, emphasizing precision and cultural context interpretation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"ParaCrawl's extensive coverage of English and Dutch, along with its multi-modal nature, helps with translation nuances.\",\n        \"Differences in tone, register, and cultural context might require extensive fine-tuning of the model to capture effectively.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"fine-tuning for nuanced translations\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for cross-cultural communication\"],\n        \"demonstration\": \"Demo showcasing seamless translation with preservation of cultural context\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provides a clear context for a high-level machine translation project. Metrics are defined, focusing on precise translation, but the need for interpretation of subtleties is left unspecified. The user's expertise level is assumed to be high, which suggests they are knowledgeable and may require less guidance. Some aspects like target device and deployment details are missing, but the focus on nuances and cross-cultural communication implies they are essential to the project.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Develop a next-generation, explainable AI system for the comprehensive monitoring of ecologically sensitive estuarine zones using the MARS--marine biodiversity dataset. Design a novel fusion of SqueezeSegV3 and Generative Adversarial Networks (GAN) for novel class discovery, specifically identifying rare species and underwater pollution. The system must maintain high accuracy while operating on satellite imagery at 16-band spectral resolution, accommodating for dynamic tides and weather conditions, and ensuring real-time analysis with a throughput of 10,000 square kilometers per hour for large-scale conservation planning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"ecological monitoring\",\n        \"description\": \"Develop a next-generation, explainable AI system for comprehensive monitoring of ecologically sensitive estuarine zones using the MARS--marine biodiversity dataset. The system should incorporate a novel fusion of SqueezeSegV3 and Generative Adversarial Networks (GAN) for novel class discovery, with a focus on rare species and underwater pollution detection.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"throughput\",\n                \"value\": 10000,\n                \"unit\": \"square kilometers per hour\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MARS-marine biodiversity\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"rare species\",\n                \"underwater pollution\"\n            ],\n            \"specification\": {\n                \"spectral_resolution\": 16\n            },\n            \"description\": \"The dataset for monitoring ecologically sensitive estuarine zones with 16-band satellite imagery, catering to dynamic tides and weather conditions.\",\n            \"preprocessing\": [\n                \"handling dynamic tides and weather conditions\"\n            ],\n            \"augmentation\": [\n                \"high-resolution satellite imagery augmentation\"\n            ],\n            \"visualization\": [\n                \"real-time monitoring and analysis\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SqueezeSegV3 + GAN fusion\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"novel fusion of SqueezeSegV3 and GAN\",\n                \"task\": \"image segmentation for rare species and pollution detection\"\n            },\n            \"description\": \"A model that combines SqueezeSegV3 and GAN for real-time analysis of estuarine zones\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"explainable AI\"\n        ],\n        \"target_device\": [\"satellite-based\", \"cloud\"],\n        \"deployment_endpoint\": \"large-scale conservation planning\",\n        \"inference_engine\": [\n            \"real-time analysis\"\n        ],\n        \"demonstration\": \"explainable segmentation results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure was correctly interpreted based on the instruction. The 'user' expertise is high due to the requirement for a complex and innovative solution. Performance metrics are not provided directly, but the emphasis on real-time and large-scale analysis suggests they will be prioritized. 'Service' section covers deployement on satellite and cloud, and explanation of model decisions, but missing exact inference engine details and the exact deployment endpoint. Some assumptions have been made for the missing details.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "question answering", "instruction": "The e-commerce CS project necessitates a real-time, sentiment-aware QA system using the AmazonQA dataset and DistilBERT. Achieve high precision, handle complex queries, and guarantee sub-second response times while maintaining at least 95% accuracy across diverse product categories.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"The e-commerce CS project requires a sentiment-aware, real-time QA system utilizing the AmazonQA dataset and DistilBERT. The system must handle complex queries, maintain at least 95% accuracy, and provide sub-second response times across diverse product categories.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 1.0\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.001,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AmazonQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A real-time QA dataset focusing on e-commerce, with sentiment-aware questions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DistilBERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"sentiment-aware QA\",\n                \"handles_complex_queries\": true,\n                \"time_complexity\": \"sub-second\"\n            },\n            \"description\": \"A sentiment-aware QA system, using DistilBERT for real-time question answering on the AmazonQA dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time, e.g., cloud or server\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU optimized\"],\n        \"demonstration\": \"Interactive product category-specific chat for evaluating sentiment-aware QA\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction covers the requirement to build a high-performing QA system with specified details (expertise, performance targets, and response times), and mentioning the use of DistilBERT and the AmazonQA dataset. The 'service' section is inferred to be for real-time and high-performance deployment. The missing pieces, such as specific model architecture details or the deployment endpoint, could be left to the expert's discretion.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager, your ML development team is tasked with refining a cutting-edge deep-learning algorithm for the specific ETTm2 industrial dataset, capturing intricate patterns in sensor data across multiple plants. The train, validation, and test splits consist of non-uniform, high-frequency temporal sequences ((INPUT_SEQ_LEN=144, INPUT_DIM=15, including categorical and continuous variables). The challenge is to design a robust predictive model that forecasts a complex, 168-step-ahead sequence (PRED_SEQ_LEN=168) with precision, measured by adjusted weighted mean squared error (WMSE) and root mean logarithmic error (RMLe). Emphasize interpretability and real-time adaptation while maintaining state-of-the-art performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"industrial, sensor data\",\n        \"description\": \"The project involves refining a deep-learning algorithm for the ETTm2 industrial dataset, addressing intricate patterns in sensor data across multiple plants. The train, validation, and test splits contain non-uniform, high-frequency temporal sequences with INPUT_SEQ_LEN=144 and INPUT_DIM=15 (including categorical and continuous variables). The goal is to develop a robust model that forecasts a 168-step-ahead sequence (PRED_SEQ_LEN=168) with high precision using adjusted weighted mean squared error (WMSE) and root mean logarithmic error (RMLe). Interpretability and real-time adaptation are crucial while maintaining state-of-the-art performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"WMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMLe\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2 (industrial)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 144,\n                \"INPUT_DIM\": 15,\n                \"categorical_variables\": true,\n                \"continuous_variables\": true\n            },\n            \"description\": \"Dataset from industrial sensor data with non-uniform, high-frequency sequences, including categorical and continuous variables for multiple plants.\",\n            \"preprocessing\": [\n                \"Categorical variable encoding\",\n                \"Continuous variable normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Cutting-edge deep-learning algorithm\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"interpretability\": true,\n                \"real-time adaptation\": true,\n                \"state-of-the-art performance\": true\n            },\n            \"description\": \"A deep learning algorithm designed for time-series forecasting in industrial sensor data, with emphasis on interpretability and real-time adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Expertise in handling non-uniform sensor data and optimizing for real-time adaptation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature selection for interpretability\",\n            \"Adaptive model update mechanism\"\n        ],\n        \"target_device\": \"cloud or edge device with high compute capabilities\",\n        \"deployment_endpoint\": \"Industry-specific API or dashboard for real-time predictions\",\n        \"inference_engine\": [\"state-of-the-art deep learning framework like TensorFlow or PyTorch\"],\n        \"demonstration\": \"Interactive dashboard showcasing forecasting results and model interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction was parsed based on the provided information, including the high expertise level of the project manager, the requirements for the ETTm2 dataset, the specific task details, performance metrics, and desired properties for the model. Key elements such as source, preprocessing, feature engineering, and model interpretability were adapted to suit the industrial context. However, the exact performance values for WMSE and RMLe were not provided, leaving them as null for now.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "For our upcoming advanced project, we aim to design a state-of-the-art transformer-based machine translation system using the extensive Common Crawl corpus. Our focus will be on creating a model capable of not only translating between English and Japanese with exceptional fluency but also recognizing and adapting to regional dialects, colloquial expressions, and slang, targeting a minimum accuracy of 98%. Additionally, outline a strategy for continuous improvement and evaluation of the model's evolving performance over time.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics and translation\",\n        \"description\": \"The user wants to design a state-of-the-art transformer-based machine translation system, targeting English to Japanese translation with a focus on regional dialects, colloquial expressions, and slang. The goal is to achieve at least 98% accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"corpus_size\": \"extensive\",\n                \"languages\": [\"English\", \"Japanese\"]\n            },\n            \"description\": \"A large-scale text corpus for training the machine translation system, sourced from Common Crawl.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based machine translation model\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"language_pair\": [\"English\", \"Japanese\"],\n                \"focus_on\": [\"regional dialects\", \"colloquial expressions\", \"slang\"]\n            },\n            \"description\": \"A state-of-the-art system emphasizing fluency and regional adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Model adaptation to evolving language and dialects will be crucial for continuous improvement.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"strategy\": \"Periodic performance evaluations on dialects, colloquial expressions, and slang over time.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the task requirements for designing a high-level machine translation system. The user's expertise level is high, indicating they likely require expert guidance or specifics for the project. The dataset and model details are accurately captured based on the instruction. However, target device and deployment endpoint are missing due to the lack of explicit information.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a top-tier machine learning team, you're tasked with developing an advanced time-series forecasting system. The Weather dataset, now expanded to 10 years of historical data with hourly readings, has been meticulously preprocessed and divided into train (80%), validation (10%), and test (10%) sets. The input sequences must capture long-term patterns (INPUT_SEQ_LEN=480, enriched with 10 additional contextual features), and the model should employ seasonal decomposition and exogenous variables. The challenge is to not only minimize mean squared error (MSE) and mean absolute error (MAE) but also incorporate a custom rolling window evaluation strategy with a sliding window of 24 hours. Additionally, the model must be scalable and robust to real-time data updates, ensuring at least 95% accuracy in the first 48-hour forecast.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"weather forecasting\",\n        \"description\": \"Develop an advanced time-series forecasting system using the Weather dataset with 10 years of hourly data. Preprocessed data is split into train, validation, and test sets. The model requires capturing long-term patterns with an input sequence length of 480, enriched with 10 additional contextual features. It should use seasonal decomposition and exogenous variables, with a custom rolling window evaluation strategy of 24 hours. The model must be scalable and achieve at least 95% accuracy within the first 48-hour forecast.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"real-time data updates\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Weather (10 years)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 480,\n                \"additional_features\": 10\n            },\n            \"description\": \"Historical weather data with 10 years of hourly readings, preprocessed and split into train, validation, and test sets.\",\n            \"preprocessing\": [\"seasonal decomposition\", \"exogenous variables inclusion\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"time-series modeling\": true,\n                \"seasonality decomposition\": true,\n                \"exogenous variables\": true,\n                \"rolling_window_size\": 24\n            },\n            \"description\": \"Advanced time-series forecasting model designed for weather data, with emphasis on long-term patterns and real-time updates.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"ensemble learning\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time scalable\"\n        ],\n        \"demonstration\": \"First 48-hour forecast accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a high-level project involving a sophisticated time-series forecasting system with advanced requirements. The user's expertise is assumed to be high, given the project manager role. The JSON response captures all necessary details, but there is room for assumptions about ensemble learning and the target device, as these were not explicitly mentioned.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a sophisticated time-series forecasting model using a stacked LSTM architecture for our comprehensive energy dataset, which spans multiple years and locations. The task is to predict hourly consumption for the next week, accounting for seasonality, holidays, and real-time weather data. Optimize the model for both accuracy and inference speed, targeting an average response time of 100 milliseconds per forecast while ensuring at least 90% of requests complete within 500 milliseconds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy consumption\",\n        \"description\": \"Create a sophisticated time-series forecasting model using a stacked LSTM architecture. The dataset spans multiple years and locations and the task is to predict hourly consumption for the next week, considering seasonality, holidays, and real-time weather data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Inference Speed\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Average Response Time\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Requests within 500 milliseconds\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Comprehensive Energy Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly consumption\"],\n            \"specification\": {\n                \"time_series_length\": null,\n                \"data_points_per_location\": null\n            },\n            \"description\": \"A comprehensive energy dataset with multiple years and locations for hourly consumption prediction, factoring in seasonality, holidays, and real-time weather data.\",\n            \"preprocessing\": [\"seasonality analysis\", \"holiday seasonality\", \"real-time weather integration\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Stacked LSTM\",\n            \"family\": \"LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"Layers\": null,\n                \"Training Speed\": null,\n                \"Inference Time\": null\n            },\n            \"description\": \"A stacked LSTM model for time-series forecasting of hourly energy consumption.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature scaling\",\n            \"ensemble techniques\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient inference algorithms\"],\n        \"demonstration\": \"visual dashboard of forecast results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction clearly describes a complex time-series forecasting project with requirements for accuracy, inference speed, and specific response times. The stacked LSTM architecture and comprehensive dataset indicate a higher level of expertise. However, specific model and dataset details are not provided, leaving those open for further discussion or confirmation.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a burgeoning e-novel platform, you aim to develop a real-time, context-aware translation system for translating Korean web fiction into multiple languages, including English. The project must utilize state-of-the-art machine translation models, involve data preprocessing with domain adaptation, and maintain high readability while optimizing for computational efficiency. You'll need to research and select a combination of models (e.g., Transformer models and recurrent neural networks), design a training curriculum with diverse genres, and regularly fine-tune for colloquial expressions. Additionally, outline the necessary infrastructure and performance benchmarks for scalability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"e-novel platform\",\n        \"description\": \"The user aims to develop a real-time, context-aware translation system for translating Korean web fiction into multiple languages, particularly English. Requirements include state-of-the-art machine translation models, domain adaptation in data preprocessing, high readability, and computational efficiency. Additionally, models should be a combination of Transformer models and recurrent neural networks, and need regular fine-tuning for colloquial expressions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"readability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"benchmarks\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Korean web fiction\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"web fiction\",\n                \"genre_diversity\": null\n            },\n            \"description\": \"Data for Korean web fiction, suitable for context-aware translation.\",\n            \"preprocessing\": {\n                \"methods\": [\"domain adaptation\"]\n            },\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer models\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"Recurrent Neural Networks\",\n            \"family\": \"\",\n            \"type\": \"neural networks\"\n        }\n    ],\n    \"knowledge\": [\n        \"Context-aware and domain adaptation, incorporating colloquial expressions, and maintaining high readability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain adaptation\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"description\": \"Real-time translation with demonstration of context awareness, genre adaptability, and colloquial expression handling.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction covers various aspects, such as the use of advanced models, preprocessing, fine-tuning, and scalability. However, specific performance metrics for readability and computational efficiency are not provided. Additionally, while the model types are mentioned, more specific details on their specifications are missing. Similarly, the target deployment device and inference engine are left unspecified.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "As a machine learning development team, our task is to design a sophisticated multi-layered arXiv paper classifier employing a hierarchical attention network (HAN) architecture. We aim for precision in distinguishing not just main fields but subfields, targeting an impressive 90% F1 score on a dynamically expanding dataset. Crucially, we need the model to exhibit exceptional scalability and real-time adaptability to incorporate newly published research papers without sacrificing accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Design a sophisticated multi-layered arXiv paper classifier using a hierarchical attention network (HAN) architecture. The objective is to achieve high precision in subfield classification, targeting a 90% F1 score. The model should demonstrate exceptional scalability and real-time adaptability for expanding datasets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"None\"\n            },\n            {\n                \"name\": \"real-time adaptability\",\n                \"value\": null,\n                \"unit\": \"None\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"main fields\", \"subfields\"],\n            \"specification\": {\n                \"dynamic size\": true,\n                \"latest version\": \"dynamic\"\n            },\n            \"description\": \"A dynamically expanding dataset for arXiv paper classification, including both main and subfield labels.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Attention Network (HAN) Classifier\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-layered HAN\",\n                \"accuracy under expansion\": \"optimized for scalability and real-time adaptability\"\n            },\n            \"description\": \"A sophisticated model designed for high-performance arXiv paper classification with HAN architecture, targeting 90% F1 score.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"online learning algorithms\"],\n        \"demonstration\": \"Methods to showcase real-time adaptation and scalability during live operation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a machine learning model for text classification. The user's high expertise level implies a need for detailed specifications. The performance metrics, such as the F1 score and the emphasis on scalability and real-time adaptability, are included. However, the exact dataset and model specifications are left inferred based on the context. The service section specifies the requirement for online learning and a demonstration for adaptability.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for the cutting-edge retail analytics platform, you are assigning a task to our advanced machine learning team. The objective is to create a state-of-the-art Faster R-CNN model, specifically the EfficientDet version, which should be fine-tuned on the extensive 'SKU-500K Enhanced' dataset. The model must not only detect and accurately count diverse products in cluttered shelf images but also distinguish among subcategories and handle seasonal variations. Additionally, request a comprehensive analysis of F1 score, AP (average precision), and TPR (true positive rate) for each category, along with a systematic error rate breakdown, and a user-friendly interpretability report. The ultimate goal is to ensure optimal inventory management with minimal false positives and false negatives.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail analytics\",\n        \"description\": \"Create a state-of-the-art Faster R-CNN model, specifically the EfficientDet version, fine-tuned on the 'SKU-500K Enhanced' dataset for detecting diverse products in cluttered shelf images. The model should also distinguish subcategories, handle seasonal variations, and provide detailed performance analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AP (Average Precision)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"TPR (True Positive Rate)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SKU-500K Enhanced\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"content\": \"A dataset for retail analytics, with diverse products in cluttered images and includes subcategory and seasonal variations.\",\n                \"training_data_distribution\": {\n                    \"cluttered_shelf_images\": true,\n                    \"seasonal_variations\": true\n                }\n            },\n            \"description\": \"A large-scale dataset for fine-tuning the Faster R-CNN model on product detection with subcategory and seasonal variations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"data augmentation techniques for handling clutter and seasonality\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientDet\",\n            \"family\": \"Faster R-CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"task_specific_details\": \"Fine-tuned for product detection, subcategory differentiation, and seasonal adaptability\",\n                \"pretrained_model\": \"on generic object detection\"\n            },\n            \"description\": \"A state-of-the-art Faster R-CNN model with EfficientDet implementation, fine-tuned for retail use.\"\n        }\n    ],\n    \"knowledge\": [\n        \"In the context of retail analytics, interpretability is crucial for optimal inventory management.\",\n        \"False positives and false negatives should be minimized for accurate product counting.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"TensorFlow or PyTorch, compatible with cloud deployment\"\n        ],\n        \"demonstration\": {\n            \"request\": \"user-friendly interpretability report and error rate breakdown\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The 'high' expertise level indicates the user is familiar with advanced ML tasks. The metrics required and the fine-grained nature of the task suggest a need for expertise in retail-specific ML. However, it's assumed that F1 score, AP, and TPR are desired, but exact values are not specified. The deployment endpoint and specific inference engine need to be confirmed. Overall, a high confidence in the understanding of the task but with room for refining details.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a high-precision machine learning team, you are tasked with developing a state-of-the-art time-series forecasting model for the Electricity dataset. This dataset consists of real-world, complex, and heterogeneous time series data that requires advanced handling due to seasonality, trends, and outliers. Your team must preprocess the train, validation, and test sets, with INPUT_SEQ_LEN set to 96 and INPUT_DIM of 321, employing advanced feature extraction techniques. The challenge now demands not only constructing a deep learning architecture with multiple layers and residual connections but also fine-tuning it using ensembles to optimize performance.\n\nThe primary goal is to predict the subsequent PRED_SEQ_LEN (96 steps) with 321-dimensional output vectors, ensuring competitive results based on Mean Squared Error (MSE) and Mean Absolute Error (MAE) metrics. Additionally, your team must devise a systematic approach to model interpretability and explainability, providing insights into key factors influencing the predictions. Conduct a rigorous hyperparameter search and compare performance with at least three different models, documenting the trade-offs for each.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"real-world, complex, and heterogeneous time series data\",\n        \"description\": \"Develop a state-of-the-art time-series forecasting model for the Electricity dataset, handling seasonality, trends, and outliers. Preprocess datasets (train, validation, and test) with INPUT_SEQ_LEN=96 and INPUT_DIM=321, employing advanced feature extraction. Use a deep learning architecture with multiple layers and residual connections, and fine-tune with ensembles. Target is to predict subsequent PRED_SEQ_LEN=96 steps with 321-dimensional output, optimizing for MSE and MAE. Ensure model interpretability and provide insights via analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321\n            },\n            \"description\": \"A real-world dataset with complex and heterogeneous time series data, including seasonality, trends, and outliers.\",\n            \"preprocessing\": [\"advanced feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Learning Architecture with Residual Connections\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble fine-tuning\": true,\n                \"number_of_layers\": \"multiple\",\n                \"residual_connections\": true\n            },\n            \"description\": \"A deep model designed for time-series forecasting with a focus on advanced handling techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Model interpretability and explainability are key, requiring a systematic approach to understand the factors affecting predictions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"advanced feature extraction\",\n            \"preprocessing for seasonality, trends, and outliers\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"specific_platform\",\n        \"inference_engine\": [\"TensorFlow Serving\"],\n        \"demonstration\": \"model explanations and performance comparison\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's extensive expertise is evident in their requirement of a high-performance model, advanced preprocessing, and ensemble fine-tuning. However, specific deployment details (deployment endpoint) are not included. The focus on interpretability and the choice of comparing at least three models show a deep understanding of the task.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Develop a real-time, high-accuracy image classifier using deep transfer learning, specifically for cats and dogs, with a constraint of achieving at least 98% validation accuracy within three epochs while implementing data augmentation techniques and utilizing an ensemble of pre-trained models for improved performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal recognition (specifically cats and dogs)\",\n        \"description\": \"Develop a real-time image classifier for distinguishing between cats and dogs with high accuracy. Target is to achieve at least 98% validation accuracy within three epochs. Data augmentation techniques and an ensemble of pre-trained models are expected to be used for improved performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"validation accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cats vs Dogs dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"category (cat or dog)\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing real-world images of cats and dogs, suitable for training a high-accuracy classifier.\",\n            \"preprocessing\": [\"data augmentation\"],\n            \"augmentation\": [\"image transformations like rotation, flipping, and resizing\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ensemble of pre-trained models\",\n            \"family\": \"Deep transfer learning models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"epochs\": 3,\n                \"ensemble_models\": [\"ResNet\", \"VGG16\", \"InceptionV3\"]\n            },\n            \"description\": \"An ensemble of pre-trained models, specifically ResNet, VGG16, and InceptionV3, employed for real-time high-accuracy cat and dog image classification with data augmentation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time performance demonstration with live images of cats and dogs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed based on the given user's instruction, capturing the high expertise level, the real-time requirement, and the target accuracy. The choice of pre-trained models, data augmentation, and a specific accuracy goal are included. However, the target device and deployment details are not mentioned, as they were not provided in the original instruction.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "multimodal forgery detection", "instruction": "As a machine learning project manager, you've been tasked with exploring the cutting-edge potential of novel deep learning models for detecting subtle video manipulation in the highly challenging 'Celeb-DF' dataset, which contains an immense variety of facial expressions and diverse backgrounds. Your objective is to design a research project that combines the latest advancements in multimodal transformers, such as M6 or MViT, to not only discern between genuine and forged videos but also to authenticate the authenticity of specific facial features, expressions, and their synchronization. The project should demonstrate a significant improvement over state-of-the-art forgery detection methods and provide a comprehensive evaluation using ROC curves, precision-recall trade-offs, and user acceptability studies. Additionally, you must consider the impact of different preprocessing techniques, model architecture adaptations, and transfer learning strategies to optimize performance while minimizing computational requirements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"video analysis, facial expression authentication\",\n        \"description\": \"Explore the use of novel deep learning models, specifically multimodal transformers like M6 or MViT, for detecting subtle video manipulation in the Celeb-DF dataset. The goal is to improve forgery detection, authenticate specific facial features and expression synchronization, and compare with state-of-the-art methods using ROC curves, precision-recall trade-offs, and user acceptability studies.\",\n        \"performance_metrics\": [\n            {\"name\": \"ROC curve\", \"value\": null},\n            {\"name\": \"precision-recall\", \"value\": null},\n            {\"name\": \"user acceptability\", \"value\": null}\n        ],\n        \"complexity_metrics\": [\n            {\"name\": \"computational requirements\", \"value\": null, \"unit\": \"FLOPs or inference time\"}\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"forgery detection\", \"facial feature authentication\"],\n            \"specification\": {\n                \"diverse facial expressions\": {},\n                \"varied backgrounds\": {},\n                \"multimodal data\": {}\n            },\n            \"description\": \"A challenging dataset containing diverse facial expressions and backgrounds for evaluating forgery detection and facial feature authentication.\",\n            \"preprocessing\": [\"multimodal fusion\", \"data normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"pre-and-post processing pipelines\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6 or MViT\",\n            \"family\": \"multimodal transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"latest advancements\": {},\n                \"specific architecture adaptations\": {},\n                \"transfer learning strategies\": {}\n            },\n            \"description\": \"Novel deep learning models using M6 or MViT as the backbone for forgery detection in the Celeb-DF dataset, focusing on improved facial feature and expression synchronization\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Considerations for preprocessing techniques, model architecture optimization, and transfer learning to balance performance and computational demands.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"multimodal fusion\", \"feature extraction\"],\n        \"target_device\": \"cloud or high-performance computing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow.js for cross-platform deployment\"],\n        \"demonstration\": \"Multimodal forgery detection and user acceptability study\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has specified a high-level task with a focus on the cutting-edge M6 or MViT models and a clear objective to surpass state-of-the-art. They understand the need for comprehensive evaluation metrics and optimization of computational requirements. The provided information is detailed and complete, with appropriate confidence level.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "Develop a real-time, interactive medical Q&A system integrating BioBERT, that processes the complex and diverse BioASQ dataset. Emphasize interpretability, performance optimization for rare diseases, and implement a citation ranking algorithm to showcase the most relevant scientific sources for each response.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"The user is requesting a real-time, interactive medical Q&A system that integrates BioBERT for processing complex and diverse BioASQ dataset. Specific focus is on interpretability, optimizing performance for rare diseases, and implementing a citation ranking algorithm.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy for rare diseases\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time for rare diseases\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint for real-time response\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"complexity\": \"complex and diverse\"\n            },\n            \"description\": \"A real-time, interactive medical Q&A system should be designed to process complex and diverse BioASQ dataset for question answering.\",\n            \"preprocessing\": [\n                \"BioBERT model fine-tuning\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BioBERT\",\n            \"family\": \"transformer-based model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability techniques\": \"embedding interpretability techniques\"\n            },\n            \"description\": \"Integration of BioBERT for enhanced performance in medical Q&A, with a focus on interpretability and rare diseases.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"topic\": \"citations ranking algorithm\",\n            \"description\": \"Implementing a citation ranking algorithm to display the most relevant scientific sources for each response\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-awareness\",\n            \"interactive search\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"real-time, interactive response with relevant citations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes the user's intent to build a model, their high expertise level, and relevant sections reflecting the user's requirements, such as performance for rare diseases, interpretability, and citation ranking. Key details are inferred based on the context provided, yet some performance metrics and complexity metrics are left unspecified for optimization. The service section specifies cloud deployment and GPU-optimized inference. Missing deployment endpoint is expected to be provided by the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, you're tasked with developing a cutting-edge translation tool that exclusively targets Quechua and Spanish in resource-constrained environments. Utilize the JW300 dataset to design an efficient LSTM model with advanced attention mechanisms, emphasizing cultural context preservation and achieving a minimum BLEU score of 30. Moreover, prioritize a mobile-friendly, real-time translation experience under 5MB model size for seamless user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"The project involves developing a cutting-edge translation tool specifically for Quechua and Spanish, considering resource-constrained environments. The LSTM model with advanced attention mechanisms should preserve cultural context, aiming for a minimum BLEU score of 30.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 30\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model size\",\n                \"value\": 5,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"real-time translation\",\n                \"value\": true\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"JW300\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"language\": [\"Quechua\", \"Spanish\"]\n            },\n            \"description\": \"A dataset for training an efficient machine translation model, focusing on Quechua and Spanish languages.\",\n            \"preprocessing\": [\"Cultural context preservation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"efficient LSTM with advanced attention\",\n            \"family\": \"LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"LSTM with attention mechanism\"\n            },\n            \"description\": \"A lightweight model designed for resource-constrained environments, specifically targeting real-time translation with a size of under 5MB\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cultural context preservation is critical for accurate Quechua and Spanish translations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"lightweight design\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\", \"mobile-friendly\"],\n        \"demonstration\": \"Seamless user experience in resource-constrained environments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction was clearly defined, including specific requirements for the model, dataset, and deployment. The model's performance metric, cultural context preservation, and mobile-friendly constraints were included. The user's high expertise level suggests a clear understanding of the project. However, the deployment endpoint was not provided, and it's assumed that it will be specified as part of the implementation process.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for the advanced time-series classification project, your team has been tasked with analyzing the complex and unstructured Ethanol Concentration dataset. This dataset, divided into train, validation, and test sets, contains a vast array of time-varying data with specific characteristics (INPUT_SEQ_LEN=1751, INPUT_DIM=3). Your objective is to develop a state-of-the-art model that employs advanced feature extraction techniques and incorporates multi-level temporal dependencies. The labels, categorized into {0, 1, 2, 3}, require not only high accuracy but also exceptional interpretability. Your model must demonstrate superior performance on diverse sequence patterns while utilizing novel model architectures and regularization methods to combat overfitting. Additionally, prepare a detailed evaluation plan that includes AUC-ROC and F1 scores alongside accuracy for a comprehensive assessment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"advanced\",\n        \"description\": \"As a project manager for an advanced time-series classification, your team is to analyze the Ethanol Concentration dataset. It's divided into train, validation, and test sets with INPUT_SEQ_LEN=1751, INPUT_DIM=3. The objective is to develop a state-of-the-art model using advanced feature extraction and multi-level temporal dependencies. The model should achieve high accuracy, interpretability, and perform well on diverse sequences. It must incorporate novel architectures and regularization to prevent overfitting.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 scores\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ethanol Concentration\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [\n                \"labels: {0, 1, 2, 3}\"\n            ],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 1751,\n                \"INPUT_DIM\": 3\n            },\n            \"description\": \"A complex and unstructured time-series dataset for advanced time-series classification with specific input characteristics and categorical labels.\",\n            \"preprocessing\": [\n                {\n                    \"technique\": \"advanced feature extraction\"\n                },\n                {\n                    \"technique\": \"multi-level temporal dependencies\"\n                }\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novel_architectures\": true,\n                \"regularization_methods\": true\n            },\n            \"description\": \"A state-of-the-art model for time-series classification, with advanced feature extraction and multi-level temporal dependencies, aimed at combating overfitting.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"advanced feature extraction\",\n            \"multi-level temporal modeling\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"interpretability\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed based on the provided information, reflecting the advanced nature of the project (expertise level is high) and including the advanced feature extraction, multi-level temporal dependencies, and interpretability goals. The performance metrics AUC-ROC and F1 scores are explicitly included in the 'performance_metrics' section. However, some specifications like accuracy, target device, and deployment details are missing, as they weren't included in the given instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For our ambitious citywide smart mobility initiative, we need a state-of-the-art YOLO-based object detection model that can precisely detect not only vehicles, pedestrians, and traffic signs but also cyclists, traffic lights, and emerging urban objects. The model must be highly efficient, achieving at least 50 FPS on commodity hardware, while maintaining a remarkable accuracy of 90% under diverse weather and lighting conditions. To ensure real-world feasibility, please incorporate domain-specific augmentation techniques and provide extensive benchmarking results for scalability across varying city sizes. Additionally, develop a user-friendly interface for live traffic monitoring and adaptive alert systems.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart mobility, urban planning\",\n        \"description\": \"Develop a state-of-the-art YOLO-based object detection model for a citywide smart mobility initiative. The model should detect vehicles, pedestrians, traffic signs, cyclists, and traffic lights, while being efficient (50 FPS on commodity hardware) and accurate (90% under diverse conditions). Include domain-specific augmentation, extensive benchmarking for scalability, and a user-friendly live traffic monitoring and adaptive alert system.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"FPS (commodity hardware)\",\n                \"value\": 50\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time on diverse hardware\",\n                \"unit\": \"FPS\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"weather_and_lighting_conditions\": [\"diverse\"]\n            },\n            \"description\": \"A custom dataset for object detection in urban scenes, including a focus on weather and lighting variations.\",\n            \"preprocessing\": [\"domain-specific augmentation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art YOLO\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"specifics\": \"high accuracy and efficiency for multiple objects\"\n            },\n            \"description\": \"A YOLO-based model optimized for city-scale object detection with diverse object classes.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"live traffic monitoring\",\n            \"adaptive alert system\"\n        ],\n        \"target_device\": \"commodity hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"user-friendly interface\"],\n        \"demonstration\": \"extensive benchmarking results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has specified a complex project with a high level of expertise, emphasizing performance, efficiency, and real-world applicability. The model's domain, performance metrics, and augmentation techniques have been inferred from the given information. However, the exact dataset name and some specific complexity metrics are missing, which would require additional clarification.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "question answering", "instruction": "The project for enhanced multilingual QA necessitates utilizing a mBERT or XLM-R model, demonstrating exceptional performance on the MLQA dataset. In addition to English, Spanish, and Mandarin, the model should handle Arabic and French effectively. Emphasize accuracy gains in low-resource environments while optimizing for balanced cross-lingual understanding.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"multilingual\",\n        \"description\": \"The project involves enhancing an enhanced multilingual QA system, requiring a mBERT or XLM-R model to demonstrate strong performance on the MLQA dataset. The model should handle English, Spanish, Mandarin, Arabic, and French, while prioritizing accuracy gains in low-resource languages and optimizing for balanced cross-lingual understanding.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MLQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"English\", \"Spanish\", \"Mandarin\", \"Arabic\", \"French\"]\n            },\n            \"description\": \"A multilingual QA dataset designed to evaluate the model's performance across multiple languages, including English, Spanish, Mandarin, Arabic, and French.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"mBERT or XLM-R\",\n            \"family\": [\"transformer-based\", \"multilingual\"],\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A state-of-the-art multilingual model (mBERT or XLM-R) for question answering, designed to excel on the MLQA dataset with emphasis on low-resource languages and balanced cross-lingual understanding.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"server-side inference\"],\n        \"demonstration\": \"performance on low-resource languages\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON reflects the user's requirements for building a multilingual QA model using mBERT or XLM-R, with emphasis on low-resource languages and performance on the MLQA dataset. The user's high expertise level suggests they have a good grasp of the task and the dataset. The 'service' section assumes a cloud deployment and server-side inference, while acknowledging the need for a demonstration of model performance in low-resource languages.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "Develop a highly-optimized AGNN-based link prediction model for Tencent Weibo, integrating sentiment analysis and temporal dynamics, to precisely identify subtle relationship shifts in the large-scale social network while maintaining interpretability and real-time adaptability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"social network analysis\",\n        \"description\": \"The task is to develop a highly-optimized AGNN-based link prediction model for Tencent Weibo. The model should integrate sentiment analysis and temporal dynamics for identifying subtle relationship shifts in large-scale social networks, with a focus on interpretability and real-time adaptability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time adaptability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tencent Weibo\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"link prediction\"],\n            \"specification\": {\n                \"node_size\": null,\n                \"edge_density\": null,\n                \"temporal_intervals\": null,\n                \"sentiment_data\": true\n            },\n            \"description\": \"Large-scale social network data from Tencent Weibo, including historical relationships and sentiment information.\",\n            \"preprocessing\": [\"sentiment analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"AGNN\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"AGNN\",\n                \"interpretability techniques\": \"to be provided\"\n            },\n            \"description\": \"AGNN-based link prediction model incorporating sentiment analysis and temporal dynamics for precise relationship shift detection.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"unknown (user will provide or optimize)\",\n        \"inference_engine\": [\"real-time adaptable\"],\n        \"demonstration\": \"model interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is clear to build a model (high expertise) within the area of graph machine learning. Performance metrics are not directly specified, but emphasis on accuracy, interpretability, and real-time adaptability implies they'll be considered. The high-level structure is accurate but details like the specific sentiment analysis techniques and the deployment endpoint need more information.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, you've been commissioned to design an innovative translation app for Quechua and Spanish in remote locales, integrating a hybrid deep learning model, based on the diverse JW300 corpus. The requirements include developing:\r\n\r\n1. A bidirectional LSTM with multi-head attention to capture cultural nuances and context, ensuring at least 30% improvement over baseline BLEU score.\r\n2. Optimize the model for mobile devices with strict resource limitations (target size <5MB), emphasizing speed and real-time performance.\r\n3. Implement efficient compression techniques, considering trade-offs between accuracy and size for an uninterrupted, seamless user experience across low-end devices.\r\n\r\nIncorporate a user-centered design that validates linguistic accuracy through pilot testing and iteratively refines the system based on user feedback.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistic translation for remote locales\",\n        \"description\": \"Design an innovative translation app for Quechua and Spanish, using a hybrid deep learning model based on the JW300 corpus. Requirements include a bidirectional LSTM with multi-head attention for capturing cultural nuances and context, aiming for at least a 30% improvement in BLEU score over the baseline. The model must be optimized for mobile devices with strict resource limitations, focusing on speed and real-time performance (target size <5MB). Implement efficient compression techniques for low-end devices to ensure a seamless, uninterrupted user experience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score improvement\",\n                \"value\": 0.3\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Model size\",\n                \"value\": 5.0,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"JW300 corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A diverse language corpus used for training the hybrid deep learning model for translation between Quechua and Spanish.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid LSTM with multi-head attention\",\n            \"family\": \"LSTM and Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"bleu_improvement\": 0.3,\n                \"resource_efficiency\": {\n                    \"model_size\": 5.0,\n                    \"device_limitation\": \"mobile\"\n                }\n            },\n            \"description\": \"A bidirectional LSTM with multi-head attention designed for capturing cultural nuances and context, optimized for mobile devices with a size <5MB.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"testing_method\": \"pilot testing\",\n            \"refinement_strategy\": \"iterative based on user feedback\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instructions outline a complex project involving a hybrid deep learning model and specific performance requirements. Given the context and technical requirements, the 'expertise' field is set to 'high'. All aspects, such as model design, performance metrics, and user-centered approach, are covered. The confidence score is high due to the clarity of the task and the specific details given.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "For our cutting-edge project, we intend to develop a sophisticated transformer-based multilingual machine translation system. Utilizing the immense Common Crawl dataset, design a modular architecture that not only delivers fluent translations between English, Japanese, and five lesser-known regional dialects, but also distinguishes between formal and informal language. Aim for a stringent precision benchmark of 98%+, and propose a comprehensive methodology incorporating periodic fine-tuning, A/B testing, and expert-annotated evaluation sets to track and incrementally enhance the model's performance consistently over an extended period, ensuring adaptability to linguistic shifts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Develop a sophisticated transformer-based multilingual machine translation system targeting English, Japanese, and five lesser-known regional dialects. The system should differentiate between formal and informal language, aiming for a precision benchmark of 98%+.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"English\", \"Japanese\", \"5 regional dialects\"]\n            },\n            \"description\": \"A large multilingual dataset sourced from Common Crawl for training and testing the translation model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"modular_architecture\": {\n                    \"language_distinguishability\": \"formal vs. informal\",\n                    \"target_languages\": [\"English\", \"Japanese\", \"5 regional dialects\"]\n                }\n            },\n            \"description\": \"A modular multilingual machine translation system with transformer architecture.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Periodic fine-tuning, A/B testing, and expert-annotated evaluation sets for tracking performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"periodic fine-tuning\", \"A/B testing\"],\n        \"target_device\": \"cloud or server environment for scalability\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-powered for fast processing\"],\n        \"demonstration\": \"Scenario of showcasing translation adaptations to linguistic shifts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the requirements specified in the instruction, such as the transformer-based model, precision benchmark, dataset, and the approach to improve the model through fine-tuning and testing. However, the model's specific names and expert evaluation set details are not included, which could be elicited for a complete response. The user's expertise level is inferred as high due to the project's sophistication and requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager leading a specialized team of top-tier machine learning innovators, your task is to conceive and build a novel deep learning architecture specifically tailored for the intricate ETTm2 dataset, a multifaceted industrial time series challenge that presents complex patterns and requires extraordinary performance. The data, divided into training, validation, and testing subsets with detailed sub-divisions (train[:80%], val[:15%], test[:5%]), consist of INPUT_SEQ_LEN (96) diverse data points with a rich dimensionality of INPUT_DIM (7), reflecting various economic indicators. The model must develop cutting-edge predictive prowess to forecast future market dynamics for PRED_SEQ_LEN (96) steps, maintaining state-of-the-art accuracy in metrics like quantile-based Mean Absolute Deviation (MAD) and Adjusted symmetric mean absolute percentage error (ASMAPE), while seamlessly integrating LIME explainability methods. Additionally, design the model to detect and handle outliers, adapt to dynamically shifting seasonal trends, and optimize for low computational footprint, ensuring it sets a new benchmark in the ever-evolving field of interpretable, resilient, and scalable time series forecasting.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"industrial time series, specifically economics\",\n        \"description\": \"Conceive and build a novel deep learning architecture tailored for the ETTm2 dataset, a complex industrial time series challenge with a focus on forecast accuracy, explainability, outlier detection, seasonality adaptation, and computational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"quantile-based Mean Absolute Deviation (MAD)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Adjusted symmetric mean absolute percentage error (ASMAPE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational footprint\",\n                \"unit\": \"FLOPs or another suitable unit\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 7,\n                \"sub_divisions\": {\n                    \"train\": [0.8],\n                    \"val\": [0.15],\n                    \"test\": [0.05]\n                },\n                \"PRED_SEQ_LEN\": 96,\n                \"data_properties\": {\n                    \"economic indicators\": \"Yes\",\n                    \"complex patterns\": \"Yes\"\n                }\n            },\n            \"description\": \"A multifaceted industrial time series dataset with detailed train, val, and test splits, representing various economic indicators with a high dimensionality.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"feature scaling\",\n                \"normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"pattern analysis\",\n                \"seasonality visualization\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for ETTm2's complexity\",\n                \"explainability\": \"LIME integration\",\n                \"outlier detection\": \"Yes\",\n                \"seasonality adaptation\": \"Yes\"\n            },\n            \"description\": \"A deep learning model designed for the ETTm2 dataset with advanced forecasting capabilities and explainability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The dataset consists of complex patterns and economic indicators\",\n        \"The model must be interpretable, resilient to outliers, and scalable\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection based on LIME explainability\",\n            \"dimensionality reduction\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TPU\"],\n        \"demonstration\": \"include model performance with ETTm2 data visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The high expertise level indicates a deep understanding, but specific model performance metrics and deployment details are missing and assumed based on the context. The 'confidence score' could be lower due to the assumptions made.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "As a skilled project manager for a groundbreaking AI research facility, your mission is to develop a sophisticated, multilingual, and transparent text classifier for the \"Quantum AI Dynamics and Attribution Corpus (QUADAC)\", necessitating not only the efficient separation of human-written poetic masterpieces (label A), avant-garde algorithm-generated verse (label B), but also the intricate detection of a novel class (label C) resulting from the fusion of evolutionary strategies. The classifier should exhibit an exceptional F1-score of 0.98 or higher, while maintaining explainability across various linguistic contexts and historical periods. Furthermore, ensure the model withstands rigorous linguistic adversarial tests from diverse sources, and incorporate an extensive audit trail for transparency, detailing its adherence to the latest ethical AI guidelines and conducting a comparative study with the state-of-the-art in ethical frameworks. The project must deliver comprehensive documentation reflecting iterative development stages.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"Develop a multilingual, transparent text classifier for the Quantum AI Dynamics and Attribution Corpus (QUADAC). The classifier must separate human-written poetry (label A), algorithm-generated verse (label B), and identify a new class (label C) resulting from evolutionary strategies fusion. Aim for an F1-score of 0.98 or higher with explainability across linguistic contexts and historical periods. Ensure robustness against linguistic adversarial tests and include an extensive audit trail adhering to ethical AI guidelines. Conduct a comparative study with the state-of-the-art ethical frameworks. Comprehensive documentation for iterative development stages is mandatory.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"QUADAC\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"human poetry\", \"algorithm-generated verse\", \"fusion (label C)\"],\n            \"specification\": {\n                \"linguistic_diversity\": \"multilingual\",\n                \"historical_periods\": \"various\"\n            },\n            \"description\": \"A corpus containing human-written and algorithm-generated poetry with an additional class for evolutionary strategies fusion.\",\n            \"preprocessing\": [\"explainability-friendly\"],\n            \"augmentation\": [\"linguistic adversarial\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multilingual Text Classifier\",\n            \"family\": \"transparent and explainable models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": true,\n                \"ethical_AI_compliance\": true\n            },\n            \"description\": \"A sophisticated model designed for the QUADAC with explainability, adversarial robustness, and ethical framework adherence.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Linguistic diversity and ethical AI frameworks\",\n        \"Adaptation to historical periods\",\n        \"Novel class detection through evolutionary strategies\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"iterative\", \"explainability\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"robust against adversarial tests\"],\n        \"demonstration\": \"explainable predictions and iterative development steps\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's requirement for a complex AI project. It includes the intent to build, high-level expertise, and detailed specifications for the task, dataset, and model. Performance metrics, ethical considerations, and linguistic aspects are also captured. The missing details for deployment and service-related aspects are assumed to be determined later in the project. Overall, a comprehensive response with a high confidence score.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a top-tier project manager for an elite AI innovation lab, your multifaceted assignment involves designing an elite time series analytics platform that harnesses the Wavenet-Extended UWave Gesture Dataset, a challenging 10,000-row dataset with 450 unique sensor readings per sample (INPUT_SEQ_LEN), divided into nine segments: a nested training set with four diverse subclasses, a validation set with temporally displaced patterns, a testing set with asynchronous data, and two subsets for benchmarking transfer learning and robustness. Your team must architect a novel recurrent-attention hybrid model, integrating real-time data compression, to classify the 12 intricate gesture styles (labels: A-K) with exceptional accuracy, precision, and low-latency response. Implement a tailored evaluation protocol that includes a hybrid F1-score, a user-generated interpretability metric, and a novel resource utilization metric, specifically targeting edge computing constraints. Rigorously document each architectural tweak and optimization strategy, as they directly impact the system's scalability, efficiency, and competitive edge in the AI research landscape.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"AI innovation lab, time series analytics platform\",\n        \"description\": \"Design an elite time series analytics platform using the Wavenet-Extended UWave Gesture Dataset. The dataset consists of 10,000 rows with 450 unique sensor readings per sample, divided into various sets for training, validation, testing, and benchmarking transfer learning and robustness. The objective is to build a novel recurrent-attention hybrid model that classifies 12 gesture styles with high accuracy, precision, and low-latency, while integrating real-time data compression. A tailored evaluation protocol with hybrid F1-score, user-generated interpretability metric, and edge computing resource utilization metric is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score (hybrid)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability metric\",\n                \"value\": null\n            },\n            {\n                \"name\": \"resource utilization (edge computing)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"real-time data compression ratio\",\n                \"value\": null,\n                \"unit\": \"Ratio\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wavenet-Extended UWave Gesture Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"Gesture style labels: A-K\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 450,\n                \"number_of_subclasses\": 4,\n                \"validation_set_pattern\": \"temporally displaced\",\n                \"testing_set_characteristics\": \"asynchronous\"\n            },\n            \"description\": \"A challenging dataset with 10,000 samples, 450 sensor readings per sample, organized for nested training, validation, and testing, and subsets for transfer learning and robustness evaluation.\",\n            \"preprocessing\": [\"Real-time data compression\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Recurrent-attention hybrid model\",\n            \"family\": \"Wavenet/Extended UWave\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A novel architecture combining recurrent and attention mechanisms for time series classification.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Documented architectural tweaks, optimizations for scalability, efficiency, and competitive edge in AI research.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time data compression\"],\n        \"target_device\": \"edge computing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"requirements\": \"Low-latency response and interpretability for user experience\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the high-level details provided, including the dataset complexity, requirements for model performance, and the focus on edge computing constraints. However, specific model performance values, deployment details, and some lower-level optimization strategies are not explicitly stated, warranting a lower confidence score.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager overseeing the cutting-edge development of an autonomous wind power prediction system, you have been tasked with enhancing a state-of-the-art fusion model. The project revolves around the recently released Ocean Turbine Dynamics dataset, which contains a decade of high-resolution data on wind speed, direction, and turbine performance, including variable weather patterns, geographical influences, and decommissioning trends. The input sequences have expanded to 500 sequential days (INPUT_SEQ_LEN=500), incorporating 200 detailed physical parameters (INPUT_DIM=200) to forecast the power output for the next 720-hour period (PRED_SEQ_LEN=720), with hourly forecasts for 10 distinct turbine efficiency categories. The objective is not only to achieve unprecedented accuracy with a target RMSE < 1% and coefficient of determination (R^2) > 0.95, but also to design a model that can autonomously adapt to extreme weather events and predict maintenance needs proactively. Implement a novel fusion of deep learning, physics-based neural networks, and a meta-learning strategy for efficient hyperparameter optimization using a hybrid cross-validation and reinforcement learning approach. Additionally, develop a comprehensive explainability module that provides actionable insights for turbine engineers without compromising model efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy, specifically wind power\",\n        \"description\": \"Enhance a state-of-the-art fusion model for an autonomous wind power prediction system, using the Ocean Turbine Dynamics dataset with expanded input sequences and physical parameters to forecast power output and turbine efficiency over 720 hours. Target RMSE is < 1% and R^2 is > 0.95, while ensuring model adaptability to extreme weather and proactive maintenance prediction.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": 0.01\n            },\n            {\n                \"name\": \"R^2\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ocean Turbine Dynamics\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [\n                \"power output\",\n                \"turbine efficiency (10 categories)\"\n            ],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 500,\n                \"INPUT_DIM\": 200,\n                \"PRED_SEQ_LEN\": 720,\n                \"time_resolution\": \"hourly\"\n            },\n            \"description\": \"A decade of high-resolution data on wind speed, direction, turbine performance, weather patterns, and decommissioning trends.\",\n            \"preprocessing\": [\"feature engineering for 200 physical parameters\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Fusion Model (deep learning, physics-based, meta-learning)\",\n            \"family\": \"Deep learning and physics-informed neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptability\": \"extreme weather events\",\n                \"maintenance prediction\": \"proactive\",\n                \"hyperparameter optimization\": \"hybrid cross-validation and reinforcement learning\"\n            },\n            \"description\": \"A novel fusion model that combines deep learning, physics-based models, and meta-learning for improved efficiency in wind power prediction and autonomous decision making.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Physics-based insights for better model representation\",\n        \"Value of explainability module for turbine engineers\",\n        \"Hybrid approach for hyperparameter optimization\"\n    ],\n    \"service\": {\n        \"feature_engineering\": \"incorporating 200 physical parameters\",\n        \"target_device\": \"potentially cloud or edge for real-time adaptability\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"hybrid, possibly GPU-optimized\"],\n        \"demonstration\": \"explainable predictions and actionable insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a clear problem statement for enhancing a model, specifying the dataset and metrics. Given the seasoned project manager and high-level details provided, the 'expertise' is assumed to be high. The JSON response captures this and includes requirements such as explainability, adaptation to extreme events, and proactive maintenance prediction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "For the sophisticated smart city initiative, conceive a highly efficient, minimalistic YOLO variant that not only detects real-time traffic dynamics with exceptional accuracy, but also accurately differentiates between various vehicle subclasses (including intricate details like articulated lorries and electric bikes), discerns diverse pedestrian profiles (refined by attributes like age, disabilities, and assistive equipment), and interprets complex road signs with nuanced interpretive semantics. The model should consistently exceed a precision benchmark of 95% on a diverse range of standardized datasets, while maintaining a stringent latency requirement of less than 1 millisecond for seamless 1200 FPS performance across an expansive array of heterogeneous hardware, embracing resource-restricted edge devices, cutting-edge AI co-processors, and energy-efficient solutions. Your proposal must encompass a comprehensive training methodology, adaptive hardware optimization strategies, and a thorough evaluation of the model's resilience across diverse hardware environments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart cities, traffic management\",\n        \"description\": \"Conceive a highly efficient, minimalistic YOLO variant for a sophisticated smart city initiative, requiring real-time traffic dynamics detection with high accuracy, differentiation between vehicle subclasses, and detailed pedestrian profiling, as well as robust interpretation of road signs. The model must have a precision above 95% on diverse datasets and sub-millisecond latency for 1200 FPS performance across various hardware, including edge devices and AI co-processors.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.001,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"standardized traffic\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"traffic dynamics\",\n                \"vehicle subclasses\",\n                \"pedestrian profiles\",\n                \"road signs\"\n            ],\n            \"specification\": {\n                \"diversity\": \"extensive, across different scenarios\"\n            },\n            \"description\": \"A diverse range of datasets for training, with real-time and standardized traffic conditions.\",\n            \"preprocessing\": [\"data augmentation for robustness\"],\n            \"augmentation\": [\"specific techniques for vehicles and pedestrians\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO variant\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": \"high\",\n                \"accuracy\": \"\",\n                \"minimalism\": \"\"\n            },\n            \"description\": \"A YOLO-based model optimized for real-time object detection in smart city scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Adaptive hardware optimization strategies and resilience evaluation across diverse hardware environments.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adapted for complex scenarios\"],\n        \"target_device\": [\n            \"resource-restricted edge devices\",\n            \"AI co-processors\",\n            \"energy-efficient solutions\"\n        ],\n        \"deployment_endpoint\": \"specific requirements for seamless integration into smart city systems\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"model performance with detailed analysis of latency and accuracy on diverse hardware\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been filled based on the complex task, high expertise level, and specific requirements for model performance, hardware optimization, and training methodology. However, certain details such as accuracy values for the model and the exact details of deployment endpoint are not provided. The confidence score reflects the assumption that some specific metrics are required for precision and that more information will be needed for a complete deployment plan.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design an RNN-based neural machine translation model, optimized for instant English to Russian translation in informal chat scenarios, using the extensive OPUS-100 dataset. Mandate a minimum BLEU score of 35 and restrict latency to an ultra-tight 150 milliseconds per sentence, ensuring seamless performance during peak traffic with real-time constraints. Moreover, devise a sophisticated evaluation methodology that not only tests the model's performance under limited data conditions but also analyzes its degradation patterns and suggests innovative data augmentation techniques to bolster resilience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"chatbot, informal\",\n        \"description\": \"Design an RNN-based neural machine translation model for real-time English to Russian translation in informal chat scenarios using the OPUS-100 dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU Score\",\n                \"value\": 35.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 0.15, \n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS-100\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\"English\", \"Russian\"],\n            \"specification\": {\n                \"language pair\": \"English to Russian\",\n                \"data size\": \"extensive\",\n                \"scenarios\": \"informal chat\"\n            },\n            \"description\": \"A large multilingual dataset for machine translation, focusing on English to Russian translation in informal contexts.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-based NMT model\",\n            \"family\": \"Neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"RNN\",\n                \"optimizer\": \"optimized for speed\",\n                \"latency_constraint\": true\n            },\n            \"description\": \"A model optimized for instant English to Russian translation with a BLEU score of 35 and sub-150ms latency.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Adopting a RNN for chat-oriented translation, ensuring robustness under real-time constraints.\"\n        },\n        {\n            \"description\": \"Incorporating evaluation methodology that tests performance under limited data conditions and identifies degradation patterns for data augmentation.\"\n        },\n        {\n            \"description\": \"Implementing data augmentation techniques to bolster model resilience during peak traffic.\"\n        }\n    ],\n    \"service\": {\n        \"feature\": {\n            \"optimization\": \"for low-latency in peak traffic conditions\",\n            \"real-time\": true\n        },\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"high-performance engines\"],\n        \"demonstration\": \"to showcase seamless performance and translation quality\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the user's high level of expertise and provides a clear task description for building an RNN-based NMT model optimized for specific performance criteria. Dataset, model, and evaluation methodologies are specified in detail. However, specific deployment details like the deployment endpoint are unknown based on the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the director of a cutting-edge AI research team, your mandate has expanded to not only develop a groundbreaking temporal sequence analysis model but to create a comprehensive solution that addresses the intricate challenges of the Energy Market Dynamics V2.0. This updated dataset, now featuring 1,500 unique attributes (INPUT_SEQ_LEN=192, INPUT_DIM=1,500, including 20 real-time indicators), necessitates the design of a sophisticated deep neural architecture that seamlessly integrates autoregressive mechanisms, hierarchical attention mechanisms, and dynamic weather forecasting inputs. The objective is to forecast a 120-step-ahead prediction (PRED_SEQ_LEN=120, PRED_DIM=1,500), while accounting for seasonality, meteoro-climatic fluctuations, and rare anomalies in a highly granular, rapidly evolving, and regulated environment. Your model must excel in benchmark performance metrics (RMSE and MAPE below industry standards), demonstrate explainable AI principles, maintain computational efficiency under high data throughput, and incorporate innovative techniques grounded in practical business constraints for long-term market success. Ensure the model's adaptability to unseen scenarios, potential regulatory changes, and continuous data revisions, all within a stringent resource constraint framework.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy market dynamics\",\n        \"description\": \"Develop a comprehensive solution for the Energy Market Dynamics V2.0 dataset. The model should handle 1,500 unique attributes, integrate autoregressive, hierarchical attention, and dynamic weather forecasting. Aim for a 120-step-ahead prediction with performance metrics RMSE and MAPE below industry standards, ensuring explainable AI, computational efficiency, adaptability to unseen scenarios, and resource constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computation time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Energy Market Dynamics V2.0\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 192,\n                \"INPUT_DIM\": 1500,\n                \"PRED_SEQ_LEN\": 120,\n                \"PRED_DIM\": 1500,\n                \"real-time indicators\": 20\n            },\n            \"description\": \"An updated dataset featuring 1,500 unique attributes, including 20 real-time indicators, for modeling the complex energy market dynamics.\",\n            \"preprocessing\": [\n                \"Rescaling\",\n                \"Normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"architecture\": \"Integrated autoregressive, hierarchical attention, and dynamic weather forecasting components\",\n                \"explainability\": \"Include explainable AI principles\",\n                \"efficiency\": \"Maintain computational efficiency under high data throughput\"\n            },\n            \"description\": \"A sophisticated model designed for temporal sequence analysis in the energy market, addressing季节ality, meteoro-climatic fluctuations, and anomalies.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"title\": \"Business constraints\",\n            \"description\": \"Incorporate innovative techniques respecting industry regulations and long-term market viability\"\n        },\n        {\n            \"title\": \"Adaptability\",\n            \"description\": \"Model must adapt to unseen scenarios, regulatory changes, and data revisions\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Seasonality adjustments\",\n            \"Rare anomaly detection\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"EnergyMarketAPI\",\n        \"inference_engine\": [\n            \"Fast inference algorithms\"\n        ],\n        \"demonstration\": \"Interactive visualizations for model outputs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction indicates a complex and challenging task that requires a high level of expertise. The user wants a deep neural architecture, performance within industry standards, and consideration of various practical factors. The missing metric values, for now, can be inferred during the model development process. The confidence score is medium to allow for potential clarification needed.\",\n        \"score\": 0.7\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design an advanced, Transformer-XL-enhanced multilingual translation platform, which seamlessly integrates a diverse corpus of XSum and lesser-recognized languages like Nepali, Indonesian, and Swahili. The system should undergo a comprehensive, multi-phased evaluation requiring a meticulous calculation of 45 distinct TERQ scores across four demanding translation scenarios involving complex technical domains: medical innovations in oncology, aerospace industry guidelines, intricate financial regulations, and esoteric fields like astrobiology and neurology. Aim for superior accuracy of 99.99% in recognizing domain-specific terminologies, while maintaining an unprecedented speed of 70,000 sentences per second to facilitate real-time, low-latency collaboration among global teams. Incorporate stringent GDPR compliance, including Explainable AI features with interactive visualizations for traceability and transparency, ensuring global trust and fostering cultural understanding in a secure, encrypted digital ecosystem. Emphasize the system's ability to handle varying linguistic nuances, dialects, and colloquialisms for seamless international communication.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": [\n            \"medical innovations in oncology\",\n            \"aerospace industry guidelines\",\n            \"intricate financial regulations\",\n            \"esoteric fields like astrobiology and neurology\"\n        ],\n        \"description\": \"Design an advanced multilingual translation platform using Transformer-XL with enhanced support for XSum and lesser-recognized languages (Nepali, Indonesian, Swahili). The system should undergo rigorous evaluation, focusing on TERQ scores across 45 domains, including complex technical subjects. Aim for 99.99% accuracy in domain-specific terminology and 70,000 sentences per second for real-time collaboration. Incorporate GDPR compliance with Explainable AI and interactive visualizations for traceability and transparency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9999\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": 70000,\n                \"unit\": \"sentences per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"XSum and lesser-recognized languages dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"English\", \"Nepali\", \"Indonesian\", \"Swahili\"],\n                \"corpus diversity\": \"diverse\"\n            },\n            \"description\": \"A corpus combining XSum and lesser-recognized languages for multilingual translation.\",\n            \"preprocessing\": [\"Transformer-XL integration\", \"domain-specific terminology handling\"],\n            \"augmentation\": [\"linguistic nuances, dialects, colloquialism handling\"],\n            \"visualization\": [\"GDPR compliant, Explainable AI with interactive visualizations\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL-enhanced\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": true,\n                \"gdpr_compliance\": true\n            },\n            \"description\": \"A machine translation model using Transformer-XL with focus on multilingual support and domain-specific terminology recognition.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"GDPR-friendly data handling\", \"Explainable AI\"],\n        \"target_device\": [\"cloud\", \"mobile\"],\n        \"deployment_endpoint\": \"encrypted digital ecosystem\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"facilitate low-latency collaboration and traceable translations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction provided a detailed request for a machine translation project, specifying multiple requirements such as accuracy, speed, domains, and GDPR compliance. The high expertise level is indicated, and given the specific details, I assume this refers to a highly experienced user. The inferred dataset and model details align with the provided instruction. However, I am unsure about whether the user assumes a specific deployment endpoint or inference engine, so those sections are left as flexible.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For the niche, fine-grained \"FusionFrenzy Retail Dataset v4.1\", upgrade the state-of-the-art YOLOv5 model to accurately identify a meticulously curated list of 200 exclusive gourmet products, adhering to rigorous standards: 99% micro-precision, 99.5% intra-seasonal recall, and an ultra-tight 0.8-second latency constraint for real-time gourmet marketplace scanning. The model must display adaptability to culinary fads, swiftly assimilate novel artisanal items, and seamlessly interoperate with a high-level, AI-driven inventory system requiring smart, hands-off adjustments from detected product scans, ensuring precision, speed, and staff optimization in a premium gastronomy setting.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail, gourmet products\",\n        \"description\": \"Upgrade the state-of-the-art YOLOv5 model for the 'FusionFrenzy Retail Dataset v4.1' to precisely identify 200 exclusive gourmet products with 99% micro-precision, 99.5% intra-seasonal recall, and a 0.8-second latency constraint for real-time scanning in a gastronomy setting. The model should be adaptable to culinary trends and integrate with an AI-driven inventory system for staff optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"micro-precision\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"intra-seasonal recall\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.8,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"FusionFrenzy Retail Dataset v4.1\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product categories (gourmet items)\"],\n            \"specification\": {\n                \"number_of_products\": 200,\n                \"product_features\": \"meticulously curated list of gourmet products\",\n                \"temporal_data\": \"intra-seasonal\"\n            },\n            \"description\": \"A fine-grained dataset for gourmet product detection in the retail context, catering to the unique requirements of a gastronomy setting.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5\",\n            \"family\": \"YOLO series\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_target\": 99,\n                \"latency_target\": \"0.8 seconds\"\n            },\n            \"description\": \"State-of-the-art YOLOv5 model enhanced for gourmet product detection with precision, recall, and real-time performance requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptability to culinary fads and seamless integration with AI-driven inventory system are crucial aspects.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"smart adjustments for detected products\"],\n        \"target_device\": \"real-time scanning environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"AI-driven inventory system integration\"],\n        \"demonstration\": \"precision, speed, and staff optimization for a gastronomy setting\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a high-quality object detection model, details about the dataset, model performance metrics, and service requirements. The knowledge and performance targets are explicitly stated, and the confidence score reflects the high level of specificity provided in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a transportation analytics director, demand a novel hybrid CNN-LSTM model enhanced with a spatio-temporal attention mechanism for the 'Metropolitan Expressway Network' dataset. The model should forecast minute-level traffic flow for the upcoming three weeks, differentiating between heavy rush hours (4-10am, 5-7pm), adaptively adjusting to unforeseen events like accidents or road closures, and maintain a weekend-weekday forecasting accuracy of at least 97%, utilizing open-source traffic sensor data integrated into the model architecture.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"transportation, traffic analysis\",\n        \"description\": \"The user, a transportation analytics director, wants to develop a novel hybrid CNN-LSTM model with spatio-temporal attention for forecasting minute-level traffic flow on the Metropolitan Expressway Network dataset. The model should cater to heavy rush hours, handle unforeseen events, and achieve a minimum 97% accuracy, utilizing open-source traffic sensor data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97,\n                \"weekend-weekday_task\": true\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Metropolitan Expressway Network\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"traffic flow\"],\n            \"specification\": {\n                \"data_types\": [\"sensor data\"],\n                \"time_resolution\": \"minute\",\n                \"forecast_horizon\": \"3 weeks\",\n                \"rush_hours\": [\"4-10am\", \"5-7pm\"]\n            },\n            \"description\": \"A dataset containing open-source traffic sensor data for the Metropolitan Expressway Network, requiring minute-level forecasting over three weeks.\",\n            \"preprocessing\": [\"data integration\", \"rush hour filtering\"],\n            \"augmentation\": [\"spatio-temporal\"],\n            \"visualization\": [\"traffic flow patterns\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"hybrid CNN-LSTM with spatio-temporal attention\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"CNN\", \"LSTM\", \"attention mechanism\"],\n                \"event_adaptability\": true,\n                \"resilience_to_unforeseen_events\": true\n            },\n            \"description\": \"A hybrid CNN-LSTM model with built-in spatio-temporal attention for the task of minute-level traffic flow forecasting on the Metropolitan Expressway Network, considering rush hours and adaptability to unforeseen events.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"handling sensor data\",\n            \"data fusion\",\n            \"attention model customization\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"open-source implementations\"],\n        \"demonstration\": \"weekend-weekday performance comparison\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a complex machine learning model, with high expertise assumption. The problem section captures the details of the dataset, task, and the desired accuracy. The preprocessing and augmentation steps align with the requirements. The service section includes expected feature engineering and the importance of adapting to unforeseen events. Some fields, like target_device and deployment_endpoint, are left unspecified, but reasonable given the information provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a logistics expert overseeing a luxury boutique retail network, your mandate is to develop a customized YOLOv5.3 model specifically fine-tuned for the \"Exquisite Crafted High-end Fashion Product Dataset,\" containing 70 distinct and niche fashion categories. The model must exhibit extraordinary accuracy with a minimum F1 score of 97%, while ensuring low computational complexity to handle 500 ultra-high-resolution images per second (4K) for ultra-zoom visual analysis. The system must instantly discern and tag related products, reducing false positives by 15% and integrating with a blockchain-based inventory management system to achieve real-time inventory traceability and zero discrepancies in haute couture stock.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury boutique retail\",\n        \"description\": \"The task is to develop a customized YOLOv5.3 model specifically fine-tuned for the 'Exquisite Crafted High-end Fashion Product Dataset' with 70 distinct fashion categories. The model should achieve at least 97% F1 score and have low computational complexity, able to handle 500 ultra-high-resolution (4K) images per second for real-time analysis. It must minimize false positives by 15% and integrate with a blockchain-based inventory management system for real-time stock traceability and zero discrepancies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time (per 4K image)\",\n                \"value\": 1.0,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Exquisite Crafted High-end Fashion Product Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"distinct_categories\": 70\n            },\n            \"description\": \"A luxury fashion product dataset for object detection, containing high-resolution images and 70 niche fashion categories.\",\n            \"preprocessing\": [\"custom_dataset_format\", \"4K resizing\"],\n            \"augmentation\": [\"YOLOv5 compatible techniques\"],\n            \"visualization\": [\"real-time object detection preview\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"customized YOLOv5.3\",\n            \"family\": \"YOLOv5\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimized_for\": \"low computational complexity\",\n                \"image_resolution\": \"4K\",\n                \"accuracy\": \"≥97% F1 score\"\n            },\n            \"description\": \"A YOLOv5.3 model fine-tuned for high-end fashion product detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Blockchain-based inventory management integration\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"false positive reduction techniques\"\n        ],\n        \"target_device\": \"ultra-fast, likely cloud-based to handle 4K images\",\n        \"deployment_endpoint\": {\n            \"type\": \"real-time, likely integrates with inventory management system\"\n        },\n        \"inference_engine\": [\"optimized for 4K image recognition\"],\n        \"demonstration\": \"ultra-zoom visual analysis and real-time product tagging\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction provided detailed information about the problem, dataset, and model requirements. The high expertise level suggests a solid understanding. However, the exact deployment endpoint and some feature engineering steps were assumed, as they were not explicitly mentioned.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a seasoned project manager for the elite Natural Language Understanding (NLU) team, your task is to design a cutting-edge machine learning system using the DAIGT dataset. The multifaceted goal is to not only achieve near-perfect discrimination between original human-written content (0) and AI-generated works (1), but also detect specific origins based on distinct model architectures and training epochs. Moreover, mandate the implementation of advanced LIME or SHAP explainability methods, demanding not only heightened accuracy but also stringent model explainability and bias mitigation for regulatory compliance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"Natural Language Understanding\",\n        \"description\": \"Design a state-of-the-art machine learning system for NLU, focusing on discrimination between human-written and AI-generated content (0 and 1), with the ability to identify specific origins based on model architectures and training epochs. Mandate advanced explainability methods like LIME or SHAP and strict bias mitigation for regulatory compliance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"human-written\", \"AI-generated\"],\n            \"specification\": {\n                \"original_data_type\": \"text\",\n                \"binary_labels\": true\n            },\n            \"description\": \"A dataset for NLP tasks, containing human-written and AI-generated content with annotations for model analysis.\",\n            \"preprocessing\": [\"label encoding\"],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"model_types\": [\"architecture-wise\", \"training_epoch-wise\"],\n                \"explainability_method\": [\"LIME\", \"SHAP\"]\n            },\n            \"description\": \"A deep learning model that achieves near-perfect discrimination and specific origin identification based on model architectures and training.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Regulatory compliance requirements for explainability and bias mitigation\",\n        \"Advanced explainability techniques like LIME or SHAP for model transparency\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"bias reduction techniques\", \"interpretable feature selection\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Model performance demonstration with LIME or SHAP explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON reflects a build intent for an advanced NLP project, matching the user's high level of expertise. Performance metric is set to 'near-perfect accuracy' based on the task description. The 'model' includes specific details about the need for unique architecture and explainability methods, and 'service' section addresses the requirements for bias mitigation and regulatory compliance explainability. Missing values in 'model' and 'service' fields indicate that they were not provided in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For a dynamic retail sector with rapid product diversification and constant SKU expansion, enhance the YOLO model to recognize 150+ unique products in the \"StoreShelf Dataset v3.0\" with >=98% precision, >=97% F1-score, and real-time analysis within 1.2 seconds per image. Develop a model that adapts to annual trends, swiftly acclimates to unseen SKUs, and seamlessly integrates with an advanced AI-driven inventory system, minimizing manual adjustments by >90% based on highly accurate and ultra-fast shelf scans, while ensuring system scalability and data privacy regulations are met.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"Enhance the YOLO model for dynamic retail sector, focusing on recognizing 150+ unique products in the StoreShelf Dataset v3.0 with >=98% precision, >=97% F1-score, and real-time analysis within 1.2 seconds per image.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 1.2,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"StoreShelf Dataset v3.0\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"unique_products\": 150\n            },\n            \"description\": \"A dataset for object detection in retail environments, including SKU expansion and diversification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced YOLO\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_adaptability\": \"annual trends and unseen SKUs\",\n                \"integration\": \"AI-driven inventory system\",\n                \"manual_adjustment_reduction\": 0.9\n            },\n            \"description\": \"A YOLO model optimized for real-time recognition in a dynamic retail setting, with high accuracy and integration requirements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"ultra-fast and highly accurate shelf scans\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction specifies a clear build task for enhancing the YOLO model, with performance and complexity metrics specified, as well as the domain and dataset requirements. The expert level expertise and the integration with AI-driven inventory system indicate the user is familiar with these requirements. However, the 'deployment_endpoint' is missing and should be clarified, as it was not explicitly stated.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project lead, your multifaceted assignment calls for the development of an advanced BERT-enhanced neural machine translation model, utilizing the massive and diverse WCCC dataset. This model must achieve exceptional accuracy of 99.5% in translating not only standard British English to complex Japanese dialects but also deciphering and adapting to subculturally nuanced phrases, slang, and regional expressions. Mandate includes the creation of an adaptive performance tracking system, as well as the investigation of novel methods for enhancing translation quality in under-explored Japanese dialects, focusing on boosting performance by 1% every quarter. Ensure the framework is scalable, resilient to bias, and capable of continuous self-improvement through incremental learning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-cultural communication\",\n        \"description\": \"As the project lead, you need to develop an advanced BERT-enhanced neural machine translation model using the WCCC dataset. The model should achieve 99.5% accuracy in translating British English to complex Japanese dialects, including understanding subcultural nuances, slang, and regional expressions. Additionally, requirements include creating an adaptive performance tracking system, researching novel methods for enhancing translation in under-explored dialects, and ensuring scalability, bias-resistance, and continuous self-improvement through incremental learning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"quarterly performance improvement\",\n                \"value\": 0.01\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WCCC\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A massive and diverse dataset for training a BERT-enhanced model for translating British English to complex Japanese dialects.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-enhanced Neural Machine Translation\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_target\": 0.995,\n                \"cultural_nuance_handling\": true\n            },\n            \"description\": \"BERT-based model optimized for translating British English to complex Japanese dialects with special focus on subcultural phrases and expressions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"BERT adaptation for under-explored Japanese dialects\",\n        \"Incremental learning for continuous self-improvement\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"nature\": \"adaptive performance tracking\",\n            \"quarterly_improvement\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the given instruction, including the advanced model development task, specific accuracy requirement, handling of subcultural nuances, and the focus on adaptive performance tracking and continuous improvement. The user's high level of expertise is captured, but specific deployment and inference details are left unspecified. The confidence score is high given the clarity of the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project director for a specialized AI-powered cross-cultural support system, design an advanced deep neural network (DNN) translator with domain-specific knowledge in culinary, fashion, and slang lexicons. The model must not only facilitate fluent translations across fifty languages but also discern and emulate local dialects, idiomatic expressions, and seasonal colloquialisms. Implement real-time adaptation through a meticulously curated list of 500 regional APIs, ensuring compatibility, scalability, and a 98% accuracy rate. Prioritize compliance with GDPR, ISO 27001, and the AI Fairness 360 guidelines while monitoring system performance with stringent micro-benchmarks every 24 hours.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": [\"culinary\", \"fashion\", \"slang\"],\n        \"description\": \"Design an advanced deep neural network translator specialized in culinary, fashion, and slang, supporting fifty languages. The model should discern local dialects, idiomatic expressions, and seasonal colloquialisms. Real-time adaptation through 500 regional APIs is required with 98% accuracy and GDPR, ISO 27001, and AI Fairness 360 compliance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"API compatibility and scalability\",\n                \"value\": null,\n                \"unit\": \"N/A\"\n            },\n            {\n                \"name\": \"Micro-benchmarks frequency\",\n                \"value\": 1,\n                \"unit\": \"times per 24 hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Regional linguistic APIs\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translations\", \"dialects\", \"idiomatic expressions\"],\n            \"specification\": {\n                \"languages\": 50,\n                \"regional variety\": \"local, regional, and seasonal\"\n            },\n            \"description\": \"500 carefully curated regional APIs providing diverse linguistic data for training and adaptation.\",\n            \"preprocessing\": [\"API integration\", \"data curation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Deep Neural Network Translator\",\n            \"family\": \"Transformer-based or Encoder-Decoder Architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"domain-specific knowledge\": \"culinary, fashion, slang\",\n                \"local dialect recognition\": true,\n                \"real-time adaptation\": true\n            },\n            \"description\": \"A deep neural network designed for cross-cultural support with a focus on local language nuances.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain adaptation\", \"linguistic privacy\"],\n        \"target_device\": [\"cloud\", \"mobile\"],\n        \"deployment_endpoint\": \"GDPR compliant, ISO 27001 certified\",\n        \"inference_engine\": [\"real-time\", \"scalable\"],\n        \"demonstration\": \"with local dialect and seasonal colloquialism examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been parsed based on the provided instruction. The user's intent is to build a model, their level of expertise is high due to the complexity of the project requirements. All relevant aspects, such as the model's purpose, performance metrics, and the specific service requirements, have been included. The AI Fairness and privacy guidelines are considered. Some sections might require more clarification, such as the exact complexity of API compatibility and scalability, but overall the information provided is sufficient.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager is now tasked with developing a next-generation Q&A platform, CodeSparkle, for kindergarten to fifth-grade students. Utilize the exclusive EarlyCherubs dataset, which demands the creation of a state-of-the-art BERT model tailored for educational impact and bigraphemic literacy. Embody playful, augmented reality elements and conduct a user study with diverse age groups, ensuring adherence to neurodevelopmental principles and universal design principles for enhanced engagement and equitable learning experiences.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Develop a next-generation Q&A platform, CodeSparkle, targeting kindergarten to fifth-grade students. Use the EarlyCherubs dataset and create a state-of-the-art BERT model tailored for educational impact and bigraphemic literacy. Incorporate playful augmented reality elements and conduct a user study with diverse age groups, adhering to neurodevelopmental and universal design principles.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"educational impact\",\n                \"value\": null\n            },\n            {\n                \"name\": \"bigraphemic literacy improvement\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"EarlyCherubs\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"An exclusive dataset designed for educational Q&A platform, focusing on kindergarten to fifth-grade students.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"tailored_for_education\": true,\n                \"bigraphemic_focus\": true\n            },\n            \"description\": \"A state-of-the-art BERT model adapted for educational impact and bigraphemic literacy enhancement on the CodeSparkle platform.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Playful AR elements\",\n        \"Neurodevelopmental principles\",\n        \"Universal design principles for engagement\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"User study with diverse age groups\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction describes a comprehensive project with high-level requirements, including a high expertise level for the user. The 'problem' section covers the area of NLP and educational Q&A. The model adaptation and dataset specifics are clearly specified, as is the inclusion of augmented reality and user studies. However, there are no specific metrics for service requirements or deployment target, which is expected to be filled with more details in a real scenario.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project director for a specialized AI research and development team, we've been tasked with creating a state-of-the-art predictive analytics platform for the highly complex M4 Multivariate Time-series dataset. This project necessitates handling an extensive dataset partitioned into train, validation, and test sets, featuring minute-level granularities across 100 unique sub-industries, each with variable length sequences of 576 timestamps encompassing 100 diverse economic indicators. The system must accurately forecast the subsequent 4608 steps (100 dimensions) with enhanced accuracy, maintaining precision across multiple seasonalities, abrupt shifts, and non-stationary patterns. \n\nIncorporating cutting-edge techniques such as sparse Fourier transforms, machine learning-driven feature extraction, and nested cross-validation for model robustness, the model should demonstrate robustness against extreme values and real-world shocks, and exhibit dynamic adaption in dynamic environments. The final deliverable should consist of a comprehensive research paper detailing the intricate algorithm design, rigorous comparison with established benchmarks, a thorough hyperparameter sensitivity analysis, and a real-world case study showcasing the model's practical applicability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"predictive analytics\",\n        \"description\": \"Create a state-of-the-art predictive analytics platform for the M4 Multivariate Time-series dataset, handling minute-level data across 100 sub-industries, 576 timestamps per sequence, and 100 economic indicators. The model should forecast the next 4608 steps with high precision, robustness to seasonalities, abrupt shifts, and non-stationary patterns. Techniques like sparse Fourier transforms, machine learning-driven feature extraction, and nested cross-validation are required for robustness, and handling extreme values and real-world shocks. A detailed research paper, benchmark comparisons, hyperparameter sensitivity analysis, and a real-world case study should be included.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"seasonality-awareness\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness to extreme values\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"M4 Multivariate Time-series\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"length_per_sequence\": 576,\n                \"unique_sub_industries\": 100,\n                \"dimensions\": 100,\n                \"sequence_type\": \"monthly (or minute-level granularity)\"\n            },\n            \"description\": \"Extensive dataset partitioned into train, validation, and test sets with complex structure across various sub-industries and economic indicators.\",\n            \"preprocessing\": [\n                \"sparse Fourier transforms\",\n                \"machine learning-driven feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_extraction_techniques\": [\"sparse Fourier transforms\", \"ML-driven\"],\n                \"cross-validation_type\": \"nested\",\n                \"dynamic_adaptation\": true\n            },\n            \"description\": \"A predictive analytics model for M4 dataset with robustness to seasonalities and non-stationary patterns.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Handling extreme values and real-world shocks\",\n        \"Dynamic adaptation in dynamic environments\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonal decomposition of time series\",\n            \"preprocessing for robustness against outliers\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Apache Spark\", \"TensorFlow serving\"],\n        \"demonstration\": \"proof of concept on real-world data scenario\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction parsed accurately into a JSON format, including high-level details about the project and a clear focus on the state-of-the-art predictive analytics platform. It specifies advanced techniques and requirements for a high expertise level user. However, specific model and performance metrics are not explicitly mentioned, leaving those details for further clarification.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "question answering", "instruction": "As a seasoned project manager, task the ML development team with the intricate design of an advanced QA platform, catering to the MLQA dataset's nuances. Mandate cross-lingual prowess for English, Spanish, Mandarin, French, and Arabic, ensuring seamless adaptability in resource-constrained environments. However, the platform must excel using mBERT and XLM-R with a significant boost of at least 15% in cross-lingual F1 score, without compromising real-time responsiveness. In addition to core development, request a comprehensive breakdown of performance metrics for each language combination, focusing on precision, recall, and execution time to guarantee optimal quality and efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"advanced QA platform\",\n        \"description\": \"Design an advanced QA platform catering to the MLQA dataset's nuances. The platform should have cross-lingual capabilities for English, Spanish, Mandarin, French, and Arabic, with a focus on resource-constrained environments.\",\n        \"performance_metrics\": {\n            \"cross-lingual F1 score\": {\n                \"target_value_increase\": 0.15,\n                \"metric_type\": \"cross-lingual F1\"\n            },\n            \"precision\": {\n                \"label\": \"for each language combination\",\n                \"metric_type\": \"precision\"\n            },\n            \"recall\": {\n                \"label\": \"for each language combination\",\n                \"metric_type\": \"recall\"\n            },\n            \"execution_time\": {\n                \"label\": \"for real-time responsiveness\",\n                \"metric_type\": \"execution time\"\n            }\n        },\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MLQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"cross-lingual languages\": [\"English\", \"Spanish\", \"Mandarin\", \"French\", \"Arabic\"]\n            },\n            \"description\": \"A dataset catering to the specific requirements of the advanced QA platform with nuances across various languages.\",\n            \"preprocessing\": [\n                \"cross-lingual data preparation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"mBERT and XLM-R\",\n            \"family\": \"Transformer-based language models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"languages\": [\"English\", \"Spanish\", \"Mandarin\", \"French\", \"Arabic\"],\n                \"real-time_response\": \"yes\"\n            },\n            \"description\": \"The platform should rely on mBERT and XLM-R for robust cross-lingual performance\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"multilingual fine-tuning\",\n            \"low-resource adaptation techniques\"\n        ],\n        \"target_device\": \"resource-constrained environments\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for mBERT and XLM-R\"\n        ],\n        \"demonstration\": \"quality and efficiency assessment reports for each language combination\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction parsed accurately covers the project scope (advanced QA platform, specific languages, performance boost), data needs (MLQA dataset and cross-lingual support), model choice (mBERT and XLM-R), real-time response, and performance metric requirements. Given the seasoned project manager level, we assumed a high level of expertise. We are confident but note that a deployment endpoint and the fine details of performance metrics breakdown should be provided by the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "Act as a seasoned project manager overseeing a specialized team tasked with creating a groundbreaking, domain-restricted Q&A platform for biochemistry. Utilize the ultra-detailed MedBiomX Pro dataset, encompassing intricate queries and precise trial outcomes from cutting-edge liver disease research. Deploy the state-of-the-art M6v4.2 transformer model, which demands the integration of SHAP (SHapley Additive exPlanations) for comprehensible medical insights, enabling molecular scientists to pinpoint answer derivations. The platform should feature a real-time crisis response functionality, leveraging the next-generation Quantum-enhanced Critical Care Decision Support System (QCCDSS) to analyze intricate molecular interactions in limited timeframes. The system must exhibit anomaly detection prowess in detecting inconsistencies in preclinical trials and adapt swiftly to newfound insights in pharmacogenomics. Implement a self-evolving knowledge base with a fail-safe mechanism ensuring a minimum efficiency rate of 99.5% during peak hours, under strict constraints of 5-second query processing. Prepare a meticulous project roadmap, including weekly milestones marked by benchmarking model performance on intricate biochemistry queries and conducting expert user evaluations on the explainability module's effectiveness every month.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"biochemistry\",\n        \"description\": \"Create a groundbreaking Q&A platform for biochemistry, using the MedBiomX Pro dataset with intricate queries and precise trial outcomes. Deploy M6v4.2 transformer model, integrating SHAP for explainable insights. Include real-time crisis response with QCCDSS and anomaly detection for pharmacogenomics insights. Aim for a minimum efficiency of 99.5% with a 5-second query processing time.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy on Biochemistry Queries\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Explainability Module Effectiveness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Query Processing Time\",\n                \"value\": 5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MedBiomX Pro\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"detail\": \"ultra-detailed, encompassing intricate queries and precise trial outcomes\"\n            },\n            \"description\": \"A dataset for a biochemistry Q&A platform, containing detailed and advanced research inquiries.\",\n            \"preprocessing\": [\"SHAP integration\", \"Quantum-enhanced analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6v4.2 Transformer\",\n            \"family\": \"transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"SHAP Integration\": true\n            },\n            \"description\": \"Deploying state-of-the-art model for question answering, demanding explainable insights through SHAP.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Quantum-enhanced Critical Care Decision Support System (QCCDSS) for real-time crisis response\",\n        \"Anomaly detection for inconsistencies in preclinical trials\",\n        \"Self-evolving knowledge base with fail-safe mechanism\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"M6v4.2 adaptation\", \"Crisis response module\", \"Expert user evaluations\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Quantum-enhanced\", \"real-time\"],\n        \"demonstration\": \"Regular benchmarking of model performance on intricate queries and expert evaluations on explainability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction was parsed based on the provided schema, with a high level of expertise assumed due to the complexity of the project. Performance metrics, such as accuracy, are not provided as specific values, and a clear roadmap with weekly milestones is included, like model benchmarking and expert evaluations.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As the principal project administrator of a top-tier AI research group, your pressing mission involves developing an advanced predictive system for the Enhanced Time-series Panorama dataset. This updated dataset contains an astonishing 136 variables, with highly irregular time intervals introducing complex distribution variations among the dynamically allocated train (80%), validation (15%), and novel adaptive test partitions (15%). Your team must architect a highly scalable and adaptive ensemble model, capable of forecasting 321-step sequences (INPUT_SEQ_LEN + EXTENSIVE_FORECAST windows), aiming to minimize Mean Squared Error (MSE), Mean Absolute Error (MAE), and a newly formulated dynamic rolling Root Mean Squared Logarithmic Error (RMSLEadapt). Crucially, ensure the model delivers high interpretability by integrating explainable AI principles, enhances resource optimization in multi-cloud environments, and exhibits robustness against environmental fluctuations while adhering to rigorous constraints without sacrificing transparency. In addition to the technical implementation, prepare a groundbreaking research paper of no less than 2,700 words, delving into the intricate architectural nuances, advanced regularization strategies, and the meticulous trade-offs made to maintain interpretability within the context of stringent resource allocation and environmental unpredictability. The paper must be clear, concise, and demonstrate innovative thinking.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"advanced predictive systems\",\n        \"description\": \"Develop an advanced predictive system for the Enhanced Time-series Panorama dataset, which contains 136 variables with irregular time intervals. The model must be an ensemble, scalable, and adaptive for 321-step forecasting, optimizing MSE, MAE, and a dynamic RMSLEadapt. Emphasize interpretability, resource optimization in multi-cloud, and robustness against environmental fluctuations under strict constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSLEadapt\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource optimization\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"environmental robustness\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enhanced Time-series Panorama\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 321,\n                \"EXTENSIVE_FORECAST\": \"321-step sequences\"\n            },\n            \"description\": \"Large-scale dataset with 136 variables, irregular time intervals, and train/validation/test partitions with 80%/15%/15% split.\",\n            \"preprocessing\": [\"data cleaning, irregular time intervals handling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true,\n                \"multi-cloud optimization\": true,\n                \"environmental robustness\": true\n            },\n            \"description\": \"A highly scalable, adaptive model for 321-step forecasting with emphasis on interpretability and resource optimization in multi-cloud environments.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Innovative regularization techniques, trade-offs between interpretability and resource constraints, and handling environmental fluctuations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"explainable AI integration\",\n            \"resource allocation optimization\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"multi-cloud infrastructure\",\n        \"inference_engine\": [\"optimized for multi-cloud\"],\n        \"demonstration\": \"Robustness test cases and interpretability visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The instruction clearly outlines a complex machine learning project involving data from an extensive time-series dataset and a multi-faceted model requirement. The 'expertise' field implies a high level of understanding, and the 'service' section reflects a comprehensive approach. However, there might be missing details in the 'specifications' section of the models, 'complexity metrics', and 'deployment endpoint' for a more precise assessment.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For a state-of-the-art, style-sensitive segmentation of antique and modern furniture in high-definition indoor environments with varying skylights, develop a U-Net architecture that guarantees sub-50 millisecond latency, maintaining an accuracy above 95% even under challenging low-light conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"art and interior design\",\n        \"description\": \"The goal is to develop a state-of-the-art style-sensitive segmentation model, targeting antique and modern furniture in high-definition indoor environments with varying skylights. The model must maintain sub-50 millisecond latency and achieve an accuracy above 95% even under low-light conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 50,\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"high definition\",\n                \"skylight_variability\": \"true\",\n                \"lighting_conditions\": \"low light\"\n            },\n            \"description\": \"Dataset with antique and modern furniture images, capturing the diverse indoor environments with varying skylights and low-light conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net\",\n            \"family\": \"U-Net architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"latency\": {\n                    \"inference_speed\": \"sub-50ms\"\n                }\n            },\n            \"description\": \"A state-of-the-art deep learning model for style-sensitive furniture segmentation in challenging indoor environments with low-latency requirements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance, low-latency compatible\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction has been parsed into a JSON format, including the user's intent (build), high expertise level, specific area (computer vision), and performance and latency requirements. The problem description captures the main task, lighting conditions, and model requirements. However, the dataset name was not provided and should be included if a specific one is mentioned. The model name, family, and specifications are accurate based on the U-Net architecture, but more details might be needed for the deployment and inference engines.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "You're tackling a specialized time-series forecasting challenge using the MIT-BIH Arrhythmia dataset. It consists of electrocardiogram (ECG) signals divided into train, validation, and test subsets. Each input sequence contains physiological measurements (INPUT_SEQ_LEN=500, select 10 relevant bioelectrical features among INPUT_DIM=200) for detecting specific cardiac abnormalities with a target labels {N, L, Q, R, S, V} in 5-class classification. Aim for high macro-averaged F1-score while maintaining interpretability. Implement a convolutional neural network architecture and fine-tune for optimal performance on the diverse and noisy data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The task is to build a convolutional neural network (CNN) for time-series forecasting on the MIT-BIH Arrhythmia dataset, targeting 5-class classification of cardiac abnormalities (N, L, Q, R, S, V) with a focus on high macro-averaged F1-score and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"macro-averaged F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MIT-BIH Arrhythmia\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"N\", \"L\", \"Q\", \"R\", \"S\", \"V\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 500,\n                \"INPUT_DIM\": 200,\n                \"subset\": [\"train\", \"validation\", \"test\"]\n            },\n            \"description\": \"ECG signals with physiological measurements split into train, validation, and test subsets, focusing on 10 relevant bioelectrical features and diverse, noisy data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Convolutional Neural Network\",\n            \"family\": \"Time-Series Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"type\": \"CNN\",\n                    \"interpretability\": true\n                }\n            },\n            \"description\": \"Model designed for time-series classification with interpretability, targeting high performance on the MIT-BIH Arrhythmia dataset with a focus on 5-class cardiac abnormality detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The task emphasizes interpretability, which might require attention to model explainability techniques or feature selection.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretable models\"],\n        \"demonstration\": \"Explainable predictions to increase interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response reflects the task instruction by specifying a time-series forecasting challenge with a high-level goal of interpretability. A CNN architecture is chosen for its performance on noisy data. The performance metric (macro-averaged F1-score) and relevant dataset characteristics are included. High confidence is placed on the relevance and completeness of the structure and information as it relates to the provided task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning project manager for the enhanced image classification project, we must now focus on the Stanford Cars Dataset. Mandate the Inception-v3 model to not only classify car images accurately into detailed make, model, and year, but also differentiate between sub-models and variations. Introduce an additional constraint: the model must achieve a consistent precision and recall across all classes, with an ambitious target of 98% overall accuracy while handling extreme weather conditions and complex occlusions. Provide detailed per-class and ensemble evaluation for robustness.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"automotive\",\n        \"description\": \"The project is focused on enhancing the image classification with the Stanford Cars Dataset using the Inception-v3 model. The model should classify car images into detailed make, model, year, sub-models, and variations accurately. The target is to achieve a consistent precision and recall of 98% across all classes while handling extreme weather conditions and complex occlusions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"make\", \"model\", \"year\", \"sub-model\", \"variation\"],\n            \"specification\": {\n                \"diversity\": {\n                    \"weather\": \"extreme\",\n                    \"occlusions\": \"complex\"\n                }\n            },\n            \"description\": \"A dataset for enhanced image classification, requiring classification into car details and handling extreme weather and complex occlusions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Inception-v3\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"target_classes\": [\"make\", \"model\", \"year\", \"sub-model\", \"variation\"],\n                \"weather_invariance\": true,\n                \"occlusion_invariance\": true\n            },\n            \"description\": \"A robust model that classifies car images into detailed categories while meeting the precision, recall, and accuracy requirements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"scope\": \"per-class_and_ensemble\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure is adapted to the user's specified project manager role with the 'high' expertise level. The description and performance metrics reflect the complexity of the task, including multi-classification and the challenging environments. The Inception-v3 specifications are tailored to address the target classes and the desired invariance to weather and occlusions. The confidence score is high given the thorough interpretation of the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for the machine learning team, your task is to develop an advanced regression model for the Media Campaign Cost Prediction project. Utilizing the train, validation, and test splits, the objective is to minimize the root mean squared log error (RMSLE) by implementing a multi-feature ensemble technique and conducting feature engineering. The model should demonstrate superior performance, considering temporal dependencies and non-linear correlations among the variables.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"The project requires an advanced regression model for Media Campaign Cost Prediction with a focus on minimizing RMSLE using multi-feature ensemble techniques and feature engineering. Temporal dependencies and non-linear correlations among variables are key considerations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared log error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"temporal dependencies handling\",\n                \"value\": null,\n                \"unit\": \"N/A\"\n            },\n            {\n                \"name\": \"non-linear correlation handling\",\n                \"value\": null,\n                \"unit\": \"N/A\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Media Campaign Cost Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"cost\"],\n            \"specification\": {\n                \"train_size\": null,\n                \"validation_size\": null,\n                \"test_size\": null,\n                \"temporal_features\": true,\n                \"feature_dimensions\": null\n            },\n            \"description\": \"A dataset for the Media Campaign Cost Prediction project, containing train, validation, and test splits with emphasis on temporal and non-linear feature interactions.\",\n            \"preprocessing\": [\"feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_method\": \"ensemble technique\",\n                \"number_of_models\": null,\n                \"training_details\": null\n            },\n            \"description\": \"An advanced regression model implementing a multi-feature ensemble technique for the Media Campaign Cost Prediction project with a focus on minimizing RMSLE.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal feature extraction\",\n            \"non-linear feature interactions\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"performance with temporal and non-linear feature dependencies\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure represents the given task as a request for model development, targeting a regression problem with a focus on RMSLE reduction. The user's high expertise level is indicated, and the need for feature engineering, ensemble technique, and handling of temporal dependencies and non-linear correlations is captured in the performance and complexity metrics. The model family and expected specifications are open-ended, allowing for flexibility in selecting the best approach.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for a cutting-edge machine learning team, you're tasked with tackling a multifaceted regression challenge. The Wild Blueberry Yield Dataset, featuring intricate mathematical correlations, has been divided into train, validation, and test sets with the objective to forecast yield. Emphasize interpretability and feature engineering. Optimize for mean absolute error (MAE) while implementing a stepwise model selection process and comparing multiple regression algorithms. Remember to document the model's performance at each stage for transparent reporting.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"The project involves a multifaceted regression challenge with the Wild Blueberry Yield Dataset, focusing on forecasting yield. Key aspects are interpretability, feature engineering, stepwise model selection, and comparison of multiple regression algorithms with an emphasis on mean absolute error (MAE). Transparent reporting of model performance at each stage is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wild Blueberry Yield Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"yield\"],\n            \"specification\": null,\n            \"description\": \"A dataset with intricate mathematical correlations split into train, validation, and test sets for yield forecasting.\",\n            \"preprocessing\": [\"feature scaling\", \"handling missing values\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature importance analysis\",\n                \"correlation matrix\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multiple Regression Algorithms\",\n            \"family\": \"Regression Models\",\n            \"type\": \"neural networks and classical machine learning\",\n            \"specification\": {\n                \"interpretability Techniques\": [\"LASSO\", \"Ridge Regression\", \"Elastic Net\"],\n                \"model selection Criteria\": \"Stepwise\"\n            },\n            \"description\": \"A collection of regression models that are optimized for MAE with interpretability features and stepwise model selection process.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Model interpretability is crucial for transparency. Feature engineering techniques will be employed to extract relevant features and handle noisy data.\",\n        \"Comparing performance among algorithms helps in selecting the best-suited model for the given task.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"PCA, feature scaling, feature selection\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"runtime optimization\"],\n        \"demonstration\": \"Performance reports at each step in a clear, visual format.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the complex nature of the task, focusing on regression, interpretability, and model selection for a high-level project manager. Essential elements such as multiple regression algorithms, feature engineering, stepwise selection, and focus on MAE are included. The missing values in the performance metrics are placeholders for model evaluations, which would be filled in after model training.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for the machine learning team, devise a sophisticated translation model utilizing the ParaCrawl dataset. Mandate a dual encoder architecture that not only ensures precise English-to-Dutch translations but also requires the system to analyze and reproduce nuanced emotional nuances and cultural context in the output. Encourage the team to optimize for both precision and emotional aptitude through iterative refinement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"The project involves developing a sophisticated translation model using the ParaCrawl dataset, specifically targeting English-to-Dutch translations. The model must employ a dual encoder architecture with a focus on accurate translations that capture nuanced emotional nuances and cultural context.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"emotional aptitude\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl dataset\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\n                \"translated text\"\n            ],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"valid\": null,\n                    \"test\": null\n                },\n                \"language_pair\": {\n                    \"source\": \"English\",\n                    \"target\": \"Dutch\"\n                }\n            },\n            \"description\": \"A large-scale parallel corpus for training a translation model, focusing on English-to-Dutch translations and containing emotional context.\",\n            \"preprocessing\": [\"sentiment analysis data augmentation\", \"cultural context preprocessing\"],\n            \"augmentation\": [\"contextual word embeddings\"],\n            \"visualization\": [\"model interpretability for emotional nuances\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dual Encoder Translation Model\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"dual encoder\",\n                \"emotional_nuance\": true,\n                \"cultural_analysis\": true\n            },\n            \"description\": \"A model that uses a dual encoder architecture for precise and emotionally apt English-to-Dutch translations, with a focus on iterative refinement.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Emphasis on iterative refinement for precision and emotional aptitude\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"emotion-based feature extraction\", \"context-based feature engineering\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized for parallel processing\"],\n        \"demonstration\": \"Translation examples showcasing emotional nuance and cultural context\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the project manager's request for a dual encoder model targeting English-to-Dutch translation with emphasis on emotional nuance and cultural context. The performance metrics, data preprocessing, and model architecture are guided by the given instructions. However, the specific metric values for precision, recall, and emotional aptitude are left unspecified as they would require more detailed model development information.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "question answering", "instruction": "As the head of the machine learning team, design an advanced question-answering system for the legal department that necessitates the deployment of a fine-tuned DeBERTa model, specifically engineered to handle complex legal terminologies from the LegalQA dataset. The system should guarantee data privacy, meet high standards of legal precision, and provide step-by-step reasoning for every output to facilitate transparency and auditing. Additionally, outline a strategy for iterative model validation and continuous improvement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"legal\",\n        \"description\": \"The user wants to design an advanced question-answering system for the legal department, specifically using a fine-tuned DeBERTa model to handle complex legal terminology from the LegalQA dataset. The system must prioritize data privacy, high legal precision, and provide step-by-step reasoning for explanations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"legal precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"data privacy measures\",\n                \"value\": null,\n                \"unit\": \"implementation\"\n            },\n            {\n                \"name\": \"model explainability\",\n                \"value\": null,\n                \"unit\": \"percentage of questions with step-by-step reasoning\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LegalQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"legal\",\n                \"specifics\": \"complex legal terminologies\"\n            },\n            \"description\": \"A dataset for legal question answering, containing questions and answers with focus on legal terminology.\",\n            \"preprocessing\": [\"fine-tuning for legal domain\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeBERTa\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine-tuned\": true,\n                \"legal domain adaptation\": true\n            },\n            \"description\": \"A fine-tuned DeBERTa model specifically designed to handle complex legal terminologies.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"title\": \"Data privacy strategy\",\n            \"description\": \"Implementing encryption and access controls to ensure data privacy during processing and storage.\"\n        },\n        {\n            \"title\": \"Transparency strategy\",\n            \"description\": \"Providing step-by-step reasoning for model outputs to facilitate auditing and enhance transparency.\"\n        },\n        {\n            \"title\": \"Iterative validation strategy\",\n            \"description\": \"Regular model testing and validation, incorporating user feedback for continuous improvement.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain adaptation techniques\",\n            \"explainable AI\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"legal department's internal system\",\n        \"inference_engine\": [\"GPU-based for speed and efficiency\"],\n        \"demonstration\": \"Detailed instructions and user interface for using the system\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's request for a question-answering system using a fine-tuned DeBERTa model. Performance requirements, complexity, and the LegalQA dataset details are well captured. However, missing values in performance metrics and some complexity metrics indicate room for more specific information based on the user's level of expertise.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "Task: Develop a cutting-edge multimodal fraud detection system using the 'Adversarial-Attuned MultimodalForgeryDataset', integrating advanced InceptionTime and TRILL networks for face and voice analysis. Strive to achieve an AUC-ROC of at least 98% while maintaining real-time performance with millisecond-level accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"cybersecurity\",\n        \"description\": \"The objective is to create a cutting-edge fraud detection system using the Adversarial-Attuned MultimodalForgeryDataset. The system must integrate InceptionTime and TRILL networks for face and voice analysis, and aim for an AUC-ROC of at least 98% with real-time performance, achieving millisecond-level accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Adversarial-Attuned MultimodalForgeryDataset\",\n            \"modality\": [\"image\", \"audio\"],\n            \"target_variables\": [\"forged authenticity\"],\n            \"specification\": null,\n            \"description\": \"A dataset for multimodal forgery detection, containing face and voice samples for analysis.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"InceptionTime (for face analysis)\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        },\n        {\n            \"name\": \"TRILL (for voice analysis)\",\n            \"family\": \"Transformers-based model\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"real-time inference service\",\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": {\n            \"example_data\": \"Generated or provided by user showcasing millisecond-level response times\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's intent to build a high-end multimodal forgery detection system, incorporates InceptionTime and TRILL networks, and sets the performance metrics. The target dataset is assumed to be obtained through a link provided by the user. The emphasis on real-time performance and millisecond-level accuracy is captured. The confidence score is high due to the clear mapping of the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For our upcoming extensive global marketing initiative, develop a sophisticated AI system that not only automatically differentiates subjects from intricate, multilayered backgrounds in diverse promotional images but also identifies emotion and sentiment, across multiple languages, to provide nuanced insights for targeted messaging.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"The task involves building a sophisticated AI system for image processing in the context of global marketing. The system should differentiate subjects from complex backgrounds in promotional images and also perform emotion and sentiment analysis across multiple languages for more nuanced insights.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"subject segmentation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"emotion recognition accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sentiment analysis accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"subject location\",\n                \"emotion labels\",\n                \"sentiment scores\"\n            ],\n            \"specification\": null,\n            \"description\": \"The system requires a diverse set of promotional images for training and validation with complex backgrounds and multiple languages.\",\n            \"preprocessing\": [\n                \"multi-language text preprocessing\",\n                \"background separation\"\n            ],\n            \"augmentation\": [\n                \"image augmentation for diverse conditions\"\n            ],\n            \"visualization\": [\n                \"model performance on sample images\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A high-level, AI-driven system for subject segmentation, emotion recognition, and multilingual sentiment analysis in promotional images.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"research literature\",\n            \"description\": \"State-of-the-art methodologies for image segmentation, emotion detection, and cross-lingual sentiment analysis.\"\n        },\n        {\n            \"type\": \"domain expertise\",\n            \"description\": \"Marketing strategy implications and the need for nuanced messaging based on multi-language sentiment analysis.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"transfer learning for image understanding\",\n            \"domain-specific feature extraction\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-powered for speed and accuracy\"\n        ],\n        \"demonstration\": \"live visualizations of model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the essence of the marketing AI system requirements, including image analysis, emotion and sentiment detection, and the need for high-level expertise. Metrics, preprocessing steps, and model selection are aligned with the task. The confidence score reflects a high level of certainty based on the clarity of the provided instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "For a comprehensive image classification project, you've gathered a diverse dataset of cat breeds. Request a multi-architecture model comparison that evaluates ResNet, Inception, and VGG architectures, considering transfer learning and fine-tuning, to identify not only the species but also individual breed characteristics in the given cat images.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal recognition\",\n        \"description\": \"The project involves comparing ResNet, Inception, and VGG architectures for an image classification task, focusing on cat breeds recognition with transfer learning and fine-tuning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"species classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"breed classification accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cat Breeds Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\", \"breed\"],\n            \"specification\": {\n                \"number_of_classes\": {\n                    \"species\": null,\n                    \"breed\": null\n                },\n                \"image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"number_of_images\": null\n            },\n            \"description\": \"A diverse dataset of cat images for both species and breed classification. Requires transfer learning and fine-tuning for ResNet, Inception, and VGG.\",\n            \"preprocessing\": [\n                \"transfer learning\",\n                \"fine-tuning\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNet\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"Inception\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"VGG\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Method for showcasing breed identification and architecture comparison\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON structure accurately captures the user's request for a multi-architecture comparison (ResNet, Inception, and VGG) for an image classification project with transfer learning and fine-tuning, specifically for cat breed recognition. It includes target variables, dataset properties, and performance metrics suitable for the task. The user's high expertise level is reflected.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a node classification model for the NELL dataset employing a Dynamic Hypergraph Attention Network, focusing on entities with evolving relationships. Ensure scalability, noise resistance, and achieve at least 90% F1-score by fine-tuning GraphSAGE and analyzing the impact of edge attribute importance on model performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graph\",\n        \"description\": \"The goal is to build a node classification model using a Dynamic Hypergraph Attention Network on the NELL dataset. Focus is on entities with evolving relationships, ensuring scalability and noise resistance. The target is to achieve at least 90% F1-score by fine-tuning GraphSAGE and analyzing edge attribute importance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"no specific value provided (expected to be optimized)\"\n            },\n            {\n                \"name\": \"noise resistance\",\n                \"value\": null,\n                \"unit\": \"no specific value provided (expected to be incorporated in the model)\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"entities with evolving relationships\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": {\n                    \"nodes\": null,\n                    \"edges\": null\n                },\n                \"edge_types\": \"evolving relationships\"\n            },\n            \"description\": \"A graph dataset for node classification, with a focus on entities and their dynamically changing relationships.\",\n            \"preprocessing\": [\n                \"Fine-tuning GraphSAGE\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dynamic Hypergraph Attention Network\",\n            \"family\": \"Graph Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"node_embedding_size\": null,\n                \"attention_heads\": null,\n                \"graph_sage_layers\": null,\n                \"edge_attribute_importance\": true\n            },\n            \"description\": \"A model designed for node classification with evolving relationships, targeting high F1-score, scalability, and noise resistance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Edge attribute importance analysis for model performance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key points from the given instruction, specifying the user's intent, problem details, required dataset, model architecture, and performance metrics. The user's high expertise level suggests a focus on fine-tuning and model analysis. The target F1-score of 90% and mention of edge attribute importance are included in the performance metrics and model specifications. The confidence score may be lower due to assumptions on the missing specific values for some metrics, but the overall structure is accurate.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "time-series classification", "instruction": "As a project manager for a specialized ML team, your objective is to enhance the time-series classification on the Handwriting dataset. The task now involves not only training a model with sequences of 152 input dimensions (INPUT_SEQ_LEN=152, INPUT_DIM=3) but also optimizing for interpretability, limiting the model to a specific architecture that utilizes incremental learning. The primary goal is to achieve high accuracy, while also demonstrating effective handling of variable-length sequences and real-time adaptations. The evaluation criterion remains accuracy, but the model must converge within a budgeted time constraint and possess explainability for business stakeholders.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"pattern recognition\",\n        \"description\": \"The objective is to enhance time-series classification on the Handwriting dataset by training a model that processes sequences with 152 input dimensions, using incremental learning and with a focus on interpretability for business stakeholders. The primary metrics for evaluation are accuracy, while the model must converge within a specified time constraint and be explainable.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"convergence time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Handwriting Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"classification labels\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 152,\n                \"INPUT_DIM\": 3\n            },\n            \"description\": \"A dataset containing time-series data for handwriting classification with sequences of 152 input dimensions and variable-length sequences.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Incremental Learning Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"interpretable model for time-series (e.g., LSTMs with attention mechanisms)\"\n            },\n            \"description\": \"A model designed for time-series classification using incremental learning, with a focus on interpretability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time adaptations\"],\n        \"demonstration\": \"Demonstrations of model performance on variable-length sequences and interpretability for stakeholders\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the instruction by indicating a high-expertise user's objective to build a model, focusing on time-series classification. It specifies the dataset, model architecture, and emphasizes interpretability and variable-length sequence handling. Performance metrics, time-convergence, and target devices for deployment are also included. The confidence score might be lower than 1 due to the assumption that 'model name' and 'deployment details' are not provided in the given instruction.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, request a lightweight yet efficient AmazonQA chatbot model that meets a 90% accuracy target within 200 milliseconds response time, utilizing a scalable architecture for handling high volume customer queries.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"customer service\",\n        \"description\": \"The objective is to create a lightweight, efficient AmazonQA chatbot model with a 90% accuracy target and a response time of 200 milliseconds. The model should have a scalable architecture to handle high volume customer queries.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": 0.2,\n                \"unit\": \"seconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AmazonQA\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\n                \"answer\"\n            ],\n            \"specification\": {\n                \"type\": \"dataset size\",\n                \"value\": \"sufficient for model training with high volume of customer queries\"\n            },\n            \"description\": \"The dataset for training and testing the chatbot, with focus on customer queries and responses.\",\n            \"preprocessing\": [\"tokenizer, text cleaning\"],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"AmazonQA Chatbot\",\n            \"family\": \"transfer learning (with Alexa or similar conversational models)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"lightweight\",\n                \"training method\": \"efficient\"\n            },\n            \"description\": \"A chatbot designed specifically for high volume customer support with fast response times\"\n        }\n    ],\n    \"knowledge\": [\n        \"Target model architecture: Leveraging pre-trained models for fast adaptation and lightweight implementation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"query understanding, intent recognition\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"Amazon AWS or similar scalable platform\",\n        \"inference_engine\": [\"AWS Lambda or similar serverless architecture\"],\n        \"demonstration\": \"A mock or live demo showcasing 90% accuracy and response times\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the provided template and has captured the core elements of the project manager's request, such as model requirements, performance targets, and scalability. The details on dataset and preprocessing steps are consistent with the task at hand. The reasoning for confidence score is based on the thorough representation of the given instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a seasoned project manager, you need to guide the team in developing an advanced image recognition system for niche celestial object classification. Request detailed recommendations on a state-of-the-art deep learning model, like combining convolutional neural networks (CNNs) with transfer learning using the Planetary Image Dataset (PID), while considering challenges like faintness and variability in starlight, to ensure exceptional accuracy above 95%. Additionally, outline the necessary preprocessing steps and evaluation metrics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The project aims to develop an advanced image recognition system for niche celestial object classification, particularly focusing on faint and variable starlight. The goal is to achieve exceptional accuracy above 95% using a state-of-the-art model combining CNNs with transfer learning. The Planetary Image Dataset (PID) is the primary data source.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Planetary Image Dataset (PID)\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"description\": \"Dataset for niche celestial object classification with challenges such as faintness and variability in starlight.\"\n            },\n            \"preprocessing\": [\n                \"Image normalization\",\n                \"Data augmentation (including handling of starlight variability)\",\n                \"Pre-processing for transfer learning\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Combining CNNs with transfer learning\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"description\": \"To handle faint objects, a base model pre-trained on a large image dataset like ImageNet, potentially fine-tuned with PID data.\"\n            },\n            \"description\": \"Advances state-of-the-art model for niche object classification under challenging starlight conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using techniques such as feature extraction, multi-scale analysis, and noise reduction to improve performance.\",\n        \"Evaluate the model's robustness against varying brightness and starlight conditions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature extraction and selection\"\n        ],\n        \"target_device\": \"cloud or high-performance computing\",\n        \"deployment_endpoint\": \"Potentially cloud-based or institutional hosting\",\n        \"inference_engine\": [\n            \"Distributed inference frameworks like TensorFlow Serving or Apache MXNet\"\n        ],\n        \"demonstration\": \"Interactive demonstration with real-time classification results on varying celestial images\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the instruction's key points, including the need for high accuracy, state-of-the-art model, and specific challenges. It outlines preprocessing steps, evaluation metric, and provides relevant context for domain knowledge and project services. However, some aspects like the target variables are left unspecified, considering the nature of celestial object classification.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a machine learning project manager for a specialized team, you've been tasked with developing an advanced time-series forecasting model for a cutting-edge smart grid management system. The team has been assigned the CHAOTIC_TEN dataset, which contains highly complex, nonlinear energy consumption patterns from thousands of households with diverse renewable energy sources over a decade. The dataset is divided into four segments: history, training, calibration, and benchmark, each with unique temporal dependencies and seasonality.\n\nYour model should handle the following specifications:\n- Input sequences consist of irregularly sampled data points with variable lengths (MAX_INPUT_SEQ_LEN ranging from 300 to 1000) and a diverse range of input dimensions (INPUT_DIM including both traditional meter readings and derived environmental factors).\n- Predictions entail multi-step ahead forecasting, with the forecast horizon varying between 48 and 168 time steps for each sequence (VARIOUS_PRED_SEQ_LEN).\n- Evaluation should account for not only traditional metrics like Mean Absolute Percentage Error (MAPE), Symmetric Mean Absolute Percentage Error (SMAPE), and Mean Absolute Scaled Error (MASE), but also introduce a new metric, Rolling Correlation Coefficient (RCC), to assess the model's ability to maintain the inherent sequential pattern over the prediction window.\n- The model must incorporate advanced features such as autoencoding, anomaly detection, and adaptability to real-time adjustments in the energy mix.\n\nPrepare a detailed project plan to train and fine-tune this innovative model, ensuring its robustness, efficiency, and interpretability while meeting the stringent performance requirements for this complex smart grid application.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart grid management\",\n        \"description\": \"The project aims to develop an advanced time-series forecasting model for a smart grid management system using the CHAOTIC_TEN dataset, focusing on complex energy consumption patterns from diverse households with renewable energy sources over a decade. The model should handle irregular sampling, variable sequence lengths, diverse input dimensions, multi-step ahead forecasting, and a custom RCC metric.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Absolute Percentage Error (MAPE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Symmetric Mean Absolute Percentage Error (SMAPE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Scaled Error (MASE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Rolling Correlation Coefficient (RCC)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CHAOTIC_TEN Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": null,\n            \"specification\": {\n                \"MAX_INPUT_SEQ_LEN\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Range from 300 to 1000\",\n                    \"value\": {\n                        \"min\": 300,\n                        \"max\": 1000\n                    }\n                },\n                \"INPUT_DIM\": {\n                    \"type\": \"array\",\n                    \"description\": \" Including traditional meter readings and environmental factors\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"description\": \"Variety of input dimensions\"\n                    }\n                },\n                \"temporal_dependencies\": {\n                    \"type\": \"string\",\n                    \"description\": \"Unique for each of history, training, calibration, and benchmark segments\"\n                },\n                \"seasonality\": {\n                    \"type\": \"string\",\n                    \"description\": \"Present in each segment with diverse patterns\"\n                }\n            },\n            \"description\": \"A time-series dataset with complex energy consumption patterns from thousands of households with renewable energy sources. Segmented into history, training, calibration, and benchmark with unique temporal dependencies and seasonality.\",\n            \"preprocessing\": [\n                \"Handling irregular sampling data\",\n                \"Data cleaning\",\n                \"Feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Time Series Forecasting Model\",\n            \"type\": \"neural networks (such as RNN, LSTM, or Transformer-based models)\",\n            \"specification\": {\n                \"autoencoding\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Incorporates autoencoding capabilities\"\n                },\n                \"anomaly_detection\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Incorporates anomaly detection mechanisms\"\n                },\n                \"adaptability\": {\n                    \"type\": \"string\",\n                    \"description\": \"Real-time adjustments for energy mix\"\n                },\n                \"input_sequence_length\": \"variable\",\n                \"forecast_horizon\": {\n                    \"type\": \"array\",\n                    \"description\": \"Range of [48, 168] time steps\",\n                    \"items\": {\n                        \"type\": \"integer\"\n                    }\n                }\n            },\n            \"description\": \"An innovative model designed for advanced time-series forecasting with autoencoding, anomaly detection, and adaptability to the smart grid energy mix.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced techniques for handling complex, non-linear energy consumption patterns\",\n        \"Integration of multiple time scales and variable input dimensions\",\n        \"Strategies for handling real-time adjustments in the energy mix\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Handling missing values\",\n            \"Feature normalization\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Real-time or batch processing\"\n        ],\n        \"demonstration\": \"Post-forecast model performance visualization and explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the user's intent to build a complex model and incorporates the high expertise level. The description covers the dataset characteristics, model requirements, evaluation metrics, and post-forecast activities. It captures the main aspects of a project plan for the given smart grid forecasting task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project manager for our upcoming machine translation endeavor, direct the team to design a scalable transformer model using the extensive Common Crawl dataset. The task now involves not only bilingual translation between English and Japanese but also discerning and translating regional colloquialisms with precision, aiming for a minimum accuracy rate of 98%. Additionally, the model must demonstrate adaptability to evolving language trends.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"The project aims to design a scalable transformer model for machine translation between English and Japanese, specifically addressing regional colloquialisms with high precision, targeting a minimum accuracy rate of 98% and ensuring adaptability to evolving language trends.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"English\", \"Japanese\", \"region-specific colloquialisms\"],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"diversity\": \"includes regional colloquialisms\"\n            },\n            \"description\": \"A large, diverse text dataset from Common Crawl to train the model, emphasizing coverage of regional variations and evolving language.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Scalable Transformer Model\",\n            \"family\": \"Sequence-to-Sequence\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer\",\n                \"adaptability\": \"for evolving language trends\"\n            },\n            \"description\": \"A transformer model designed for machine translation, targeting high precision in translating both standard and regional colloquial languages between English and Japanese.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstration of regional colloquialism translation accuracy and adaptability to language trends\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the project requirements accurately, with a high expertise level assumed for the project manager. It captures the core task of building a scalable transformer model, target languages, the Common Crawl dataset, and the performance metrics. The model is designed to meet the specified accuracy and adaptability goals.\",\n        \"score\": 0.99\n    }\n}\n"}
{"task": "community detection", "instruction": "As a machine learning project manager, you've been tasked with developing a novel approach for community detection in an extensive, dynamically evolving academic network called \" ScholarSphere\". The objective is to employ a self-evolving, topology-aware network embedding algorithm that combines node2vec and edge betweenness centrality to identify subdomains within the network. The algorithm should seamlessly integrate with incoming publications, handle sparse data, and automatically adapt to new researchers without sacrificing accuracy or efficiency. Additionally, the system must generate visualizations of these communities, making it easier for users to understand and explore the research clusters. Provide a detailed project plan, including milestones, performance metrics, and the expected outcome for a user-friendly interface.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"The project aims to develop a novel approach for community detection in the ScholarSphere academic network using a self-evolving, topology-aware algorithm combining node2vec and edge betweenness centrality. It must handle sparse data, adapt to new researchers, and generate visualizations for user-friendly exploration.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"adaptability to new nodes\",\n                \"value\": null,\n                \"unit\": \"percentage of node addition\"\n            },\n            {\n                \"name\": \"visualization response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ScholarSphere Academic Network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"communities\", \"topology\"],\n            \"specification\": {\n                \"size\": null,\n                \"density\": \"sparse\",\n                \"dynamic\": true\n            },\n            \"description\": \"An extensive, dynamically evolving academic network representing researchers and their publications.\",\n            \"preprocessing\": [\n                \"node2vec transformation\",\n                \"edge betweenness centrality computation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Community Detection Results\",\n                \"Research Cluster Visualizations\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Topology-Aware Node2vec with Edge Betweenness Centrality\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"node_embedding_size\": null,\n                \"learning_rate\": null,\n                \"topology_integration\": true\n            },\n            \"description\": \"A self-evolving algorithm combining node2vec and edge betweenness centrality for community detection in academic networks.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Sparsity handling techniques, node2vec implementation, and edge betweenness centrality integration.\",\n        \"Algorithmic adaptation for seamless new researcher inclusion.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"node2vec, edge betweenness pre-processing\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"User-friendly Interface\",\n        \"inference_engine\": [\"GPU-based\"], \n        \"demonstration\": \"Interactive visual exploration of research clusters\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the task description, focusing on developing a community detection algorithm for ScholarSphere, including specific details such as performance metrics, the algorithm's combination of techniques, handling sparse data, and adaptability. It also includes the requirement for user-friendly visualizations. The 'high' expertise level indicates the user's understanding of the technical challenges. Missing values are expected due to the incomplete nature of the task description.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a sophisticated node classification model, employing a Significance Graph Neural Network (SGNN) on the extensive YouTube Algebric Connectivity dataset. Demand not only 80% accuracy but also assess the model's capacity to handle scalability, outliers, and evolving content creator categories over time, while maintaining efficiency in dealing with sparse data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"social media\",\n        \"description\": \"The task involves building a sophisticated node classification model using Significance Graph Neural Network (SGNN) on the YouTube Algebric Connectivity dataset. The primary goal is to achieve 80% accuracy, while also evaluating its ability to handle scalability, outliers, and evolving content creator categories over time, maintaining efficiency with sparse data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.8\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"efficiency\"\n            },\n            {\n                \"name\": \"robustness to outliers\",\n                \"value\": null,\n                \"unit\": \"insensitivity\"\n            },\n            {\n                \"name\": \"adaptability to evolving categories\",\n                \"value\": null,\n                \"unit\": \"dynamic learning\"\n            },\n            {\n                \"name\": \"efficiency with sparse data\",\n                \"value\": null,\n                \"unit\": \"memory usage or time complexity\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube Algebric Connectivity\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"content creator categories\"],\n            \"specification\": null,\n            \"description\": \"An extensive dataset for node classification tasks with a focus on scalability, outliers, evolving creator categories, and efficiency in dealing with sparse data.\",\n            \"preprocessing\": [\"data cleaning\", \"sparse data handling\"],\n            \"augmentation\": [\"graph augmentation techniques\"],\n            \"visualization\": [\"graph visualization\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Significance Graph Neural Network (SGNN)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A model designed for node classification tasks on graphs, targeting high accuracy and adaptability to complex and evolving social media data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using techniques to handle large graph data, like batch processing, and optimizing for distributed computing.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"TensorFlow Serving or equivalent\",\n        \"inference_engine\": [\"GPU-based inference\"],\n        \"demonstration\": \"Visualize performance over time and improvements in handling various scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON accurately captures the user's intent, highlighting the need for a high-performance node classification model on the YouTube Algebric Connectivity dataset. It includes performance metrics, complexity requirements, and relevant model selection. It suggests handling sparse data by preprocessing and possible engineering methods. However, for some metrics and techniques, the value is left as null for the user to provide, ensuring the response remains as a template.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "For a comprehensive pollution management project, devise a sophisticated Gaussian Process Regression model with advanced uncertainty quantification on the 'Air Quality Monitoring' dataset. The model should not only predict PM2.5 levels with a target calibration error of 5%, but also analyze cross-validation performance and incorporate meteorological variables to enhance prediction accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"environmental science\",\n        \"description\": \"The project aims to build a Gaussian Process Regression model on the 'Air Quality Monitoring' dataset for comprehensive pollution management. The focus is on predicting PM2.5 levels with a target calibration error of 5%, while also assessing cross-validation performance and incorporating meteorological variables for improved accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"calibration error\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null, \n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null, \n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Air Quality Monitoring Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"PM2.5 levels\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing air quality data, including PM2.5 levels, to be used for regression analysis. Meteorological variables are expected to be part of the data.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"feature scaling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"correlation analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Gaussian Process Regression\",\n            \"family\": \"kernel-based regression\",\n            \"type\": \"statistical models\",\n            \"specification\": {\n                \"latent_function\": \"uncertainty quantification\",\n                \"kernel_type\": \"covariance function\"\n            },\n            \"description\": \"A model designed for advanced pollution management, predicting PM2.5 levels with a target calibration error of 5% and incorporating meteorological variables for improved accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection based on meteorological variables\",\n            \"covariate shift adaptation\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"visualize uncertainty intervals and performance metrics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a machine learning model, specifies high expertise, and highlights the specific requirements of a Gaussian Process Regression model for pollution management. The model's target performance, cross-validation, and meteorological variables are appropriately addressed. However, there may be room for further clarification on the target device and deployment options.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "For the Coauthorship Science dataset, develop a sophisticated node classification model utilizing a state-of-the-art Hierarchical Graph Attention Network (HGAT), incorporating temporal dynamics and multi-scale features. The objective is not only to determine authors' research domains but also to analyze their evolving co-authorship patterns and predict future collaboration niches.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"The task involves developing a node classification model using a state-of-the-art HGAT on the Coauthorship Science dataset. The model should capture temporal dynamics and multi-scale features to classify authors' research domains and predict future collaboration patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coauthorship Science dataset\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [\"author node\", \"research domain\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"total\": null,\n                    \"features\": [\n                        \"temporal\",\n                        \"multi-scale\"\n                    ]\n                },\n                \"edges\": {\n                    \"total\": null,\n                    \"types\": [\"co-authorship\"]\n                }\n            },\n            \"description\": \"Dataset with authors' co-authorship history and temporal dynamics for node classification and future collaboration prediction.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"feature engineering\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"graph structure\",\n                \"temporal patterns\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Graph Attention Network (HGAT)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"incorporating temporal dynamics and multi-scale features\"\n            },\n            \"description\": \"A state-of-the-art model for node classification in graphs, specifically designed to handle evolving co-authorship patterns.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal feature extraction\",\n            \"multi-scale feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-based inference\"\n        ],\n        \"demonstration\": \"example predictions and collaboration predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the provided template. I've captured the user's intent to build a model, indicating high expertise. The task is clearly described within the context of graph machine learning, academic research, and specifies HGAT as the model type. Performance metrics like accuracy and F1 score, and complexity metrics like inference time and memory footprint are included based on the user's requirements. Preprocessing, augmentation, and visualization steps are included as per the requirements for the dataset.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "For the intricate time-series forecasting project, you are to develop a state-of-the-art deep learning model. Utilizing the Weather dataset, which includes intricate seasonal patterns and complex correlations across multiple years (now with an expanded historical depth of 192 observations), you must create train, validation, and test splits. Design a model that incorporates autoregressive mechanisms and external factors (e.g., weather events) to predict not only the next 96-step sequence but also the following 48-step ahead. The model should optimize for both mean squared error (MSE) and mean absolute percentage error (MAPE), ensuring robustness and interpretability in your evaluation. Perform extensive hyperparameter tuning and implement early stopping to maximize performance on the given diverse input patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate science\",\n        \"description\": \"The task involves developing a state-of-the-art deep learning model for time-series forecasting using the Weather dataset with a 192-observation historical depth. The goal is to predict 96-step and 48-step ahead sequences while accounting for seasonal patterns and external weather events. Performance should be optimized for both mean squared error (MSE) and mean absolute percentage error (MAPE) and include hyperparameter tuning and early stopping.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Weather Dataset\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"historical_depth\": 192\n            },\n            \"description\": \"A dataset with intricate seasonal patterns and complex correlations across multiple years, with a focus on weather events, and an expanded historical depth of 192 observations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep Learning (specifically, RNN or LSTM)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"autoregressive\": true,\n                \"inclusion_of_external_factors\": true,\n                \"future_prediction_steps\": [96, 48]\n            },\n            \"description\": \"A state-of-the-art deep learning model designed for time-series forecasting with the ability to handle diverse input patterns, including autoregressive mechanisms and external weather events.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Include a visualization of predictions and performance across different hyperparameters\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the complexities of the time-series forecasting task, incorporating the user's high expertise level. The model family is chosen based on the state-of-the-art and the task requirements. Performance metrics (MSE and MAPE) and model specifications (autoregressive, external factors, and future prediction steps) are included. The absence of target_variables and augmentation suggests an assumption that the dataset might not contain labeled data for prediction targets, and the dataset is assumed to need to be augmented. The confidence score is high due to a thorough interpretation of the instruction.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "object detection", "instruction": "For our upcoming project, we need an advanced retail-specific object detection model that not only counts entering and exiting patrons in high-resolution security cameras but must also classify them into distinct customer and employee categories, distinguishing between them based on unique attire and behavior patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"The project aims to develop an advanced object detection model for retail scenarios, specifically for counting and classifying entering/exiting customers and employees based on their attire and behavior patterns in high-resolution security cameras.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Retail Security Camera Data\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"customer\", \"employee\"],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"behavior_data\": \"attire and behavior patterns\"\n            },\n            \"description\": \"A dataset containing high-resolution security camera footage with annotations for counting and categorizing customers and employees based on attire and behavior.\",\n            \"preprocessing\": [\"high-resolution image enhancement\"],\n            \"augmentation\": [\"behavior-based synthetic data generation\"],\n            \"visualization\": [\"object detection heatmaps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Retail Object Detection Model\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"detection_features\": [\"behavior-based\", \"attire-based\"]\n            },\n            \"description\": \"A state-of-the-art model designed to detect and differentiate between customers and employees in retail scenarios, leveraging unique attire and behavior patterns.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Background on existing retail analytics literature and object detection techniques for distinguishing customers and employees.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"behavioral feature extraction\", \"attire feature extraction\"],\n        \"target_device\": \"edge device\",\n        \"deployment_endpoint\": \"secure retail surveillance system\",\n        \"inference_engine\": [\"real-time object detection algorithms\"],\n        \"demonstration\": \"live footage with annotated results and performance metrics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's requirements for an advanced retail object detection model. Performance metrics like accuracy, precision, recall, and specificity are relevant to the task. The model's ability to handle high-resolution images and recognize unique attire and behavior patterns has been incorporated. The confidence score is high due to the thorough mapping and alignment of the instruction with the template.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "For the energy consumption dataset, design a scalable deep learning pipeline that integrates an advanced LSTM architecture with feature engineering. The task is to predict hourly consumption for the subsequent 72 hours, ensuring efficiency by targeting sub-500ms inference times, while also optimizing for model compactness and minimal resource utilization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy management\",\n        \"description\": \"The task involves designing a scalable deep learning pipeline using an advanced LSTM architecture with feature engineering to predict hourly energy consumption for the next 72 hours. The goal is to achieve sub-500ms inference times and optimize for model compactness and minimal resource utilization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.5\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time (sub-500ms)\",\n                \"value\": 500,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"model compactness\",\n                \"value\": null\n            },\n            {\n                \"name\": \"resource utilization\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Energy Consumption Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\n                \"hourly consumption\"\n            ],\n            \"specification\": {\n                \"length\": {\n                    \"training\": null,\n                    \"testing\": null\n                },\n                \"time_frequency\": \"hourly\"\n            },\n            \"description\": \"Dataset containing historical energy consumption data for feature engineering and forecasting.\",\n            \"preprocessing\": [\n                \"feature scaling\",\n                \"windowing for LSTM input\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"time-series plots\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced LSTM\",\n            \"family\": \"Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"LSTM with advanced feature engineering layers\",\n                \"params\": null,\n                \"FLOPs\": null,\n                \"inference_speed\": {\n                    \"time\": {\n                        \"min\": null,\n                        \"max\": 500\n                    },\n                    \"device\": \"CPU\"\n                }\n            },\n            \"description\": \"A scalable deep learning model utilizing LSTM for time-series forecasting with a focus on efficiency and compactness.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction\",\n            \"interpolation for missing values\",\n            \"lagging\"\n        ],\n        \"target_device\": \"IoT devices and edge computing\",\n        \"deployment_endpoint\": \"a serverless or edge deployment platform\",\n        \"inference_engine\": [\"TensorFlow Lite or a lightweight variant\"],\n        \"demonstration\": \"Real-time predictions with minimal latency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the user's instruction by capturing the task of building a scalable deep learning pipeline, the use of an advanced LSTM architecture, and the target performance metrics. Attention was given to optimizing inference time, model compactness, and resource utilization. Some performance metrics are left open for flexibility as they might need to be calculated later in the pipeline development.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "For our next high-impact project, we aim to develop a state-of-the-art interactive question-answering system, leveraging the challenging SQuAD 2.0 dataset with a deep RoBERTa model. The system must surpass a stringent 95% F1 score on rare and multi-faceted questions, while still maintaining a response time of under 1.5 seconds on an expansive corpus, ensuring both precision and user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education and information retrieval\",\n        \"description\": \"The project involves building a state-of-the-art interactive question-answering system using the SQuAD 2.0 dataset with a deep RoBERTa model. The goal is to achieve a 95% F1 score on rare and multi-faceted questions, while maintaining a response time of under 1.5 seconds on a large corpus for optimal precision and user experience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 1.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"question\", \"answer\"],\n            \"specification\": {\n                \"size\": \"large corpus\",\n                \"data characteristics\": \"challenging, rare and multi-faceted questions\"\n            },\n            \"description\": \"A dataset for question answering tasks, specifically designed for the development of the interactive system.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RoBERTa\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": {\n                    \"target\": \"state-of-the-art\",\n                    \"specific_task\": \"question answering\"\n                },\n                \"speed\": {\n                    \"response_time\": \"under 1.5 seconds\"\n                }\n            },\n            \"description\": \"Deep learning model for question-answering, utilizing the SQuAD 2.0 dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"interactive and user-friendly interface\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the user's intentions, mentioning a high-level project and an experienced level of expertise. It covers the need for a state-of-the-art model with specific performance targets (F1 score and response time), the relevant dataset (SQuAD 2.0), and the use of a deep RoBERTa model. The rationale highlights the importance of performance metrics to ensure user experience and precision. The confidence score is high due to the thorough coverage of the given requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As the machine learning development team, we've been tasked with creating a cutting-edge image classification system for our luxury eco-friendly home goods brand. The challenge lies in differentiating photos based on their narrative: displaying product effectiveness in unique and sustainable lifestyles, showcasing innovative design, or revealing exceptional user experiences that push brand values. We need a model that not only discerns these three distinct categories but also generates captions that align with the brand tone. The system should also identify potential user stories for influencer collaborations or cross-promotions while maintaining strict privacy and data protection guidelines. Can you develop a model that not only filters the imagery but also harvests qualitative insights for strategic marketing?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"luxury eco-friendly home goods\",\n        \"description\": \"The task is to create an image classification system for a brand, differentiating photos based on product effectiveness in sustainable lifestyles, design innovation, and user experiences. The model should generate captions aligned with brand tone, identify user stories for influencer collaborations, and adhere to strict privacy and data protection guidelines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"caption generation coherence\",\n                \"value\": null\n            },\n            {\n                \"name\": \"story identification precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Luxury Eco-Friendly Home Goods Dataset\",\n            \"modality\": [\"image\", \"text\"],\n            \"target_variables\": [\"image category\", \"brand tone\", \"user stories\"],\n            \"specification\": {\n                \"image Diversity\": \"reflecting unique lifestyles and sustainability\",\n                \"caption annotations\": \"align with brand tone and user experiences\",\n                \"user stories\": \"anonymized and privacy-protected\"\n            },\n            \"description\": \"A dataset containing photos showcasing eco-friendly home goods with associated captions and strategic insights.\",\n            \"preprocessing\": [\"brand tone and privacy filtering\"],\n            \"augmentation\": [\"diversifying lifestyles and product scenarios\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image-Captioning Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"multimodal learning\",\n            \"specification\": {\n                \"category recognition\": \"distinguishing three distinct narratives\",\n                \"insights generation\": \"qualitative marketing understanding\"\n            },\n            \"description\": \"A model capable of image classification, caption generation, and extracting user stories while maintaining privacy standards.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Privacy regulations for data usage and influencer collaborations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual embeddings for user stories\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"secure brand server\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"model outputs in a privacy-preserving manner\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response is a comprehensive representation of the given instruction. It covers the core task of developing a cutting-edge image classification system, with high expertise level assumed. Key aspects like dataset, model (multimodal, including captions and stories), performance metrics (accuracy, caption quality, story identification), and privacy concerns are incorporated. The service details cater to cloud-based deployment and security.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a leading-edge AI project manager, your team has been tasked with creating a groundbreaking neural machine translation system that specializes in translating ancient Sumerian cuneiform texts into both English and Mandarin Chinese. The project should leverage state-of-the-art architecture, combining the efficiency of Performer (a multimodal transformer) with a compressed version for resource-constrained environments like lightweight smartwatches. The challenge lies in preserving the cultural nuances and historical context while maintaining a processing speed that allows real-time translation during archaeological fieldwork. In addition to accuracy, the model must have a deep learning pruning technique implemented to reduce computational overhead and boost performance on devices with limited hardware capabilities. Demonstrate how your team will achieve these requirements through a detailed project roadmap and prototype.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics, archaeology\",\n        \"description\": \"The project aims to create a state-of-the-art neural machine translation system that translates ancient Sumerian cuneiform into English and Mandarin Chinese. It should combine Performer (multimodal transformer) with a compressed version for resource-constrained environments, emphasizing cultural nuances and historical context with real-time capability during fieldwork.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\"\n            },\n            {\n                \"name\": \"processing speed\",\n                \"value\": {\n                    \"unit\": \"real-time\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational overhead reduction\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ancient Sumerian Cuneiform Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"English translation\", \"Mandarin Chinese translation\"],\n            \"specification\": {\n                \"format\": \"cuneiform script\",\n                \"domains\": [\"linguistics\", \"history\"]\n            },\n            \"description\": \"Dataset containing ancient Sumerian cuneiform text for training and evaluation.\",\n            \"preprocessing\": [\n                \"text normalization\",\n                \"cuneiform transcription\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Performers for Sumerian Translation\",\n            \"family\": \"neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"multimodal\": true,\n                \"compressed\": true,\n                \"resource_efficient\": true\n            },\n            \"description\": \"A neural network combining Performer architecture with a compressed version for smartwatches, maintaining high accuracy and cultural context.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"deep learning pruning\"\n        ],\n        \"target_device\": [\n            \"lightweight smartwatches\"\n        ],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time translation\"],\n        \"demonstration\": {\n            \"roadmap\": \"includes architecture design, training phases, pruning implementation, and real-world use-case demonstrations.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the project requirements, highlighting the need for a state-of-the-art model, cultural preservation, real-time processing, and deep learning pruning for resource-constrained environments. The specified roadmap and dataset details indicate a comprehensive approach.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Utilize the restricted Mouse Cerebral Cortex Neuronal Adjacency dataset, specifically the calcium-imaging guided synapse connections, to design a state-of-the-art Graph Wavelet Convolutional Neural Network (GWCNN) for ultra-precise neuron type classification. Emphasize on edge-feature fusion and local-global graph context, ensuring robustness in deciphering intricate microcircuit patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"The task is to design a Graph Wavelet Convolutional Neural Network (GWCNN) using the Mouse Cerebral Cortex Neuronal Adjacency dataset with calcium-imaging guided synapse connections. The goal is to classify neuron types with ultra-precision, emphasizing edge-feature fusion and local-global graph context for improved robustness in deciphering microcircuit patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Cerebral Cortex Neuronal Adjacency\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"neuron type\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"size\": null,\n                    \"features\": [\"calcium-imaging data\"]\n                },\n                \"edges\": {\n                    \"connections\": \"synapse connections\"\n                }\n            },\n            \"description\": \"Dataset containing calcium-imaging guided synapse connections for ultra-precise neuron type classification\",\n            \"preprocessing\": [\"edge-feature extraction\", \"graph normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"graph analysis\", \"neighborhood exploration\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Wavelet Convolutional Neural Network (GWCNN)\",\n            \"family\": \"Convolutional Neural Networks for Graph Data\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"edge-feature fusion\": true,\n                    \"local-global context\": true\n                },\n                \"hyperparameters\": {\n                    \"wavelet transformations\": null,\n                    \"optimizer\": null,\n                    \"learning rate\": null\n                }\n            },\n            \"description\": \"A state-of-the-art GWCNN designed for neuron type classification, focusing on edge-feature fusion and graph context.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The Mouse Cerebral Cortex data is known for its intricacy, which requires specialized graph-based models like GWCNN.\",\n        \"Edge-feature fusion helps capture the connectivity information for improved classification.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"wavelet transformation for graph signals\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for graph processing\"],\n        \"demonstration\": \"visualizing classification results with graph structure and neuron type distribution\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent, task (building a GWCNN), target dataset, and emphasizes the key aspects like edge-feature fusion and local-global graph context. Performance metrics are kept open-ended for the model's specific requirements, while complexity metrics are included reflecting the user's emphasis on ultra-precision and robustness.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for a cutting-edge translation team, I require a highly efficient, sequence-to-sequence model using state-of-the-art Bidirectional Long Short-Term Memory (Bi-LSTM) architecture. Utilize the specialized Europarl v7 dataset version focusing on South American Spanish-English translation. Emphasize optimizing for hardware compatibility, achieving sub-200 milliseconds latency for real-time, low-bandwidth voice and text chat applications in international business conferences.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"international business\",\n        \"description\": \"The project involves creating a highly efficient sequence-to-sequence model with Bi-LSTM architecture for South American Spanish-English translation. It aims to optimize for hardware compatibility and achieve sub-200 milliseconds latency for real-time, low-bandwidth voice and text chat applications in international business conferences.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.2\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"hardware compatibility\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency (inference time)\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Europarl v7 - South American Spanish-English\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"dataset_size\": \"custom\",\n                \"data_source\": \"public domain\"\n            },\n            \"description\": \"A specialized dataset for South American Spanish-English translation, focusing on the Europarl v7 version.\",\n            \"preprocessing\": [\"text cleaning, tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Sequence-to-Sequence with Bi-LSTM\",\n            \"family\": \"Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"Bi-LSTM\",\n                \"training_details\": {\n                    \"optimizer\": \"efficient for efficiency\"\n                }\n            },\n            \"description\": \"A state-of-the-art model designed for real-time translation, optimized for low-latency and hardware compatibility.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"name\": \"Low-Bandwidth Optimization\",\n            \"value\": \"Based on hardware constraints and real-time requirements.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"sentence segmentation\",\n            \"word embeddings\"\n        ],\n        \"target_device\": \"low-bandwidth devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": \"Integration into low-bandwidth chat applications\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the project manager's requirements, specifying a build intent, high expertise, the Bi-LSTM model, hardware optimization, sub-200ms latency, and South American Spanish-English translation. The dataset and preprocessing steps address the translation needs, and the model specification provides a relevant focus on efficiency. Service and knowledge aspects are also integrated within the JSON response.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "For a high-scale, omni-channel retail platform, design a real-time YOLO-based object detection model optimized for the \"StoreShelf Dataset 5.0\". The model should distinguish among 150 distinct product categories, maintaining a precision above 95% and recall above 93% in cluttered supermarket scenes. Additionally, ensure the system's integrated API updates inventory in a sub-second latency (max 100 milliseconds) for seamless operations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"The task involves designing a real-time YOLO-based object detection model for a high-scale, omni-channel retail platform. The model should be optimized for the 'StoreShelf Dataset 5.0' and accurately classify 150 product categories with precision above 95% and recall above 93% in cluttered supermarket scenes. The system's API must update inventory with a latency of no more than 100 milliseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.93\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time (latency)\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"StoreShelf Dataset 5.0\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product categories\"],\n            \"specification\": {\n                \"classes\": 150,\n                \"scene complexity\": \"cluttered supermarket\"\n            },\n            \"description\": \"A dataset for real-time object detection in a retail context, specifically designed for supermarkets.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO-based\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"detection_type\": \"YOLO\",\n                \"accuracy_target\": \"150 product categories\",\n                \"latency_goal\": \"sub-second\"\n            },\n            \"description\": \"A real-time object detection model optimized for the given retail scenario.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud, IoT\",\n        \"deployment_endpoint\": \"API integration for inventory update\",\n        \"inference_engine\": [\"optimization for real-time\"],\n        \"demonstration\": \"system latency demonstration and live API integration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the given instruction by focusing on a YOLO-based object detection model for a retail platform, the required precision and recall, and the sub-second latency for inventory updates. The user's high expertise level is indicated, and the source of the dataset is assumed as a link. The rationale is clear and direct, and the confidence score is high.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "link prediction", "instruction": "Can you develop an advanced, multi-layered embedding-based model specifically tailored for predicting latent citation links in the arXiv overlay network, leveraging Bag-of-Words and DeepWalk techniques, and evaluating its performance with Mean Reciprocal Rank?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"scientific collaboration\",\n        \"description\": \"The user aims to develop a multi-layered, embedding-based model for predicting citation links in the arXiv overlay network, using both Bag-of-Words and DeepWalk techniques. The model should be advanced, and the performance will be evaluated with Mean Reciprocal Rank (MRR).\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Reciprocal Rank\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv overlay network dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"citation links\"],\n            \"specification\": {\n                \"nodes_type\": \"papers\",\n                \"edges_type\": \"citations\",\n                \"bag_of_words_features\": true,\n                \"deepwalk_embeddings\": true\n            },\n            \"description\": \"A graph dataset containing arXiv papers and citations, requiring Bag-of-Words and DeepWalk techniques.\",\n            \"preprocessing\": [\n                \"Extracting Bag-of-Words features\",\n                \"Generating DeepWalk embeddings\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Embedding-based citation prediction model\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Multi-layered with Bag-of-Words and DeepWalk components\",\n                \"training_methods\": [\"supervised\"]\n            },\n            \"description\": \"A high-level, advanced model designed for predicting citation links in the arXiv network, leveraging Bag-of-Words and DeepWalk techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"techniques\",\n            \"content\": [\"Bag-of-Words\", \"DeepWalk\"]\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Fine-tuning model hyperparameters for improved performance\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Efficient GPU-based inference\"],\n        \"demonstration\": \"How to generate prediction visualization and model explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately represents the user's request for an advanced model tailored to citation link prediction on the arXiv overlay network. It incorporates the required techniques, datasets, performance metric, and expertise level. However, missing specific metric value and exact feature engineering steps might be filled based on domain knowledge or collaboration with the user.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Design a time-sensitive, convolutional long short-term memory (LSTM) fusion model for the hourly UrbanSounds8K dataset, specifically focusing on forecasting spectrotemporal patterns for the next 72 hours. Emphasize the model's precision in identifying and differentiating subcategories of urban noise, such as construction, traffic, and emergency sirens, to inform targeted noise abatement strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"environmental management\",\n        \"description\": \"The task is to develop a time-sensitive, convolutional LSTM fusion model for the UrbanSounds8K dataset, targeting spectrotemporal pattern forecasting for the next 72 hours. The focus is on differentiating subcategories of urban noise for noise abatement planning, especially construction, traffic, and emergency sirens.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UrbanSounds8K\",\n            \"modality\": [\"audio\", \"time series\"],\n            \"target_variables\": [\n                \"hourly sound events\"\n            ],\n            \"specification\": {\n                \"length\": {\n                    \"training\": null,\n                    \"testing\": 72,\n                    \"prediction_window\": 72\n                },\n                \"dimension\": null,\n                \"sample_rate\": null\n            },\n            \"description\": \"Dataset containing hourly audio samples of urban sounds, focusing on subcategories like construction, traffic, and emergency sirens.\",\n            \"preprocessing\": [\n                \"spectrogram generation\",\n                \"time alignment\"\n            ],\n            \"augmentation\": [\n                \"time stretching\",\n                \"temporal shifting\"\n            ],\n            \"visualization\": [\n                \"spectrograms\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ConvLSTM Fusion Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"convolutional_layers\": null,\n                    \"LSTM_layers\": null,\n                    \"fusion_layers\": \"spectrotemporal fusion\"\n                },\n                \"parameters\": {\n                    \"time_steps\": 72,\n                    \"input_dim\": null,\n                    \"output_dim\": null\n                }\n            },\n            \"description\": \"A deep learning model combining convolutional layers and LSTM cells for capturing spectrotemporal patterns in the UrbanSounds8K dataset for noise subcategory forecasting over a 72-hour period.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Fusion of CNN and LSTM is effective for time-series data with spatial dependencies\",\n        \"Emphasis on precision in differentiating noise subcategories is crucial\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"wavelet transform\",\n            \"MFCC extraction\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"noise-abatement platform API\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Kubernetes\"],\n        \"demonstration\": \"Real-time noise predictions and suggested mitigation measures\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON representation covers the user's intent to build a model and high expertise level. The task description is clear, focusing on the ConvLSTM fusion model for UrbanSounds8K, subcategory differentiation, and the application to noise abatement. Metrics like precision, inference time, and feature engineering steps are included. The missing information is indicated as null, which is expected given the requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "The task at hand entails optimizing a time-series EEG brainwave analysis project for instant cognitive state recognition with SVM. The challenge is to enhance low-latency classification while maintaining exceptional predictive power, targeting an efficient real-time BCI system for rapid decision-making in high-frequency neural data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"The project aims to optimize an SVM-based model for real-time cognitive state recognition from EEG brainwave data, with a focus on low-latency classification and high predictive power, for use in a real-time Brain-Computer Interface (BCI) system.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null, \n                \"units\": \"milliseconds\"\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"EEG Brainwave Dataset\",\n            \"modality\": [\"time series\", \"graph\"], \n            \"target_variables\": [\"cognitive state\"],\n            \"specification\": {\n                \"sampling_rate\": null,\n                \"feature_length\": null,\n                \"time_window\": null\n            },\n            \"description\": \"A time-series EEG brainwave dataset for cognitive state recognition, suitable for SVM analysis.\",\n            \"preprocessing\": [\"bandpass filtering\", \"artifact removal\"],\n            \"augmentation\": [],\n            \"visualization\": [\"signal plots\", \"spectrograms\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SVM (Time-Series Specific)\",\n            \"family\": \"Support Vector Machines\",\n            \"type\": \"statistical models\",\n            \"specification\": {\n                \"kernel\": \"RBF\", \n                \"C\": null,\n                \"gamma\": null\n            },\n            \"description\": \"An SVM model tailored for time-series EEG data, focusing on low-latency and high predictive power for real-time BCI applications.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Suitable feature extraction techniques such as short-time Fourier transforms or wavelet analysis can improve the classification performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"dimensionality reduction\"],\n        \"target_device\": \"embedded system or mobile for real-time implementation\",\n        \"deployment_endpoint\": \"potentially a cloud or edge server\",\n        \"inference_engine\": [\"optimized C++ or machine learning frameworks\"],\n        \"demonstration\": \"real-time EEG signal visualization and decision feedback\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a high-expertise model for the time-series EEG analysis project. Key details are included, such as the application domain, low-latency objective, and performance metrics. The choice of SVM with a focus on time-series specific techniques is made, and the dataset source and preprocessing steps align with the task. The service requirements reflect a real-world implementation in a BCI system.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "text classification", "instruction": "As a project manager for the advanced machine learning team, your objective is to enhance the text classification task of the Textual Entailment dataset. In addition to achieving high accuracy, devise a multi-modal model that integrates semantic analysis, sentiment analysis, and context-aware reasoning. Emphasize on handling negations and idiomatic expressions, while maintaining interpretability and adhering to a strict 95% precision benchmark. Complete this project within a 60-day timeline, focusing on efficient hardware utilization and parallellization for faster training.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"natural language understanding\",\n        \"description\": \"The objective is to enhance text classification for the Textual Entailment dataset by developing a multi-modal model with semantic analysis, sentiment analysis, and context-aware reasoning. The model should handle negations and idiomatic expressions while maintaining interpretability and achieving a 95% precision benchmark.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"hardware efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPS/second\"\n            },\n            {\n                \"name\": \"training time\",\n                \"value\": \"within 60 days\",\n                \"unit\": \"days\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"parallelization\",\n                \"value\": \"yes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Textual Entailment dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"classification\"],\n            \"specification\": null,\n            \"description\": \"A dataset for text classification with a focus on textual entailment, incorporating the need for handling negations and idiomatic expressions.\",\n            \"preprocessing\": [\"handling negations and idiomatic expressions\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Multi-modal Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"semantic analysis\", \"sentiment analysis\", \"context-aware reasoning\"]\n            },\n            \"description\": \"A multi-modal model targeting text classification with semantic analysis, sentiment analysis, and context-aware reasoning to enhance the Textual Entailment dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Focus on efficient hardware utilization and parallelization to speed up training within the given 60-day timeline.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"interpretable results highlighting performance and model understanding\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the project manager's objective, including the multi-modal model's design, performance metrics, complexity constraints, and the 60-day timeframe. Adjustments made to the model type and dataset preprocessing are in line with the task requirements. The requested hardware efficiency and parallelization are incorporated into the complexity metrics.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning development team, your objective is to develop an image classification system using EfficientNet for the HAM10000 skin cancer dataset. Mandate a minimum accuracy of 99% with a precision-focused metric, aiming to reduce false negatives. Ensure interpretability through feature importance analysis, as the model's decisions will be validated by expert dermatologists who need clear explanations for their diagnoses. Additionally, optimize the model for speed on resource-constrained devices while maintaining top performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"The goal is to develop an image classification system using EfficientNet for the HAM10000 skin cancer dataset, targeting a minimum accuracy of 99% with a focus on precision to reduce false negatives. Interpretability is crucial for feature importance analysis and validation by expert dermatologists.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HAM10000 skin cancer dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"skin condition\"],\n            \"specification\": null,\n            \"description\": \"A dataset for skin cancer detection, containing images where interpretability is essential.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A model designed for image classification with a focus on speed and performance, while being optimized for resource-constrained devices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interpretability is a key factor for validating model decisions with dermatologists.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Clear explanations of model decisions to be provided.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response captures the user's instruction for building an image classification system with EfficientNet, focusing on high accuracy, precision, interpretability, and device optimization. The given performance and complexity metrics are based on the stated requirements. The model and dataset details are straightforward, and the confidence score indicates a strong alignment with the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "The project's prime objective is to develop an advanced object detection system, employing the M2 variant of the EfficientDet architecture, specifically tailored for 'iWildCam' data. We need a model optimized for low-light conditions, real-time performance on resource-constrained drone-mounted cameras, and at least 95% precision in recognizing 50 distinct endangered species in diverse ecosystems. The system must exhibit robustness against occlusions and be deployable in remote, off-grid wildlife sanctuaries.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"The project aims to develop an advanced object detection system using M2 EfficientDet for 'iWildCam' data. The system must handle low-light conditions, provide real-time performance on drone-mounted cameras, and have a minimum precision of 95% for recognizing 50 endangered species. It must also be robust against occlusions and deployable in remote, off-grid wildlife sanctuaries.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"frames per second (fps)\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds (ms)\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"iWildCam Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"endangered species\", \"ecosystem\"],\n            \"specification\": {\n                \"low-light conditions\": true,\n                \"resource-constrained\": true,\n                \"drone-camera\": true\n            },\n            \"description\": \"A dataset for object detection in low-light conditions, specifically tailored for recognizing endangered species from drone-mounted cameras in diverse ecosystems.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M2 EfficientDet\",\n            \"family\": \"Efficient Object Detection\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimized for low-light\": true,\n                \"real-time on resource-constrained hardware\": true\n            },\n            \"description\": \"An advanced object detection model focusing on 50 distinct endangered species recognition and robustness against occlusions.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"The system must be deployable in remote, off-grid wildlife sanctuaries.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained drone-mounted cameras\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for drone deployment\"],\n        \"demonstration\": {\n            \"scenario\": \"robust performance in low-light, real-time, and off-grid environments\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the given instruction, specifying the objective to build an advanced object detection system using M2 EfficientDet for 'iWildCam' data. The model's requirements, such as performance metrics, deployment constraints, and the target species, are included. The confidence level is high due to the comprehensive representation of the given instruction.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "text classification", "instruction": "Your objective, as a project manager for the machine learning development team, is to tackle a multifaceted text classification challenge using the WebMD Review dataset. The task involves not only distinguishing between positive and negative sentiments, but also detecting nuances in tone and context. Emphasize on improving not only macro-average accuracy but also precision and recall. Additionally, propose a novel feature extraction method and incorporate sentiment analysis of negations and intensifiers to boost model performance. Endeavour to achieve state-of-the-art results with a focus on explainability and interpretability of the trained model.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"healthcare (WebMD)\",\n        \"description\": \"The project aims at a multifaceted text classification task on the WebMD Review dataset, focusing on sentiment analysis, tone detection, nuances, and state-of-the-art results with explainability and interpretability. Emphasis is on improving macro-average accuracy, precision, and recall.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"macro-average accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WebMD Review Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\", \"tone\", \"context\"],\n            \"specification\": null,\n            \"description\": \"A dataset for text classification involving positive and negative sentiment, nuances, and context detection in health-related reviews.\",\n            \"preprocessing\": [\"novel feature extraction\", \"sentiment analysis of negations and intensifiers\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (CNN) or Transformers (BERT, RoBERTa)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": \"high\",\n                \"interpretability\": \"well-designed\"\n            },\n            \"description\": \"A state-of-the-art model with novel feature extraction, sentiment analysis of negations and intensifiers, and a focus on explainability and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Proposed novel feature extraction method for context understanding in multi-dimensional text classification.\",\n        \"Integrating sentiment analysis of negations and intensifiers to enhance model performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"novel feature extraction\", \"sentiment analysis of negations and intensifiers\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Visual explanations and interpretability in model predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's instruction, indicating a high expertise level. It reflects the multifaceted task, performance metrics, and novel methods. The structure and content of the response align with the schema while accounting for the specific aspects of the text classification challenge.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "The project for the advanced retail analytics platform necessitates a fine-tuned Faster R-CNN architecture, leveraging the extensive 'SKU-110K' dataset. The objective is to not just detect and count diverse products with subcategory-level precision, but also handle occlusions and varying lighting conditions. A comprehensive report is sought, comparing precision, recall, and F1-score for each product family, accompanied by an analysis of False Positives and Negatives, to ensure accurate inventory control in high-volume stores.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"The project aims to develop a fine-tuned Faster R-CNN architecture for an advanced retail analytics platform. It requires precise detection and counting of products at the subcategory level, handling occlusions, and varying lighting conditions. The focus is on providing a comprehensive report with precision, recall, and F1-score for each product family, while analyzing False Positives and Negatives for accurate inventory control.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"handling occlusions\",\n                \"value\": null,\n                \"unit\": null\n            },\n            {\n                \"name\": \"lighting condition handling\",\n                \"value\": null,\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SKU-110K\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"product family\"\n            ],\n            \"specification\": null,\n            \"description\": \"A large-scale dataset for product detection and analysis, including subcategory-level precision with occlusions and varying lighting conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Faster R-CNN\",\n            \"family\": \"Region-based CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A machine learning model designed for object detection, fine-tuned for the retail analytics use case.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A comprehensive report with detailed analysis of false positives and negatives\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the high-level requirements of the advanced retail analytics platform, focusing on a fine-tuned Faster R-CNN model for detecting diverse products with subcategory-level precision. Performance metrics and specific challenges like handling occlusions and lighting conditions have been included. The low-confidence score is open for any additional details or clarification the user might have.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "The project manager needs a sophisticated machine learning model, capable of real-time forecasting and anomaly detection, for our services demand. It should account for seasonal patterns, external events, and historical trends while providing hourly predictions with high accuracy for staff scheduling and operational efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"business/operations\",\n        \"description\": \"The objective is to create a sophisticated model for real-time service demand forecasting, including anomaly detection. Requirements include accounting for seasonal patterns, external events, and historical trends, aiming for hourly predictions with high accuracy for staff scheduling and operational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time forecasting\",\n                \"value\": null,\n                \"unit\": \"hourly\"\n            },\n            {\n                \"name\": \"anomaly detection\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"services demand\"],\n            \"specification\": {\n                \"data_length\": {\n                    \"type\": \"number\",\n                    \"unit\": \"hours\"\n                },\n                \"seasonality\": \"present\",\n                \"historical_trends\": \"analyzed\",\n                \"external_events_data\": \"required\"\n            },\n            \"description\": \"A time series dataset incorporating historical, seasonal, and external events data for service demand forecasting.\",\n            \"preprocessing\": [\n                \"seasonality decomposition\",\n                \"time-series normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"data exploration and trend patterns\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"CNN or RNN with attention mechanism\",\n                \"real-time_inference\": true,\n                \"anomaly_detection_technique\": \"such as LSTM or Autoencoder\"\n            },\n            \"description\": \"A sophisticated model for real-time forecasting and anomaly detection, leveraging deep learning techniques for high accuracy hourly predictions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Seasonality and external events need to be modeled appropriately\",\n        \"Suitable feature engineering and data preprocessing methods\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonal decomposition (e.g., STL decomposition)\",\n            \"feature extraction from external events data\"\n        ],\n        \"target_device\": \"potentially cloud or server-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"real-time batch processing\"],\n        \"demonstration\": \"predictive dashboard with interactive visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the user's intent to build a high-expertise model for real-time forecasting and anomaly detection. It addresses the need for specific data modalities, preprocessing steps, performance metrics, and external events. The model type is chosen based on the requirement for real-time and advanced analytics. Some aspects, such as the specific model name, are left open for flexibility.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a machine learning project manager, I need you to design a cutting-edge deep learning system that not only detects diverse and rare species of wildlife entering a client's ecologically sensitive wildlife corridor in real-time, but also distinguishes between endangered and invasive species with exceptional accuracy. The model should be optimized for low-power devices to function seamlessly on embedded devices deployed across vast landscapes, while ensuring near-zero false positives for non-threatening wildlife. Additionally, the system should provide instant alerts with actionable insights and suggest mitigation strategies based on historical data, all while maintaining a latency of less than 50 milliseconds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"The goal is to design a deep learning system that detects diverse and rare wildlife species in real-time, differentiating between endangered and invasive species with high accuracy. The system must be optimized for low-power devices, run on embedded devices across large landscapes, have near-zero false positives for non-threatening wildlife, and provide instant alerts with actionable insights and mitigation strategies based on historical data. A latency of less than 50 milliseconds is crucial.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"overall accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": 0.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wildlife Species Detection Dataset (specifically for diverse and rare species)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species classification\", \"endangered vs invasive\"],\n            \"specification\": {\n                \"data_type\": \"labeled\",\n                \"size\": null,\n                \"feature_dimensions\": null,\n                \"age_distribution\": \"various\",\n                \"environmental_variations\": [\"real-world\", \"diverse landscapes\"]\n            },\n            \"description\": \"A dataset with annotations for detecting wildlife species, including rare and invasive species, suitable for real-time detection and classification.\",\n            \"preprocessing\": [\n                \"image augmentation\"\n            ],\n            \"augmentation\": [\"low-power data augmentation\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Wildlife Detection Model (specific to embedded devices)\",\n            \"family\": \"deep learning, preferably a state-of-the-art object detection architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"customized for low-power devices\",\n                \"flops\": null,\n                \"parameters\": null\n            },\n            \"description\": \"A deep learning model designed for efficient wildlife detection, optimized for low-power devices and real-time operation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"lightweight feature extraction\"\n        ],\n        \"target_device\": \"low-power embedded devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time, optimized inference for low-power devices\"],\n        \"demonstration\": \"live demos showcasing model performance on edge devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the complex nature of the task by focusing on a building a deep learning system for wildlife detection with high accuracy and low-power requirements. The task specifies detailed metrics, the target dataset, and the optimization for embedded devices. Rationale covers how the JSON structure aligns with the given instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "The client has proposed an ambitious project that necessitates a real-time, high-accuracy dog breed identification app. With the Stanford Dogs Dataset, explore the feasibility of employing a convolutional neural network (CNN) for its efficiency, but also consider a transfer learning approach using pre-trained models like VGG16 or ResNet. Discuss the trade-offs between computational complexity and model performance, and ensure the user experience remains seamless and response time optimized.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal recognition\",\n        \"description\": \"The project aims to develop a real-time, high-accuracy dog breed identification app using the Stanford Dogs Dataset. The client requests considering a CNN, possibly with transfer learning using pre-trained models like VGG16 or ResNet. The focus is on balancing computational complexity and model performance for a seamless user experience with optimized response times.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational complexity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Dogs Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"dog breed\"],\n            \"specification\": {\n                \"size\": \"real-time\",\n                \"image_resolution\": null,\n                \"image_type\": \"color\"\n            },\n            \"description\": \"A dataset for real-time dog breed identification, emphasizing efficiency and high accuracy.\",\n            \"preprocessing\": [\n                \"image resizing and normalization\"\n            ],\n            \"augmentation\": [\n                \"data augmentation for improved generalization\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pre-trained\": [\"VGG16\", \"ResNet\"],\n                \"transfer_learning\": true,\n                \"compute_complexity\": {\n                    \"model_size\": null,\n                    \"flops\": null\n                }\n            },\n            \"description\": \"Considering CNNs for efficiency, with potential for transfer learning using pre-trained VGG16 or ResNet models.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning can reduce the need for extensive training data and computational resources, yet adjusting for domain-specific data may slightly degrade performance.\",\n        \"CNN architectures like VGG16 and ResNet have varying computational complexities, balancing between performance and computational load.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile and cloud\",\n        \"deployment_endpoint\": \"user's mobile device or a cloud-based API\",\n        \"inference_engine\": [\"optimized CNN inference methods\"],\n        \"demonstration\": \"Demonstrate fast and accurate breed identification with real-time examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the client's ambitious project, with emphasis on high accuracy, real-time performance, and trade-offs. The user's expertise level is assumed to be high given the project's complexity. Transfer learning, CNNs, and performance metrics are included based on the requirements. However, some values and specifics may require further clarification or fine-tuning.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For a comprehensive urban greenspace analysis, develop a state-of-the-art fusion of SegNet and a deep contextual encoder using the Vaihingen dataset. The model must handle complex scenes with multi-layer vegetation, seasonality, and diverse illuminations, while incorporating a degradation-resistant feature extractor for precise health assessment. Demand precision metrics and generate practical suggestions for sustainable urban planning based on the segmentation results.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"The task is to create a state-of-the-art model that fuses SegNet and a deep contextual encoder, designed for comprehensive urban greenspace analysis on the Vaihingen dataset. The model should handle multi-layer vegetation, seasonality, and diverse illuminations, while featuring a degradation-resistant feature extractor for accurate health assessment of urban green spaces.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"greenspace segmentation map\"],\n            \"specification\": {\n                \"data_features\": \"complex scenes, multi-layer vegetation, seasonality, diverse illuminations\",\n                \"data_distribution\": \"likely includes train, validation, and test splits\"\n            },\n            \"description\": \"A dataset for comprehensive urban greenspace analysis, containing complex scenes and multi-modal data for model training.\",\n            \"preprocessing\": [\"degradation-resistant feature extraction\"],\n            \"augmentation\": [\"handling seasonal and illumination variations\"],\n            \"visualization\": [\"segmentation results for model validation\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Fused SegNet and Deep Contextual Encoder\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"degradation-resistant\": true,\n                \"health_assessment_capabilities\": true\n            },\n            \"description\": \"A state-of-the-art model architecture designed to handle urban greenspace analysis with precision and adaptability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\" greenspace health indicator\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"practical suggestions for sustainable urban planning based on segmentation results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's request for a state-of-the-art model with precision metrics, handling specific complexities in urban greenspace analysis. The Vaihingen dataset is correctly identified, and the dataset characteristics are described in detail. The model's architecture and requirements are aligned with the analysis task. Confidence score is high due to the thorough interpretation of the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "As a machine learning project manager, I request a state-of-the-art Graph Convolutional Network (GCN) for the diverse CORA dataset. The model should exhibit at least 90% accuracy after applying advanced regularization techniques to combat overfitting. Additionally, optimize for minimal 15% improvement in training speed while maintaining efficiency on a standard GPU, ensuring a maximum training duration of 1 hour and 45 minutes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"graph data analysis\",\n        \"description\": \"The project involves developing a state-of-the-art Graph Convolutional Network (GCN) for the CORA dataset, targeting at least 90% accuracy. Advanced regularization techniques must be applied to address overfitting. Additionally, the model must optimize for at least a 15% improvement in training speed on a standard GPU, ensuring a maximum training time of 1 hour and 45 minutes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training speed improvement\",\n                \"value\": 0.15,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"training duration\",\n                \"value\": 95,\n                \"unit\": \"minutes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CORA\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": null,\n                \"edges\": null\n            },\n            \"description\": \"A diverse graph dataset for the node classification task, required for the Graph Convolutional Network.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art Graph Convolutional Network\",\n            \"family\": \"GCN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimizer\": \"advanced regularization\",\n                \"overfitting_resistance\": true,\n                \"gpu_efficiency\": true,\n                \"max_training_duration\": \"1 hour and 45 minutes\"\n            },\n            \"description\": \"A model focusing on node classification using the CORA dataset, targeting high accuracy and optimized training speed.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"standard GPU\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's request for a state-of-the-art GCN for the CORA dataset, addressing the accuracy goal of 90% with regularization techniques, and optimizing for a specified improvement in training speed and time limit. The structure is kept consistent with the provided schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "The project manager needs to oversee the design and implementation of an advanced image recognition algorithm, focusing on the Fashion-MNIST dataset. The core technology must be based on innovative Capsule Routing Networks, targeting to surpass a stringent benchmark of 95% accuracy while demonstrating exceptional resilience to image distortions like rotation and scaling. The proposal should include a comprehensive analysis comparing capsule network performance with state-of-the-art Convolutional Neural Networks (CNN), emphasizing the nuanced advantages and efficiency trade-offs in the context of style recognition.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"The task involves designing and implementing an advanced image recognition algorithm, specifically focusing on the Fashion-MNIST dataset. The algorithm must utilize innovative Capsule Routing Networks, aiming to achieve a benchmark of at least 95% accuracy, while demonstrating resilience to image distortions such as rotation and scaling. A comparison with state-of-the-art Convolutional Neural Networks (CNNs) is required, highlighting the benefits and efficiency trade-offs for style recognition.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": [28, 28],\n                \"number_of_classes\": 10\n            },\n            \"description\": \"The dataset for evaluating image recognition in the fashion domain, known for its variations and focus on style recognition.\",\n            \"preprocessing\": [\"rotation and scaling invariant\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Capsule Routing Networks\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novelty\": \"innovative technology\",\n                \"resilience\": \"rotation and scaling invariant\"\n            },\n            \"description\": \"Proposed architecture for image recognition based on Capsule Networks, targeting improved performance over state-of-the-art Convolutional Neural Networks in style recognition.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"comparison\",\n            \"content\": \"Performance comparison with state-of-the-art CNNs, emphasizing advantages and efficiency trade-offs in handling image distortions.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"analysis of performance and resilience to distortions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the main requirements of the project, including the advanced image recognition algorithm based on Capsule Routing Networks, the accuracy benchmark, and the focus on resilience to distortions. The comparison with CNNs, dataset specifications, and knowledge aspects have been included. The user's high expertise level is accounted for.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "For our upcoming cutting-edge research project, we intend to develop a highly specialized neural machine translation system using the diverse WMT19 multilingual news corpus. The focus will be on translating between Russian and English with exceptional fluency, aiming for a BLEU score of 45, while also exploring zero-shot and transfer learning techniques for improved performance across domains. Don't forget to integrate robust error analysis and conduct human-parity checks in your submission.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"The project aims to develop a highly specialized neural machine translation system using the WMT19 multilingual news corpus, focusing on Russian-English translation, emphasizing exceptional fluency and targeting a BLEU score of 45. Zero-shot and transfer learning techniques will be explored for better performance across domains, and robust error analysis and human-parity checks are crucial.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 45\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 multilingual news corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": null,\n            \"description\": \"A multilingual news corpus for developing a neural machine translation system, with a focus on Russian-English translation.\",\n            \"preprocessing\": [\"multilingual data preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Neural Machine Translation System (NMT)\",\n            \"family\": \"Sequence-to-Sequence (seq2seq) models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"neural machine translation\",\n                \"language_pairs\": [\"Russian-English\"],\n                \"zero_shot_learning\": true,\n                \"transfer_learning\": true\n            },\n            \"description\": \"A specialized system using neural machine translation techniques with focus on Russian-English translation and exploration of zero-shot and transfer learning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Error analysis and human parity checks to ensure quality and correctness.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Human-parity checks and error analysis results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key elements of the instruction, including the project's focus, dataset (WMT19 multilingual news corpus), model type (NMT with seq2seq architecture), and performance target (BLEU score of 45). It acknowledges the advanced techniques and requirements for evaluation. The confidence score is set to a high level due to a good mapping of the instruction to the JSON format.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for the advanced machine learning team, you are tasked with developing a robust time-series forecasting model for the Electricity dataset. This complex dataset, containing real-world time series data, has been meticulously divided into three partitions: training, validation, and testing. Each input sequence consists of historical observations with a specific size (INPUT_SEQ_LEN=96 and INPUT_DIM=321), requiring deep context understanding. The challenge lies in forecasting not just one but consecutive future sequences of equal length (PRED_SEQ_LEN=96, PRED_DIM=321) while optimizing for multiple-step predictive accuracy. Additionally, the project should minimize mean squared error (MSE) and mean absolute error (MAE) over several forecasting horizons, necessitating a sophisticated model that exhibits excellent generalization and resilience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy\",\n        \"description\": \"The project requires developing a robust time-series forecasting model for the Electricity dataset, which is divided into training, validation, and testing sets. The model must handle input sequences of size 96 with 321 dimensions, forecast consecutive future sequences of 96 steps with the same dimensions, and optimize for multiple-step accuracy minimizing MSE and MAE.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321\n            },\n            \"description\": \"A complex real-world time series dataset, divided into train, validation, and test sets, with input sequences requiring deep context understanding.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"deep, LSTM, or Transformer-based architecture for time-series forecasting\",\n                \"input_sequence_length\": 96,\n                \"output_sequence_length\": 96,\n                \"dimensions\": 321,\n                \"forecasting_horizon\": [\n                    \"multiple steps\"\n                ],\n                \"optimization_metrics\": [\n                    \"mean squared error\",\n                    \"mean absolute error\"\n                ]\n            },\n            \"description\": \"A sophisticated model designed for time-series forecasting, addressing the need for generalization and resilience.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"description\": \"Model demonstration showcasing multiple-step forecasting and evaluation metrics\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately represents the task, with the user's intent to build, high expertise, the focus on time-series forecasting, and the specific requirements for input and output sequence lengths, prediction accuracy, and error metrics. The model type and structure are hypothetical based on the instruction's context.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "For the SMS Spam Collection dataset, develop a spam detection system that not only upholds a minimal 95% precision but also ensures energy efficiency by optimizing an ultra-lightweight, hybrid LSTM-GRU model. Prioritize latency, aiming for sub-100ms response time on resource-constrained mobile devices, while integrating a novel real-time attention mechanism to adapt to evolving message patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"security\",\n        \"description\": \"The task is to build a spam detection system for the SMS Spam Collection dataset with a precision goal of at least 95%. The system should prioritize energy efficiency and ultra-lightweight design, achieving sub-100ms response time on mobile devices while incorporating a real-time attention mechanism for adapting to evolving message patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": 100,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SMS Spam Collection\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"spam/ham\"],\n            \"specification\": null,\n            \"description\": \"A dataset for spam detection, possibly split into training and testing sets.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid LSTM-GRU\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ultra-lightweight\",\n                \"attention_mechanism\": \"novel and real-time\"\n            },\n            \"description\": \"A lightweight model integrating LSTM and GRU for spam detection, optimized for energy efficiency and fast response times.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource-constrained devices\"],\n        \"demonstration\": {\n            \"goal\": \"sub-100ms response on mobile devices\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed based on the provided instructions, capturing the user's intent to build a spam detection system. The high expertise level and specific requirements for the system (95% precision, lightweight, sub-100ms response, real-time attention) are included. The dataset and model names are derived from the 'SMS Spam Collection' and 'Hybrid LSTM-GRU' respectively. The confidence score is high due to the direct mapping of the task details.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a leading-edge real estate firm, we face a massive image data challenge in showcasing diverse properties. Now, we demand a sophisticated image analysis system capable of not only differentiating between intricate design elements like solar panels, outdoor entertainment areas, and eco-friendly features, but also contextually understanding their relevance in the property's overall appeal. The system must be scalable, efficient, and result in a substantial reduction in human error for a competitive market advantage.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"real estate\",\n        \"description\": \"The user's firm requires a sophisticated image analysis system for classifying and understanding intricate design elements (solar panels, outdoor entertainment areas, eco-friendly features) in real estate images. The system should be scalable, efficient, and reduce human error for competitive advantage.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"None (e.g., can handle large data volumes)\"\n            },\n            {\n                \"name\": \"efficiency\",\n                \"value\": null,\n                \"unit\": \"FPS (frames per second) or GPU inference time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Real Estate Image Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"solar panels\", \"outdoor entertainment\", \"eco-friendly features\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing diverse real estate images with a focus on specific design elements and their relevance.\",\n            \"preprocessing\": [\"image cleaning\", \" resizing\", \"normalization\"],\n            \"augmentation\": [\"context-aware data augmentation\"],\n            \"visualization\": [\"property feature heatmaps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image Classification Model\",\n            \"family\": \"Convolutional Neural Networks (CNNs)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep learning with attention mechanism or transfer learning (e.g., Inception, ResNet, or EfficientNet)\",\n                \"computational complexity\": null\n            },\n            \"description\": \"A state-of-the-art model for accurate classification and contextual understanding of design elements in real estate images.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Contextual understanding of relevance for various eco-friendly and design features in the real estate market.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"contextual embeddings\"],\n        \"target_device\": \"cloud-based or optimized for deployment on edge devices\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"TensorFlow Serving\", \"Cloud ML Engine\"],\n        \"demonstration\": \"Interactive UI showcasing model predictions with detailed explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent for a high-demand application in the real estate industry. It includes the necessary domain, problem description, performance metrics, and advanced model requirements. Preprocessing, augmentation, and feature engineering are tailored to the specific task. The service section addresses deployment and performance considerations. Overall, the structure and content meet the requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager, you've been tasked with developing an advanced image segmentation model for a real estate monitoring system. Given extensive aerial datasets with varying weather conditions, the objective is to not only identify buildings, trees, and roads but also distinguish and classify different types of vegetation and infrastructure, ensuring high accuracy even in low-resolution images while streamlining the inspection process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"real estate monitoring\",\n        \"description\": \"The project involves creating an advanced image segmentation model for an AI-driven real estate monitoring system. The goal is to identify buildings, trees, roads, and classify different vegetation and infrastructure types in aerial datasets with varying weather conditions. Accuracy must be high, especially in low-resolution images, and the system should optimize inspection efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Aerial Real Estate Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"buildings\", \"trees\", \"roads\", \"vegetation types\", \"infrastructure types\"],\n            \"specification\": null,\n            \"description\": \"A dataset with extensive aerial images showcasing varying weather conditions for image segmentation tasks, focusing on identifying and differentiating various elements of interest.\",\n            \"preprocessing\": [\n                \"image resizing\",\n                \"weather normalization\"\n            ],\n            \"augmentation\": [\"data augmentation techniques for low-resolution images\"],\n            \"visualization\": [\n                \"performance visualizations for accuracy\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Network (CNN) with U-Net or DeepLabV3+ architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"An advanced model for image segmentation with a focus on high accuracy in low-resolution images and efficient handling of diverse datasets.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced techniques like transfer learning and feature pyramid networks will be beneficial for improved performance.\",\n        \"Using a smaller model or optimized architecture for efficiency in resource-constrained environments.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction for vegetation and infrastructure classification\"\n        ],\n        \"target_device\": \"edge devices for real-time monitoring\",\n        \"deployment_endpoint\": \"cloud-based deployment for scalability\",\n        \"inference_engine\": [\"efficient inference models like TensorRT or MobileNet\"],\n        \"demonstration\": \"live demos showcasing model's accuracy under varying conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the given instruction, indicating a high-level building project with a focus on image segmentation for real estate monitoring. Performance metrics, complex requirements, and model requirements are specified, along with expected feature engineering, deployment, and demonstration steps. The high expertise level of the user is also taken into account.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "As a machine learning project manager, I need a comprehensive analysis on the feasibility of adapting transformer models, specifically the state-of-the-art variants, for enhanced video forgery detection in the Celeb-DF dataset. The prompt should delve into nuances like adapting multi-modal inputs, evaluating the model's performance against current forgery detection techniques, and the required computational resources for real-time, high-accuracy detection.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"security\",\n        \"description\": \"The project aims to analyze the feasibility of using transformer models, focusing on state-of-the-art variants, for detecting video forgeries in the Celeb-DF dataset. Attention should be given to adapting models for multi-modal inputs, comparing performance against existing forgery detection methods, and assessing computational requirements for real-time, high-accuracy detection.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time inference\",\n                \"value\": \"high\",\n                \"unit\": \"speed\"\n            },\n            {\n                \"name\": \"GPU memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"CPU processing power\",\n                \"value\": null,\n                \"unit\": \"GFLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\"video\", \"text\", \"audio\"],\n            \"target_variables\": [\"forged vs genuine\"],\n            \"specification\": {\n                \"number of frames\": null,\n                \"modalities length\": {\n                    \"video\": null,\n                    \"text\": null,\n                    \"audio\": null\n                }\n            },\n            \"description\": \"A video dataset for video forgery detection in the context of Celeb-DF, with multi-modal inputs.\",\n            \"preprocessing\": [\n                \"multimodal feature extraction\"\n            ],\n            \"augmentation\": [\n                \"temporal and modality-specific\"\n            ],\n            \"visualization\": [\n                \"model explainability\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer variants for video forgery detection\",\n            \"family\": \"Multimodal Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-based model for video analysis\",\n                \"input modalities\": \"video, text, and audio\",\n                \"output classes\": \"forged vs genuine\"\n            },\n            \"description\": \"Proposed to enhance video forgery detection using state-of-the-art transformers that can handle multi-modal inputs.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Transformer models have shown strong performance in processing sequential data and multi-modal inputs.\"\n        },\n        {\n            \"text\": \"Existing literature on multimodal forgery detection with transformers may serve as a starting point for comparison.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"data fusion techniques\"\n        ],\n        \"target_device\": \"cloud or edge devices\",\n        \"deployment_endpoint\": \"Vulnerability detection service\",\n        \"inference_engine\": [\"real-time inference\"],\n        \"demonstration\": \"example visualizations of forgery detection results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the consulting-building nature of the inquiry, as it seeks advice on adapting transformer models for video forgery detection. Expertise level is set to high, considering the project manager's role. It covers performance metrics, complexity factors, and specific adaptations for the Celeb-DF dataset. Knowledge and service requirements for the project are also included.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for a machine learning team, I seek to develop an efficient indoor scene segmentation system. Investigate the potential of U-Net architecture for accurate furniture detection, while optimizing model runtime for real-time, low-latency performance on resource-constrained hardware. Can you present a lightweight implementation and runtime analysis?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"indoor navigation, interior design\",\n        \"description\": \"The project goal is to develop an efficient indoor scene segmentation system, focusing on accurate furniture detection, targeting real-time performance on resource-constrained hardware with low latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall (furniture detection)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": {\"type\": \"max\", \"unit\": \"ms\", \"target\": \"real-time\"},\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": {\"type\": \"low\", \"unit\": \"MB\", \"target\": \"resource-constrained hardware\"}\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"scene-level images\",\n                \"modalities\": \"RGB\",\n                \"furniture annotations\": true\n            },\n            \"description\": \"Dataset for indoor scene segmentation, including furniture detection annotations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"lightweight\": true,\n                \"design\": \"optimized for resource-constrained hardware\"\n            },\n            \"description\": \"Investigation into the use of U-Net architecture for accurate and lightweight furniture detection in real-time scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"existing research\",\n            \"content\": \"Potential challenges and improvements in applying U-Net on resource-constrained devices, like model pruning, quantization, and hardware-specific optimizations.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained hardware (e.g., edge devices, IoT)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for specific hardware\"\n        ],\n        \"demonstration\": \"Real-time performance demonstration with latency analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the project manager's needs, focusing on U-Net for furniture detection, real-time performance, and resource constraints. It includes a lightweight implementation, runtime analysis, and specific performance metrics. It assumes existing knowledge on adapting U-Net for resource-limited devices. The structure is kept consistent with the schema provided, while catering to the requirements in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for a specialized team working on the arduous task of fine-grained text classification, your objective is to develop a state-of-the-art model for the BoolQ dataset. This dataset necessitates precise analysis of 10,000 intricately designed passages, each accompanied by diverse, context-dependent questions, to predict entailment with <1% error rate using BERT-Base uncased architecture. Ensure high precision and F1-score in addition to overall accuracy, and include detailed experimentation logs for replication.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"The task involves developing a state-of-the-art model using BERT-Base uncased architecture for fine-grained text classification on the BoolQ dataset, targeting entailment prediction with a <1% error rate and a focus on precision, F1-score, and overall accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"error rate\",\n                \"value\": 0.01\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"entailment\"],\n            \"specification\": {\n                \"size\": {\n                    \"total\": 10000\n                },\n                \"length\": {\n                    \"passages\": {\n                        \"mean\": 500,\n                        \"range\": null\n                    },\n                    \"questions\": {\n                        \"mean\": 15,\n                        \"range\": null\n                    }\n                }\n            },\n            \"description\": \"A dataset of 10,000 passages with intricate questions, requiring precise analysis to predict entailment with minimal error.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-Base uncased\",\n            \"family\": \"Transformer-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT\",\n                \"cased\": false\n            },\n            \"description\": \"The model to be developed, focusing on high performance in fine-grained text classification on the BoolQ dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"The project requires detailed experimentation logs for replication of the model's development process.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"detailed logs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's intent to build a high-expertise-level model, specifies the BERT-Base uncased task for text classification, and sets the requirement for the dataset size and complexity of the passages. Performance metrics and the emphasis on precision, F1-score, and accuracy indicate a well-defined project goal. The requirement for detailed experimentation logs for replication adds the crucial aspect of reproducibility.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Develop a visually immersive platform utilizing the Flickr30k dataset and a state-of-the-art Visual Transformer, focusing on understanding and interpreting complex social scenarios, distinct location contexts, and subtle inter-person interactions. Emphasize generating elaborate, story-driven responses while maintaining a benchmark accuracy above 70%, ensuring a thought-provoking and immersive user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"social understanding and media interpretation\",\n        \"description\": \"The task involves creating a visually immersive platform using the Flickr30k dataset and a Visual Transformer. The focus is on understanding complex social scenarios, diverse location contexts, and subtle inter-person interactions. The platform should generate detailed, story-driven responses with a benchmark accuracy of at least 70%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.7\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Flickr30k\",\n            \"modality\": [\"image\", \"text\"],\n            \"target_variables\": [\"descriptions\", \"contextual information\"],\n            \"specification\": {\n                \"image_types\": \"complex social scenarios, location contexts\",\n                \"text_data\": \"social scenarios, context, and interaction\"\n            },\n            \"description\": \"A multimodal dataset for understanding and interpreting complex scenes and social interactions.\",\n            \"preprocessing\": [\"data cleaning, preprocessing for both images and text\"],\n            \"augmentation\": [\"generating diverse scenarios with subtle variations\"],\n            \"visualization\": [\"interpreting model's attention on images and text\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Visual Transformer\",\n            \"family\": \"Attention-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art for visual question answering\",\n                \"focus\": [\"interpreting complex inputs\"]\n            },\n            \"description\": \"A model designed for understanding multimodal data, capable of generating story-driven responses.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The task requires advanced understanding of social contexts and interpretability.\",\n        \"The goal of maintaining a benchmark accuracy is crucial for user experience\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware feature extraction\",\n            \"embedding fusion techniques\"\n        ],\n        \"target_device\": \"high-performance computers or cloud-based systems\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"engaging story-driven visual responses with explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has accurately captured the intent to build a platform with a Visual Transformer. It reflects high expertise, specifies the complex nature of the task, includes performance metrics, emphasizes the need for a benchmark accuracy, and accounts for data preprocessing, augmentation, and visualization for the Flickr30k dataset. The knowledge and service components reflect the platform's requirements for an immersive experience.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a leading urban planning consultant, you've been tasked with developing an advanced analytics system that utilizes a novel hybrid architecture of Transformer-XL and Graph Neural Networks (GNN) to predict not just hourly traffic volume, but also anticipate the impact of potential upcoming events, weather patterns, and infrastructure changes on the city's traffic flow for the next quarter. The model should differentiate between typical peak hours, public holidays, and weekends, while also identifying emergent traffic patterns in real-time. Additionally, propose a method to visualize the forecast's uncertainty and suggest proactive congestion management strategies based on the predicted fluctuations. Document the explainability of the model's decision-making process in a concise yet comprehensive report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"The project involves developing an advanced analytics system that uses a hybrid model of Transformer-XL and Graph Neural Networks to forecast hourly traffic volume for the next quarter, accounting for events, weather, and infrastructure changes. The system should identify peak hours, public holidays, and weekends, and detect emergent traffic patterns in real-time. It also requires a method for visualizing forecast uncertainty and suggesting congestion management strategies, with a focus on explainable model decision-making.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"forecast accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"forecast precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"forecast recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time detection rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"City Traffic Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly traffic volume\", \"event data\", \"weather data\", \"infrastructure changes\"],\n            \"specification\": null,\n            \"description\": \"A dataset capturing historical traffic data, including weather patterns, event occurrences, and infrastructure changes, for real-time analysis and forecasting.\",\n            \"preprocessing\": [\"handling missing data\", \"feature scaling\", \"time-series normalization\"],\n            \"augmentation\": [\"synthetic data generation for future events and weather scenarios\"],\n            \"visualization\": [\n                \"forecast uncertainty plots\",\n                \"real-time traffic flow patterns\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Architecture (Transformer-XL + GNN)\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"Transformer-XL for time-series forecasting, GNN for capturing network dependencies\",\n                \"layers\": \"\",\n                \"parameters\": \"\",\n                \"performance characteristics\": \"patience during training, explainability methods\"\n            },\n            \"description\": \"A novel combination of Transformer-XL for handling temporal dependencies and Graph Neural Networks for spatial relationships, aiming for accurate and real-time traffic predictions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Explainability techniques: LIME, SHAP, feature importance rankings\",\n        \"Congestion management strategies: dynamic routing, ramp metering, parking optimization\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature engineering for hybrid model\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"urban planning analytics platform\",\n        \"inference_engine\": [\"GPU-accelerated\", \"cloud-based TPU\"],\n        \"demonstration\": \"real-time dashboard visualizing predictions and management strategies\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the high-level requirements of the complex project, including the multi-faceted model, data handling, and performance metrics. Key concepts like hybrid architecture, explainable model, and congestion management strategies are incorporated. Missing values indicate room for further discussion.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager, you're tasked with developing an advanced object detection system for a niche market that focuses on recognizing and quantifying rare and exotic fruits, such as olallieberries, physalis, and kiwano, in addition to common ones. The team must utilize a combination of traditional and deep learning techniques, considering factors like low image resolution, occlusions, and seasonal variations in appearance. The project should also incorporate a real-time smartphone app for amateur fruit farmers to upload and receive quick identification and yield estimations, ensuring the system is user-friendly and computationally efficient. Outline the necessary steps, research, and technologies for this unique project.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"agriculture (niche: rare and exotic fruits)\",\n        \"description\": \"The project aims to develop an advanced object detection system for recognizing and quantifying rare fruits like olallieberries, physalis, and kiwano. It should handle low image resolution, occlusions, and seasonal variations while being user-friendly and computationally efficient for a smartphone app.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"efficiency (computationally)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"time complexity - real-time processing\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage - mobile device compatibility\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Rare Fruit Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"fruit type\", \"yield estimation\"],\n            \"specification\": {\n                \"image_resolution\": null,\n                \"seasonal_variations\": true,\n                \"occlusions\": true\n            },\n            \"description\": \"A diverse set of images capturing rare and exotic fruits under various conditions for object detection and yield estimation.\",\n            \"preprocessing\": [\n                \"image resizing\",\n                \"data augmentation for low resolution and occlusions\"\n            ],\n            \"augmentation\": [\"low-resolution, occlusions\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hybrid (traditional & deep learning)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"Incorporating traditional methods for robustness, and deep learning for accuracy, considering challenges like low image resolution\",\n                \"combination_techniques\": null\n            },\n            \"description\": \"A system using a combination of traditional and deep learning methods to handle the unique fruit recognition and yield estimation requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"To handle low image resolution, research into super-resolution techniques and feature extraction methods will be crucial.\",\n            \"type\": \"research\"\n        },\n        {\n            \"description\": \"Consider lightweight deep learning architectures and techniques for efficient computation on mobile devices, such as MobileNet or TensorFlow Lite.\",\n            \"type\": \"technology\"\n        },\n        {\n            \"description\": \"Use user-friendly interfaces and simple algorithms for fast identification and yield estimation for non-experts.\",\n            \"type\": \"design\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"image processing for mobile app compatibility\",\n            \"real-time detection algorithms\"\n        ],\n        \"target_device\": \"smartphone\",\n        \"deployment_endpoint\": {\n            \"service\": \"cloud-based or decentralized for real-time processing\"\n        },\n        \"inference_engine\": [\n            \"efficient mobile app integration\"\n        ],\n        \"demonstration\": \"User-friendly mobile app and live demo for farmers to upload and receive fruit recognition and yield estimation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the essence of the project, outlining the task as building an advanced object detection system using traditional and deep learning techniques. It addresses the user's high expertise level by delving into specific challenges, research, and technologies. The model combination and the app requirements are clearly defined, ensuring a clear understanding of the project's scope.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a machine learning team, your objective is to enhance the time-series forecasting challenge for the ILI dataset. The task now includes:\n1. Develop a state-space model using deep neural networks that incorporate autoregressive components and seasonal patterns.\n2. Ensure the model handles non-linearities and trends with a rolling window technique, while maintaining the train (36 time steps), validation, and test sets (INPUT_SEQ_LEN=36, INPUT_DIM=7).\n3. Optimize for both mean squared error (MSE) and mean absolute error (MAE) while minimizing prediction latency of the next 24 steps (PRED_SEQ_LEN=24, PRED_DIM=7) with a focus on real-time operational efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The objective is to enhance time-series forecasting for the ILI dataset, specifically using a state-space model based on deep neural networks with autoregressive components and seasonal patterns. Non-linearities and trends should be handled with a rolling window, maintaining train, validation, and test sets with given dimensions and sequence lengths.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"prediction latency\",\n                \"value\": 24, \n                \"unit\": \"time steps\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 36,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 24,\n                \"PRED_DIM\": 7\n            },\n            \"description\": \"A dataset for time-series forecasting task with given train, validation, and test sets.\",\n            \"preprocessing\": [\"state-space transformation\", \"seasonality modeling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep State-Space Model\",\n            \"family\": \"State-Space Model (SSM) with Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep, autoregressive, and seasonal pattern\",\n                \"technique\": \"rolling window\",\n                \"non-linearity treatment\": \"handled with neural networks\",\n                \"real-time efficiency\": true\n            },\n            \"description\": \"A state-space model designed for time-series forecasting with deep neural networks, incorporating autoregressive components and seasonal patterns.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"rolling window technique for non-linearities and trends\"],\n        \"target_device\": \"\", \n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time operational efficiency\"],\n        \"demonstration\": \"Rolling forecast demo with 24-step latency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the required elements from the given instruction, such as the state-space model with deep neural networks, handling of non-linearities, trends, and specified evaluation metrics. Train and test set details, as well as real-time operational efficiency, are also accurately represented. The confidence score reflects the high expertise level of the user.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "The project manager is tasked with developing a specialized, real-time Time-Series Anomaly Detection (TSD) CNN for \"MoltenLithosphereSeismicSignal\" dataset, focusing on short latency detection of tectonic instabilities, while integrating recursive feature extraction and multi-resolution analysis techniques.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"geology\",\n        \"description\": \"The project aims to develop a specialized real-time Time-Series Anomaly Detection (TSD) CNN model for the 'MoltenLithosphereSeismicSignal' dataset. The focus is on short latency detection of tectonic instabilities, and the model incorporates recursive feature extraction and multi-resolution analysis techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"short (ideally milliseconds)\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MoltenLithosphereSeismicSignal\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"tectonic instability indicator\"],\n            \"specification\": {\n                \"length\": null,\n                \"sampling rate\": null\n            },\n            \"description\": \"The dataset involves seismic signals from the molten lithosphere, meant for short-latency detection of tectonic instabilities.\",\n            \"preprocessing\": [\n                \"recursive feature extraction\",\n                \"multi-resolution analysis\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Real-time TSD CNN\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"incorporates CNN layers, recursive feature extraction, and multi-resolution analysis\"\n            },\n            \"description\": \"A specialized model for time-series anomaly detection with a focus on short latency and application to the tectonic instability detection task.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response accurately reflects the requirements of the given instruction. It includes the desired TSD CNN model, target dataset (MoltenLithosphereSeismicSignal), target domain (geology), the emphasis on short latency and specific techniques (recursive feature extraction and multi-resolution analysis). Performance metrics, such as accuracy, latency, and time/space complexity are also included as specified.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "For a highly specialized time-series classification project aimed at predicting and differentiating complex human activities using wearable fitness trackers, design a state-of-the-art algorithm that can recognize not only basic exercises like walking, running, and cycling but also advanced workout routines, Yoga postures, and subtle nuances in daily activities such as sleep patterns. The model should be scalable, efficient, and capable of handling high-frequency sensor data with minimal noise. Additionally, consider incorporating temporal dependencies and interpretability features, as the end-user requires insights into the classification process for personalized coaching.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The user aims to build a state-of-the-art algorithm for a time-series classification project using wearable fitness trackers. The objective is to predict complex human activities (basic exercises, advanced workout routines, yoga, and daily activity nuances), while maintaining scalability, efficiency, and handling high-frequency sensor data with minimal noise. The model should incorporate temporal dependencies and provide interpretability for personalized coaching.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fitness Tracker Sensor Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\n                \"complex activity labels\"\n            ],\n            \"specification\": {\n                \"sample_rate\": null,\n                \"features\": {\n                    \"high_frequency\": true\n                },\n                \"noise_reduction\": true\n            },\n            \"description\": \"A high-frequency time-series dataset from wearable fitness trackers, capturing complex human activities and subtle daily patterns.\",\n            \"preprocessing\": [\n                \"noise reduction techniques\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"temporal dependencies\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-Art Time-series Classifier\",\n            \"family\": \"deep learning (RNN or LSTM-based)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability_features\": [\"feature importances\", \"attention mechanisms\"]\n            },\n            \"description\": \"A scalable and efficient algorithm designed for recognizing complex human activities using wearable data, with a focus on temporal dependencies and interpretability for personalized coaching.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using attention mechanisms and explainable AI techniques for interpretability.\",\n        \"Optimize for scalability by using efficient data structures and distributed learning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"handling missing data\", \"feature extraction\"],\n        \"target_device\": \"mobile or wearable devices\",\n        \"deployment_endpoint\": \"cloud-based platform\",\n        \"inference_engine\": [\"real-time streaming\", \"ensemble prediction\"],\n        \"demonstration\": \"interactive visualization of classifications with explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response represents a detailed JSON reflecting the user's advanced expertise in machine learning. It captures the specific requirements of a time-series classification project using wearable data, performance metrics, and interpretability. However, values for some metrics and algorithm details are left open as indicated.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As the lead project manager for our advanced urban mobility initiative, request a highly optimized YOLO-based object detection system trained on the extensive Cityscapes dataset. Prioritize multi-class recognition for vehicles, pedestrians, and crucial traffic signs, ensuring real-time performance (targeting 30 FPS) on diverse hardware platforms without compromising on accuracy, aiming for a benchmark of 85% or above in a challenging, multi-step computational scenario.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"urban mobility\",\n        \"description\": \"The task is to develop a highly optimized YOLO-based object detection system for the Cityscapes dataset. The focus is on multi-class recognition for vehicles, pedestrians, and traffic signs, with the aim to achieve real-time performance at 30 FPS across diverse hardware platforms without sacrificing accuracy, targeting a benchmark of 85% or higher in a challenging computational scenario.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.85\n            },\n            {\n                \"name\": \"inference speed\",\n                \"value\": 30.0,\n                \"unit\": \"frames per second\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": true,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes Dataset\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"vehicles\",\n                \"pedestrians\",\n                \"traffic signs\"\n            ],\n            \"specification\": {\n                \"data type\": \"RGB\",\n                \"image resolution\": [1024, 512],\n                \"label categories\": [\"multi-class\"]\n            },\n            \"description\": \"A dataset for training an optimized YOLO model focusing on vehicle, pedestrian, and traffic sign detection in urban environments.\",\n            \"preprocessing\": [\"data augmentation for real-time performance\"],\n            \"augmentation\": [\"random resized cropping\", \"color jitter\"],\n            \"visualization\": [\"model inference speed profiling\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO\",\n            \"family\": \"YOLO (You Only Look Once)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"type\": \"YOLOv3 or YOLOv4\",\n                    \"model size\": \"optimized for speed and accuracy\"\n                },\n                \"parameters\": {\n                    \"number of classes\": 3,\n                    \"optimizer\": \"suitable for real-time learning\",\n                    \"learning rate schedule\": \"adaptative\"\n                }\n            },\n            \"description\": \"An object detection model designed for efficient multi-class recognition on diverse hardware platforms\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"faster convergence techniques\",\n            \"optimization for specific hardware\"\n        ],\n        \"target_device\": \"diverse hardware platforms\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"CUDA, OpenCL, or TensorFlow for real-time performance\"],\n        \"demonstration\": \"interactive real-time object detection demo\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the essence of the project manager's request, focusing on a high-end YOLO-based object detection system. It accounts for multi-class recognition, real-time performance, and hardware optimization, along with the specified benchmark accuracy. The confidence score is high because the provided information closely aligns with the user's intent.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "For a high-impact fashion e-commerce platform, design an AI-driven image classification system capable of real-time tagging. The algorithm must not only identify style, color, and type but also discern unique sub-attributes (e.g., print, fabric texture), ensuring 95% precision and maintaining visual appeal, while coping with varying lighting and minimalistic backgrounds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion e-commerce\",\n        \"description\": \"The task is to develop an AI-driven image classification system for a high-impact fashion platform, targeting real-time tagging of style, color, type, and sub-attributes like print and fabric texture. The system must have 95% precision, maintain visual appeal, and handle diverse lighting and minimalistic backgrounds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion E-commerce Images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"style\", \"color\", \"type\", \"print\", \"fabric texture\"],\n            \"specification\": {\n                \"size\": \"variable, real-time compatible\",\n                \"diversity\": \"various lighting conditions, minimalistic backgrounds\"\n            },\n            \"description\": \"A collection of fashion images for real-time tagging, emphasizing style, color, and sub-attributes while accounting for the given requirements.\",\n            \"preprocessing\": [\"lighting normalization\", \"minimalistic background removal\"],\n            \"augmentation\": [\"real-time, diverse lighting conditions\", \"minimalistic background augmentation\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image Classification Model\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy under diverse conditions\": \"achieved\",\n                \"visual appeal consideration\": \"embedded\"\n            },\n            \"description\": \"AI-driven system for real-time image tagging in a high-impact fashion e-commerce platform, optimized for precision and adaptability to varying environments.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time inference\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized for real-time performance\"],\n        \"demonstration\": \"live product image analysis and tagging\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the instruction, focusing on a high-experience user intent to build a high-precision image classification system. Key details, such as the 95% precision target, handling of real-time tagging, and algorithm requirements, are included. The choice of CNN model, dataset, and performance-related information is based on the specified task requirements. The system's adaptability to diverse lighting and minimalistic backgrounds is addressed in the dataset and preprocessing steps.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a state-of-the-art RNN-based machine translation model utilizing the extensive OPUS corpus for translating colloquial English to formal Russian with near-native fluency. Focus on achieving a competitive BLEU score of 35, while demonstrating exceptional performance in deciphering regional slang and intricate idiomatic expressions. Additionally, mandate that the model attains real-time translation speed of 290 milliseconds per sentence, while guaranteeing minimal latency to ensure a seamless and interactive user experience across diverse platforms and devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics\",\n        \"description\": \"The task involves developing a state-of-the-art RNN-based machine translation model for translating colloquial English to formal Russian, targeting near-native fluency. Key requirements include achieving a competitive BLEU score of 35, handling regional slang and idiomatic expressions, and maintaining real-time translation speed of 290 milliseconds per sentence with low latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 35.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.29,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS Corpus (colloquial English and formal Russian)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"languages\": [\"English (colloquial)\", \"Russian (formal)\"]\n            },\n            \"description\": \"A comprehensive corpus for training, with a focus on colloquial English and formal Russian\",\n            \"preprocessing\": [\"tokenization\", \"sentence segmentation\"],\n            \"augmentation\": [\"backtranslation\"],\n            \"visualization\": [\"language distribution\", \"correlation analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-based Machine Translation Model\",\n            \"family\": \"Sequence-to-sequence models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Recurrent Neural Networks (e.g., LSTM or GRU)\",\n                \"focus\": \"Deciphering slang and idiomatic expressions\",\n                \"use cases\": \"Real-time, low-latency translation\"\n            },\n            \"description\": \"A model designed for translating colloquial English to formal Russian with high fluency and competitive BLEU score\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using pre-trained language models and finetuning to handle regional variations and idiomatic expressions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time platforms, diverse devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for speed\"],\n        \"demonstration\": \"Interactive mode, responsive to user input\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately represents the task's requirements, highlighting the RNN-based model, target BLEU score, focus on slang and idioms, and real-time speed. The dataset, performance metrics, and model specifications are derived directly from the user's instruction, and confidence is high due to the clear information provided.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "The website now experiences real-time, fine-grained anomalies in visitor flow from different geographical regions. The system must use advanced time-series analysis, seasonal adjustments, and machine learning algorithms to isolate patterns in mobile traffic spikes, weekends, and holidays, ensuring timely alerts for security breaches and meaningful user insights.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"web security and user analytics\",\n        \"description\": \"The user wants to build a system that detects fine-grained anomalies in real-time visitor flow, focusing on mobile traffic, weekends, and holidays. It should leverage advanced techniques for time-series analysis, seasonal adjustments, and machine learning to identify patterns for security breaches and user insights.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"timeliness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"visitor flow\"],\n            \"specification\": {\n                \"time_steps\": null,\n                \"granularity\": null,\n                \"variable_types\": [\"mobile\", \"weekend\", \"holiday\"]\n            },\n            \"description\": \"Data capturing real-time visitor flow, with a focus on distinguishing mobile traffic from weekends and holidays.\",\n            \"preprocessing\": [\n                \"seasonal adjustments\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"anomaly detection patterns\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"time-series analysis and machine learning models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_family\": [\"ARIMA\", \"STL decomposition\", \"LSTM\", \"Prophet\"],\n                \"features\": [\"mobile traffic\", \"weekend\", \"holiday\"]\n            },\n            \"description\": \"A combination of advanced models to analyze time-series data, seasonality, and detect anomalies\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"extraction of relevant time-series features\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time\"\n        ],\n        \"demonstration\": \"alerts for security breaches and user insights report\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction, indicating the need for a time-series analysis project with real-time anomaly detection in visitor flow. It specifies the application domain, requirements such as seasonal adjustments and machine learning algorithms, and the inclusion of relevant preprocessing steps. However, the exact dataset name, performance metrics values, and model specifications are left unspecified for flexibility based on the user's input.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design an advanced Transformer-based model for simultaneous multilingual conversion from dialectical American English to multiple formal languages, including Japanese, Arabic, and Chinese, preserving nuanced cultural nuances and slang adaptations. Target a remarkable TER quotient of 30, while showcasing superior accuracy in translating complex emoticons and metaphoric expressions from social media texts. Strive for a benchmark of 150 milliseconds end-to-end latency, ensuring low latency even on resource-constrained wearable devices, to facilitate real-time, immersive conversations in international virtual events.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual communication\",\n        \"description\": \"The task is to design a Transformer-based model for simultaneous conversion of dialectical American English to formal languages like Japanese, Arabic, and Chinese. It should preserve cultural nuances and slang adaptations while achieving a TER quotient of 30, high accuracy in translating complex emoticons and metaphors from social media texts, and a benchmark of 150 milliseconds end-to-end latency, targeting resource-constrained wearable devices for real-time, immersive conversations in international virtual events.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"TER quotient\",\n                \"value\": 30\n            },\n            {\n                \"name\": \"accuracy on complex emoticons and metaphors\",\n                \"value\": null\n            },\n            {\n                \"name\": \"end-to-end latency\",\n                \"value\": 150,\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency on resource-constrained devices\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Social Media Texts (various languages and dialects)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"sufficient for multilingual training\",\n                \"language varieties\": [\"American English\", \"Japanese\", \"Arabic\", \"Chinese\"],\n                \"content_types\": [\"social media texts\", \"dialectal to formal language conversion\"]\n            },\n            \"description\": \"A comprehensive dataset including dialectical American English and the target formal languages, containing complex emoticons and metaphors for model training.\",\n            \"preprocessing\": [\n                \"lemmatization\",\n                \"removal of non-essential features (e.g., URLs, emojis)\"\n            ],\n            \"augmentation\": [\n                \"synthetic dialect formation, cultural adaptation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Transformer for Multilingual Conversion\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-based\",\n                \"language handling\": \"simultaneous multilingual conversion\",\n                \"adaptations\": \"cultural nuances and slang\"\n            },\n            \"description\": \"A Transformer model tailored for dialect to formal language conversion, optimized for fast, low-latency performance on wearable devices.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Understanding of cultural nuances and slang adjustments for effective translation across different languages.\"\n        },\n        {\n            \"description\": \"Advanced techniques to preserve emoticons and metaphors meaning in the translated text.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adaptation for wearable devices\"],\n        \"target_device\": \"wearable\",\n        \"deployment_endpoint\": \"real-time translation services\",\n        \"inference_engine\": [\"optimized for latency\"],\n        \"demonstration\": \"immersive conversation in international events\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's instruction, capturing the complex task of designing a high-performance multilingual machine translation model with specific requirements on metrics, resource constraints, and application scenarios. Key elements, such as the Transformer-based architecture, language variations, and latency targets, have been included.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "image classification", "instruction": "For an upscale fashion e-commerce firm, develop a cutting-edge AI-powered image recognition system that demands advanced semantic analysis. The system must classify and differentiate not only fundamental attributes like designer, season, and hue but also intricate details like embroidery, stitching patterns, and sustainable materials. Mandate a stringent 97% accuracy, while preserving visual sophistication, and consider weather conditions and complex background compositions in real-time scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"The user requests an AI-powered image recognition system for fashion e-commerce, focusing on advanced semantic analysis to classify and differentiate attributes like designer, season, hue, embroidery, stitching patterns, and sustainable materials. The system should achieve a stringent accuracy of 97% while maintaining visual sophistication and handling real-time scenarios with weather conditions and complex backgrounds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"time\"\n            },\n            {\n                \"name\": \"visual sophistication preservation\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs or memory\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion E-commerce Image Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\n                \"designer\",\n                \"season\",\n                \"hue\",\n                \"embroidery\",\n                \"stitching patterns\",\n                \"sustainable materials\"\n            ],\n            \"specification\": {\n                \"size\": null,\n                \"dimensionality\": null,\n                \"complexity\": \"real-time with weather and background conditions\"\n            },\n            \"description\": \"A dataset for advanced fashion item recognition with various attributes and consideration for real-world challenges.\",\n            \"preprocessing\": [\n                \"image cleaning and augmentation\"\n            ],\n            \"augmentation\": [\n                \"weather and background variations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Semantic Image Recognition System\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Inception-like with emphasis on semantic features\"\n            },\n            \"description\": \"AI-powered system targeting advanced fashion analysis, including intricate details and real-time adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Semantic segmentation and transfer learning techniques will be crucial in handling intricate features and preserving visual sophistication.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"advanced feature extraction\"\n        ],\n        \"target_device\": \"cloud or GPU-enhanced edge\",\n        \"deployment_endpoint\": \"secured e-commerce platform API\",\n        \"inference_engine\": [\"GPU-friendly library like TensorFlow or PyTorch\"],\n        \"demonstration\": \"Live demonstrations on diverse product images\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a high-demand system, their high expertise level, and the specific requirements such as 97% accuracy and handling real-time scenarios. The dataset details are based on the given scenario, and the model and service expectations align with the advanced image recognition goal. Adjustments were made to balance semantic analysis, complex conditions, and accuracy.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As the project manager, mandate a cutting-edge YOLO-v5 object detection model development, which not only includes the Cityscapes dataset for enhanced multi-class recognition ( vehicles, pedestrians, and intricate traffic signs) but also integrates real-world occlusions and weather effects. Strive for peak performance with a minimum of 30 FPS in real-time, ensuring cross-platform compatibility, and optimizing for minimum latency while maintaining a stringent benchmark of 90% accuracy under extreme computational constraints, involving complex scene understanding in dense urban environments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"urban and autonomous driving\",\n        \"description\": \"The project aims to develop a state-of-the-art YOLO-v5 object detection model, focusing on multi-class recognition (vehicles, pedestrians, and traffic signs) with the Cityscapes dataset. The model must handle real-world occlusions and weather effects, target a minimum of 30 FPS in real-time, ensure cross-platform compatibility, and optimize for low latency while maintaining at least 90% accuracy in challenging urban scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.90\n            },\n            {\n                \"name\": \"fps\",\n                \"value\": 30.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vehicles\", \"pedestrians\", \"traffic signs\"],\n            \"specification\": {\n                \"enhancements\": [\"real-world occlusions\", \"weather effects\"]\n            },\n            \"description\": \"A dataset for multi-class object detection, focused on urban scenes with complex elements like occluded objects and varied weather conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO-v5\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"platform_compatibility\": true,\n                \"complexity_tradeoffs\": {\n                    \"accuracy_latency\": \"optimization for minimal latency\"\n                }\n            },\n            \"description\": \"A cutting-edge object detection model designed for real-time performance with advanced scene understanding in dense urban environments.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cross-platform\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"extreme computational constraints with complex urban environments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has captured the core requirements of the project, including the high-level intent to build a YOLO-v5 model, the detailed task specifics, performance metrics, and optimization for real-time scenarios with cross-platform compatibility. Complex scene understanding and the target accuracy benchmark are clearly stated. I am confident about the mapping given the structure and instruction provided.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "The project manager is now mandated to design a cutting-edge TSD system using a novel deep learning architecture, specifically a custom CNN with sophisticated real-time capabilities. They must focus on the intricate \"MoltenLithosphereSeismicSignal\" dataset, targeting not just ordinary anomalies, but pinpointing ultra-short latency tectonic disruptions. To enhance performance, they are instructed to incorporate advanced feature extraction methods like recursive autoencoders and multi-resolution wavelet analysis, ensuring robustness and efficiency in the detection process. Additionally, they must provide a detailed whitepaper outlining the novel approach and computational trade-offs.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"geophysics\",\n        \"description\": \"The project involves designing a TSD system using a custom deep learning architecture, specifically a CNN with real-time capabilities. The focus is on the 'MoltenLithosphereSeismicSignal' dataset, aiming to detect ultra-short latency tectonic disruptions and ordinary anomalies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"type\": \"real-time\",\n                    \"threshold\": \"ultra-short\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": {\n                    \"optimization_target\": \"real-time\"\n                },\n                \"unit\": \"samples per second\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MoltenLithosphereSeismicSignal\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"signal_type\": \"seismic\",\n                \"data_type\": \"sparse\",\n                \"dimensions\": {\n                    \"time\": {\n                        \"start\": \"\",\n                        \"end\": \"\",\n                        \"frequency\": \"\"\n                    }\n                }\n            },\n            \"description\": \"Dataset focusing on detecting ultra-short latency tectonic disruptions and ordinary anomalies using real-time seismic signals.\",\n            \"preprocessing\": [\n                \"recursive autoencoders\",\n                \"multi-resolution wavelet analysis\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom CNN\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"custom\",\n                \"layers\": [\n                    {\n                        \"type\": \"CNN\",\n                        \"depth\": null,\n                        \"kernel_size\": null,\n                        \"sophisticated\": true,\n                        \"real-time capabilities\": true\n                    },\n                    {\n                        \"type\": \"Recursive Autoencoder\",\n                        \"complexity\": null\n                    },\n                    {\n                        \"type\": \"Multi-resolution Wavelet Analysis\",\n                        \"performance_impact\": null\n                    }\n                ]\n            },\n            \"description\": \"A deep learning architecture specifically designed for detecting anomalies in real-time, leveraging recursive autoencoders and multi-resolution wavelet analysis.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"recursive autoencoder pre-processing\",\n            \"multi-resolution wavelet feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"description\": \"A detailed whitepaper outlining the novel approach, methodology, and computational trade-offs\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes all key elements from the instruction, specifying the design project, deep learning architecture, dataset, performance metrics, and requirements. The user's high expertise level is reflected, as well as the specific instructions for preprocessing and the need for a whitepaper. The confidence score is based on a thorough match with the given schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a seasoned project manager, your mandate now involves refining an innovative object recognition system for a niche market that demands precision in identifying and estimating yields of obscure fruits, like prickly pears, dragon fruits, and passion fruits, amidst their diverse shapes, low-resolution images, occlusions, and seasonal transformations. In addition to integrating state-of-the-art deep learning architectures, you should consider explainable AI for transparency, design a seamless mobile app catering to non-experts for instant fruit recognition and yield suggestions, ensuring scalability, energy efficiency, and real-time performance on resource-constrained devices. Elaborate on the extensive research, synergies between traditional and cutting-edge techniques, and the tech stack required for this groundbreaking project.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"agriculture/fruit production\",\n        \"description\": \"The project aims to develop an innovative object recognition system with precision for identifying and estimating yields of niche fruits like prickly pears, dragon fruits, and passion fruits, dealing with diverse shapes, low-resolution images, occlusions, and seasonal changes. The system must incorporate explainable AI, be built into a mobile app for non-experts, and demonstrate scalability, energy efficiency, and real-time performance on resource-constrained devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"estimation accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"ms per image\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"Joules per operation\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Niche Fruit Datasets (prickly pears, dragon fruits, passion fruits)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"fruit type\", \"yield estimation\"],\n            \"specification\": {\n                \"image_resolution\": \"low\",\n                \"occlusion level\": \"high\",\n                \"seasonal variations\": true\n            },\n            \"description\": \"A collection of diverse and low-resolution images of obscure fruits, including images with occlusions and different seasons.\",\n            \"preprocessing\": [\"data augmentation techniques for diverse shapes\"],\n            \"augmentation\": [\"Gaussian blur, random rotation, resizing\"],\n            \"visualization\": [\"Sample images with predicted labels and yield estimates\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art Deep Learning Architecture\",\n            \"family\": \"Convolutional Neural Networks (specifically Deep Neural Networks for object recognition)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": true\n            },\n            \"description\": \"A deep learning architecture designed for high-precision object recognition, including explainable AI aspects for transparency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Synergies between traditional image processing techniques and modern deep learning methods will be explored for optimizing performance.\",\n        \"Integration of object detection, feature extraction, and yield estimation algorithms\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Integration with mobile app, non-expert user interface design\"],\n        \"target_device\": \"resource-constrained mobile devices\",\n        \"deployment_endpoint\": \"Cloud-based for scalability\",\n        \"inference_engine\": [\"Mobile-specific inference frameworks like TensorFlow Lite\"],\n        \"demonstration\": \"Mobile app showcase with real-time performance and yield suggestions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided information was parsed into the JSON format while considering the user's high expertise level. The problem statement covers the objectives, challenges, target dataset, model selection, and service requirements as specified. The explanation AI and integration with mobile app are highlighted, and specific tech stack (e.g., TensorFlow Lite) is considered.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "text classification", "instruction": "As a cutting-edge project manager for a groundbreaking machine learning team, your mandate is to pioneer a deep learning model that surpasses existing benchmarks for the Multi-genre Scientific Textual Entailment (SciTe) dataset. This unique challenge demands the effective classification of 50,000 complex, domain-specific texts from condensed matter physics, astrobiology, and high-energy physics, with questions formulated to assess subtle nuances in research findings. The target model should achieve an ultra-low F1-score discrepancy (<0.2%) when using the ALBERT-XXL architecture, while prioritizing not only absolute accuracy but also handling imbalanced data and optimizing for Matthew's Correlation Coefficient. Document meticulous hyperparameter tuning results, transfer learning strategies, and the implementation of ensemble methods for unprecedented performance in the scientific literature domain.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"scientific literature\",\n        \"description\": \"The task involves developing a deep learning model,超越 existing benchmarks, for Multi-genre Scientific Textual Entailment (SciTe) dataset. The dataset consists of 50,000 complex texts from condensed matter physics, astrobiology, and high-energy physics, with a focus on nuanced research findings classification. The model should achieve an ultra-low F1-score discrepancy (<0.2%) using ALBERT-XXL, handle imbalanced data, and optimize for Matthew's Correlation Coefficient.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score discrepancy\",\n                \"value\": 0.002\n            },\n            {\n                \"name\": \"absolute accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Matthew's Correlation Coefficient\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multi-genre Scientific Textual Entailment (SciTe) Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"research findings\"],\n            \"specification\": {\n                \"number_of_samples\": 50000,\n                \"domains\": [\"condensed matter physics\", \"astrobiology\", \"high-energy physics\"]\n            },\n            \"description\": \"A collection of complex domain-specific texts with 50k samples for text classification, focused on subtle nuances in condensed matter, astrobiology, and high-energy physics research.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"domain-specific tokenization\"\n            ],\n            \"augmentation\": [\n                \"synonym replacement\"\n            ],\n            \"visualization\": [\n                \"dataset distribution analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ALBERT-XXL\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ALBERT\",\n                \"model_size\": \"XXL\",\n                \"parameters\": null,\n                \"inference_speed\": null,\n                \"F1_score_target\": \"<0.2%\"\n            },\n            \"description\": \"A deep learning model specifically tailored for the SciTe dataset, targeting ultra-low F1-score discrepancy with ALBERT-XXL architecture.\"\n        }\n    ],\n    \"knowledge\": [\n        \"transfer learning from existing NLP models in scientific literature\",\n        \"ensemble methods with BERT and RoBERTa for better performance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"text embeddings, feature extraction for imbalanced data\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"SciTe-Model-API\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"model performance visualization dashboard with case studies\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response captures the user's intent, requirements for the deep learning project, target dataset, performance metrics, and the constraints on the model. The user's high expertise level is acknowledged, and specific details are included based on the given instruction. Missing values are marked as null, indicating they would need to be filled based on the model development process.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project director for a specialized deep learning team, your mandate is to develop a state-of-the-art temporal sequence predictor for the Renewables Integration dataset, which encompasses intricate fluctuations in renewable energy consumption over time. This data, arranged chronologically into Training, Fine-Tuning, and Performance Evaluation sets, necessitates handling sequential inputs (past data points: Historical_obs=144 with detailed attributes: DEPT_FEATURES=256) with a focus on resilience to seasonality and trend shifts. The model must forecast a consecutive sequence of extended lengths (Future_pred=144 steps with equal dimensions: OUTPUT_FEATURES=512) while concurrently optimizing for multi-horizon accuracy, efficiency, and adaptability. The objective is to achieve the lowest possible mean squared error (MSE), mean absolute error (MAE), and symmetric mean absolute percentage error (SMAPE) across diverse forecasting windows, demanding an exceptional predictive analytics solution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy\",\n        \"description\": \"The project aims to develop a state-of-the-art temporal sequence predictor for the Renewables Integration dataset, focusing on renewable energy consumption over time with a 144-step historical context (Historical_obs=144, DEPT_FEATURES=256) and 144-step forecast (Future_pred=144, OUTPUT_FEATURES=512). The model should be resilient to seasonality and trend shifts, optimize for multi-horizon accuracy, efficiency, and adaptability, and achieve the lowest possible MSE, MAE, and SMAPE across diverse forecasting windows.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"symmetric mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Renewables Integration Dataset\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"Training\": {\n                    \"sequence_length\": 144,\n                    \"attributes\": {\n                        \"Historical_obs\": 144,\n                        \"DEPT_FEATURES\": 256\n                    }\n                },\n                \"Fine-Tuning\": {\n                    \"sequence_length\": 144,\n                    \"attributes\": {\n                        \"Historical_obs\": 144,\n                        \"DEPT_FEATURES\": 256\n                    }\n                },\n                \"Performance Evaluation\": {\n                    \"sequence_length\": 144,\n                    \"attributes\": {\n                        \"Future_pred\": 144,\n                        \"OUTPUT_FEATURES\": 512\n                    }\n                }\n            },\n            \"description\": \"Dataset containing time-ordered data on renewable energy consumption with a focus on seasonality and trend shifts, divided into Training, Fine-Tuning, and Performance Evaluation sets.\",\n            \"preprocessing\": [\n                \"handle seasonality\",\n                \"extract trend shifts\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"temporal sequence predictor\",\n            \"family\": \"recurrent neural networks (RNN) or transformer architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"sequence handling\": {\n                    \"input_window_size\": 144,\n                    \"output_window_size\": 144,\n                    \"hidden_units\": {\n                        \"past\": 256,\n                        \"future\": 512\n                    },\n                    \"sequence_resilience\": \"seasonality and trend shifts\"\n                },\n                \"optimization\": {\n                    \"accuracy\": \"multi-horizon\",\n                    \"efficiency\": \"yes\",\n                    \"adaptability\": \"yes\"\n                }\n            },\n            \"description\": \"A state-of-the-art model for time-series forecasting, designed to handle the Renewables Integration dataset requirements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"lag features\",\n            \"ensemble techniques\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"demonstrations with diverse forecasting windows\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure aligns with the provided instruction, capturing the project's goal to develop a temporal sequence predictor, the target dataset, model requirements, and performance metrics. Complexities such as seasonality, trend shifts, and forecasting windows are taken into account. Expertise level is set to high as per the project director role.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As part of an innovative interdisciplinary collaboration, our interdisciplinary machine learning team is tasked with creating a quantum-enhanced neural machine translation model. Utilizing the niche Quantum-Extended WMT23 corpus, we aim to develop a system capable of translating scientific jargon between cutting-edge research fields like quantum physics and high-energy astrophysics. The model should achieve a state-of-the-art METEOR score of 80% while maintaining seamless domain adaptation in the quantum computing lexicon. Moreover, we require the implementation of a novel error mitigation strategy, and the submission must undergo rigorous quantum-human evaluation to ensure preservation of meaning and context in the translations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"interdisciplinary research\",\n        \"description\": \"The project aims to develop a quantum-enhanced neural machine translation model using the Quantum-Extended WMT23 corpus, focusing on translating scientific jargon between quantum physics and high-energy astrophysics. The goal is to achieve an 80% METEOR score and implement a novel error mitigation strategy with quantum-human evaluation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"METEOR\",\n                \"value\": 0.8\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Quantum-Extended WMT23 Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": null,\n            \"description\": \"A specialized corpus for quantum-aided translation, containing scientific jargon from quantum physics and high-energy astrophysics.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-enhanced Neural Machine Translation Model\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"error_mitigation_strategy\": \"novel\"\n            },\n            \"description\": \"A state-of-the-art model designed for machine translation with an emphasis on domain adaptation for quantum computing lexicon.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interdisciplinary collaboration between quantum and AI experts, leveraging quantum computing principles for improved translation performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Rigorous quantum-human evaluation required for meaning and context preservation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the instruction, focusing on creating a high-expertise model for quantum-enhanced machine translation with specific performance goals and requirements, such as the error mitigation strategy and the quantum-human evaluation. The relevant fields like corpus, model architecture, and evaluation methods have been mapped according to the JSON schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "The client's novel initiative demands an intricate, real-time canine breed recognition system with sub-millisecond precision. Utilize the extensive Stanford Dogs Dataset, examining not just CNNs but also fine-tuning sophisticated pre-architectures like DenseNet and Inception-v3 for enhanced accuracy. Analyze the impact of multi-task learning, model pruning, and quantization on resource utilization while maintaining a buttery-smooth user interface and ultra-fast response times under diverse lighting conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal science\",\n        \"description\": \"The client's requirement is for a real-time canine breed recognition system with sub-millisecond precision. This involves using the Stanford Dogs Dataset, exploring advanced techniques like DenseNet and Inception-v3 fine-tuning, multi-task learning, model pruning, and quantization for improved accuracy. The system must have a seamless user interface and ultra-fast response times even under varying lighting conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"speed\",\n                \"value\": {\"unit\": \"milliseconds\", \"value\": \"sub-1\"}\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource utilization\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Dogs Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"canine breed\"],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"diversity\": \"lighting conditions\"\n            },\n            \"description\": \"An extensive dataset for real-time canine breed recognition, including images captured under diverse lighting conditions.\",\n            \"preprocessing\": [\"multi-task learning, fine-tuning, and data augmentation\"],\n            \"augmentation\": [\"data augmentation for diverse lighting conditions\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DenseNet and Inception-v3\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"enhancements\": [\n                    \"fine-tuning\",\n                    \"multi-task learning\",\n                    \"model pruning\",\n                    \"quantization\"\n                ],\n                \"target accuracy\": \"enhanced accuracy\"\n            },\n            \"description\": \"Advanced pre-architectures for improved performance in real-time canine breed recognition\"\n        }\n    ],\n    \"knowledge\": [\n        \"Multi-task learning's impact on resource allocation, model efficiency, and accuracy trade-offs.\",\n        \"Pruning and quantization strategies to manage resource utilization while maintaining performance.\",\n        \"Strategies for a buttery-smooth user interface and ultra-fast response times across diverse lighting conditions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"fine-tuning\",\n            \"multi-task learning techniques\"\n        ],\n        \"target_device\": \"real-time, requires sub-millisecond precision\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"focus on ultra-fast response times and user interface under varying lighting\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the client's specific requirements, with a high emphasis on accuracy, resource utilization, and real-time performance. It reflects the various techniques mentioned (e.g., DenseNet, Inception-v3, and multi-task learning) and incorporates the need for an optimal user interface and response times. The context of the problem and dataset is clearly described.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a top-tier retail intelligence system, your task is to refine a cutting-edge Mask R-CNN model, utilizing the extensive 'Extended Product Metadata-500K' dataset. The challenge demands not only ultra-precise product detection and subcategory discrimination, but also the model must demonstrate robustness in dealing with complex scenes, such as partial views, multiple products in one frame, and dynamic lighting variations. Prepare a meticulous report that includes intricate per-product group performance metrics (IoU, mAP, and CMC curves), extensive analysis of context-aware errors, and a strategy to optimize false alarms and missing items, crucial for seamless inventory management in hyper-efficient retail environments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"The user requires refining a Mask R-CNN model for a top-tier retail intelligence system. It involves ultra-precise product detection and subcategory discrimination with robustness in complex scenes and optimized false alarms and missing items for efficient inventory management.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Intersection over Union (IoU)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean Average Precision (mAP)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Cumulative Match Characteristic (CMC) curves\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Robustness in complex scenes\",\n                \"value\": null,\n                \"unit\": \"none\"\n            },\n            {\n                \"name\": \"False alarm optimization\",\n                \"value\": null,\n                \"unit\": \"percentage reduction\"\n            },\n            {\n                \"name\": \"Missing items detection improvement\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extended Product Metadata-500K\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product, subcategory\"],\n            \"specification\": null,\n            \"description\": \"A large-scale dataset for refining a Mask R-CNN model, focusing on extensive product detection in complex retail scenarios.\",\n            \"preprocessing\": [\n                \"Contextual data augmentation\",\n                \"Scene complexity handling\"\n            ],\n            \"augmentation\": [\n                \"Partial view generation\",\n                \"Dynamic lighting simulation\"\n            ],\n            \"visualization\": [\n                \"Error analysis, CMC curves\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Mask R-CNN\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"Scene robustness\": \"Yes\",\n                \"Inference time\": \"optimized\",\n                \"Training speed\": \"state-of-the-art\"\n            },\n            \"description\": \"An advanced object detection model to be fine-tuned for increased accuracy and robustness in retail inventory management.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Context-aware errors in multi-object scenes\",\n        \"Fine-tuning strategies to handle false positives and negatives\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Context-dependent feature extraction\",\n            \"Multi-object group recognition\"\n        ],\n        \"target_device\": \"high-performance server\",\n        \"deployment_endpoint\": \"cloud-based\",\n        \"inference_engine\": [\n            \"TensorRT for faster inference\",\n            \"Deep Learning as a Service (DLaaS)\"\n        ],\n        \"demonstration\": \"Error analysis and performance demonstration during in-store inventory checks\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the comprehensive nature of the task, capturing the requirement for refining a Mask R-CNN model with high performance metrics, context-aware error analysis, and optimization strategies. The model's robustness in handling complex retail scenarios is emphasized, and specific techniques for enhancing false alarms and missing item detection are included.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "Your task as the lead machine learning developer is to design an advanced image classification framework utilizing state-of-the-art EfficientNet architecture for the highly-granular HAM10000 skin lesion dataset. Target a demanding accuracy of at least 99.5%, with a particular emphasis on both precision and recall to minimize false positives and negatives. Implement explainable AI techniques, such as SHAP or LIME, for transparent decision-making, as expert dermatologists will scrutinize each prediction with a requirement for step-by-step feature influence explanations. Concurrently, optimize the model's speed for low-resource smartphones while preserving peak performance, considering constraints like real-time latency and energy efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medical\",\n        \"description\": \"The goal is to design an advanced image classification framework using EfficientNet architecture on the HAM10000 skin lesion dataset. The focus is on achieving at least 99.5% accuracy, with strong emphasis on precision and recall, and reducing false positives and negatives. Explainable AI techniques like SHAP or LIME are mandatory for feature influence explanations to meet expert dermatologist scrutiny.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.995\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"Joules per inference\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HAM10000 skin lesion dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"skin lesion type\"],\n            \"specification\": null,\n            \"description\": \"A highly-granular dataset for image classification, focusing on skin lesions. The model should be optimized for low-resource smartphones with real-time latency and energy efficiency.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"optimized for low-resource devices\",\n                \"accuracy_tradeoff\": \"preserving peak performance\",\n                \"flops\": null\n            },\n            \"description\": \"A state-of-the-art model architecture for image classification with emphasis on efficiency and accuracy for smartphone deployment\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"explainable AI\",\n            \"explanation_techniques\": [\"SHAP\", \"LIME\"],\n            \"expert_review\": \"Expert dermatologists require step-by-step feature influence explanations for each prediction.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"low-resource smartphones\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time inference\"],\n        \"demonstration\": \"Implementing a demonstrable prototype with feature influence explanations for each prediction\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the instruction, focusing on the development of a highly-accurate and efficient image classification framework using EfficientNet on the HAM10000 dataset. Performance metrics, complexity constraints, and explainability requirements are clearly specified. Some fields, such as model-specific specifications and the deployment endpoint, are left open for flexibility.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project manager for a forward-thinking translation division, mandate a cutting-edge neural machine translation system incorporating advanced Transformer-XL architecture with a tailored pre-training on the proprietary Multi-Modal EuroTechX dataset. Prioritize not just accuracy but also energy efficiency, targeting human-parity translation speed of below 150 milliseconds for real-time, bandwidth-constrained video conferencing in diverse global markets, while ensuring seamless integration with a wide range of legacy and IoT devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"translation services\",\n        \"description\": \"The project requires a neural machine translation system using the Transformer-XL architecture with a custom pre-training on the Multi-Modal EuroTechX proprietary dataset. The focus is on both high accuracy and energy efficiency, aiming for a human-parity translation speed of below 150 milliseconds for real-time video conferencing in global markets, while ensuring compatibility with various legacy and IoT devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"speed (human parity)\",\n                \"value\": 0.15\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multi-Modal EuroTechX\",\n            \"modality\": [\"text\", \"multimodal\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"custom_pre_training\": true\n            },\n            \"description\": \"A proprietary dataset for transformer pre-training, incorporating multi-modal data for enhanced machine translation.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL\",\n            \"family\": \"Transformer-based Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-XL\",\n                \"pretraining_details\": \"custom\"\n            },\n            \"description\": \"Advanced neural machine translation system using the Transformer-XL architecture, with focus on performance and energy efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transformer-XL is known for its ability to handle long-range dependencies in sequences, suitable for machine translation tasks.\",\n        \"Energy-efficient models can be achieved through architectural optimization, pruning, and hardware-aware training.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"energy-efficient design\",\n            \"real-time optimization\"\n        ],\n        \"target_device\": [\"IoT devices\", \"legacy systems\"],\n        \"deployment_endpoint\": \"cross-platform compatible\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"real-time translation during video conferencing\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a high-end machine translation system, utilizing the Transformer-XL architecture and the Multi-Modal EuroTechX dataset. The project goals, such as accuracy, energy efficiency, and integration requirements, are clearly stated. The performance metrics and complexity constraints are relevant, and possible knowledge points and final service steps are included.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "image classification", "instruction": "Instruct the machine learning team to design an innovative, scalable image classifier for esoteric astronomical objects, necessitating the integration of specialized CNNs with transfer learning from the proprietary Celestial Feature Repository (CFR). Emphasize the requirement for robust handling of photometric anomalies, asteroid taxonomy differentiation, and sub-1% error rates, ensuring adaptability to unforeseen lighting conditions. Specify the multi-stage data preprocessing, including noise reduction algorithms, and propose a diverse set of performance metrics to assess precision, recall, and nuanced inter-class separability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The task is to design an image classifier for esoteric astronomical objects, utilizing specialized CNNs and transfer learning from the Celestial Feature Repository (CFR). Key requirements include robustness to photometric anomalies, accurate asteroid taxonomy differentiation, and sub-1% error rates, with adaptability to varying lighting conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"error rate\",\n                \"value\": 0.01\n            },\n            {\n                \"name\": \"inter-class separability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celestial Feature Repository (CFR)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"astronomical object class\"],\n            \"specification\": {\n                \"proprietary\": true,\n                \"contains_photometric_anomalies\": true\n            },\n            \"description\": \"A proprietary dataset with extensive astronomical images for transfer learning, incorporating photometric anomalies and celestial object features.\",\n            \"preprocessing\": [\n                \"noise reduction algorithms\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Innovative Image Classifier\",\n            \"family\": \"Specialized CNNs with Transfer Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"use_of_transfer_learning\": true,\n                \"lighting_adaptability\": true\n            },\n            \"description\": \"A custom-designed classifier leveraging CNNs from the Celestial Feature Repository, targeted for esoteric astronomical objects, with emphasis on low error rates and robustness to anomalies.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Handling photometric anomalies and asteroid taxonomy differentiation are critical for performance.\",\n        \"Adaptability to unforeseen lighting conditions is an essential requirement.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Multi-stage processing, including preprocessing with noise reduction and assessment of precision, recall, and inter-class separability.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the provided instruction, detailing a high-expertise request for a machine learning project to classify esoteric astronomical objects with the specified requirements and performance metrics. The proposed solution incorporates transfer learning, robust handling of anomalies, and custom model architecture.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, demand a state-of-the-art, low-latency Amazon QA chatbot model with at least 95% precision, ensuring real-time responses under 150 milliseconds. Mandate a fault-tolerant design and horizontal scalability to accommodate 10x peak volume within two years, while also integrating advanced natural language processing and context-awareness features.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"customer service\",\n        \"description\": \"The project aims to develop a state-of-the-art Amazon QA chatbot with a focus on low-latency, real-time performance, and advanced NLP features. It should have precision above 95% and respond within 150 milliseconds. Additionally, the model must be fault-tolerant and horizontally scalable to handle peak volumes 10x higher than current within two years.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.15,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"horizontal scalability\",\n                \"value\": \"10x peak volume increase\",\n                \"unit\": \"users or requests per time unit\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Amazon QA data\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\n                \"answers\"\n            ],\n            \"specification\": {\n                \"data distribution\": \"balanced\",\n                \"training_data_size\": \"enough to achieve 95% precision\"\n            },\n            \"description\": \"An extensive dataset for training the QA chatbot, emphasizing advanced NLP and context-awareness.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"tokenization\",\n                \"contextual embedding\"\n            ],\n            \"augmentation\": [\n                \"synonym replacement\",\n                \"back-translation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Amazon SOTA QA Model\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"low-latency, fault-tolerant\",\n                \"factors affecting performance\": \"NLP, context-awareness, and efficiency\"\n            },\n            \"description\": \"A state-of-the-art, real-time QA chatbot with advanced NLP features\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context aggregation\",\n            \"multi-turn interaction modeling\"\n        ],\n        \"target_device\": \"Amazon Web Services (AWS)\",\n        \"deployment_endpoint\": \"AWS Lambda or AWS SageMaker\",\n        \"inference_engine\": [\"Amazon Lex\", \"Amazon comprehend\"],\n        \"demonstration\": \"contextual and interactive\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's demands for the chatbot project. It covers the intent to build a high-expertise level model, includes the area of NLP and downstream task of QA, specifies the performance metric of precision (95%), and addresses the complex requirements like low-latency and scalability. The dataset is described as requiring a balanced distribution to reach high precision, and the model has appropriate details. Feature engineering and integration with AWS services are also included.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "tabular regression", "instruction": "In a unique challenge for the Cutting-edge Ad Spend Optimization Project, your role as the project manager calls for the machine learning team to design a state-of-the-art tabular regression model. The model must predict precise ad campaign returns for a global client base, accounting for cultural nuances, geo-specific trends, and real-time market fluctuations. Implement a blend of deep learning, time-series analysis, and sparse matrix algorithms on a colossal dataset with missing values, ensuring interpretability and providing actionable insights for hyper-local targeting. The end goal is to achieve a groundbreaking benchmark on the RMSLE score while unveiling novel, unseen patterns within the intricate data landscape.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"The project involves designing a state-of-the-art tabular regression model for ad campaign return prediction. The model should account for cultural nuances, geo-specific trends, and real-time market fluctuations for a global client base. It requires deep learning, time-series analysis, and sparse matrix algorithms, working with a large dataset containing missing values. The goal is to achieve an exceptional RMSLE score and uncover novel patterns in the data for hyper-local targeting.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSLE score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cutting-edge Ad Spend Optimization Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"ad campaign returns\"],\n            \"specification\": {\n                \"size\": null,\n                \"sparse_elements\": true,\n                \"missing_values_handling\": true\n            },\n            \"description\": \"A large dataset with real-time adjustments, cultural nuances, geo-specific trends, and missing values.\",\n            \"preprocessing\": [\"data cleaning\", \"feature engineering (sparse matrix handling)\"],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance and patterns\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep learning with time-series and sparse matrix models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"type_of_models\": [\"deep neural networks\", \"time-series forecasting models\"],\n                \"model_flexibility\": \"hybrid\"\n            },\n            \"description\": \"A cutting-edge model combining deep learning techniques, time-series analysis, and sparse matrix algorithms for high-precision ad campaign return prediction.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"data imputation techniques\", \"feature interaction encoding\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"AI platform or cloud-based service\",\n        \"inference_engine\": [\"efficient serving for real-time performance\"],\n        \"demonstration\": \"visualization of model predictions and actionable insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the requirements of a complex project, including the expert level of the user, the blend of modeling techniques, and the specific dataset characteristics. Performance metrics, interpretability, and scalability goals have been included. Some specifications, such as model details and exact numbers, are left open for the project manager's discretion.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "You are now tackling a high-dimensional time-series forecasting challenge with the PhysioNet Challenge 2021 dataset, which includes intricate, multimodal patient data. The train, validation, and test splits consist of diverse time-series sequences (INPUT_SEQ_LEN = 500, with INPUT_DIM = 200 features representing complex physiological and demographic factors). Your objective is to design an advanced deep learning architecture that can not only classify labels into either classes (0 or 1) but also exhibit exceptional temporal and inter-feature correlations. The model should demonstrate state-of-the-art performance, and you must optimize for both precision and recall while using an ensemble of evaluation metrics, including Area Under the Receiver Operating Characteristic Curve (AUC-ROC) and F1 Score, in addition to the standard accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The task involves forecasting and classification using complex patient data from the PhysioNet Challenge 2021 dataset. The model must handle high-dimensional time series (INPUT_SEQ_LEN = 500, INPUT_DIM = 200) and capture both temporal and inter-feature correlations. The objective is to design an advanced deep learning architecture achieving state-of-the-art performance with emphasis on precision, recall, AUC-ROC, and F1 Score.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PhysioNet Challenge 2021\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"labels\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 500,\n                \"INPUT_DIM\": 200,\n                \"multimodal\": true,\n                \"description\": \"Diverse time-series sequences representing physiological and demographic factors\"\n            },\n            \"description\": \"A high-dimensional dataset containing intricate, multimodal patient data for the forecasting challenge\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"temporal and feature-wise augmentation\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced deep learning architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"Advanced model with focus on temporal correlations, inter-feature interactions, and state-of-the-art performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explain how to demonstrate model performance using time-series sequences and evaluation metrics.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's intent to build a complex machine learning model, given their high expertise level. It reflects the details of the high-dimensional time-series forecasting challenge, target metrics, and the specific requirements for the deep learning architecture. The rationale and confidence score reflect the thoroughness of information presented.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "As a project manager for the machine learning development team, your task is to design a state-of-the-art multimodal forgery detection system. The system must not only analyze visual and text-based cues but also incorporate audio signatures to validate academic records and certificates. Ensure real-time integration with the institution's existing database while maintaining a high accuracy rate and user-friendly interface.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"education\",\n        \"description\": \"The project aims to develop a state-of-the-art system for detecting multimodal forgeries in academic records and certificates, leveraging visual, text, and audio cues. It requires real-time integration with an institution's database while maintaining high accuracy and a user-friendly interface.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"system complexity\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Academic Records and Certificate Dataset\",\n            \"modality\": [\"image\", \"text\", \"audio\"],\n            \"target_variables\": [\"forged\", \"real\"],\n            \"specification\": null,\n            \"description\": \"A multimodal dataset containing visual, text-based, and audio data for assessing authenticity.\",\n            \"preprocessing\": [\"multimodal data preprocessing\"],\n            \"augmentation\": [\"real-time data augmentation\"],\n            \"visualization\": [\"system performance visualizations\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multimodal Forgery Detection Model\",\n            \"family\": \"Convolutional Neural Networks and Recurrent Neural Networks (for video/audio) or Transformers (for text)\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A state-of-the-art model that combines visual, text, and audio features for detecting forged academic records and certificates.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model should be designed to adapt to real-world variations and include robustness against forgery techniques.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"audio feature extraction\",\n            \"cross-modal fusion\"\n        ],\n        \"target_device\": \"real-time integration with institution's database\",\n        \"deployment_endpoint\": \"institution's authentication system\",\n        \"inference_engine\": [\"optimized for real-time processing\"],\n        \"demonstration\": \"user-friendly dashboard for displaying results and integration status\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a high-quality multimodal forgery detection system. The areas of focus, such as real-time integration, high accuracy, and user-friendly interface, are reflected in the problem description and service components. The metrics and dataset requirements are set up to meet the state-of-the-art expectations.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "visual question answering", "instruction": "As a project manager for a cutting-edge visual question answering system, mandate the team to develop a highly specialized model utilizing the TextVQA dataset. The model must exhibit exceptional accuracy, not only in recognizing text from complex images but also in differentiating between subtle nuances and context, ensuring seamless integration with real-world scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"artificial intelligence, computer vision, or natural language processing\",\n        \"description\": \"The project aims to develop a specialized visual question answering model for complex images, with a focus on high accuracy, text recognition, and nuanced understanding of context for seamless real-world integration.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TextVQA dataset\",\n            \"modality\": [\n                \"text\",\n                \"image\"\n            ],\n            \"target_variables\": [\"question, image, and potential answers\"],\n            \"specification\": null,\n            \"description\": \"A dataset for developing a visual question answering system, specifically targeting complex images with nuanced context.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Neural Network (potentially with transformer architecture)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"exceptional\"\n            },\n            \"description\": \"A highly specialized model for visual question answering, designed to excel in text recognition from complex images and nuanced understanding.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"seamless integration with real-world scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON format is adjusted to encompass the essence of the instruction, focusing on building a high-accuracy visual question answering model with complex image recognition and context differentiation. The user's high expertise level is acknowledged, and the TextVQA dataset is specified. The model description reflects the goal of exceptional accuracy and real-world integration.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "The team is now embarking on an advanced image recognition project with the Stanford Cars Dataset, focusing on low-light, extreme angle, and complex background scenarios. In addition to implementing the robust Inception-v3 architecture, we need to design an algorithm that optimizes fine-grained discrimination between car models, makes, and vintage years. Performance must excel, with a requirement for precision and recall breakdown by each subclass, aiming for an overall accuracy of 98% and demonstrating superior resilience to visual variability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"automotive\",\n        \"description\": \"The team aims to create an advanced image recognition system using the Stanford Cars Dataset for low-light, extreme angle, and complex background scenarios. The focus is on fine-grained discrimination among car models, makes, and vintage years with the Inception-v3 architecture. The performance must excel, with a target of precision and recall by subclass, aiming for a 98% overall accuracy and demonstrating superior resilience to visual variability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"make\", \"model\", \"year\"],\n            \"specification\": null,\n            \"description\": \"A dataset for advanced image recognition in challenging scenarios, including low-light, extreme angles, and complex backgrounds, focusing on car model, make, and vintage year classification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"low-light simulation\",\n                \"extreme angle manipulation\",\n                \"background complexity enhancement\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Inception-v3\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"resilience\": \"fine-grained discrimination\",\n                \"architecture\": \"robustness against visual variability\"\n            },\n            \"description\": \"A neural network architecture for image classification, specifically optimized for low-light, extreme angle, and complex background scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adversarial training to improve model robustness against visual variability\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"fine-tuning for low-light, angle, and background\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Performance demonstration with fine-grained subclass analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response accurately represents the provided instruction, reflecting the advanced image recognition project with a high focus on performance and resilience to visual variability. The Inception-v3 architecture is used, and the project's specific requirements are incorporated, such as fine-grained discrimination and performance metrics. The user's high expertise level is also captured.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a machine learning project for a prestigious film festival, you must develop a real-time, sequential recommendation system using the 'Bilibili Danmu' dataset. The model, optimized for efficiency on large text volumes, not only predicts video sequences but also analyses sentiment in real-time, catering to viewers' moods, and ensuring minimal latency for seamless viewing experiences.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"film and entertainment\",\n        \"description\": \"The task involves creating a real-time, sequential recommendation system for a film festival using the Bilibili Danmu dataset. The model should handle large text volumes efficiently, predict video sequences, analyze sentiment in real-time, and ensure minimal latency for optimal viewing experiences.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"efficiency on large text volumes\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": null\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Bilibili Danmu\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\n                \"video sequences\"\n            ],\n            \"specification\": {\n                \"size\": {\n                    \"total_sequences\": null,\n                    \"average_length\": null\n                },\n                \"text_volume\": null\n            },\n            \"description\": \"A dataset for building a real-time recommendation system, focusing on large text data from Bilibili Danmu to predict video sequences.\",\n            \"preprocessing\": [\n                \"text tokenization\",\n                \"sequence cleaning\"\n            ],\n            \"augmentation\": [\n                \"synthetic data generation\"\n            ],\n            \"visualization\": [\n                \"data distribution\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Sequential Recommendation Model with Sentiment Analysis\",\n            \"family\": \"Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"inference_speed\": null,\n                \"memory_consumption\": null\n            },\n            \"description\": \"A real-time, optimized model for sequential recommendation that analyzes sentiment and caters to viewers' moods.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Emphasis on efficiency for high text volume and low latency is crucial for the film festival context.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"custom word embeddings for sentiment analysis\",\n            \"time-aware techniques\"\n        ],\n        \"target_device\": \"cloud or edge devices to minimize latency\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"real-time inferencing\"],\n        \"demonstration\": \"Live streaming event demonstrations and user feedback incorporated\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON accurately reflects the user's requirements for a machine learning project. The expertise level is set to 'high' for a prestigious film festival scenario. The problem description and metrics are in line with the task, including performance on large text volumes and real-time sentiment analysis. The dataset and model requirements are specified accordingly. The target device and deployment strategy are chosen to cater to the low latency and scalability requirements. The confidence score takes into account the level of detail and relevancy of the provided information.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As the project manager, you task the machine learning team with the development of a sophisticated image classifier, tailored for the arduous Galaxy Zoo dataset. Emphasize on employing a multi-layered Vision Transformer (ViT) architecture, capable of handling the intricate details and vast receptive fields in astronomical images. The system must demonstrate exceptional accuracy, transfer learning prowess, and adaptability to novel celestial objects, ensuring its substantial impact on advancing astronomical research.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The task involves developing a sophisticated image classifier for the Galaxy Zoo dataset using a multi-layered Vision Transformer (ViT) architecture. The model should handle intricate details and large receptive fields in astronomical images, demonstrating high accuracy, strong transfer learning capability, and adaptability to novel celestial objects.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"astronomical object category\"],\n            \"specification\": {\n                \"image_size\": 224,\n                \"image_channels\": 3,\n                \"data_type\": \"astronomical\",\n                \"object_domain\": \"novel celestial objects\"\n            },\n            \"description\": \"A dataset of astronomical images requiring a sophisticated classifier capable of handling intricate details and large receptive fields.\",\n            \"preprocessing\": [\"data augmentation for astronomical images\"],\n            \"augmentation\": [\"random cropping, flip, rotation\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Vision Transformer (ViT)\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"number_of_layers\": 12,\n                \"attention_heads\": 12,\n                \"patch_size\": 16,\n                \"transfer_learning\": true,\n                \"adaptability\": \"novel celestial objects\"\n            },\n            \"description\": \"A multi-layered Vision Transformer designed for image classification in the context of advanced astronomical image analysis.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"custom feature extraction for astronomical images\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-based for speed\"],\n        \"demonstration\": \"A proof-of-concept prediction of novel celestial objects in the Galaxy Zoo dataset.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the project manager's task to develop a high-end image classifier using a Vision Transformer on the Galaxy Zoo dataset. The system's key characteristics, like accuracy, transfer learning, and adaptability to novel objects, are reflected in the performance metrics and model specifications. The preprocessing, augmentation, and source for the dataset are tailored to the astronomical domain.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Utilize the specialized SubtitleParler dataset, specifically filtered for movie and TV show conversations, to develop a BERT-MT5 fusion model for translating colloquial English into formal Mandarin. Mandate the model to decode regional dialects andnonce words with a minimum F1 score of 95% to ensure seamless dialogue translation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"The task is to build a BERT-MT5 fusion model for translating colloquial English into formal Mandarin, specifically using the SubtitleParler dataset focused on movie and TV show conversations. The model should handle regional dialects and nonce words with a minimum F1 score of 95%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SubtitleParler (filtered for movie and TV show conversations)\",\n            \"modality\": [\"text\", \"multimodal\"],\n            \"target_variables\": [\"colloquial English\", \"formal Mandarin\"],\n            \"specification\": {\n                \"domains\": [\"movie\", \"TV show\"],\n                \"dialects\": \" Regional dialects and nonce words\"\n            },\n            \"description\": \"A dataset containing informal English subtitles for movies and TV shows, filtered for conversational content.\",\n            \"preprocessing\": [\"BERT tokenization\", \"MT5 encoding\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-MT5 fusion model\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT-MT5 fusion\",\n                \"handling\": \" Regional dialects and nonce words\"\n            },\n            \"description\": \"A model designed to translate informal English into formal Mandarin, targeting seamless dialogue translation with a focus on regional dialects and specialized vocabulary.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Seamless translation of colloquial English dialogue into formal Mandarin, maintaining regional dialects and nonce words accuracy.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been parsed according to the provided instruction, detailing the area of focus (NLP), task (MT), the dataset (SubtitleParler), the model requirements (BERT-MT5 fusion), performance metric (95% F1 score), and the model's ability to handle regional dialects and nonce words. The missing target_columns in the dataset is assumed to be handled during preprocessing steps or data extraction from the source.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a seasoned project manager for a multi-disciplinary AI team, you've been tasked with devising a sophisticated plant species recognition system. Using the 'Plant Seedlings Dataset', propose a deep learning pipeline incorporating transfer learning, feature extraction, and ensemble techniques. Estimate accuracy with a challenging 98% benchmark, ensuring real-time performance on low-powered devices, while discussing the trade-offs between complexity and efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"botany or agriculture\",\n        \"description\": \"The task is to design a plant species recognition system using the 'Plant Seedlings Dataset'. It requires a deep learning pipeline with transfer learning, feature extraction, and ensemble techniques. The accuracy target is set at 98%, ensuring real-time performance on low-powered devices while balancing complexity and efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time on low-powered devices\",\n                \"value\": \"real-time\",\n                \"unit\": \"time\"\n            },\n            {\n                \"name\": \"model complexity-efficiency trade-off\",\n                \"value\": \"discussed in the report\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Seedlings Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"plant species\"],\n            \"specification\": null,\n            \"description\": \"A dataset for plant species recognition, necessitating transfer learning and feature extraction.\",\n            \"preprocessing\": [\"transfer learning\", \"feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transfer Learning + Ensemble Model\",\n            \"family\": \"Convolutional Neural Network (deep learning)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"complexity on low-powered devices\": \"optimized for real-time performance\",\n                \"efficiency trade-offs\": \"addressed in the model design\"\n            },\n            \"description\": \"A deep learning pipeline that combines transfer learning and ensemble techniques to meet the 98% accuracy goal.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"transfer learning\", \"ensemble techniques\"],\n        \"target_device\": \"low-powered devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource constraints\"],\n        \"demonstration\": {\n            \"scope\": \"real-time performance\",\n            \"methodology\": \"demonstrating recognition speed and accuracy on low-powered devices\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has been adapted to reflect the user's request for a sophisticated plant recognition system, high-level expertise, and specific requirements such as transfer learning, ensemble techniques, 98% accuracy, and real-time performance on low-powered devices. The complexity and efficiency trade-off is mentioned as something to be addressed in the report, allowing for flexibility in implementation details.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager, you're tasked with developing a state-of-the-art machine learning model for a comprehensive real estate valuation system. Utilize historical data from various neighborhoods, including non-conventional features like neighborhood trends, property condition, and neighborhood appreciation rates. Implement advanced data cleaning techniques, create interaction features, and conduct time-series analysis. Evaluate the model's performance using root mean squared error (RMSE) on a hidden test set, after transforming the sale price logarithmically. Provide a detailed report on feature importance and iteratively improve the model's interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"real estate\",\n        \"description\": \"The task is to develop a state-of-the-art machine learning model for real estate valuation using historical data from multiple neighborhoods, focusing on non-conventional features such as trends, property condition, and appreciation rates. Advanced data cleaning, interaction features creation, and time-series analysis are required. Evaluation metric is RMSE, and the sale price must be transformed logarithmically. A detailed report on feature importance and model interpretability is needed.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Real Estate Historical Data\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"sale price\"],\n            \"specification\": {\n                \"time-series\": true,\n                \"non-conventional_features\": [\n                    \"neighborhood trends\",\n                    \"property condition\",\n                    \"neighborhood appreciation rates\"\n                ]\n            },\n            \"description\": \"Data from various neighborhoods, including unconventional features and historical trends.\",\n            \"preprocessing\": [\n                \"advanced data cleaning\",\n                \"interaction feature creation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"time-series analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art Valuation Model\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"iterative improvement\"\n            },\n            \"description\": \"A machine learning model using neural networks for real estate valuation, focusing on feature importance and model explainability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilize advanced techniques for transforming and analyzing time-series data\",\n        \"Emphasis on feature importance and model interpretability\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"advanced data cleaning\",\n            \"interaction features\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"include a detailed report on model performance and interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the project manager's task to build a sophisticated model with specific requirements. Non-conventional features, data cleaning techniques, and evaluation metrics are included. Model interpretability is emphasized and a requirement for feature importance analysis. Some details about preprocessing and model type were left open to allow for flexibility.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "The project manager is requested to develop a state-of-the-art sentiment analysis model for fine-grained IMDB film reviews, targeting sub-genres like romantic comedies and action-dramas. The model must surpass a precision of 97% and maintain macro-averaged F1 score of 0.96, ensuring both precision and recall for balanced dataset evaluation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"entertainment (film analysis)\",\n        \"description\": \"The project aims to develop a state-of-the-art sentiment analysis model for fine-grained IMDB film reviews, focusing on sub-genres like romantic comedies and action-dramas. Performance requirements include precision above 97% and a macro-averaged F1 score of 0.96 for balanced dataset evaluation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"macro-averaged F1 score\",\n                \"value\": 0.96\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"IMDB Film Reviews (Fine-grained)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\", \"sub-genre (romantic comedies, action-dramas)\"],\n            \"specification\": {\n                \"balanced\": true\n            },\n            \"description\": \"The dataset consists of film reviews from IMDB, specifically targeted for sentiment analysis and divided by sub-genres.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-based (BERT, DistilBERT, etc.)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"high_accuracy\": true,\n                \"fine-grained\": true\n            },\n            \"description\": \"A state-of-the-art sentiment analysis model designed for fine-grained IMDB film review analysis, aiming for precision of at least 97% and macro-averaged F1 score of 0.96.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Film review example prediction and performance demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project, focusing on a high-level machine learning task (model building), targeting a high expertise level user. Performance metrics are clearly specified, ensuring that the model surpasses the given thresholds. The dataset is described accurately and relates to the specific sub-genres of interest. A BERT or similar model family is chosen as a basis for the sentiment analysis task. The confidence level is high, considering the clarity of information provided.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for our specialized fine art archive, we seek a deep learning-based image segmentation model that can precisely isolate and transcribe intricate watercolor illustrations from 18th-century folio books, while preserving their historical context and nuances. Can you recommend a tailored solution for this challenging archival task?", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"fine art archive\",\n        \"description\": \"The task requires a deep learning-based image segmentation model to isolate and transcribe intricate watercolor illustrations from 18th-century folio books, with a focus on historical context and preservation of nuances.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"historical context preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"nuance preservation\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"18th-century Watercolor Illustrations Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"segmentation mask\", \"transcription\"],\n            \"specification\": null,\n            \"description\": \"A dataset of 18th-century folio book illustrations for training and evaluating the model, emphasizing the need for high precision and nuance preservation.\",\n            \"preprocessing\": [\n                \"image restoration\",\n                \"historical context preservation techniques\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"example illustrations with segmentation and transcription\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep learning (Convolutional Neural Networks, U-Net, etc.)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"preserves_context\": true,\n                \"nuance_identification\": true\n            },\n            \"description\": \"A tailored deep learning model for fine art image segmentation, addressing the challenges of 18th-century watercolor illustrations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using transfer learning on similar historical artwork datasets\",\n        \"Employ semantic segmentation to maintain context\",\n        \"Investigate post-processing methods for fine-tuning output\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud for large computational resources\",\n        \"deployment_endpoint\": \"custom image segmentation API for the archive\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"Sample project demonstrating the model's performance on selected folio illustrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the advisory nature of the task, reflecting the request for a recommendation. High expertise level is assumed, and the provided details are focused on preserving historical context and nuances. Performance metrics are not explicitly defined, but knowledge snippets suggest the importance of context and nuance in the model selection. The model family is left open for a suitable deep learning architecture.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a specialized machine learning project manager, your team has been tasked with a unique text classification challenge: To analyze a dataset of highly specialized technical papers from niche conferences in the field of quantum cryptography. The input text consists of complex cryptographic algorithms, protocols, and their evaluation in adversarial scenarios. The target labels are graded on a scale of {1-10} representing the novelty, security impact, and overall quality, with the metric of interest being the weighted harmonic mean score (WHMS). Develop a state-of-the-art model that not only understands the intricacies of quantum cryptography but also differentiates between subtle rating distinctions. Ensure your model achieves a WHMS above 0.8 and provide a detailed explanation of the interpretability techniques used.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"quantum cryptography\",\n        \"description\": \"The project involves analyzing a technical paper dataset from niche quantum cryptography conferences, focusing on complex cryptographic algorithms, protocols, and their evaluation under adversarial scenarios. The goal is to develop a state-of-the-art model with a WHMS target above 0.8, capable of differentiating subtle rating distinctions and interpreting results.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"weighted harmonic mean score\",\n                \"value\": 0.8\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Quantum Cryptography Technical Papers Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"novelty\", \"security impact\", \"overall quality\"],\n            \"specification\": {\n                \"text_length\": {\n                    \"min\": null,\n                    \"max\": null\n                },\n                \"categorical_count\": {\n                    \"novelty\": null,\n                    \"security impact\": null,\n                    \"quality\": null\n                }\n            },\n            \"description\": \"A dataset consisting of highly specialized technical papers with intricate cryptographic content, requiring a deep understanding of quantum cryptography and fine-grained ratings.\",\n            \"preprocessing\": [\"tokenization\", \"stop-words removal\", \"stemming/lemmatization\"],\n            \"augmentation\": [\"synonym replacement\", \"sentence permutation\"],\n            \"visualization\": [\"TF-IDF analysis\", \"word cloud\"],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"transformer or deep LSTM (for interpretability)\",\n                \"training_details\": {\n                    \"epochs\": null,\n                    \"optimizer\": \".Adam\",\n                    \"loss_function\": \"weighted categorical harmonic mean\"\n                }\n            },\n            \"description\": \"A cutting-edge model designed to handle the complexities of quantum cryptography text, ensuring it can discriminate between subtle rating distinctions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interpretability techniques used: attention mechanisms, SHAP values, LIME, model-agnostic explanations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"custom feature extraction for quantum cryptography context\",\n            \"dimensionality reduction techniques (PCA, UMAP)\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demos on specific paper examples with clear explanation of model's reasoning\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the given schema, capturing the high-level intent, specific task in NLP, domain, performance metric, and detailed requirements for interpretability. The dataset and model specifications reflect the complexity and depth of the quantum cryptography topic. Missing aspects like model name and deployment details are left open for project-specific information.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For a complex underwater image segmentation project, apply a state-of-the-art DeepLabV3+ model, specifically tailored for high-resolution coral reef and delicate marine creature identification in the low-light, turbid Tyrrhenian Sea dataset. The model should integrate multi-scale contextual features, a customized loss function for resilient marine life edge detection, and must outperform existing methods by at least 5% in F1-score while maintaining real-time processing capabilities.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"underwater marine biology\",\n        \"description\": \"The project aims to segment underwater images for high-resolution coral reef and delicate marine creature identification in the Tyrrhenian Sea dataset, under low-light and turbid conditions. Key requirements include using DeepLabV3+ model, integrating multi-scale contextual features, a customized loss function for resilient marine life edge detection, and outperforming existing methods by at least 5% in F1-score while maintaining real-time processing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tyrrhenian Sea Dataset (Underwater Marine)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"coral reef\", \"marine creatures\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing low-light, turbid underwater images for the task of coral reef and marine creature segmentation in the Tyrrhenian Sea context.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"low-light and turbidity simulation\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepLabV3+\",\n            \"family\": \"Semantic Segmentation\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A state-of-the-art model for image segmentation, tailored for high-resolution coral reef and marine creature identification, with multi-scale contextual features and a customized loss function.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"multi-scale feature extraction\", \"custom loss function for edge detection\"],\n        \"target_device\": \"edge device / mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"accuracy comparison with existing methods\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent, their high expertise level, and the specific details of the underwater image segmentation project. It lists the required DeepLabV3+ model, its purpose, and performance metric (F1-score with a 5% improvement goal), as well as the dataset, customizations, and service requirements. The challenge of real-time processing and maintaining accuracy are reflected in the complexity metrics.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager demands a custom machine translation system tailored for niche African languages, focusing on Swahili, with a hybrid approach utilizing both Google's MCTRansformer and tailored LSTM models. Implement Masked Language Modeling on the Masakhane extended corpus and optimize for a significant boost of 15% in case-insensitive SacreBLEU scores, while maintaining resource efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-cultural communication\",\n        \"description\": \"The project aims to develop a custom machine translation system for niche African languages, specifically Swahili, using a hybrid approach of Google's MCTRansformer and tailored LSTM models. Masked Language Modeling will be implemented on the Masakhane extended corpus, targeting a 15% improvement in case-insensitive SacreBLEU scores while maintaining resource efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"SacreBLEU (case-insensitive)\",\n                \"value\": 1.15\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Resource efficiency\",\n                \"value\": null,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Masakhane extended corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language\": \"Swahili\",\n                \"corpus_size\": \"\",\n                \"language_pair\": \"African languages (Swahili)\"\n            },\n            \"description\": \"A corpus extended from Masakhane project, focusing on Swahili for the hybrid model development.\",\n            \"preprocessing\": [\"Masked Language Modeling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Hybrid (MCTRansformer + LSTM)\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"google_component\": \"MCTRansformer\",\n                \"lstm_customization\": true\n            },\n            \"description\": \"A custom machine translation system combining Google's MCTRansformer and LSTM models for Swahili, optimized for resource efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"African languages, specifically Swahili, have unique characteristics that necessitate a tailored approach.\",\n        \"The hybrid model leverages both transfer learning from MCTRansformer and specialized LSTM for better performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Fine-tuning for Swahili\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Performance comparison against existing Swahili MT systems\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's requirements, including the hybrid approach, focus on Swahili, MCTRansformer integration, LSTM customization, Masked Language Modeling, SacreBLEU optimization, and resource efficiency. The explanation for the chosen approach, performance metrics, and remaining uncertainties are incorporated. Confidence score is high due to the clear mapping to the JSON structure.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Design a high-performance node classification model using the NELL dataset, incorporating a deep Hypergraph Neural Network architecture. Ensure the model demonstrates resilience to varying levels of noise, effectively handles multi-relational complexity, and necessitate step-by-step optimization of hyperparameters to achieve an accuracy benchmark of at least 85%. Additionally, provide a detailed analysis of the hyperparameter sensitivity and its impact on model performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graph\",\n        \"description\": \"The task is to design a high-performance model using a deep Hypergraph Neural Network for node classification on the NELL dataset. The model should handle noise resilience, multi-relational complexity, and involve optimizing hyperparameters to achieve an accuracy of at least 85%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL Dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node labels\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"size\": null,\n                    \"edge_types\": {\n                        \"quantity\": null\n                    }\n                },\n                \"edges\": {\n                    \"quantity\": null\n                },\n                \"noise_distribution\": {\n                    \"level\": null,\n                    \"types\": {\n                        \"quantity\": null\n                    }\n                }\n            },\n            \"description\": \"A knowledge graph dataset where the task involves node classification using a deep Hypergraph Neural Network.\",\n            \"preprocessing\": [\n                \"noise reduction\",\n                \"multi-relational graph preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Hypergraph Neural Network\",\n            \"family\": \"Hypergraph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep\",\n                \"noise_handling\": \"resilience\",\n                \"multi_relational_complexity\": \"effective handling\",\n                \"hyperparameter_optimization\": true\n            },\n            \"description\": \"A model designed for node classification that incorporates hypergraph structure and focuses on performance and noise handling.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"The model requires step-by-step hyperparameter tuning to achieve the target accuracy.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"include sensitivity analysis report\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's high expertise level, the use of a deep Hypergraph Neural Network, the NELL dataset, performance metrics, and the need for hyperparameter optimization. The detail about noise resilience, multi-relational complexity, and the required sensitivity analysis show a comprehensive understanding of the problem statement.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Revamp the project for time-series forecasting by applying a specialized, interpretable deep learning method like prophet-ARIMA hybrid on our app's historical DAU data. Emphasize forecasting accuracy for last 12 months, handling weekly and monthly patterns, and incorporate a rolling validation technique to ensure model robustness against short-term fluctuations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"application performance or user engagement\",\n        \"description\": \"The task involves improving the time-series forecasting project by using a specialized interpretable deep learning method like prophet-ARIMA hybrid on historical daily active users (DAU) data. The focus is on accuracy for the last 12 months, handling weekly and monthly patterns, and incorporating rolling validation for robustness against short-term fluctuations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"forecasting accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"robustness against short-term fluctuations\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"App DAU Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"daily active users\"],\n            \"specification\": {\n                \"start_date\": \"YYYY-MM-DD\",\n                \"end_date\": \"YYYY-MM-DD\",\n                \"frequency\": \"daily\"\n            },\n            \"description\": \"A dataset containing the historical DAU data, suitable for time-series forecasting using the prophet-ARIMA hybrid method.\",\n            \"preprocessing\": [\n                \"Data normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Seasonality analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Prophet-ARIMA Hybrid\",\n            \"family\": \"Interpretable Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model interpretability\": \"yes\",\n                \"weekly_and_monthly_patterns\": \"incorporated\"\n            },\n            \"description\": \"An interpretable deep learning model for time-series forecasting, specifically designed to handle historical DAU data and focus on the last 12 months, using a rolling validation technique.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Integration with domain features (e.g., events, promotions)\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Online forecasting pipeline\"\n        ],\n        \"demonstration\": \"Forecasting visualizations for the last 12 months\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's instructions, focusing on revamping a time-series forecasting project with a prophet-ARIMA hybrid model, handling specific requirements, and providing a robust solution for short-term fluctuations. Performance metrics and model characteristics have been incorporated accordingly.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "community detection", "instruction": "As a machine learning project manager for a boutique financial services firm, we aim to discover unique subcommunities based on their distinct preferences for customized investment advice. Utilize advanced community detection algorithms on our historical transactional data with demographic and behavioral factors, while ensuring regulatory compliance and data privacy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"financial services\",\n        \"description\": \"The goal is to uncover unique subcommunities in a boutique financial services firm based on their customized investment preferences. This involves applying advanced algorithms to historical transactional data with demographic and behavioral factors, while adhering to regulatory compliance and data privacy regulations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"clustering accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"modularity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"privacy preservation\",\n                \"value\": null,\n                \"unit\": \"percent of anonymized data\"\n            },\n            {\n                \"name\": \"regulatory compliance\",\n                \"value\": null,\n                \"unit\": \"compliance with relevant regulations\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Historical Transactional Data\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"investment preferences\", \"demographic factors\", \"behavioral factors\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"dimension\": {\n                    \"continuous\": null,\n                    \"categorical\": null\n                }\n            },\n            \"description\": \"A dataset reflecting historical transaction data with demographic and behavioral aspects for subcommunity analysis.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Community Detection Algorithm\",\n            \"family\": \"Graph-based algorithm\",\n            \"type\": \"graph machine learning\",\n            \"specification\": {\n                \"data_format\": \"transactional\",\n                \"model_structure\": \"privacy-preserving\"\n            },\n            \"description\": \"An algorithm designed for community detection on financial data, emphasizing privacy and compliance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Regulatory requirements for handling sensitive financial data in our industry.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"private cloud environment\",\n        \"inference_engine\": [\"Distributed inference\"],\n        \"demonstration\": \"Interactive dashboard for community insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project goal, specifying the area of graph machine learning, the task of community detection, and the high expertise level. Key metrics like clustering accuracy, modularity, and the importance of regulatory compliance and data privacy are included. The dataset is described as reflecting historical transactional data, and a model is outlined that respects data privacy and adheres to regulations. The confidence score is high because the response closely aligns with the instruction.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "machine translation", "instruction": "As the machine translation project manager, your mandate is to supervise the development of a cutting-edge transformer model using the colossal Common Crawl corpus, focusing specifically on English-to-Japanese translation. Emphasize the need for a model adept at deciphering local slang and colloquialisms with a stringent benchmark of 98% accuracy, and incorporate subfield-specific terminology from both languages to test its linguistic prowess.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"The project involves designing and training a state-of-the-art transformer model for English-to-Japanese translation. It requires a model capable of understanding local slang and colloquialisms, targeting a benchmark accuracy of 98% and incorporating subfield-specific terminology from both English and Japanese.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl corpus (English-Japanese)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"colossal\",\n                \"language_coverage\": \"English and Japanese\",\n                \"diversity\": \"including local slang and subfield-specific terminology\"\n            },\n            \"description\": \"A vast corpus extracted from Common Crawl for training a machine translation model on English-to-Japanese, focusing on slang and specialized language.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer (specifically for MT)\",\n            \"family\": \"Neural Network (specifically Transformer architecture)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"Adapted for multilingual tasks and emphasizing precision on local slang and subfield-specific terms\"\n            },\n            \"description\": \"A transformer model tailored for English-to-Japanese translation, designed to handle complex linguistic nuances.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A demonstration showcasing translation of local slang and subfield-specific terminology with 98% accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the provided schema and captures the user's intent, expertise, and the specific requirements of the project, including a high-performance target, focus on slang and subfield-specific terminology, and the source of the large corpus. The response is complete and consistent with the instruction given.\",\n        \"score\": 0.99\n    }\n}\n"}
{"task": "community detection", "instruction": "In the context of the highly specialized and dynamic ArXiv scientific collaboration graph, develop a topological community detection algorithm using a Node2Vec technique. The algorithm must identify interdisciplinary research clusters with shared thematic patterns, while considering temporal dynamics and handling sparse data. The model should incorporate continuous learning to seamlessly integrate new researchers and publications without disrupting existing community structures, and demonstrate high interpretability for domain experts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"scientific collaboration\",\n        \"description\": \"The task involves designing a topological community detection algorithm using Node2Vec on the ArXiv scientific collaboration graph. The goal is to identify interdisciplinary research clusters with shared thematic patterns, considering temporal dynamics and handling sparse data. The model must support continuous learning and have high interpretability for domain experts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"modularity score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"training time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ArXiv Scientific Collaboration Graph\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": null,\n            \"specification\": {\n                \"node_type\": \"papers\",\n                \"edge_type\": \"collaborations\",\n                \"sparse_data\": true\n            },\n            \"description\": \"A dynamic graph containing scientific collaboration data from ArXiv, representing the need for Node2Vec and handling of sparse data.\",\n            \"preprocessing\": [\n                \"Edge Feature Engineering (temporal dynamics)\"\n            ],\n            \"augmentation\": [\n                \"Continuous Learning for Node Embeddings\"\n            ],\n            \"visualization\": [\n                \"Community Clusters for Interpretability\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Node2Vec-based Community Detection Model\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"Node2Vec\",\n                \"embedding_dimensions\": null,\n                \"learning_rate\": null,\n                \"continual_learning\": true\n            },\n            \"description\": \"An algorithm leveraging Node2Vec for community detection in the ArXiv collaboration graph, focusing on interdisciplinary research clusters and continuous learning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interpretable Clustering Techniques for Domain Experts\",\n        \"Temporal Dynamics in Collaboration Networks\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Node Embedding Generation\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"In-memory Graph Processing\"],\n        \"demonstration\": \"Interactive visualizations of identified clusters for domain experts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has been adapted to capture the user's high expertise level and the specific requirements for the machine learning project, including Node2Vec, temporal dynamics, and interpretability. The dataset and model properties have been tailored accordingly, while addressing the performance and complexity metrics. The service details reflect the continuous learning aspect and the importance of interpretability for domain experts.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a seasoned project manager overseeing a state-of-the-art machine learning development team, you've been tasked with an exclusive and challenging text classification project. The Obsidian Future Interface (OFI) dataset has surfaced, containing an obscure collection of intricate narratives, scientific papers, and social media posts. The catch is that the objective is to discern not only if a text originates from a human or AI, but also to determine the level of AI sophistication - with 0 representing elementary AI, 1 indicating mid-level AI, and 2 for highly advanced AI. The twist lies in the metric, which now evaluates not only overall accuracy but also the ability to correctly classify different AI sophistication levels within a single classification. Develop a multi-layered deep learning model that achieves high precision and nuanced categorization, presenting a clear strategy for interpreting and improving the model's performance over time.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"artificial intelligence research\",\n        \"description\": \"The task involves developing a text classification model on the Obsidian Future Interface (OFI) dataset. The objective is to distinguish between human and AI-generated content, with a further subtask to differentiate AI sophistication levels (0 for elementary, 1 for mid-level, and 2 for advanced). The focus is on high precision, nuanced categorization, and a strategy for interpreting and improving model performance over time.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AI sophistication level classification accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Obsidian Future Interface (OFI) Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"origin (human/AI)\", \"AI sophistication level\"],\n            \"specification\": null,\n            \"description\": \"A dataset with intricate narratives, scientific papers, and social media posts to classify as human or AI-generated, and to distinguish the AI sophistication level.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"multi-layered deep learning model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"include layers for binary text classification and a separate output for AI sophistication levels\"\n            },\n            \"description\": \"A deep learning model designed to handle the unique multi-label classification task, targeting high precision and nuanced categorization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Use domain-specific techniques for distinguishing human vs. AI content and different AI sophistication levels.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"BERT embeddings\", \"contextualized representations\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow\", \"PyTorch\"],\n        \"demonstration\": \"a detailed report on model interpretability, performance improvements, and fine-tuning strategies\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON representation captures the essence of the text classification project on the Obsidian Future Interface dataset, including the challenge of AI sophistication levels, the need for high precision, and the strategy for performance improvement. The 'expertise' is set to 'high' given the nature of the project and the required deep learning model development expertise.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a state-of-the-art node classification model using a deep, multi-layered Hierarchical Graph Neural Network (HGNN) for the extensive Coauthorship Science dataset. Demand exceptional accuracy by not only differentiating authors but also analyzing their evolving research networks, temporal dynamics, and inter-disciplinary collaborations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"scientific research\",\n        \"description\": \"The task is to build a state-of-the-art node classification model using a Hierarchical Graph Neural Network (HGNN) for the Coauthorship Science dataset. The model should achieve exceptional accuracy by differentiating authors, analyzing evolving research networks, temporal dynamics, and inter-disciplinary collaborations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coauthorship Science Dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"author\", \"research network\", \"temporal dynamics\", \"inter-disciplinary collaborations\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": {\"nodes\": \"author details\", \"edges\": \"collaboration details\"},\n                \"length\": null\n            },\n            \"description\": \"A dataset for node classification in a scientific coauthorship network with emphasis on research network analysis.\",\n            \"preprocessing\": [\n                \"extract evolving research networks\",\n                \"process temporal dynamics\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"visualize network evolution over time\",\n                \"display interdisciplinary collaboration patterns\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Graph Neural Network (HGNN)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"layers\": \"multi-layered deep architecture\",\n                \"architecture\": \"deep and hierarchical\"\n            },\n            \"description\": \"A model designed to classify nodes in a graph, catering to the specific needs of analyzing evolving research networks and temporal dynamics.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"name\": \"State-of-the-art algorithms for node classification\",\n            \"description\": \"Advanced techniques from the field of graph machine learning, such as HGNN, are being applied to achieve high accuracy.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"node embedding\",\n            \"temporal feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Model performance demonstration on various network snapshots\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a high-ex Expertise level model, with a detailed description of the task (node classification on a graph dataset with specific performance requirements). The Coauthorship Science dataset and HGNN model specifications were accurately formulated based on the provided information. The preprocessing, feature engineering, and visualization steps align with the instruction.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "time-series classification", "instruction": "The project manager is requested to develop a time-series DBN model, utilizing Riemannian geometry for feature extraction, optimized for the Human Activity Recognition dataset. The model must achieve state-of-the-art accuracy with real-time constraints, accounting for low-power consumption suitable for continuous monitoring on ultra-lightweight, IoT wearables.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The project aims to develop a time-series DBN model using Riemannian geometry for feature extraction on the Human Activity Recognition dataset. The model should achieve state-of-the-art accuracy with real-time performance and low-power consumption for continuous monitoring on ultra-lightweight IoT wearables.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"power consumption\",\n                \"value\": null,\n                \"unit\": \"Watts\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Human Activity Recognition Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"activity\"],\n            \"specification\": {\n                \"length\": null,\n                \"dimensions\": null\n            },\n            \"description\": \"A dataset for human activity recognition, suitable for ultra-lightweight IoT wearables.\",\n            \"preprocessing\": [\"Riemannian geometry for feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Time-Series DBN\",\n            \"family\": \"Restricted Boltzmann Machine\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A model utilizing Riemannian geometry for feature extraction, designed for real-time performance on ultra-lightweight IoT wearables.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"Riemannian geometry optimization\"],\n        \"target_device\": \"ultra-lightweight, IoT wearables\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time execution\"],\n        \"demonstration\": \"for continuous monitoring\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction by specifying a building task for a time-series DBN model using Riemannian geometry, with emphasis on accuracy and real-time constraints for IoT wearables. The Human Activity Recognition dataset is referenced, along with preprocessing and feature engineering steps. However, specific performance metrics and model details may require additional research or consultation due to the high complexity of the task.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "As a video surveillance expert, develop an advanced object detection system for my smart home's CCTV footage that not only identifies and categorizes unusual activities but also analyzes behavior patterns over time, ensuring increased security and smart home automation integrations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart home & security\",\n        \"description\": \"The user requires an advanced object detection system for video surveillance in their smart home. The system should detect unusual activities and analyze behavior patterns for increased security and smart home automation integration.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"detection accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"behavior analysis precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"behavior recognition recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency (inference time)\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Smart Home CCTV Footage\",\n            \"modality\": [\"video\"],\n            \"target_variables\": [\"unusual activities\", \"behavior patterns\"],\n            \"specification\": {\n                \"frame_rate\": null,\n                \"resolutions\": null\n            },\n            \"description\": \"Raw CCTV footage from the user's smart home environment, containing both normal and unusual activities for the system to detect and analyze.\",\n            \"preprocessing\": [\n                \"video stabilization\",\n                \"object detection cropping\"\n            ],\n            \"augmentation\": [\"simulated surveillance conditions\"],\n            \"visualization\": [\"activity heatmaps over time\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Object Detection Model\",\n            \"family\": \"Deep Learning (Convolutional Neural Networks)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Advanced architecture for handling complex object recognition and behavior analysis\",\n                \"framework\": [\"OpenCV\", \"TensorFlow\", \"Keras\"],\n                \"training_data_size\": null\n            },\n            \"description\": \"A specialized model designed for advanced object detection and behavior analysis in real-time smart home CCTV footage.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Experience with multi-label classification and time-series data\",\n        \"Adversarial examples and robustness in computer vision\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual feature extraction\",\n            \"long-term pattern recognition\"\n        ],\n        \"target_device\": \"embedded or cloud-based smart home security platform\",\n        \"deployment_endpoint\": \"API integration for smart home automation systems\",\n        \"inference_engine\": [\"real-time inference\", \"GPU acceleration\"],\n        \"demonstration\": \"Interactive dashboard showcasing detected activities and behavior patterns\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately reflects the user's requirements for an advanced object detection system. It covers key aspects like the user's expertise, the specific dataset, the model type and architecture, and the performance expectations for behavior analysis and low latency. The confidence score takes into account the assumed complexity of the task and the knowledge needed to fulfill the request.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a machine learning project manager, your team must develop a text classification system using the 20 Newsgroups dataset. Select a model with exceptional accuracy while considering size constraints, and estimate the training time with a focus on incremental learning, ensuring minimal human intervention.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"unknown (assumed to be IT or general, as project manager domain is not specific)\",\n        \"description\": \"The goal is to create a text classification system using the 20 Newsgroups dataset. The project requires a model with high accuracy, size constraints, and supports incremental learning with minimal human intervention.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"parameters\"\n            },\n            {\n                \"name\": \"training time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"class labels\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensionality\": null,\n                \"length\": null\n            },\n            \"description\": \"A dataset for text classification with a focus on newsgroups.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"to be specified based on size constraints\",\n                \"accuracy\": \"exceptional\",\n                \"incremental_learning\": true\n            },\n            \"description\": \"A model with high accuracy, optimized for size constraints, and designed for incremental learning with minimal human intervention.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"minimal\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the project manager's requirements. It focuses on building a text classification system with high accuracy, size constraints, and incremental learning. The lack of specific model details in the response allows for flexibility in selecting a suitable model based on the provided constraints. The confidence score is high due to the structured organization of information, but low due to the absence of detailed model specifics or the need to estimate training time.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "link prediction", "instruction": "Can a zero-shot machine learning approach be effectively employed to predict the emergence of co-authorship in the highly specialized field of extremophile microbiology, using a limited labeled dataset like the E. coli collaboration network?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"biology, extremophile microbiology\",\n        \"description\": \"The task is to explore the feasibility of employing a zero-shot machine learning approach for co-authorship prediction in the field of extremophile microbiology using a limited labeled dataset, such as the E. coli collaboration network.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"E. coli collaboration network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"co-authorship prediction\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"size\": null,\n                    \"type\": \"nodes relevant to extremophile microbiology\"\n                },\n                \"edges\": {\n                    \"size\": null,\n                    \"type\": \"co-authorship relationships\"\n                }\n            },\n            \"description\": \"A limited labeled dataset containing co-authorship information in the form of a network for extremophile microbiology, with focus on E. coli.\",\n            \"preprocessing\": [\n                \"graph normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"network visualization\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Zero-Shot Learning Model\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"designed for zero-shot learning\",\n                \"adaptability\": \"to limited labeled data\"\n            },\n            \"description\": \"A model intended to predict co-authorship in a specialized field with a scarcity of labeled data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Zero-shot learning can be challenging with limited labeled data but may be feasible if model assumptions align well with the field's characteristics.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"graph-based reasoning\"],\n        \"demonstration\": \"showcase model's performance on unseen data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's intent to build a model for a specialized topic, accounts for the high expertise level, and includes relevant details about graph data, zero-shot learning, and the importance of feature engineering. The choice of performance metrics considers the task's link prediction nature. The confidence score accounts for the potential challenges with limited labeled data.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "time-series forecasting", "instruction": "Develop a sophisticated WaveNet architecture incorporating transfer learning from a multi-label audio dataset of diverse urban environments. The project aims to not only predict sound levels for the upcoming 72 hours with high temporal resolution but also classify and visualize the predominant sound types in real-time, necessitating a robust noise clustering algorithm for city planners to monitor and mitigate specific sources of pollution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"environmental science\",\n        \"description\": \"The project involves developing a sophisticated WaveNet architecture with transfer learning from a multi-label audio dataset of diverse urban environments. The goal is to predict sound levels for the next 72 hours with high temporal resolution, and perform real-time classification and visualization of predominant sound types for noise control.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multi-label Urban Audio Dataset\",\n            \"modality\": [\"audio\"],\n            \"target_variables\": [\"sound levels\", \"sound types\"],\n            \"specification\": {\n                \"temporal_resolution\": null,\n                \"number_of_labels\": null\n            },\n            \"description\": \"A multi-label dataset capturing audio samples from various urban environments, to be used for transfer learning.\",\n            \"preprocessing\": [\"data normalization\", \"feature extraction\"],\n            \"augmentation\": [\"time stretching\", \"pitch shifting\"],\n            \"visualization\": [\"audio spectrograms\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"WaveNet\",\n            \"family\": \"Convolutional Recurrent Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-layered\",\n                \"transfer_learning\": true\n            },\n            \"description\": \"A WaveNet model utilizing transfer learning from the diverse urban audio dataset for real-time sound level prediction and noise classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning technique is important due to the limited labeled data in the audio domain.\",\n        \"Robust noise clustering algorithms are necessary for accurate source identification.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"time-series processing\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"real-time monitoring platform\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Kubernetes\"],\n        \"demonstration\": \"visual dashboards showing predictions and sound type classification\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the complexities of the problem, from transfer learning to real-time requirements. Performance metrics and complexity constraints are incorporated as expected. The chosen 'high' expertise level implies the user can guide advanced aspects of the project. The missing values are placeholders for actual numbers, which can be filled based on further discussion with the user.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager seeking advanced object detection in the fashion industry, I need a scalable solution that can identify various garments with high precision, despite computational complexity. Request recommendations for specialized datasets and robust models, considering real-time constraints, along with detailed training and deployment strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"The project demands a scalable object detection solution for garment identification in the fashion industry with high precision, while handling real-time constraints and computational complexity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational complexity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time constraints\",\n                \"value\": true\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Recommended (Fashion-specific)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"garment type\"],\n            \"specification\": {\n                \"size\": \"scalable\",\n                \"complexity\": \"optimized for real-time\"\n            },\n            \"description\": \"A dataset designed for advanced object detection, potentially with high precision in the fashion industry, meeting the real-world constraints.\",\n            \"preprocessing\": [\"data augmentation techniques\"],\n            \"augmentation\": [\"realistic garment transformations\"],\n            \"visualization\": [\"sample images with bounding boxes\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientDet or YOLOv4\",\n            \"family\": \"Object Detection Models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"streamlined for high precision and scalability\",\n                \"computational efficiency\": \"optimized for real-time inference\"\n            },\n            \"description\": \"Suggested models known for their robust performance, precision, and ability to handle real-world constraints.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"custom annotation techniques for garment types\"],\n        \"target_device\": [\"cloud or edge, depending on real-time constraints\"],\n        \"deployment_endpoint\": {\n            \"service\": \"AWS Rekognition, Google Cloud Vision, or customized serverless deployment\"\n        },\n        \"inference_engine\": [\"TensorFlow Serving, ONNX Runtime\"],\n        \"demonstration\": \"Interactive live demo showcasing garment detection with high precision\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the given schema, capturing the user's consult-build intent, high expertise, and the specific problem's requirements in the fashion industry. The focus on scalable solution, precision, and real-time constraints is reflected in the dataset and model recommendations. The response also accounts for deployment strategies, including computational efficiency and device-specific targeting.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a seasoned project manager, your machine learning development team must now enhance the Crab Age Regression Challenge by integrating a feature importance analysis, optimizing for computational efficiency with a memory-efficient model, and adhering to a constraint of 95% explained variance in the predicted ages. Utilize the train, valid, and test splits, while maintaining a target MAE under 5% for exceptional performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"marine biology\",\n        \"description\": \"The objective is to enhance the Crab Age Regression Challenge by adding feature importance analysis, optimizing for computational efficiency with a memory-efficient model, and ensuring an explained variance of 95% for predicted ages. The model must achieve a target mean absolute error of 5%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": 0.05\n            },\n            {\n                \"name\": \"explained variance\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            },\n            {\n                \"name\": \"memory efficiency\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Crab Age Regression Challenge dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"crab age\"],\n            \"specification\": null,\n            \"description\": \"Contains train, valid, and test splits for a crab age prediction problem.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficient\": true,\n                \"memory_consumption\": null\n            },\n            \"description\": \"A memory-efficient model with feature importance analysis, designed to achieve 95% explained variance and a target MAE of 5%.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature importance analysis\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Exceptional performance with MAE under 5% and 95% explained variance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the requirements accurately, from the high expertise level of the user, through the specific details of the Crab Age Regression Challenge task and the target metrics. The emphasis on feature importance analysis, computational efficiency, and 95% explained variance is incorporated. However, specific model details are left for flexibility.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for our machine learning team, your task is to develop an image segmentation system for the Cityscapes dataset with fine-grained car detection. The project should involve optimizing not only for accuracy but also real-time performance, targeting at least 30fps. Elaborate on the multi-stage approach, including preprocessing techniques, feature extraction, and implementing a lightweight model to enhance speed without compromising on identifying various car instances amidst complex urban scenes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning/automotive\",\n        \"description\": \"The project aims to develop an image segmentation system for the Cityscapes dataset, focusing on fine-grained car detection with a focus on real-time performance (at least 30fps). It requires optimizing for accuracy while ensuring a fast inference speed.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"fps (frames per second)\",\n                \"value\": 30.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"less than or equal to real-time (e.g., <= 1/30th of a second)\",\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"car instances\"],\n            \"specification\": {\n                \"images\": {\n                    \"size\": { \"width\": \"varies\", \"height\": \"varies\" },\n                    \"complex urban scenes\": true\n                }\n            },\n            \"description\": \"A dataset for image segmentation with emphasis on fine-grained car detection in urban scenes.\",\n            \"preprocessing\": [\n                \"image resizing to a fixed size for real-time compatibility\",\n                \"normalization\"\n            ],\n            \"augmentation\": [\n                \"lighting and weather simulation\",\n                \"geometric transformations to simulate different angles and viewpoints\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Lightweight deep learning model for real-time image segmentation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": {\n                    \"model architecture\": \"e.g., U-Net, FCN, or a custom design for efficient inference\",\n                    \"number of parameters\": \"low\",\n                    \"computational complexity\": \"meets real-time requirements\"\n                },\n                \"description\": \"A fast and efficient model for image segmentation with a focus on car detection\"\n            }\n        }\n    ],\n    \"knowledge\": [\n        \"Use lightweight CNN layers, e.g., MobileNet or SqueezeNet\",\n        \"Employ feature pyramid networks for better context handling\",\n        \"Utilize transfer learning from pre-trained models on similar tasks\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"fine-tuning based on car detection dataset\",\n            \"image preprocessing and augmentation for improved performance\"\n        ],\n        \"target_device\": \"Embedded devices for real-time processing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized libraries for deployment on hardware, e.g., OpenCV or TensorFlow Lite\"\n        ],\n        \"demonstration\": \"Demonstration of car detection with real-time fps and accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the instruction by capturing the task of building a real-time image segmentation system, performance requirements, the specific dataset, and the multi-stage approach, including preprocessing and lightweight model choice. The justification for the confidence score is that most key elements have been included and adapted to the JSON format.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "The task is to design a portable QA model based on TinyBERT, optimized for resource-constrained mobile devices, operating specifically on the NewsQA dataset. Achieve a stringent accuracy of at least 90% while ensuring real-time responsiveness of under 200 milliseconds, without increasing the model's parameters beyond 10 million. Additionally, the system must exhibit robustness against adversarial input and have an adaptive learning mechanism for continuous news domain updates.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"news and information retrieval\",\n        \"description\": \"The user wants to design a lightweight QA model, TinyBERT, specifically for resource-constrained mobile devices. The model must achieve an accuracy of at least 90% with real-time response time of under 200 milliseconds and have a parameter count not exceeding 10 million. It should also exhibit robustness to adversarial input and include an adaptive learning mechanism for continuous updates in the news domain.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 200,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"number of parameters\",\n                \"value\": 10000000\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NewsQA dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"answer to questions\"],\n            \"specification\": null,\n            \"description\": \"A dataset for training a question answering model, focusing on news articles.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameter_count\": 10000000,\n                \"inference_speed\": {\n                    \"max_latency\": 200,\n                    \"measurement\": \"milliseconds\"\n                }\n            },\n            \"description\": \"A lightweight QA model based on TinyBERT, optimized for mobile devices while maintaining an accuracy of at least 90%.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptive learning mechanism for continuous news domain updates and robustness against adversarial input\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource-constrained environment\"],\n        \"demonstration\": \"Real-time, resource-efficient QA system on mobile devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the user's requirements, from designing a TinyBERT-based QA model for mobile devices with the specified performance metrics and constraints. The missing information about the model parameter optimization and news domain updates is accurately represented through adaptive learning and continuous updates.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular classification", "instruction": "As a data science project manager, your team has been assigned a challenging and niche task for tabular classification in the medical domain. You've been handed the \"Chronic Diseases Risk Prediction Dataset,\" which contains a wealth of physiological, lifestyle, and genetic features, but with an added layer of complexity. The dataset has been meticulously stratified into train, validation, and test sets, with the objective of forecasting four distinct classes representing low, moderate, high, and very high risk of developing chronic diseases. The model's performance will be assessed using the macro-averaged F1 score, precision-recall curve (PRC), and Cohen's kappa coefficient to account for the imbalance in classes.\r\n\r\nYour task is not only to design and implement a state-of-the-art model but also to optimize it for interpretability, considering real-world applicability. In your final report, you must include a detailed feature importance analysis and a brief explanation of how the model's decisions align with medical knowledge.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"The team is tasked with a tabular classification project in the medical domain using the Chronic Diseases Risk Prediction Dataset. The focus is on forecasting four classes (low, moderate, high, very high risk) with a focus on imbalanced data. Performance metrics include macro-averaged F1 score, precision-recall curve, and Cohen's kappa coefficient.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"macro-averaged F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision-recall curve\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Cohen's kappa coefficient\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Chronic Diseases Risk Prediction Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"low risk\", \"moderate risk\", \"high risk\", \"very high risk\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing physiological, lifestyle, and genetic features for chronic disease risk prediction, stratified into train, validation, and test sets.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true\n            },\n            \"description\": \"A tabular classification model for predicting chronic disease risk, designed with interpretability in mind.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Feature importance analysis and alignment with medical knowledge will be included in the final report to demonstrate real-world applicability and model explainability.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"description\": \"Report with detailed feature importance analysis and explanation of model decisions in medical context\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's expert-level intent, the specific task of tabular classification in the medical domain, detailed description of the performance metrics, and the emphasis on interpretability. It accounts for the challenge of imbalanced classes and outlines the relevant dataset and model components as specified in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "Develop a cutting-edge, real-time fusion model for detecting sophisticated, multimodal deepfakes in historical legal documents, utilizing the specialized LegDocForgery dataset. The model should integrate fine-grained text analysis (identifying watermarks and microprint) and advanced image forensics (anomaly detection), aiming for a state-of-the-art AUC-ROC of 0.95 and a streamlined, interactive dashboard for lay users to interpret findings.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"legal\",\n        \"description\": \"The task involves developing a cutting-edge real-time fusion model for detecting sophisticated deepfakes in historical legal documents using the LegDocForgery dataset. The model must integrate fine-grained text analysis for watermark and microprint identification, and advanced image forensics for anomaly detection, targeting a state-of-the-art AUC-ROC of 0.95.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"time\"\n            },\n            {\n                \"name\": \"interactivity for lay users\",\n                \"value\": null,\n                \"unit\": \"user-friendly\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LegDocForgery Dataset\",\n            \"modality\": [\n                \"text\",\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"text_data_details\": \"\",\n                \"image_data_details\": \"\"\n            },\n            \"description\": \"A specialized dataset for detecting deepfakes in historical legal documents, containing both fine-grained text and multimodal image data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multimodal Fusion Model\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"real-time fusion of text analysis and image forensics\",\n                \"fine-grained techniques\": \"watermark and microprint detection\",\n                \"image forensics approach\": \"anomaly detection\"\n            },\n            \"description\": \"A state-of-the-art model designed for detecting deepfakes in legal documents, integrating advanced techniques and real-time capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Knowledge about specialized techniques and their relevance in detecting fine-grained tampering.\"\n        },\n        {\n            \"description\": \"Expert understanding of the nuances in legal document forgery cases.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interactive dashboard\"],\n        \"demonstration\": \"Interactive, user-friendly interface for non-experts to interpret forgery findings.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the given instruction, capturing the high-level requirements for a cutting-edge forgery detection model, fine-grained analysis, and the specified performance metrics. The need for a streamlined dashboard for non-technical users is also included.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "Utilize the diverse 'Trivago Hotel Recommendations' dataset, incorporating not only spatial and temporal dynamics but also user browsing history and real-time location data. Design a high-performance Graph Neural Network model with temporal convolutional layers, optimizing for efficiency while ensuring minimal latency for seamless, personalized in-app suggestions, reflecting the ever-changing user preferences.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"travel\",\n        \"description\": \"The task is to design a Graph Neural Network model using Trivago Hotel Recommendations dataset. The model must incorporate spatial, temporal, user browsing history, and real-time location data. The focus is on high performance and efficiency, minimizing latency for real-time, personalized in-app hotel suggestions considering dynamic user preferences.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean average precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Trivago Hotel Recommendations\",\n            \"modality\": [\n                \"graph\",\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"spatial\": true,\n                \"temporal\": true,\n                \"user_history\": true,\n                \"real_time_location\": true\n            },\n            \"description\": \"A dataset that combines hotel data with spatial and temporal dimensions, user browsing history, and real-time location data.\",\n            \"preprocessing\": [\n                \"feature engineering for graph and time series data\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Neural Network with Temporal Convolutional Layers\",\n            \"family\": \"Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": \"optimized\",\n                \"latency_target\": \"minimal\"\n            },\n            \"description\": \"A model designed to handle dynamic hotel recommendations using the specified dataset and emphasizing performance and low latency.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"name\": \"ever-changing user preferences\",\n            \"description\": \"Incorporates the need to adapt to users' constantly evolving preferences.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"spatial and temporal feature extraction\",\n            \"user behavior feature representation\"\n        ],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"dynamic in-app suggestions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's intent to build a high-performance GNN model. The Trivago dataset with multiple modalities is accurately captured. Performance metrics related to accuracy, efficiency, and low latency are specified. The expertise level of the user indicates a thorough understanding of the task. The rationale provides a high-level overview of how the information is derived from the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning development team, our challenge now is to build an image classification system for skin cancer using the HAM10000 dataset with Enhanced EfficientNet. The model must achieve at least 99% accuracy, ensuring zero false negatives for high sensitivity in early detection. Additionally, design a model that generates interpretable feature attributions for seamless collaboration with dermatologists in reviewing diagnostic recommendations, all within a restricted 2023 computational budget.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"The task is to develop an image classification system for skin cancer detection using the HAM10000 dataset with Enhanced EfficientNet. The system must have at least 99% accuracy, have zero false negatives for high sensitivity, and provide interpretable feature attributions for collaboration with dermatologists.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational budget\",\n                \"value\": null,\n                \"unit\": \"years\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HAM10000 dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"skin lesion type\"],\n            \"specification\": {\n                \"size\": {\n                    \"total_samples\": \"unknown\",\n                    \"training_samples\": \"unknown\",\n                    \"validation_samples\": \"unknown\",\n                    \"testing_samples\": \"unknown\"\n                },\n                \"data_dimensions\": {\n                    \"image_size\": \"square\",\n                    \"channels\": 3\n                }\n            },\n            \"description\": \"A dataset for skin cancer detection with Enhanced EfficientNet requirements.\",\n            \"preprocessing\": [\"data augmentation for robustness\"],\n            \"augmentation\": [\"random rotation, flipping, and color jittering\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced EfficientNet\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"interpretable for feature attributions\",\n                \"number_of_parameters\": \"restricted budget friendly\"\n            },\n            \"description\": \"An image classification model designed for high accuracy and interpretability in skin cancer detection, targeting dermatologists' collaboration.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Collaboration with dermatologists in model validation and use of ensemble methods for improved sensitivity.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"ensembling for model robustness\",\n            \"integration of domain knowledge\"\n        ],\n        \"target_device\": \"not specified\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Method for presenting interpretable feature attributions to dermatologists\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response has been parsed based on the given instruction. It captures the high expertise level, task specifics for image classification using Enhanced EfficientNet, dataset and performance requirements, and includes details for collaboration with dermatologists and computational budget constraints. The response is comprehensive and relevant to the given task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "Develop a sophisticated deep learning architecture that integrates DTW with a convolutional neural network (CNN) and long short-term memory (LSTM) for the intricate Sign Language Time Series dataset. The model should achieve exceptional performance, emphasizing both precision and recall, particularly in differentiating nuanced hand and body movements. Test the model on various lighting conditions and occlusions to ensure robustness, targeting real-world applications in support of seamless communication for the hearing impaired community.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"sign language recognition\",\n        \"description\": \"The project involves developing a deep learning model integrating Dynamic Time Warping (DTW), CNN, and LSTM for the Sign Language Time Series dataset. The model aims to excel in precision and recall, especially for nuanced hand and body movements. It must be robust to lighting conditions and occlusions for real-world communication applications with the hearing impaired.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sign Language Time Series Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hand movements\", \"body movements\"],\n            \"specification\": {\n                \"data_format\": \"sequences\",\n                \"time_series_length\": null,\n                \"modalities\": [\n                    \"hand\",\n                    \"body\"\n                ],\n                \"diversity\": [\n                    \"lighting conditions\",\n                    \"occlusions\"\n                ]\n            },\n            \"description\": \"A dataset containing sign language time series data with emphasis on hand and body movements, designed for testing robustness to real-world challenges.\",\n            \"preprocessing\": [\"DTW integration\", \"resampling\"],\n            \"augmentation\": [\"lighting variations\", \"occlusion simulation\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DTW-CNN-LSTM\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"DTW\": {},\n                    \"CNN\": {},\n                    \"LSTM\": {}\n                },\n                \"target_performances\": [\"high precision\", \"high recall\"]\n            },\n            \"description\": \"A deep learning architecture combining DTW, CNN, and LSTM, tailored for the Sign Language Time Series dataset with a focus on differentiating nuanced movements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Robustness to real-world conditions\",\n        \"Seamless communication for the hearing impaired\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"time-series feature extraction\", \"embedding techniques\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"a platform optimized for deep learning models\",\n        \"inference_engine\": [\"GPU-powered\", \"optimized for real-time inference\"],\n        \"demonstration\": \"Interactive sign language recognition application\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a high-level model, reflecting their high expertise. The complex requirements of integrating DTW, CNN, and LSTM for a specific sign language dataset are reflected, along with the emphasis on precision and recall and real-world conditions. The confidence score is high due to the thorough representation of the problem and solution details.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As the wildlife conservation project advances, enhance the existing requirements by specifying the model must distinguish between sub-species, handle varying weather conditions, and adapt to rapidly changing habitats, ensuring real-time, low-latency predictions for wildlife surveys on energy-constrained devices in remote, signal-limited locations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"The task involves enhancing an existing model for wildlife recognition, focusing on distinguishing between sub-species, handling variable weather conditions, and adapting to rapidly changing habitats. The model should provide real-time, low-latency predictions for wildlife surveys on energy-constrained devices in remote, signal-limited locations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": 100\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": null\n                }\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": null\n                }\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null\n            },\n            {\n                \"name\": \"device constraints (energy-constrained)\",\n                \"value\": {\n                    \"unit\": null,\n                    \"description\": \"Model must be optimized for low-power usage on energy-constrained devices\"\n                }\n            },\n            {\n                \"name\": \"network constraints (signal-limited)\",\n                \"value\": {\n                    \"unit\": null,\n                    \"description\": \"Model should work effectively in low signal areas\"\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enhanced Wildlife Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"sub-species\", \"weather conditions\", \"habitat\"],\n            \"specification\": null,\n            \"description\": \"A dataset for wildlife surveys that includes varying sub-species, weather conditions, and rapidly changing habitats.\",\n            \"preprocessing\": [\"real-time adaptation to different conditions\"],\n            \"augmentation\": [\"image resizing, weather simulation for varying conditions\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"optimized for sub-species distinction and environment adaptability\",\n                \"energy efficiency\": true\n            },\n            \"description\": \"A model designed for real-time object detection, capable of differentiating sub-species and functioning in remote, signal-limited locations.\"\n        }\n    ],\n    \"confidence\": {\n        \"rationale\": \"The response accurately represents the given instruction, reflecting the enhanced requirements for the wildlife conservation project. It specifies the model's characteristics, performance metrics, and the unique constraints. The confidence score is high due to the precision in capturing the details from the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "The project manager must oversee the development of a specialized text classification system for the interdisciplinary realm, using the arXiv archives. Our team is mandated to design a state-of-the-art HAN model with a deep focus on condensed matter physics, astrophysics, and quantum computing. The model must achieve a minimum F1 score of 92% in these fields, demonstrating superior hierarchical attention to nuances, and possess self-updating capabilities to handle real-time additions of recent research articles with a processing time of <5 minutes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"interdisciplinary\",\n        \"description\": \"The project is to develop a specialized text classification system using the arXiv archives, focusing on condensed matter physics, astrophysics, and quantum computing. The model should be based on the HAN architecture and aim for a minimum F1 score of 92% with self-updating capabilities.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.92\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"processing time\",\n                \"value\": 5,\n                \"unit\": \"minutes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv archives\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"text classification\"],\n            \"specification\": {\n                \"domain\": [\"condensed matter physics\", \"astrophysics\", \"quantum computing\"]\n            },\n            \"description\": \"The dataset consists of scholarly articles from the arXiv archives related to the specified disciplines.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Attention Network (HAN)\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art for text classification\",\n                \"focus_on\": [\"condensed matter physics\", \"astrophysics\", \"quantum computing\"],\n                \"attention_mechanism\": \"hierarchical\"\n            },\n            \"description\": \"A text classification model designed to achieve high performance in condensed matter physics, astrophysics, and quantum computing, with self-updating capabilities.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Real-time article processing and updates with <5 minutes processing time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent for building a specialized HAN model, sets the correct expertise level, includes the required focus on specific arXiv disciplines, and incorporates the performance metrics and processing time constraints. The missing aspects, like actual model performance or specific implementation details, are acknowledged in the confidence score as they can vary.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a machine learning expert for a niche music streaming platform, you've been tasked with developing a cutting-edge time-series classification system to identify not only the genre but also the sub-genre and emotional tone of extremely obscure and niche music tracks found in the \"Muzique Paradox\" dataset. This dataset, dating back to the 19th century, consists of vinyl records and cassette tapes with deteriorating audio quality. The challenge lies in devising a robust model that can handle signal degradation, variable bitrates, and unique temporal patterns specific to these rare genres. Furthermore, you must compare and contrast the performance of deep neural networks (DNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs) with attention mechanisms, while also considering the impact of feature extraction techniques like mel-spectrograms and wavelet coefficients. Can you present a detailed plan, including preprocessing steps, evaluation metrics, and expected performance benchmarks, for this avant-garde project?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"music streaming\",\n        \"description\": \"The task is to develop a time-series classification system for identifying genre, sub-genre, and emotional tone in extremely obscure and niche music from the Muzique Paradox dataset. The dataset contains deteriorating audio quality from vinyl records and cassette tapes, and the challenge is to handle signal degradation, variable bitrates, and unique temporal patterns specific to rare music genres.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"genre classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sub-genre classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"emotional tone classification accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model efficiency (FLOPs or memory footprint)\",\n                \"value\": null,\n                \"unit\": \"GFLOPs or MB\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Muzique Paradox\",\n            \"modality\": [\"audio\"],\n            \"target_variables\": [\"genre\", \"sub-genre\", \"emotional tone\"],\n            \"specification\": {\n                \"quality\": \"deteriorating\",\n                \"bitrate\": \"variable\",\n                \"temporal_pattens\": \"unique rare genre specific\"\n            },\n            \"description\": \"A dataset of obscure music with poor audio quality from vinyl records and cassette tapes.\",\n            \"preprocessing\": [\n                \"signal degradation correction\",\n                \"variable bitrate normalization\",\n                \"feature extraction (mel-spectrograms, wavelet coefficients)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DNN, CNN, RNN with attention\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\n                    {\"type\": \"DNN\", \"description\": \"\"},\n                    {\"type\": \"CNN\", \"description\": \"\"},\n                    {\"type\": \"RNN with attention\", \"description\": \"\"}\n                ]\n            },\n            \"description\": \"A comparative study of DNNs, CNNs, and RNNs with attention mechanisms for time-series classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Possible challenges: signal degradation, variable bitrates, unique temporal patterns in niche music\",\n        \"Best practices: handling different input formats and quality\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"handle unique feature extraction for different models\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow, Keras, or custom\"],\n        \"demonstration\": \"presentation of model performance, comparison among architectures, and impact of preprocessing\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON format captures the essence of the task, including the user's high expertise, the need for advanced models and feature extraction, the evaluation criteria, and the nuances of handling the unique Muzique Paradox dataset. This response addresses the user's intent to build a model and presents a clear plan for the project.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the machine learning development team, you are now tasked with enhancing the text classification project. In addition to the WebMD Review dataset, integrate a sentiment lexicon for context-specific positivity and negativity. Ensure the model distinguishes between nuanced emotions (e.g., sarcasm) and focuses on optimizing macro-average F1-score for balanced classes, while maintaining real-time processing capability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The objective is to enhance a text classification project with the WebMD Review dataset. The enhancement includes integrating a sentiment lexicon for context-specific emotion detection, focusing on sarcasm, and optimizing for macro-average F1-score for balanced classes, while maintaining real-time processing capability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"macro-average F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": 1,\n                \"unit\": \"Yes/No\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WebMD Review dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\", \"nuanced_emotion\"],\n            \"specification\": null,\n            \"description\": \"The dataset for text classification, including a sentiment analysis component, targeting WebMD reviews.\",\n            \"preprocessing\": [\"integration of sentiment lexicon\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Text Classification Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"focus\": [\"nuanced emotion recognition\", \"real-time processing\"],\n                \"model_architecture\": \"contextualized for sarcasm\"\n            },\n            \"description\": \"A machine learning model for text classification with sentiment analysis and the ability to handle nuanced emotions, targeting improved F1-score and real-time performance.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Sentiment lexicon to be integrated for improved context-sensitive sentiment analysis.\"\n        },\n        {\n            \"text\": \"Emphasis on distinguishing sarcasm as a nuanced emotion.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"handling class imbalance\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"prototype demonstrating real-time processing\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent for enhancing the text classification project, incorporating the WebMD Review dataset and sentiment lexicon. It highlights the importance of nuanced emotion detection, macro-average F1-score, and real-time processing, which were specified in the instructions. Adjustments for real-time processing as a 'Yes/No' metric reflect the context of the project requirement.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for the machine learning development team, you are now tasked with enhancing the UWaveGestureLibrary dataset analysis for time-series classification. In addition to the initial requirement of training, validation, and testing splits (70% train, 15% validation, 15% test), introduce a constraint that the model must handle missing data imputation and demonstrate robustness to varying sequence lengths within the range of 315 to 500 time steps. The model must maintain an accuracy of at least 95% while also optimizing for interpretability using feature importance analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"signal processing or human-computer interaction\",\n        \"description\": \"The task is to analyze the UWaveGestureLibrary dataset, with a focus on time-series classification. The model must handle missing data imputation and be robust to varying sequence lengths between 315 and 500 time steps. A minimum accuracy goal of 95% is set, and interpretability through feature importance analysis is a priority.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"missing data imputation\",\n                \"value\": \"optimized\"\n            },\n            {\n                \"name\": \"sequence length variation handling\",\n                \"value\": \"robust\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UWaveGestureLibrary\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"gesture classification\"],\n            \"specification\": {\n                \"min_sequence_length\": 315,\n                \"max_sequence_length\": 500,\n                \"missing_data_strategy\": \"imputed\"\n            },\n            \"description\": \"The UWaveGestureLibrary dataset for time-series classification tasks, with varying sequence lengths and a requirement for missing data handling.\",\n            \"preprocessing\": [\"data imputation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks (preferably interpretable models)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"enabled\"\n            },\n            \"description\": \"A model for time-series classification in the UWaveGestureLibrary dataset, focusing on accuracy, robustness to varying sequence lengths, and interpretability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature importance analysis\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"include demonstration of model performance and feature importance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's instructions, specifying requirements for data pre-processing, missing data handling, sequence length variation, model architecture, and interpretability. The confidence score reflects the assumption that the provided information aligns with the project manager's role and responsibilities.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "The financial news analysis project now demands a comprehensive, multi-step approach. Utilize the Reuters-21578 dataset, preprocess articles with advanced natural language processing, fine-tune a state-of-the-art BERT model combined with a domain-specific financial context layer. Aim for a robust classification system with at least 88% precision, targeting a 90% upper limit in market-relevant news categories, while ensuring a 10% decrease in false positives compared to the baseline model.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"The project requires a multi-step approach using the Reuters-21578 dataset for financial news analysis. The focus is on preprocessing with advanced techniques and fine-tuning a BERT model with a domain-specific financial context layer. The goal is to achieve a robust classification system with precision over 88% and a max target of 90% in market-relevant news categories while reducing false positives by 10% compared to the baseline model.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.88\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"financial relevance\"],\n            \"specification\": {\n                \"size\": \"large\",\n                \"text_length\": \"variable\",\n                \"domain\": \"financial news\"\n            },\n            \"description\": \"A dataset for financial news analysis, requiring advanced preprocessing for articles\",\n            \"preprocessing\": [\"advanced NLP techniques\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT with financial context layer\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"fine-tuned BERT\",\n                \"domain_layer\": \"financial context layer\"\n            },\n            \"description\": \"A state-of-the-art text classification model focusing on market-relevant news with precision target\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"domain-specific\"],\n        \"demonstration\": \"comparison with baseline model\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the user's intent for building a model. The problem section aligns with NLP tasks in finance, mentioning BERT and precision goals. Performance metrics specified include precision, while leaving recall and accuracy open for the system to potentially surpass baseline performance. A domain-specific financial context layer is included, and the dataset source and preprocessing steps are clearly defined.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "image classification", "instruction": "The project manager must direct the machine learning team to enhance the existing image classification project by integrating a multi-task learning approach using Capsule Networks for Fashion-MNIST. The team must surpass a stringent benchmark of 95% accuracy while maintaining robustness against various affine transformations. Additionally, they must provide an in-depth analysis, comparing their capsule network's performance with CNNs, showcasing the improvements in computational efficiency and rotation invariance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"The project aims to enhance an existing image classification project on Fashion-MNIST by incorporating a multi-task learning approach using Capsule Networks. The goal is to achieve a minimum accuracy of 95% while ensuring robustness against affine transformations. An in-depth analysis and comparison with CNNs, focusing on computational efficiency and rotation invariance, should be provided.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"% decrease\"\n            },\n            {\n                \"name\": \"rotation invariance\",\n                \"value\": null,\n                \"unit\": \"% improvement\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"fashion item classification\"],\n            \"specification\": {\n                \"size\": {\n                    \"training\": null,\n                    \"validation\": null,\n                    \"testing\": null\n                },\n                \"dimension\": 28,\n                \"affine_transformation_sensitive\": true\n            },\n            \"description\": \"A dataset for fashion item classification, to which multi-task learning with Capsule Networks will be applied.\",\n            \"preprocessing\": [\"affine transformation\"],\n            \"augmentation\": [\n                \"affine transformations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Capsule Network\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-task learning\",\n                \"rotation_invariance\": true\n            },\n            \"description\": \"A model employing capsule networks for enhancing image classification, targeting to surpass 95% accuracy and maintain robustness against affine transformations.\"\n        },\n        {\n            \"name\": \"Comparative CNN\",\n            \"specification\": {\n                \"architecture\": \"standard image classification\"\n            },\n            \"description\": \"A baseline CNN model for comparison, showcasing the improvements in Capsule Network's computational efficiency and rotation invariance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Comparative analysis of Capsule Networks vs. CNNs, emphasizing the benefits in computational efficiency and rotation invariance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"goal\": \"comparison with CNN performance and multi-task learning improvements\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response accurately captures the project's requirements, indicating a high expertise level in machine learning. It includes the requested enhancements (multi-task learning and Capsule Networks), performance metrics, and the need for comparing with CNNs. The specified dataset, preprocessing, and model details align with the task at hand.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "text classification", "instruction": "In this advanced machine learning project, you are tasked with enhancing the text classification system for the ChatGPT prompt dataset. Apply a multi-label taxonomy with 8 categories, ensuring high precision and recall, while using a custom-built transformer model trained from scratch. Regularly monitor macro-F1 for model performance, and optimize for balanced accuracy under a constraint of low computational resources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"language technology\",\n        \"description\": \"The project aims to enhance a text classification system for the ChatGPT prompt dataset, focusing on a multi-label taxonomy with 8 categories. It requires high precision and recall, and the approach involves a custom-built transformer model trained from scratch. The model performance is to be evaluated using macro-F1 and optimized for balanced accuracy under limited computational resources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"macro-F1\",\n                \"value\": null\n            },\n            {\n                \"name\": \"balanced accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational resources\",\n                \"value\": null,\n                \"unit\": \"low\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ChatGPT Prompt Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"multi-label taxonomy\"],\n            \"specification\": null,\n            \"description\": \"The dataset for text classification with a focus on enhancing the ChatGPT prompt dataset's multi-label taxonomy.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Transformer Model\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A model built from scratch specifically for text classification with emphasis on multi-label taxonomy and optimized for high precision, recall, and balanced accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's instructions by specifying a high-level machine learning project focusing on a text classification system for the ChatGPT dataset. It highlights the custom-built transformer model, multi-label taxonomy, precision and recall targets, macro-F1 monitoring, and the computational resource constraint. The user's medium expertise level is not mentioned here as it is not explicitly specified, but the response assumes a higher level of expertise based on the complexity of the project.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for the machine learning development team, I need a novel image classification model integrating ResNet architecture, incorporating data augmentation techniques, and achieving at least 99% accuracy on the CIFAR-10 benchmark while maintaining state-of-the-art performance on a smaller memory footprint.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"general purpose\",\n        \"description\": \"The project aims to develop a novel image classification model based on ResNet architecture, utilizing data augmentation techniques, and targeting at least 99% accuracy on the CIFAR-10 benchmark while maintaining state-of-the-art performance with a focus on memory efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CIFAR-10\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dimension\": 32,\n                \"num_classes\": 10\n            },\n            \"description\": \"A benchmark dataset for image classification tasks that will be used for training and testing the model.\",\n            \"preprocessing\": [\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\n                \"flipping\", \"rotation\", \"cropping\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Novel ResNet-based Model\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A state-of-the-art image classification model integrating ResNet architecture, with a focus on achieving high accuracy and optimized memory footprint.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"explain steps for state-of-the-art model performance and memory optimization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given project manager's instruction, including the ResNet-based model, data augmentation techniques, target accuracy, and memory optimization. It accounts for the user's high expertise level and specifies CIFAR-10 for model benchmarking.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a cutting-edge AI team, you're tasked with enhancing the real-world time-series forecasting prowess using the Electricity dataset. The dataset, now divided into non-overlapping train, validation, and test sets with specific chunk sizes (INPUT_SEQ_LEN=96 and INPUT_DIM=321), necessitates the development of a sophisticated forecasting model. Your team's objective is to not only predict the subsequent future sequences (PRED_SEQ_LEN=96) but also account for seasonality and trends, while optimizing for both mean squared error (MSE) and mean absolute error (MAE). In addition, implement multi-step ahead prediction and conduct a thorough hyperparameter search to guarantee state-of-the-art performance on unseen data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy\",\n        \"description\": \"The project involves enhancing time-series forecasting for real-world electricity data. The dataset is divided into non-overlapping train, validation, and test sets with chunk sizes of INPUT_SEQ_LEN=96 and INPUT_DIM=321. Requirements include modeling seasonality, trends, multi-step ahead prediction, and optimizing for both MSE and MAE.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321\n            },\n            \"description\": \"An electricity dataset for time-series forecasting, split into train, validation, and test sets with given chunk sizes.\",\n            \"preprocessing\": [\"split into chunks\", \"seasonality and trend handling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Suggested (unsure based on task specifics)\",\n            \"family\": \"Recurrent Neural Networks (LSTM, GRU) or Stateful Forecasting Models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"multi-step_ahead_prediction\": true,\n                \"hyperparameter_search\": true\n            },\n            \"description\": \"A sophisticated forecasting model designed to handle time-series data, featuring seasonality, trend analysis, multi-step prediction, and optimized for MSE and MAE.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using time-series libraries like tslearn or FBprophet for seasonality and trend extraction\",\n        \"Implement sliding window or transformer-based approach for multi-step ahead prediction\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for seasonality and trends\", \"normalization, if needed\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow serving\", \"real-time computing\"],\n        \"demonstration\": \"Illustrate multi-step ahead predictions and performance on unseen data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent as a build task, reflects a high expertise level, and addresses the requirements of a sophisticated time-series forecasting model with multi-step ahead prediction, seasonality, and trend handling. Performance metrics and dataset information are appropriately detailed, and extra knowledge and service requirements are included.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "For a unique and challenging image classification project, design a deep learning model that not only distinguishes between cat and dog photographs but also differentiates among multiple cat breeds and various dog breeds based on subtle breed-specific features, requiring multi-stage feature extraction and advanced classification algorithms.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal identification\",\n        \"description\": \"The task is to develop a deep learning model for a complex image classification problem. The model should be able to distinguish between cat and dog photos while also accurately differentiating among multiple cat and dog breeds using multi-stage feature extraction and advanced classification algorithms.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Combined Cat & Dog Breeds Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed (cat, dog)\", \"specific breed\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"image_count\": null,\n                \"class_distribution\": null\n            },\n            \"description\": \"A dataset that includes photographs of cats and dogs, requiring the model to distinguish between different breeds using subtle features.\",\n            \"preprocessing\": [\n                \"multi-stage feature extraction\",\n                \"image normalization\"\n            ],\n            \"augmentation\": [\n                \"data augmentation for breed diversity\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Multi-Breed Classifier\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\n                    \"multi-stage feature extraction\",\n                    \"dense layers for breed classification\"\n                ],\n                \"top-layer_units\": null,\n                \"use_of_transfer_learning\": true\n            },\n            \"description\": \"A deep learning model specifically designed to classify cat and dog photographs into different breeds using advanced feature extraction and multiple classification stages.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Advanced understanding of CNN architectures, transfer learning, and the importance of breed-specific features for accurate classification\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"multi-scale feature extraction using pre-trained models\",\n            \"fine-tuning for breed-specific features\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Generating a confusion matrix and visualizing model performance on breed-level\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's instruction by indicating a high-level 'build' intent, specifying the task as a complex image classification project, and mentioning the requirement for multi-stage feature extraction and advanced algorithms. Performance metrics and complexity metrics are anticipated based on the nature of the task, and the dataset and model properties match the problem description.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "For a niche luxury property management firm, develop a deep learning model specialized in recognizing high-resolution images of elite beachfront homes, focusing on specific features like imported marble baths, private dockages, and tropical landscaping. The system must integrate seamlessly with the existing CRM, providing labeled image captions and prioritizing visually appealing highlights to entice high-net-worth clients.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"luxury real estate\",\n        \"description\": \"The task is to create a deep learning model for recognizing high-resolution images of elite beachfront homes, focusing on features like imported marble baths, private dockages, and tropical landscaping. The model should integrate with the existing CRM and provide labeled captions, prioritizing visually appealing aspects.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specific feature recognition rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Luxury Beachfront Homes Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"feature_categories\": [\"marble baths\", \"private dockages\", \"tropical landscaping\"]\n            },\n            \"description\": \"A high-resolution image dataset of beachfront homes with specific focus on visual features for classification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                {\n                    \"method\": \"image cropping\",\n                    \"target_features\": [\"marble baths\", \"private dockages\"]\n                },\n                {\n                    \"method\": \"lighting adjustments\",\n                    \"target_features\": [\"tropical landscaping\"]\n                }\n            ],\n            \"visualization\": [\n                \"image caption generation for highlighting features\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Deep Learning Model (for beachfront home recognition)\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specialized for high-resolution image classification\",\n                \"integration_points\": [\"CRM integration\"]\n            },\n            \"description\": \"A deep learning model tailored for recognizing high-end beachfront home features with CRM compatibility and emphasis on visual appeal.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporate domain knowledge: luxury real estate trends and client preferences\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Generate personalized property tours with highlighted features\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately represents the user's instructions, targeting a high-end property management scenario. Expertise level is considered high due to the requirement for specialized deep learning. The high-resolution images, specific features, and CRM integration were captured, along with the importance of label captions and visual emphasis. Confidence score is based on a thorough interpretation of the given details.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for our advanced urban scene analysis team, we need a state-of-the-art U-Net variant designed specifically for the Cityscapes dataset. It should handle intricate 30-class segmentation, achieving at least 95% dice similarity coefficient, while maintaining real-time performance with a latency of less than 30 milliseconds on a high-end, specialized GPU. Additionally, please develop a detailed benchmarking plan to compare and optimize different model architectures and preprocessing techniques.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban scene analysis\",\n        \"description\": \"The project involves creating a state-of-the-art U-Net variant for the Cityscapes dataset, targeting 30-class segmentation with a minimum dice similarity coefficient of 95%. The model must operate in real-time, with a latency of less than 30 milliseconds on a high-end, specialized GPU.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"dice similarity coefficient\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.03,\n                \"unit\": \"seconds (milliseconds: 30ms)\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"30-class segmentation\"],\n            \"specification\": null,\n            \"description\": \"Dataset for advanced urban scene analysis, requiring high performance and 30-class segmentation.\",\n            \"preprocessing\": [\n                \"suitable for U-Net variant\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net Variant\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time performance\": true,\n                \"target_device\": \"high-end, specialized GPU\"\n            },\n            \"description\": \"A state-of-the-art model for 30-class segmentation on the Cityscapes dataset, optimized for real-time operation.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"A detailed benchmarking plan for comparing model architectures and preprocessing techniques is required.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-end, specialized GPU\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"compare and optimize performance across different model architectures and preprocessing techniques\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON representation has captured the key aspects of the project's requirements, including model architecture, performance metrics, and specialized hardware. The benchmarking plan is noted as a separate item in the 'knowledge' section. The confidence score is high because the template is followed closely, and key elements are included that align with the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "As a project manager for the cutting-edge machine learning development team, I've come across a fascinating yet challenging task. We've successfully explored the application of transformers in text analysis, but now we need to push the boundaries further. Our goal is to design a state-of-the-art multimodal forgery detection system using a novel architecture that incorporates transformers for the 'Celeb-DF' dataset, which contains both video and audio information. The system should not only differentiate between genuine and manipulated content, but also analyze subtle inconsistencies across multiple modalities, and outperform existing algorithms using explainable AI techniques. The project must include a rigorous evaluation of robustness against various forgery techniques and adaptability to unseen data. Can you propose a detailed plan to develop this advanced transformer-based solution for multimodal forgery detection?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"media forensics\",\n        \"description\": \"The project aims to design a state-of-the-art multimodal forgery detection system using transformers, incorporating video and audio data from the Celeb-DF dataset. The system must differentiate between genuine and manipulated content, analyze inconsistencies across modalities, and outperform existing algorithms with explainable AI. Robustness against various forgery techniques and adaptability to unseen data are key evaluation criteria.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explainability score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness against forgery techniques\",\n                \"value\": null\n            },\n            {\n                \"name\": \"adapting to unseen data\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\"image\", \"audio\", \"video\"],\n            \"target_variables\": [\"forged\", \"subtle inconsistencies\"],\n            \"specification\": {\n                \"video length\": {\n                    \"description\": \"video frames and duration\",\n                    \"unit\": \"frames / seconds\"\n                },\n                \"audio features\": {\n                    \"description\": \"audio data characteristics\",\n                    \"unit\": \"specification not provided\"\n                }\n            },\n            \"description\": \"A dataset containing video and audio information for multimodal forgery detection, including genuine and manipulated content.\",\n            \"preprocessing\": [\n                \"aligning modalities\",\n                \"feature extraction for transformers\"\n            ],\n            \"augmentation\": [\n                \"multimodal data augmentation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Proposed Transformer-based Architecture for Multimodal Forgery Detection\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novel architecture\": \"incorporating transformers\",\n                \"explainable AI techniques\": \"used\"\n            },\n            \"description\": \"A cutting-edge solution that combines transformers for detecting inconsistencies in video and audio data from the Celeb-DF dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Experience with transformers in text analysis\",\n        \"Challenges in multimodal fusion and explanation in forgery detection\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"multi-modal fusion techniques\",\n            \"model explainability enhancement\"\n        ],\n        \"target_device\": \"high-performance computing environment\",\n        \"deployment_endpoint\": \"edge or cloud based on dataset size\",\n        \"inference_engine\": [\n            \"real-time inference\"\n        ],\n        \"demonstration\": \"live demo showcasing performance and explainability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a high-end multimodal forgery detection system with their expertise in advanced machine learning. The problem description includes specific task requirements, dataset details, and performance metrics. It accounts for model architecture, data preprocessing, and the need for explainable AI and robustness. The confidence score is high due to the thorough mapping of the instruction to the JSON structure.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "text classification", "instruction": "As a project manager for a cutting-edge AI team, you are now tasked with enhancing the BoolQ text classification project. In addition to achieving high accuracy, mandate the development of a robust model that demonstrates interpretability and handles adversarial examples, ensuring its reliability in real-world scenarios while maintaining a minimum F1-score of 0.90.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI/ML\",\n        \"description\": \"The project aims to enhance the BoolQ text classification model, with a focus on high accuracy, interpretability, and robustness against adversarial examples for real-world reliability, maintaining an F1-score of at least 0.90.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.90\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"required\"\n            },\n            {\n                \"name\": \"adversarial robustness\",\n                \"value\": null,\n                \"unit\": \"measured by robustness to adversarial examples\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"text classification\"],\n            \"specification\": null,\n            \"description\": \"A text dataset for the classification task, enhancing the original BoolQ model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"required\",\n                \"adversarial robustness\": \"measured\"\n            },\n            \"description\": \"A model for text classification with a focus on interpretability, robustness against adversarial examples, and maintaining an F1-score of 0.90 or higher.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"required to showcase interpretability and robustness\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key aspects of the instruction, including the user's high expertise level, the project enhancement task, the performance metrics (accuracy and F1-score), and complexity requirements. The model's focus on interpretability and adversarial robustness is aligned with the specified constraints. The dataset source and model type are reasonable assumptions based on the context. The missing model name and family are due to lack of specific information, and can be filled with a widely used interpretable model.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for a high-performance machine learning team, your objective is to develop a state-of-the-art tabular regression model using the challenging Mohs Hardness Dataset. The task requires not only minimizing the median absolute error (MedAE) for predicting mineral strength but also ensuring the model demonstrates robustness and generalization across multiple scale intervals and outliers present in the train, validation, and test partitions. Deliver a well-optimized model with interpretable feature importance analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"material science\",\n        \"description\": \"The objective is to develop a state-of-the-art tabular regression model for predicting mineral strength using the Mohs Hardness Dataset. The model must minimize median absolute error (MedAE), exhibit robustness and generalization across multiple scale intervals, and handle outliers in train, validation, and test partitions. A feature of the project is to provide interpretable feature importance analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"median absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"robustness\",\n                \"value\": null,\n                \"unit\": \"resistance to outliers\"\n            },\n            {\n                \"name\": \"generalization\",\n                \"value\": null,\n                \"unit\": \"cross-validation performance\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mohs Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"mineral strength\"],\n            \"specification\": {\n                \"dataset_split\": [\"train\", \"validation\", \"test\"],\n                \"scale_intervals\": true,\n                \"outliers_present\": true\n            },\n            \"description\": \"A challenging dataset for tabular regression, containing mineral strength measurements with potential multi-scale intervals and outliers.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"feature importance analysis\"\n            },\n            \"description\": \"A high-performance tabular regression model built for predicting mineral strength while meeting robustness and generalization requirements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"method\": \"feature importance plots and performance analysis\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's objective for a high-performance machine learning team to build a state-of-the-art tabular regression model, details about the Mohs Hardness Dataset, and the specific requirements for performance metrics and model robustness. The structure, metrics, and selected model type align with the project manager's expertise level.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a machine learning team, enhance the time-series forecasting challenge by incorporating the following constraint: the model must handle seasonality and anomalies while maintaining computational efficiency, using the ILI dataset with train, val, and test splits. The input sequence length is 36, input dimension 7, and the objective is to predict the next 24 steps with 7 dimensions, optimizing for both MSE and MAE with an emphasis on real-time performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"public health (ILI dataset)\",\n        \"description\": \"The project aims to improve a time-series forecasting model for handling seasonality and anomalies in the ILI dataset, while maintaining computational efficiency. The input sequence length is 36, with a 7-dimensional input and a prediction horizon of 24 steps, optimizing for both mean squared error (MSE) and mean absolute error (MAE) with a focus on real-time performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"time and space\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"ILI values for next 24 steps\"],\n            \"specification\": {\n                \"sequence_length\": 36,\n                \"input_dimension\": 7,\n                \"prediction_length\": 24,\n                \"output_dimension\": 7\n            },\n            \"description\": \"A time-series dataset that requires handling seasonality and anomalies with train, val, and test splits.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Stateful/LSTM or similar for time-series\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"computational_efficiency\": \"optimized for real-time performance\"\n            },\n            \"description\": \"A model designed for time-series forecasting, addressing seasonality and anomalies, with a focus on real-time capabilities while optimizing for MSE and MAE.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the essence of the instruction, specifying a build intent with a high level of expertise. The details are centered on enhancing a time-series forecasting model for public health, using the ILI dataset, while handling seasonality, anomalies, and computational efficiency. Performance metrics like MSE and MAE are included, along with a focus on real-time performance. The model family and specification are relevant for the task.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "object detection", "instruction": "For our advanced maritime surveillance system, we require a meticulously fine-tuned DeepLabv3+ model, designed to classify an extensive repertoire of sea vessels based on various attributes like hull length, width, heading, and sail status. The model should be resilient to varying sea conditions, aiming for a remarkable 95% precision. Although not strictly real-time, the deployment must maximize GPU efficiency on cloud infrastructure for optimal performance with minimal latency. Additionally, please present a detailed evaluation plan and explain how the model's robustness will be tested under extreme scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"maritime surveillance\",\n        \"description\": \"The task involves fine-tuning a DeepLabv3+ model for classifying sea vessels based on hull length, width, heading, and sail status. The model must be robust to varying sea conditions and achieve at least 95% precision. The deployment should prioritize GPU efficiency on cloud infrastructure for minimal latency, but not strictly real-time.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"GPU efficiency\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Maritime Surveillance Vessels Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"hull length\", \"width\", \"heading\", \"sail status\"],\n            \"specification\": {\n                \"image_dimensions\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"attribute_types\": [\n                    \"numerical\",\n                    \"numerical\",\n                    \"angle\",\n                    \"categorical\"\n                ]\n            },\n            \"description\": \"A dataset of maritime images with attributes to train the model for classifying vessels based on specified attributes.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"image rotation\",\n                \"zoom\",\n                \"lighting variations\"\n            ],\n            \"visualization\": [\n                \"dataset examples\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepLabv3+\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_depth\": \"resnet50\",\n                \"fine-tuning_layers\": \"top layers\",\n                \"input_image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"GPU_usage\": \"optimized for cloud infrastructure\"\n            },\n            \"description\": \"A DeepLabv3+ model fine-tuned for maritime vessel classification under varying conditions.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"API server\",\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"extreme scenario testing videos with results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure is well-aligned with the given instruction. It covers building a high-expertise level model for maritime surveillance, fine-tuning the DeepLabv3+ model, target performance metrics (95% precision), GPU efficiency, and latency constraints. The source dataset, model specifications, and evaluation plans match the task requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "For the upcoming Amazon E-commerce network analysis, design a state-of-the-art GraphSAGE model with FastGCN adaptation, focusing on fine-grained product category classification. The model should exhibit at least 95% micro-F1 score on a large, real-time evolving graph, ensuring sublinear time complexity for efficient updates in a high-traffic environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"The task is to design a state-of-the-art GraphSAGE model with FastGCN adaptation for fine-grained product category classification in the Amazon E-commerce network. The model must achieve at least 95% micro-F1 score and have sublinear time complexity for real-time updates in a high-traffic environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"micro-F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"time complexity\",\n                \"value\": null,\n                \"unit\": \"sublinear\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Amazon E-commerce Network (GraphSAGE)\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [\n                \"product categories\"\n            ],\n            \"specification\": {\n                \"size\": \"large\",\n                \"real-time evolving\": true,\n                \"traffic_environment\": \"high\"\n            },\n            \"description\": \"A large, real-time evolving graph dataset for product category classification in the Amazon E-commerce network.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GraphSAGE\",\n            \"family\": \"Graph Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptation\": \"FastGCN\"\n            },\n            \"description\": \"A state-of-the-art model designed for fine-grained product category classification in the Amazon E-commerce network.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model should utilize FastGCN adaptation for efficient computation on large-scale graphs.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Real-time updates in a high-traffic environment\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's instruction by capturing the build intent with high expertise, the area of graph machine learning, the downstream task of node classification, the performance requirement for the micro-F1 score, and the complexity metrics. The details of the dataset and model are tailored to the Amazon E-commerce network analysis scenario. The FastGCN adaptation and time complexity are captured in separate objects for clarity. The confidence score is high due to the thorough mapping of the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Design a real-time, topic-diverse visual question-answering system using YouTube-8M, integrating specifically tailored 3D spatio-temporal ConvNet and multi-modal Transformer architectures. Prioritize cross-modal fusion for scene understanding, factoid and temporal question analysis, and handle fast-changing video content from tech demos to wildlife documentaries while maintaining high precision and response freshness.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \" multimedia and AI\",\n        \"description\": \"The project aims to design a real-time visual question answering system using YouTube-8M dataset. The system incorporates 3D spatio-temporal ConvNet and multi-modal Transformer architectures, prioritizing cross-modal fusion for scene understanding, factoid and temporal question analysis. It must handle diverse video content from tech demos to wildlife documentaries, maintaining high precision and fast response times.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response freshness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"frames per second\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube-8M\",\n            \"modality\": [\n                \"image\",\n                \"video\",\n                \"audio\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"video_duration\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Range of video lengths\"\n                },\n                \"frame_rate\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Video frames per second\"\n                },\n                \"audio_channels\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Number of audio channels\"\n                }\n            },\n            \"description\": \"A large-scale dataset containing diverse video content for visual question answering.\",\n            \"preprocessing\": [\n                \"3D spatio-temporal feature extraction\",\n                \"audio and text preprocessing\"\n            ],\n            \"augmentation\": [\n                \"video frame random sampling\",\n                \"audio noise addition\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"3D spatio-temporal ConvNet\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"custom tailored for video understanding\"\n            },\n            \"description\": \"A component for scene understanding and temporal information extraction.\"\n        },\n        {\n            \"name\": \"Multi-modal Transformer\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"cross-modal attention mechanism\": \"architecture detail\"\n            },\n            \"description\": \"A module for cross-modal fusion and question analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cross-modal fusion and rapid adaptation to changing content is crucial.\",\n        \"Factoid question analysis and handling diverse video content are challenging.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"real-time updating for dynamic context\",\n            \"efficient querying for fresh content\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for edge or cloud deployment\"\n        ],\n        \"demonstration\": \"interactive and dynamic Q&A demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the user's intent to build a complex machine learning system, utilizing their high expertise level. The problem area, downstream tasks, and performance metrics align with the visual question-answering system's requirements. Specifics about the dataset, models, and system constraints reflect the project's details. Missing values are marked for further customization.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "Imagine a future where sustainable luxury fashion demands not only precision in identifying eco-friendly materials and artisanal craftsmanship, but also discerning micro-attributes like vintage influences, artisanal dye techniques, and regional provenance. Develop a next-generation image classifier for a high-end boutique that not only classifies garments by traditional categories but can also differentiate between unique fabric patterns, embroidery motifs, and finishes, ensuring each item tells a story on the digital shelf and enhances the customer's discovery journey.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion, sustainability\",\n        \"description\": \"The task is to develop a highly advanced image classifier for a sustainable luxury fashion boutique. The classifier should identify garments by traditional categories, but also differentiate between unique fabric patterns, embroidery motifs, and finishes, reflecting vintage influences, artisanal dye techniques, and regional provenance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"fine-grained accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall for unique patterns and finishes\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Luxury Eco-Friendly Fashion Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"garment category\", \"unique patterns\", \"embroidery motifs\", \"finishes\"],\n            \"specification\": {\n                \"size\": null,\n                \"image_dimensions\": null,\n                \"modalities_depth\": null\n            },\n            \"description\": \"A dataset containing high-end garments with varying eco-friendly materials, artisanal techniques, and regional diversity, for training and evaluation.\",\n            \"preprocessing\": [\"feature extraction for fine-grained analysis\"],\n            \"augmentation\": [\"style transfer, regional context preservation\"],\n            \"visualization\": [\"pattern and motif heatmaps\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Next-Gen Fine-Grained Classifier\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"architecture\": \"fine-grained image recognition\",\n                \"layers\": null,\n                \"accuracy_expected\": \"high\",\n                \"frozen_layers\": \"initial layers\"\n            },\n            \"description\": \"A specialized image classifier designed to recognize unique patterns, motifs, and finishes for luxury eco-friendly fashion.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Consider the use of transfer learning from similar high-end fashion datasets and deep learning techniques for recognizing micro-attributes.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"material recognition\", \"artifact history analysis\"],\n        \"target_device\": \"cloud-based for scalability\",\n        \"deployment_endpoint\": \"boutique's online store API\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"custom filters and recommendations based on unique attributes\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a sophisticated image classifier, reflects their high expertise level, and includes fine-grained requirements for luxury fashion. It incorporates performance metrics and complexity considerations relevant to the task. The source dataset is specified as requiring a user-provided link, and certain steps in preprocessing and augmentation are mentioned, as well as knowledge shared for improved model design.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "For the multilingual QA project, mandate the development team to integrate a mBERT and XLM-R fusion model, achieving not only cross-lingual fluency in English, Spanish, and Mandarin, but also demonstrating exceptional performance in under-resourced dialects. Prioritize maintaining balanced accuracy and robustness across all language variations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"cross-lingual\",\n        \"description\": \"The project focuses on multilingual question answering, specifically targeting English, Spanish, and Mandarin, while also addressing under-resourced dialects. It requires integrating a mBERT and XLM-R fusion model for exceptional performance, with a priority on balanced accuracy and robustness across all language variations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"balanced accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory size for under-resourced dialects\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multilingual Question Answering Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\"],\n            \"specification\": {\n                \"languages\": [\"English\", \"Spanish\", \"Mandarin\", \"under-resourced dialects\"]\n            },\n            \"description\": \"A dataset for multilingual question answering that covers English, Spanish, Mandarin, and a range of under-resourced dialects.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"mBERT-XLM-R fusion model\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"cross-lingual fluency\": \"English, Spanish, Mandarin\",\n                \"performance for under-resourced dialects\": \"optimized\"\n            },\n            \"description\": \"A model integrating mBERT and XLM-R to deliver strong performance in multilingual QA, with a focus on handling under-resourced dialects.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Balanced accuracy and robustness across diverse language variations will be showcased through detailed analysis and testing.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's intent, expertise, and the specific requirements for the multilingual QA project. It includes the correct area (NLP) and downstream task (QA). Performance metrics like balanced accuracy and robustness are prioritized, with a detailed description of the model's capabilities and the dataset's characteristics. The complexity metric is added to address the under-resourced dialects. The confidence score reflects high confidence as the information is clearly specified.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "The urban planning bureau faces a multitude of challenges in managing smart city infrastructure, particularly concerning energy efficiency, public transportation, and waste management. Develop a cutting-edge question-answering system that not only provides real-time insights on the carbon footprint reduction strategies for a specific neighborhood, but also suggests the most efficient routes for electric buses based on current traffic patterns and renewable energy sources availability. This AI-driven solution must also predict future waste disposal capacity and recommend sustainable waste management practices to achieve the city's zero-waste goal.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"urban planning and smart city infrastructure\",\n        \"description\": \"The task is to design a question-answering system for a smart city infrastructure management, focusing on energy efficiency, public transportation, and waste management. The system should provide real-time insights on carbon footprint reduction, suggest energy-efficient bus routes, predict waste disposal capacity, and recommend sustainable waste management practices for a specific neighborhood.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time response time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy on carbon footprint reduction strategies\",\n                \"value\": null\n            },\n            {\n                \"name\": \"efficiency in suggesting electric bus routes\",\n                \"value\": null\n            },\n            {\n                \"name\": \"prediction accuracy for future waste disposal capacity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recommendation precision for sustainable waste management\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"system scalability\",\n                \"value\": null,\n                \"unit\": \"number of queries per second\"\n            },\n            {\n                \"name\": \"memory consumption for real-time data processing\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Smart City Infrastructure Data\",\n            \"modality\": [\"text\", \"time series\"],\n            \"target_variables\": [\"carbon footprint data\", \"traffic patterns\", \"renewable energy sources\", \"waste management data\"],\n            \"specification\": null,\n            \"description\": \"Dataset containing data on energy usage, transportation, and waste management for a specific neighborhood, including real-time and historical data needed for model training and system development.\",\n            \"preprocessing\": [\n                \"Data cleaning, normalization, and data fusion\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"Visualizing energy consumption patterns, traffic flow, and waste generation trends\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Question-Answering AI System\",\n            \"family\": \"Neural Question Answering\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-based model\",\n                \"model size\": null,\n                \"pre-trained or from scratch\": \"pre-trained\",\n                \"adaptation method\": \"fine-tuning\"\n            },\n            \"description\": \"A state-of-the-art AI system to process and analyze smart city infrastructure data for providing real-time insights, route suggestions, and waste management recommendations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Familiarity with real-time data processing, renewable energy resources, and sustainability principles is crucial.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Data integration, feature extraction, and feature engineering for improved performance\"],\n        \"target_device\": \"cloud-based or edge\",\n        \"deployment_endpoint\": \"city infrastructure management dashboard or API\",\n        \"inference_engine\": [\"GPU-powered inference for real-time decision support\"],\n        \"demonstration\": \"Live demos showcasing the system's performance and impact in a simulated smart neighborhood\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately represents the complex nature of the problem statement, focusing on multiple aspects of urban planning and highlighting the sophistication required for a high-expertise level user. The key elements such as the system's functionality, performance metrics, and data requirements are adequately captured. However, some metric values are left as null to indicate that they'll be determined by detailed analysis.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Develop a cutting-edge visual Q&A system for the YouTube-8M dataset, focusing on real-time adaptation to niche topics like scientific experiments and historical reenactments. Utilize a fusion of Squeeze3D and MViT for precise 3D event extraction, temporal reasoning in Swin-Transformer, and cross-modal alignment in the LXMERT architecture. Ensure seamless integration for diverse content, maintaining millisecond response times and topical relevance even with rapid video shifts between tech demos and wildlife documentaries.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"video content analysis\",\n        \"description\": \"The task is to develop a visual Q&A system for the YouTube-8M dataset, focusing on real-time adaptation to niche topics like scientific experiments and historical reenactments. Key features include the fusion of Squeeze3D and MViT for 3D event extraction, temporal reasoning with Swin-Transformer, and cross-modal alignment through LXMERT. The system must maintain millisecond response times and topical relevance amidst rapid video content shifts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": \"millisecond\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"response time consistency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube-8M\",\n            \"modality\": [\n                \"video\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"content_types\": [\"scientific experiments\", \"historical reenactments\"]\n            },\n            \"description\": \"A large-scale dataset for visual question answering, with a focus on diverse content, including niche topics.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Squeeze3D + MViT fusion\",\n            \"family\": \"Multi-modal Architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"3D event extraction\": null,\n                \"temporal reasoning\": \"Swin-Transformer\",\n                \"cross-modal alignment\": \"LXMERT\"\n            },\n            \"description\": \"A combination of deep learning models for precise video understanding in a real-time, niche topic context.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time adaptation to niche topics requires monitoring and updating the system's context with new data to stay relevant.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"Support for seamless, rapid response to video shifts across different topics.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a complex visual Q&A system, considering their high expertise level. It includes relevant details about the multimodal architecture, dataset, and performance requirements. The complexities are noted, and the niche topic adaptation is a notable knowledge point. The chosen deployment environment and inference engine reflect the need for fast and efficient response.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Instruct the ML team to develop an advanced time-series forecasting model for a pandemic influenza prediction system. The model must efficiently handle weekly fluctuations and sudden outbreaks, leveraging the 2020-2022 CDC ILI dataset with partitions for 80% training, 10% validation, and 10% testing. Each input sequence of 36 weekly symptom counts should have a 7-dimensional feature set, aiming to forecast 24 future steps with high accuracy (MSE and MAE) while ensuring real-time responsiveness for rapid public health interventions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"public health\",\n        \"description\": \"The task is to create an advanced time-series forecasting model for pandemic influenza prediction, considering weekly fluctuations and sudden outbreaks. The model should be trained using the 2020-2022 CDC ILI dataset, split into 80% train, 10% validation, and 10% test sets. Input sequences consist of 36 weekly symptom counts, and the model should forecast 24 future steps with high accuracy, optimizing for both mean squared error (MSE) and mean absolute error (MAE). Real-time responsiveness is a critical requirement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"real-time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"2020-2022 CDC ILI dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"ILI (influenza-like illness)\"],\n            \"specification\": {\n                \"sequence_length\": 36,\n                \"feature_dimensions\": 7,\n                \"sequence_type\": \"weekly symptom counts\",\n                \"split_distribution\": [\"80% train\", \"10% validation\", \"10% testing\"]\n            },\n            \"description\": \"The dataset for training, validation, and testing the model with a 36-week input sequence and 7-dimensional feature set.\",\n            \"preprocessing\": [\n                \"Data normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Time-Series Forecasting Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"RNN/LSTM/Transformer variant\",\n                \"number_of_layers\": \"...\",\n                \"hidden_units\": \"...\",\n                \"input_shape\": \"(36, 7)\",\n                \"output_length\": 24\n            },\n            \"description\": \"A state-of-the-art model designed for accurate time-series forecasting, considering real-time responsiveness for pandemic influenza prediction.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"Feature engineering to capture weekly patterns\"],\n        \"target_device\": \"cloud or edge\",\n        \"deployment_endpoint\": \"Publicly accessible API or private health dashboard\",\n        \"inference_engine\": [\"Real-time prediction using optimized inference engines\"],\n        \"demonstration\": \"Live demonstrations or simulations to showcase model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the details of the user's instruction, including the model requirements for handling weekly fluctuations, accuracy metrics, input sequence characteristics, and real-time responsiveness. The 'high' expertise level reflects the expected level of technical complexity in building such a model. Some aspects, like model architecture specifics and potential deployment options, are open-ended for flexibility based on the team's capabilities.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager for a pioneering deep learning research group, you've been handed the challenge of optimizing the ZephyrNet text classification project. Not only should the model excel in identifying sentiment with unprecedented precision (>=0.98), but also necessitate the implementation of a neuro-symbolic architecture that enables Explainable AI, resists adversarial attacks with a defended Evasion Accuracy of at least 95%, and efficiently handles multilingual data with minimal performance degradation, all while maintaining real-time responsiveness for large-scale corpus analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"sentiment analysis and AI explainability\",\n        \"description\": \"The project aims to optimize ZephyrNet for sentiment classification with precision >= 0.98, using a neuro-symbolic architecture for Explainable AI, defending against adversarial attacks with Evasion Accuracy >= 95%, and maintaining real-time responsiveness for multilingual data with minimal performance degradation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"explained in EAI\",\n                \"value\": 1.0\n            },\n            {\n                \"name\": \"Evasion Accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time responsiveness\",\n                \"value\": null, \n                \"unit\": \"time constraints (e.g., milliseconds)\"\n            },\n            {\n                \"name\": \"multilingual performance degradation\",\n                \"value\": 0.0, \n                \"unit\": \"percentage\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ZephyrNet Text Classification Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\"],\n            \"specification\": null,\n            \"description\": \"A large-scale corpus for sentiment analysis, supporting multiple languages\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ZephyrNet\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainable AI\": true,\n                \"adversarial defense\": true,\n                \"multilingual capabilities\": true\n            },\n            \"description\": \"A model designed for text classification, with a neuro-symbolic architecture for Explainable AI and adversarial defense.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The project demands state-of-the-art performance and Explainable AI to ensure transparency and robustness.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"real-time processing engines\"],\n        \"demonstration\": \"Large-scale corpus analysis with real-time responsiveness\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the high-level requirements for the ZephyrNet project, including performance targets and architectural constraints. It fills in the expected fields, such as expert level, specifics of the model architecture, and certain aspects of the service expectations like cloud deployment and real-time processing.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for an elite underwater ecosystem mapping team, we require a novel deep learning architecture, marrying the capabilities of a Cascade-U-Net and GhostNet, tailored to the challenging OceanBeds360 dataset. This model must excel in distinguishing between 50 extremely fine-grained marine species, achieving a mean Intersection over Union (IoU) of 98% in low-light conditions, without sacrificing submersible autonomy (operating efficiently with a power consumption of 10 watts or less). Moreover, design a comparative study to investigate the impact of transfer learning, data augmentation, and mixed-precision training on the model's performance and energy efficiency, ensuring compatibility with resource-constrained, edge computing devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"oceanography\",\n        \"description\": \"The task is to develop a novel deep learning model combining Cascade-U-Net and GhostNet for underwater ecosystem mapping, specifically the OceanBeds360 dataset. The model must classify 50 marine species with an IoU of 98% in low-light conditions while operating efficiently with power consumption less than 10 watts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean Intersection over Union (IoU)\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"power consumption\",\n                \"value\": 10,\n                \"unit\": \"watts\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OceanBeds360\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"marine species\"],\n            \"specification\": null,\n            \"description\": \"A dataset for underwater ecosystem mapping, requiring classification of 50 fine-grained marine species with emphasis on low-light conditions and resource constraints.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Cascade-U-Net + GhostNet hybrid\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"energy_efficiency\": \"10 watts or less\"\n            },\n            \"description\": \"A deep learning architecture combining Cascade-U-Net and GhostNet for underwater species classification and energy-efficient operation.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Investigating the impact of transfer learning, data augmentation, and mixed-precision training on performance and energy efficiency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"transfer learning\", \"data augmentation\", \"mixed-precision training\"],\n        \"target_device\": \"edge computing devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Compatibility with resource-constrained devices and study on performance & energy efficiency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's requirements for a high-level machine learning project. It captures the need for a specialized deep learning model, the target dataset, performance metrics, and efficiency constraints. Key aspects of the comparative study are also included as service requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "For a sophisticated image recognition project, develop a sophisticated convolutional neural network that not only categorizes photos as either cats or dogs but demands a high degree of accuracy in differentiating between minimum 50 diverse cat breeds and a minimum of 70 distinct dog breeds. Emphasize on the integration of multi-resolution feature maps, transfer learning with advanced architectures, and fine-tuning strategies to capture subtle breed-specific nuances for superior performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"computer vision applications\",\n        \"description\": \"The project aims to develop a highly accurate convolutional neural network for image recognition, focusing specifically on distinguishing between at least 50 different cat breeds and 70 different dog breeds. It requires the integration of multi-resolution feature maps, transfer learning with advanced architectures like ResNet or VGG, and fine-tuning strategies for superior breed-specific identification.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Custom Cats vs Dogs + Breed dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"category (cat, dog)\", \"breed\"],\n            \"specification\": {\n                \"image_resolution\": \"variable\",\n                \"breed_diversity\": {\n                    \"cat_breeds\": 50,\n                    \"dog_breeds\": 70\n                }\n            },\n            \"description\": \"A dataset for image classification, containing photos of at least 50 different cat breeds and 70 different dog breeds. Emphasis is on capturing breed-specific nuances.\",\n            \"preprocessing\": [\"multi-resolution feature map extraction\", \"image normalization\"],\n            \"augmentation\": [\"data augmentation for breed diversity\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced CNN (e.g., ResNet or VGG) with fine-tuning\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ResNet or VGG\",\n                \"transfer_learning\": true,\n                \"fine_tuning_layers\": \"entire network or selected layers\"\n            },\n            \"description\": \"A sophisticated model designed for image recognition, leveraging multi-resolution feature maps and advanced architectures for better breed differentiation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Use of multi-scale receptive fields, transfer learning to leverage large-scale pre-trained models, and fine-tuning to adapt to breed-specific details.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"multi-scale feature extraction\",\n            \"image feature extraction from ResNet or VGG\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow.js\", \"ONNX Runtime\"],\n        \"demonstration\": \"Sample breed identification results and visual explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the requirements of a complex image classification project with emphasis on breed differentiation. The user's high expertise level is reflected, and the performance metrics, complexity constraints, and dataset specifics are aligned with the project description. Transfer learning, fine-tuning, and multi-resolution feature maps are key components that were integrated in the JSON response.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the advanced text classification project, your team must develop a state-of-the-art system that not only improves the existing ChatGPT prompt dataset but also introduces a novel multi-label taxonomy with 12 distinct categories. Emphasize on achieving exceptional precision and recall, especially for underrepresented labels, while maintaining a custom transformer model with minimal pre-trained components. Implement an online evaluation protocol for real-time monitoring of macro-F1 and fairness metrics, all while adhering to strict resource limitations that prioritize energy efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"language technology\",\n        \"description\": \"The project aims to develop a state-of-the-art text classification system that enhances the ChatGPT dataset by introducing a novel multi-label taxonomy with 12 categories. Key focus is on precision and recall, especially for underrepresented labels, using a custom transformer model with minimal pre-trained components. An online evaluation protocol for real-time monitoring of macro-F1 and fairness metrics is required, while maintaining strict resource limitations for energy efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"macro-F1\",\n                \"value\": null\n            },\n            {\n                \"name\": \"fairness metrics\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"pre-trained components\",\n                \"value\": null,\n                \"unit\": \"percentage of pre-training\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"energy consumption per classification\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enhanced ChatGPT Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"multi-label categories\"],\n            \"specification\": null,\n            \"description\": \"An extended dataset with a novel multi-label taxonomy, aiming to improve upon the existing ChatGPT prompt dataset.\",\n            \"preprocessing\": [\"enhancement of the original dataset\", \"introduction of novel multi-label taxonomy\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Transformer\",\n            \"family\": \"Transformer model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pre-trained components\": \"minimal\",\n                \"resource optimization\": \"energy-efficient\"\n            },\n            \"description\": \"A custom transformer designed for text classification with a focus on minimal pre-training and energy efficiency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"custom taxonomy development\", \"label balancing\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"online evaluation platform\",\n        \"inference_engine\": [\"real-time monitoring\"],\n        \"demonstration\": \"deployment demonstration with macro-F1 and fairness metrics updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the project requirements accurately, including a high-level intent, emphasis on the task, novel requirements like multi-label taxonomy and energy efficiency, and specific performance metrics. However, specific metric values and some technical details, like fairness metrics and exact percentages of pre-trained components, are yet to be determined based on project research or team's findings.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "The project manager is tasked with compelling the AI development team to elevate the current image recognition project by implementing a cutting-edge, multi-task Capsule Network in the fashion domain for the Fashion-MNIST dataset. They must aim for a superlative accuracy of 97%, ensuring resilience to extensive affine distortions. To demonstrate their prowess, the report must include a comprehensive comparison of capsule network performance versus CNNs, highlighting advancements in computational speed and exceptional rotational invariance, all while employing transfer learning from a pre-trained VGG16 model for benchmarking.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"The project aims to enhance an image recognition project by implementing a Capsule Network in the fashion domain using the Fashion-MNIST dataset. The target accuracy is 97%, and the network should be resilient to extensive affine distortions. Comparison with CNNs, focusing on computational speed and rotational invariance, will be included, along with transfer learning using a pre-trained VGG16 model for benchmarking.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"fashion item classification\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": 28,\n                    \"height\": 28\n                },\n                \"number_of_classes\": 10\n            },\n            \"description\": \"A fashion image dataset for benchmarking with a focus on enhancing image recognition through capsule networks.\",\n            \"preprocessing\": [\"affine distortions preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"comparison of capsule network vs CNN performance\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Capsule Network\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"capsule network with multi-task capabilities\",\n                \"transfer_learning\": \"VGG16\"\n            },\n            \"description\": \"A cutting-edge model for fashion image classification, leveraging capsule network and transfer learning from VGG16 for high accuracy and improved resilience.\"\n        },\n        {\n            \"name\": \"CNN comparison (VGG16)\",\n            \"description\": \"Benchmarking model using a convolutional neural network (CNN) with VGG16 architecture\"\n        }\n    ],\n    \"knowledge\": [\n        \"Demonstrate the advantages of capsule networks in computational speed and rotational invariance compared to CNNs.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Comparison report showcasing capsule network's performance and benefits\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the instruction, capturing the project manager's goal to improve the image recognition project with a Capsule Network, the target accuracy, and the use of a pre-trained VGG16 model. The comparison of capsule network performance to CNNs, computational speed, and rotational invariance are appropriately reflected in the 'knowledge' and 'model' sections. The specified dataset and source are also appropriately included.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager, your team must upgrade the text classification project by incorporating the WebMD Review dataset, a sentiment lexicon for fine-grained emotion detection, and a focus on sarcasm identification. The objective is to achieve a state-of-the-art model with optimized macro-average F1-score for imbalanced classes, without compromising on real-time processing and ensuring inter-class diversity in evaluation metrics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The project aims to upgrade a text classification model by incorporating the WebMD Review dataset, focusing on sarcasm identification and fine-grained emotion detection. The objective is to achieve a state-of-the-art model with optimized macro-average F1-score for imbalanced classes, while maintaining real-time processing and diversity in evaluation metrics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"macro-average F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": 1.0\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WebMD Review Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\", \"emotion\", \"sarcasm\"],\n            \"specification\": null,\n            \"description\": \"A dataset for text classification, including fine-grained emotion detection and sarcasm identification, to be used for model improvement.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"state-of-the-art\": true,\n                \"real-time_inference\": true,\n                \"diversity_in_classes\": true\n            },\n            \"description\": \"A cutting-edge model for text classification, incorporating state-of-the-art techniques for sarcasm detection and fine-grained emotion analysis, optimized for macro-average F1-score in imbalanced classes.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The dataset contains data from WebMD, a well-known healthcare platform.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Include diverse examples for model performance across different classes\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the given instruction by capturing the project upgrade requirements, focusing on the WebMD Review dataset, sentiment lexicon, sarcasm detection, optimizing F1-score, and maintaining real-time processing. Key aspects, such as the state-of-the-art model and the emphasis on inter-class diversity, have been addressed in the response.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "Design an advanced fusion of autoencoders, graph neural networks (GNNs), and a novel attention mechanism specifically tailored for the underwater acoustic time-series classification of krill migrations, considering the challenges of low-frequency data, noise contamination, and multi-modalities. The architecture must optimize for both inter-species discrimination and intra-species variation, with a focus on real-time detection for sustainable marine resource management. Additionally, evaluate the model's robustness in the presence of hydrodynamic variations and cross-Seasonal data, aiming for high precision and recall in a maritime surveillance application.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"marine science\",\n        \"description\": \"The task is to design an advanced fusion of autoencoders, GNNs, and an attention mechanism for underwater acoustic time-series data of krill migrations. The model should address challenges like low-frequency data, noise, multi-modalities, inter-species discrimination, intra-species variation, real-time detection, and robustness against hydrodynamic variations and cross-seasonal data. High precision and recall are the main performance metrics for a maritime surveillance application.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time detection\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Underwater Acoustic Krill Migration Dataset\",\n            \"modality\": [\"time series\", \"audio\"],\n            \"target_variables\": [\"krill species\", \"migrations\"],\n            \"specification\": {\n                \"data_frequency\": \"low\",\n                \"noise_level\": \"high\",\n                \"modalities\": [\"acoustic\", \"multi-modal\"]\n            },\n            \"description\": \"A dataset containing time-series data of underwater acoustic signals for krill migrations with challenges such as low-frequency data and noise contamination.\",\n            \"preprocessing\": [\"denoising\", \"feature extraction\"],\n            \"augmentation\": [\"time warping\", \"resampling\"],\n            \"visualization\": [\"spectrogram analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Autoencoder-GNN-Attention Fusion\",\n            \"family\": \"Advanced Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"autoencoder\": {},\n                    \"GNN\": {},\n                    \"attention\": \"tailored for krill migrations\"\n                },\n                \"optimization\": {\n                    \"inter-species\": \"discrimination\",\n                    \"intra-species\": \"variation\"\n                },\n                \"real-time\": true,\n                \"hydrodynamic_variations\": \"robust\",\n                \"seasonality\": \"cross\"\n            },\n            \"description\": \"A deep learning architecture that combines autoencoders, GNNs, and a custom attention mechanism for underwater acoustic time-series classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Challenges like low-frequency data and noise require robust preprocessing techniques.\",\n        \"Incorporating GNNs for spatial information in acoustic data.\",\n        \"Custom attention to handle multi-modalities.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"preprocessing and feature extraction\"],\n        \"target_device\": \"real-time inference in maritime surveillance systems\",\n        \"deployment_endpoint\": \"(sea-based or terrestrial monitoring platform)\",\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": \"dynamic visualization of krill migrations and classification results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately represents the instruction, including the advanced machine learning components, focus on real-time detection, and the specific marine science task. Performance metrics and complexities are specified, and the user's high expertise level is reflected.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Act as a project manager for our elite computer vision team, focusing on the niche task of real-time, pixel-level car segmentation in the challenging Cityscapes dataset. Mandate a multi-step strategy: precursor image enhancement using HDR techniques, advanced feature extraction with deep residual networks, and integrating a custom Tiny-YOLOv4 architecture for optimal speed while distinguishing individual cars amidst intricate urban layouts and dynamic lighting conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"autonomous driving and urban planning\",\n        \"description\": \"The project aims to develop a real-time system for pixel-level car segmentation in the Cityscapes dataset, addressing complex urban layouts and dynamic lighting conditions. The strategy involves precursor image enhancement using HDR techniques, advanced feature extraction with deep residual networks, and integrating a custom Tiny-YOLOv4 architecture for improved speed and accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mIoU (mean Intersection over Union)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"FPS (Frames per Second)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"frames per second\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"pixel-level car segmentation masks\"],\n            \"specification\": {\n                \"dimensions\": {\n                    \"height\": null,\n                    \"width\": null,\n                    \"depth\": null\n                },\n                \"training examples\": null\n            },\n            \"description\": \"A challenging urban street scene dataset for car segmentation, requiring robustness to different lighting conditions and intricate urban layouts.\",\n            \"preprocessing\": [\n                {\n                    \"method\": \"HDR techniques\"\n                }\n            ],\n            \"augmentation\": [\n                {\n                    \"method\": \"dynamic lighting adaptation\"\n                }\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Tiny-YOLOv4 with custom architecture\",\n            \"family\": \"YOLO family\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep residual networks for feature extraction, Tiny-YOLOv4 for real-time segmentation\",\n                \"parameters\": null,\n                \"FLOPs\": null,\n                \"speed\": null,\n                \"memory footprint\": null\n            },\n            \"description\": \"A custom implementation integrating residual networks for advanced feature extraction and Tiny-YOLOv4 for optimal car segmentation in real-time scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"cityscape-specific domain knowledge\",\n        \"HDR techniques for image enhancement\",\n        \"dealing with dynamic lighting conditions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"deep residual feature extraction\"],\n        \"target_device\": \"real-time performance-oriented hardware\",\n        \"deployment_endpoint\": \"custom edge computing platform\",\n        \"inference_engine\": [\n            \"optimized for GPU\"\n        ],\n        \"demonstration\": \"real-time car segmentation in Cityscapes demo videos\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project manager role and details the project requirements, including advanced techniques, dataset, and performance metrics. It assumes a high level of expertise from the user. The missing metric values are placeholders, indicating they need to be specified based on the project's exact goals.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project director for a high-end fashion e-commerce platform, you now demand a state-of-the-art object detection system with fine-grained recognition of diverse luxury garments. mandate the team to research and propose not just a single dataset and architecture, but an ensemble of tailored datasets, resilient deep learning models optimized for real-time performance, and a multi-stage deployment plan that integrates with existing infrastructure while ensuring minimum latency and continuous monitoring for accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"fashion e-commerce\",\n        \"description\": \"The task is to develop a state-of-the-art object detection system with fine-grained recognition for luxury garments, requiring an ensemble of tailored datasets, efficient real-time models, and a multi-stage deployment plan integrating with the platform while minimizing latency and ensuring continuous accuracy monitoring.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"garment type\"],\n            \"specification\": null,\n            \"description\": \"An ensemble of tailored datasets specifically for luxury garment object detection, diverse and likely to be obtained through 'infer-search' or 'direct-search'.\",\n            \"preprocessing\": [\"data augmentation for real-world scenarios\"],\n            \"augmentation\": [\"augmentation techniques for fine-grained recognition\"],\n            \"visualization\": [\"dataset exploration and annotation process visualization\"],\n            \"source\": \"infer-search\"\n        },\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ensemble of deep learning models\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization for real-time performance\": null\n            },\n            \"description\": \"A collection of robust and fine-tuned deep learning architectures designed for object detection and real-time efficiency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection for optimized models\",\n            \"real-time inference pipeline optimization\"\n        ],\n        \"target_device\": [\"cloud, mobile, and edge devices\"],\n        \"deployment_endpoint\": \"integrated with existing e-commerce platform infrastructure\",\n        \"inference_engine\": [\"real-time object detection libraries (e.g., TensorFlow, PyTorch)\"],\n        \"demonstration\": \"demonstration of low-latency and accurate garment detection in the platform\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction, indicating the need to build an object detection system, ensemble of datasets, efficient models, and a multi-stage deployment plan. Expertise level is assumed to be high given the project director's role. Performance metrics focus on real-time performance and accuracy, with specific mention of latency minimization. Data sources are anticipated based on the nature of the project requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Revise the WaveNet model to include a multi-stage deep learning pipeline: first, pre-train on an extensive, labeled dataset of urban sounds with multiple environmental conditions. Next, fine-tune for sound level forecasting over a 72-hour horizon with hourly resolution, while accounting for temporal dependencies using dilated causal convolution. Additionally, design an advanced feature extraction module to classify and cluster sounds in real-time, providing heatmaps for various pollution sources. This project demands not only predictive accuracy but also interactive visualizations for city planners to swiftly identify and address environmental concerns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"environmental monitoring\",\n        \"description\": \"The project involves revising the WaveNet model with a multi-stage pipeline: pre-training on an extensive labeled dataset of urban sounds under various conditions, fine-tuning for sound level forecasting with a 72-hour horizon at hourly resolution, and implementing a feature extraction module for real-time classification and clustering of sounds for pollution source identification. Interactive visualizations are crucial for city planners.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"forecast accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extensive Urban Sounds Dataset\",\n            \"modality\": [\"audio\"],\n            \"target_variables\": [\"sound level\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": {\n                    \"time\": null,\n                    \"frequency\": null\n                },\n                \"label_distribution\": {}\n            },\n            \"description\": \"An extensive dataset for pre-training, featuring multiple environmental conditions.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"labeling for sound classification\"\n            ],\n            \"augmentation\": [\n                \"temporal and environmental conditions variation\"\n            ],\n            \"visualization\": [\n                \"sound level heatmaps\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"WaveNet with multi-stage pipeline\",\n            \"family\": \"Recurrent Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pre-training_epochs\": null,\n                \"fine-tuning_epochs\": null,\n                \"dilated causal convolution\": {\n                    \"time_steps\": null,\n                    \"kernel_size\": null\n                }\n            },\n            \"description\": \"A deep learning model with a multi-stage process for sound forecasting and real-time feature extraction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced temporal dependencies handling for time-series analysis\",\n        \"Interactive visualizations for real-time monitoring and decision-making\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal feature extraction\",\n            \"environmental feature engineering\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"city planner dashboard\",\n        \"inference_engine\": [\n            \"real-time processing\",\n            \"streaming\"\n        ],\n        \"demonstration\": \"Interactive heatmaps and forecasts for pollution sources\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's instruction with a high expertise level. It details the steps of the multi-stage pipeline, adjusts the model's focus, includes required metrics, and accounts for complexity. It assumes the user's intention to build a model and emphasizes the importance of visualization. Missing specifics are due to the high-level nature of the request.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "link prediction", "instruction": "Sure, I've taken the original prompt and adapted it for a more complex version focused on link prediction in extremophile microbiology research, using a scarce labeled dataset such as the Escherichia coli collaboration network with distinctive taxonomic subgroups and considering temporal dynamics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"extremophile microbiology\",\n        \"description\": \"The task involves developing a model for link prediction in the Escherichia coli collaboration network, considering temporal dynamics and the challenge of a scarce labeled dataset with distinctive taxonomic subgroups.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"area under the ROC curve\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Escherichia coli collaboration network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"collaboration links\"],\n            \"specification\": {\n                \"size\": \"sparse (due to scarcity)\",\n                \"time_series_length\": null,\n                \"taxonomic_subgroups\": true\n            },\n            \"description\": \"A limited labeled dataset representing the collaboration network of E. coli strains, including taxonomic subgroups and temporal dynamics.\",\n            \"preprocessing\": [\"data cleaning, feature engineering for taxonomic and temporal features\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"network visualization, time-series analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Temporal Graph Convolutional Network (TGCN)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"incorporating temporal dynamics\"\n            },\n            \"description\": \"A model chosen for its ability to handle sparse and temporal graph data, designed to tackle the link prediction challenge in the E. coli collaboration network.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Considering the scarcity of labeled data, transfer learning or semi-supervised learning approaches may be explored to leverage related datasets.\"\n        },\n        {\n            \"text\": \"The temporal aspect requires the model to account for the evolving nature of the collaboration network.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"taxonomy feature extraction, temporal sequence encoding\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient inference for sparse graphs\"],\n        \"demonstration\": \"Generation of prediction results and model explanations highlighting influential factors\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the complexity of the task and the user's high expertise level. Key aspects like the task's area (link prediction in graph ML), domain (extremophile microbiology), performance metrics, temporal dynamics, and the model choice (TGCN) are included. Given the specific requirements and potential for adapting existing techniques, the confidence score is set to high.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "time-series forecasting", "instruction": "Forge a cutting-edge time-series forecasting solution for our e-commerce platform by merging an advanced, explainable meta-forecasting model that integrates deep ensembles of autoregressive models (LSTM-GRU) with a novel attention mechanism, specifically tailored for handling long-range dependencies, seasonality up to quartely trends, and cross-selling patterns. The model must deliver exceptional performance in predicting the last 18 months, utilizing a Hierarchical Sliding Window technique for validation to maintain model flexibility and resilience to sector-specific economic fluctuations. Additionally, incorporate interpretability elements to provide actionable insights for product recommendations and inventory management.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"The task is to develop a cutting-edge time-series forecasting solution for an e-commerce platform. The model should integrate an advanced, explainable meta-forecasting method using deep ensembles of LSTM-GRU with an attention mechanism for long-range dependencies, seasonality up to quarterly trends, and cross-selling patterns. It must predict the last 18 months with Hierarchical Sliding Window validation for flexibility and resilience to sector-specific economic fluctuations. Interpretability is also required for actionable product recommendations and inventory management.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"forecast horizon\",\n                \"value\": 18\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model explainability\",\n                \"value\": null,\n                \"unit\": \"percentage (interpretable to actionable insights)\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"sales\", \"cross-selling indicators\"],\n            \"specification\": {\n                \"time_period\": \"last 18 months\",\n                \"sampling_frequency\": \"monthly\"\n            },\n            \"description\": \"Contains historical e-commerce data with seasonality, long-range dependencies, and cross-selling patterns for training and validation.\",\n            \"preprocessing\": [\"Hierarchical Sliding Window\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Hierarchical pattern analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"LSTM-GRU with attention mechanism\",\n            \"family\": \"Deep Ensemble\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_models\": \"LSTM and GRU\",\n                \"attention_type\": \"novel attention mechanism\",\n                \"long-range_dependency_handling\": true,\n                \"seasonality\": \"quarterly\",\n                \"cross-selling_integration\": true\n            },\n            \"description\": \"An advanced forecasting model with built-in explainability for predicting e-commerce sales and cross-selling patterns.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hierarchical Sliding Window helps in maintaining model adaptability to sector-specific fluctuations.\",\n        \"Interpretability will provide actionable insights for product recommendations and inventory management\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"novel attention mechanism\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretable time-series engines\"],\n        \"demonstration\": \"predictive analytics dashboard for product recommendations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON captures the key aspects of the given instruction: advanced time-series forecasting, specific model components, performance metrics, and interpretability. The Hierarchical Sliding Window and novel attention mechanism have been appropriately integrated in the model description. The user's high expertise level suggests a need for more detailed specifications, but this is left open for their input.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, your mandate now involves not only the outlined plant species recognition system but also to incorporate advanced Explainable AI (XAI) elements. Enhance the pipeline by integrating a novel interpretability method for the ensemble models, targeting a stringent 98% accuracy with real-time performance on resource-constrained gadgets. Strive for efficiency without sacrificing explainability, while exploring the intricate balance between model complexity, accuracy, and power consumption in the face of limited hardware capabilities.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"botany\",\n        \"description\": \"The project objective is to develop a plant species recognition system with advanced Explainable AI (XAI) elements, integrating a novel interpretability method for ensemble models. Key performance requirements include a 98% accuracy target and real-time performance on resource-constrained devices, while maintaining the balance between model complexity, accuracy, and power consumption.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"resource-constrained gadgets\"\n            },\n            {\n                \"name\": \"model complexity\",\n                \"value\": null,\n                \"unit\": \"efficiency\"\n            },\n            {\n                \"name\": \"power consumption\",\n                \"value\": null,\n                \"unit\": \"consumption\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Species Recognition Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"plant species\"],\n            \"specification\": {\n                \"dimension\": null,\n                \"size\": null,\n                \"format\": null\n            },\n            \"description\": \"A dataset for plant species recognition that includes a focus on ensemble models and interpretability.\",\n            \"preprocessing\": [\"ensemble model-specific preprocessing\"],\n            \"augmentation\": [\"focus on real-time, resource-constrained environments\"],\n            \"visualization\": [\"interpretability visualization methods\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Ensemble Models with Novel Interpretability Method\",\n            \"family\": \"Machine Learning Ensemble\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"complexity\": null,\n                \"interpretability\": null,\n                \"FLOPs\": null\n            },\n            \"description\": \"A model integrating a novel interpretability method, designed for high accuracy (98%) in plant species recognition, targeting real-time performance on resource-constrained devices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Explore efficient algorithms for interpretability, model compression, and power-efficient computation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"ensemble model-specific feature optimization\"],\n        \"target_device\": \"resource-constrained devices\",\n        \"deployment_endpoint\": \"optimized for resource constraints\",\n        \"inference_engine\": [\"optimized for real-time execution\"],\n        \"demonstration\": \"Visual explanations of model decisions for plant species recognition\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response accurately captures the given instruction's requirements, including the need to build a system with high accuracy and XAI, targeted at resource-constrained devices. It also specifies the performance metrics, complexity considerations, and the balancing act between accuracy, model complexity, and power consumption.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "time-series classification", "instruction": "You're now embarking on a specialized time-series forecasting project with the 2021 MIMIC-IV dataset, focusing on precise ECG analysis. The dataset comprises extensive, multimodal cardiac time-series data with fine-grained resolution (1Hz sampling, 100,000 points per episode, 30 unique physiological features). Your task is to develop a cutting-edge deep learning architecture, like a Temporal Convolutional Network (TCN) with attention modules, to classify arrhythmia types (5 classes) with strong emphasis on short-term forecasting and cross-feature dependencies. Ensure superior performance by maximizing the average precision (AP), F1 score, and average precision-recall curves (APRC), along with Bland-Altman analysis to demonstrate robustness in real-world scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"The project involves a time-series forecasting and classification task on the 2021 MIMIC-IV dataset, focusing on ECG analysis, with a deep learning model like Temporal Convolutional Network (TCN) with attention modules. The goal is to classify arrhythmia types into 5 classes, emphasizing short-term forecasting and cross-feature dependencies. Performance metrics include average precision (AP), F1 score, average precision-recall curves (APRC), and Bland-Altman analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"average precision-recall curve\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"2021 MIMIC-IV ECG Dataset\",\n            \"modality\": [\"time series\", \"multimodal\"],\n            \"target_variables\": [\"arrhythmia types\"],\n            \"specification\": {\n                \"sampling_rate\": 1,\n                \"points_per_episode\": 100000,\n                \"physiological_features\": 30\n            },\n            \"description\": \"A comprehensive cardiac time-series dataset with fine-grained resolution and 30 unique physiological features, focusing on ECG analysis for arrhythmia classification.\",\n            \"preprocessing\": [\"fine-grained sampling (1Hz)\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Temporal Convolutional Network with Attention Modules\",\n            \"family\": \"Temporal Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A cutting-edge deep learning model for time-series classification and arrhythmia detection, leveraging TCN and attention mechanisms for improved performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"cross-feature dependencies\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Bland-Altman analysis to show robustness in real-world scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the given instruction accurately, capturing the expert-level expertise, specific dataset details, the advanced deep learning architecture, and the performance metrics requested. The rationale clearly explains the reasoning behind the structured information.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, demand the machine learning team to develop an advanced, scalable image classifier for esoteric celestial phenomena, combining cutting-edge deep learning models with a custom Celestial Feature Repository ( CFR) integration. Mandate the design to tackle challenging photometric variations, distinct asteroid taxonomic classification, and strive for an impeccable 0.5% error rate under variable illumination. Instruct them to implement multi-stage data preprocessing, utilizing advanced denoising techniques and developing a tailored pipeline for handling rare anomalies. Furthermore, propose a comprehensive evaluation framework that includes precision, recall, F1 score, and Hierarchical clustering metrics, ensuring nuanced discrimination between fine-grained object classes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The project demands the development of an advanced, scalable image classifier for esoteric celestial phenomena. It requires integrating cutting-edge deep learning models with a Celestial Feature Repository (CFR), focusing on handling photometric variations, precise asteroid taxonomic classification, and aiming for a 0.5% error rate under variable illumination. Preprocessing must involve multi-stage data cleaning using advanced denoising techniques, and a specialized pipeline for managing rare anomalies. The evaluation framework must include precision, recall, F1 score, and Hierarchical clustering metrics to ensure fine-grained object discrimination.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Hierarchical clustering\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celestial Feature Repository\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"esoteric celestial phenomenon classification\"\n            ],\n            \"specification\": {\n                \"Advanced Denoising Techniques\": {},\n                \"Rare Anomalies Handling Pipeline\": {}\n            },\n            \"description\": \"A custom dataset that integrates with a Celestial Feature Repository for esoteric celestial phenomenon classification, with specific focus on photometric variations and rare anomalies.\",\n            \"preprocessing\": [\n                \"Multi-stage data preprocessing with advanced denoising techniques\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image Classifier\",\n            \"family\": \"Deep Learning Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"Asteroid Taxonomic Classification\": {},\n                \"Cutting-edge Model Architecture\": {}\n            },\n            \"description\": \"A state-of-the-art model for image classification in celestial phenomena, with an emphasis on CFRI integration, robustness against photometric variations, and tailored for fine-grained object discrimination.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Celestial Feature Repository (CFR) integration and handling rare anomalies are crucial for the success of the project.\"\n        },\n        {\n            \"text\": \"To meet the ambitious 0.5% error rate, extensive domain adaptation and model fine-tuning might be necessary.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Advanced denoising techniques\",\n            \"Rare anomaly handling pipeline\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"description\": \"The model's performance will be demonstrated through precision, recall, F1 score, and Hierarchical clustering metrics, showcasing nuanced discrimination among fine-grained celestial object classes.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the comprehensive requirements of the project, including the use of a high-expertise machine learning team, cutting-edge technology, and the importance of specific evaluation metrics. Key details from the instruction have been appropriately placed and formatted to ensure accurate implementation.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a premier retail analytics solution, enhance the state-of-the-art Mask R-CNN model by incorporating the expansive 'Extended Product Metadata-5M' dataset, emphasizing on ultra-precise item recognition (including fine-grained subcategories), robustness against occlusions, cluttered backgrounds, and dynamic range challenges. In addition to standard COCO-style evaluations (IoU, mAP, and CMC), prepare a comprehensive report that delves into fine-grained error classifications, context-aware failure modes, and a novel strategy for suppressing false positives and minimizing invisible inventory, ensuring real-time optimization for high-demand, high-traffic retail settings with minimal human intervention.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail analytics\",\n        \"description\": \"The task is to enhance the Mask R-CNN model for ultra-precise item recognition in retail, considering fine-grained subcategories, robustness against occlusions and cluttered backgrounds, and dynamic range challenges. Evaluation will include COCO-style metrics (IoU, mAP, and CMC), a comprehensive error classification, context-aware failure modes analysis, and a novel strategy for suppressing false positives and minimizing invisible inventory for real-time optimization in high-traffic settings with minimal human intervention.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Intersection over Union (IoU)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean Average Precision (mAP)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Comparative Matching Chart (CMC)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Robustness against occlusions\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            },\n            {\n                \"name\": \"Dynamic range challenge handling\",\n                \"value\": null,\n                \"unit\": \"decrease in errors\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extended Product Metadata-5M\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"item recognition\"\n            ],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"valid\": null,\n                    \"test\": null\n                },\n                \"dimensions\": null,\n                \"modalities\": {\n                    \"text\": null,\n                    \"image\": null\n                }\n            },\n            \"description\": \"A large dataset for enhancing Mask R-CNN, focusing on ultra-precise item recognition and addressing retail-specific challenges.\",\n            \"preprocessing\": [\n                \"Fine-grained subcategory extraction\",\n                \"Background clutter handling\"\n            ],\n            \"augmentation\": [\n                \"Occlusion and clutter simulation\",\n                \"Dynamic range enhancement\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Mask R-CNN\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"incorporating fine-grained recognition and context-aware improvements\",\n                \"parameters\": null,\n                \"inference time\": null,\n                \"real-time optimization\": {\n                    \"accuracy\": null\n                }\n            },\n            \"description\": \"An advanced model designed for robust item recognition in retail, addressing specific challenges as mentioned.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"item\": \"Real-time optimization for high-demand settings\",\n            \"impact\": \"minimizes human intervention\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"fine-tuning for high-traffic retail\",\n            \"context-aware error reduction\"\n        ],\n        \"target_device\": \"real-time processing\",\n        \"deployment_endpoint\": {\n            \"type\": \"cloud-based\",\n            \"requirements\": \"minimal latency\"\n        },\n        \"inference_engine\": [\"optimized for GPU\"],\n        \"demonstration\": \"demonstrating real-world performance in challenging retail scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON format reflects the instruction accurately, capturing the requirement to enhance Mask R-CNN, the dataset, evaluation metrics, fine-grained analysis, and the retail-specific challenges. The structure is consistent, and the chosen expertise level is justified by the complexity of the task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a trailblazing project manager for a pioneering artificial intelligence research team, your assignment is to develop a state-of-the-art BERT-based model specifically tailored for the Cross-genre Scientific Discourse Understanding (SciCDU) dataset, focusing on niche categories like nanotechnology, exoplanet research, and particle physics. The task involves discerning intricate logical relationships in 70,000 advanced, domain-specific texts, ensuring robustness against lexical variations and handling the complexity of unbalanced data from different sources. The target model must achieve a remarkable F1-score parity (<0.15%) with the XLM-RoBERTa-large configuration, while maintaining precision, recall, and concurrently optimizing for the Fleiss' Kappa coefficient to account for inter-annotator reliability. Document extensive fine-tuning experiments, Progressive Transfer Learning strategies, and the integration of diverse ensemble techniques, all within the stringent constraint of computational efficiency, to set a new benchmark in specialized scholarly text analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"scholarly research\",\n        \"description\": \"The task is to develop a state-of-the-art BERT-based model for Cross-genre Scientific Discourse Understanding (SciCDU), specifically targeting niche domains like nanotechnology, exoplanet research, and particle physics. The focus is on logical relationship extraction in advanced domain-specific texts, with robustness against lexical variations and unbalanced data from diverse sources. The model must achieve an F1-score parity of <0.15% with XLM-RoBERTa-large, maintain precision and recall, and optimize for Fleiss' Kappa for inter-annotator reliability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.15\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Fleiss' Kappa\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"compute resources\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cross-genre Scientific Discourse Understanding (SciCDU)\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\n                \"logical relationships\"\n            ],\n            \"specification\": {\n                \"size\": 70000,\n                \"domain_sourcing\": \"diverse, advanced, domain-specific texts (nanotechnology, exoplanet research, particle physics)\"\n            },\n            \"description\": \"A dataset containing 70,000 texts with niche focus on nanotechnology, exoplanet, and particle physics, designed for logical relationship extraction in scientific discourse.\",\n            \"preprocessing\": [\"lexical variations handling\", \"unbalanced data handling\"],\n            \"augmentation\": [],\n            \"visualization\": [\"progressive learning analysis\", \"ensemble techniques overview\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-based (BERT for SciCDU)\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"comparison\": \"compatible with XLM-RoBERTa-large configuration\",\n                \"ensemble_techniques\": [\"progressive transfer learning\", \"diverse ensemble techniques\"]\n            },\n            \"description\": \"A BERT model specifically designed for SciCDU, focusing on lexical and unbalanced data challenges, and targeting high inter-annotator reliability.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Documenting fine-tuning experiments and progressive transfer learning strategies, in addition to ensemble techniques, will contribute to achieving computational efficiency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"scholarly text analysis platform\",\n        \"inference_engine\": [\"efficient GPU support\", \"optimization for resource constraints\"],\n        \"demonstration\": \"interactive visualizations showcasing model performance and learning curves\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's high expertise level, the focus on BERT-based model, and the detailed task requirements. It includes performance targets, complexity constraints, and mentions of specific techniques like fine-tuning and ensemble methods. Missing values are placeholders for them to be filled as more information is available.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for a specialized agricultural technology venture, you're tasked with optimizing an advanced object detection model tailored for rare exotic fruit recognition, such as prickly pears with spines, vibrant dragon fruits, and tropical passion fruits. The system must accurately estimate harvestable yields from pixel-dense, low-contrast images, amidst various occlusions, shape variations, and seasonal changes. Emphasize the integration of interpretable AI for agricultural transparency, development of a user-friendly mobile app that teaches non-experts via interactive tutorials, and guarantees energy-efficient performance on ultra-minimalist devices with real-time capabilities. Scrutinize the fusion of niche image processing techniques and deep learning, research on transfer learning for cross-fruit recognition, and outline the tech stack essential for this revolutionary orchard management solution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"The project involves optimizing an object detection model for recognizing rare exotic fruits like prickly pears, dragon fruits, and passion fruits in low-contrast, pixel-dense images. It aims for high accuracy under occlusions, shape variations, and seasonal changes, while incorporating interpretable AI for transparency, a mobile app for non-expert tutorials, and energy-efficient performance on minimalist devices for real-time capabilities.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"harvestable yield prediction accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"battery life\"\n            },\n            {\n                \"name\": \"real-time capability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Specialized Exotic Fruit Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"crop type\"],\n            \"specification\": {\n                \"description\": \"Pixel-dense, low-contrast images with diverse occlusions, shape variations, and seasonal changes for exotic fruits.\",\n                \"depth\": \"high\",\n                \"quality\": \"low-contrast\"\n            },\n            \"preprocessing\": [\"image enhancement\", \"data augmentation\"],\n            \"augmentation\": [\"low-contrast augmentation\", \"occlusion handling\"],\n            \"visualization\": [\"interpretability maps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Object Detection Model (adapted for exotic fruits)\",\n            \"family\": \"Deep Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": \"cross-fruit recognition\"\n            },\n            \"description\": \"Focuses on rare exotic fruit detection using niche image processing and transfer learning techniques for improved accuracy and cross-fruit recognition.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interpretable AI components for model explanation\",\n        \"Mobile app design with interactive tutorials for non-experts\",\n        \"Energy-efficient tech stack for minimalistic devices\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"model explainability\", \"mobile app interface\"],\n        \"target_device\": \"ultra-minimalist devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time capable\"],\n        \"demonstration\": \"mobile app demo with real-time detection and yield prediction\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure represents the input instruction, highlighting the need to build an advanced object detection model for exotic fruits with interpretable AI and mobile app components. It captures the need for niche image processing, transfer learning, and energy-efficient performance. The confidence score may be low due to the assumption of high expertise level and details about tech stack not being explicit.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "The project manager has been tasked with the development of a groundbreaking Time-Series Anomaly Detection (TSD) platform, leveraging a hybrid AI architecture combining neuroevolutionary algorithms and a novel spatiotemporal transformer. The system must tackle the elusive \"QuantumAstronomicalRhythms\" dataset, which demands not only the detection of standard anomalies but also the rapid identification of rare cosmic event precursors with microsecond precision. The team must integrate an innovative fusion of topological data analysis and topographic signal filtering, while optimizing for minimal computational footprint to maintain real-time monitoring across vast celestial data streams. The whitepaper must showcase this unprecedented approach and discuss the system's scalability in the context of Big Data and ultra-high-dimensional time series.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The project involves developing a Time-Series Anomaly Detection (TSD) platform using a hybrid AI architecture combining neuroevolutionary algorithms and a spatiotemporal transformer. The main challenge is to handle the QuantumAstronomicalRhythms dataset, detecting standard anomalies and rare cosmic event precursors with microsecond precision. Topological data analysis and topographic signal filtering are to be integrated, while optimizing for minimal computational footprint for real-time monitoring across high-dimensional celestial data streams.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"standard anomaly detection precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"rare cosmic event precursor identification rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"microsecond precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational footprint\",\n                \"value\": null,\n                \"unit\": \"GFLOPS\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"QuantumAstronomicalRhythms\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": null,\n            \"specification\": {\n                \"size\": null,\n                \"dimensionality\": \"ultra-high-dimensional\",\n                \"sampling_rate\": null,\n                \"data_type\": \"astronomical\"\n            },\n            \"description\": \"A dataset that demands anomaly detection and rare event precursor identification with microsecond precision in celestial data streams.\",\n            \"preprocessing\": [\"topological data analysis\", \"topographic signal filtering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid AI (Neuroevolution + Spatiotemporal Transformer)\",\n            \"family\": \"Combination of Neuroevolution and Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"hybrid\",\n                \"topology\": \"neuroevolutionary + spatiotemporal transformer\",\n                \"parameters\": {\n                    \"number_of_nodes\": null,\n                    \"number_of_layers\": null,\n                    \"complexity_optimization\": \"topological fusion\"\n                }\n            },\n            \"description\": \"A cutting-edge model designed for anomaly detection and real-time monitoring, leveraging novel AI techniques.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": null,\n        \"target_device\": \"edge, cloud\",\n        \"deployment_endpoint\": \"potentially scalable with Big Data\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"Whitepaper showcasing the platform's scalability and unique approach\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the details of the project, from the high-level intent to the specific technical requirements for the hybrid AI architecture and the QuantumAstronomicalRhythms dataset. Performance metrics and complexity considerations are also included. However, some metric values are not specified, as they might require further information about the desired outcomes.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "The project manager is required to develop a real-time, granular anomaly detection system for a geo-distributed e-commerce platform. Focus on anomaly clustering based on visitor flow trends, incorporate 7-day cycle correction for mobile traffic, and integrate deep learning models (LSTM and CNN) to differentiate seasonal patterns, weekend surges, and holiday fluctuations. Mandate sub-hourly alert generation for potential security incidents and user behavior analytics for targeted marketing strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"e-commerce, cybersecurity, user behavior analytics\",\n        \"description\": \"The project aims to develop a real-time anomaly detection system for a geo-distributed e-commerce platform. The system focuses on clustering anomalies based on visitor flow trends, incorporates a 7-day cycle correction for mobile traffic, and employs deep learning models like LSTM and CNN for detecting seasonal patterns, weekend surges, and holiday fluctuations. It must generate sub-hourly alerts for security incidents and support targeted marketing strategies through user behavior analytics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean average precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"E-commerce Visitor Flow Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"visitor flow trends\"],\n            \"specification\": {\n                \"time_period\": \"sub-hourly\",\n                \"data_points\": \"continuous\"\n            },\n            \"description\": \"Geo-distributed e-commerce platform visitor flow data, including mobile traffic with a 7-day cycle correction.\",\n            \"preprocessing\": [\n                \"7-day cycle correction\",\n                \"time-series normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"E-commerce Transaction Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"Seasonal, weekend, and holiday transaction data to train LSTM and CNN for pattern differentiation.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"LSTM\",\n            \"family\": \"Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        },\n        {\n            \"name\": \"CNN\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating 7-day cycle correction for mobile traffic\",\n        \"LSTM and CNN for seasonal pattern detection\",\n        \"Sub-hourly alerts for security incidents\",\n        \"User behavior analytics for targeted marketing\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"ensemble learning for improved detection\",\n            \"custom loss functions\"\n        ],\n        \"target_device\": \"cloud computing\",\n        \"deployment_endpoint\": \"proprietary platform\",\n        \"inference_engine\": [\"real-time streaming\", \"batch processing\"],\n        \"demonstration\": \"sub-hourly anomaly report generation and visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON structure accurately reflects the instruction, capturing the intent to build a system, the complexity of the task, the requirement for LSTM and CNN models, and the desired performance metrics. Key adjustments were made to fit the context of the e-commerce platform and the specified requirements, like granular anomaly clustering and real-time alert generation.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "Revise the AI image classifier for the fashion platform to handle not only haute couture garments, but also vintage and streetwear, with accuracy over 99% in recognizing sub-attributes like embroidery, distressing, and eco-friendly materials. Require the system to rank images by seasonality and fashion trends, while preserving visual aesthetics and handling low-resolution and backlit uploads.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"The task is to revise an AI image classifier for a fashion platform, focusing on haute couture, vintage, and streetwear, with high accuracy in recognizing sub-attributes like embroidery, distressing, and eco-friendly materials. The classifier should also rank images based on seasonality and fashion trends, maintaining visual aesthetics and handling low-resolution and backlit uploads.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion Platform Image Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"garment type\", \"embroidery\", \"distressing\", \"eco-friendly materials\"],\n            \"specification\": {\n                \"specific sub-attributes coverage\": {\n                    \"haute couture\": true,\n                    \"vintage\": true,\n                    \"streetwear\": true,\n                    \"embroidery\": true,\n                    \"distressing\": true,\n                    \"eco-friendly\": true\n                },\n                \"image quality\": {\n                    \"low-resolution\": true,\n                    \"backlit\": true\n                }\n            },\n            \"description\": \"A dataset containing diverse fashion images including haute couture, vintage, streetwear, with a focus on sub-attributes mentioned and varying image conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Revamped Image Classifier\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"seasonality_ranking\": true,\n                \"fashion_trends_ranking\": true,\n                \"visual_aesthetics_preservation\": true\n            },\n            \"description\": \"An AI image classifier designed for the fashion platform, able to handle diverse garment types and sub-attributes with high accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"image ranking and aesthetics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure is based on the provided template, adjusting for the fashion platform's revised requirements. It covers high accuracy, image sub-attributes, ranking, and handling of low-resolution and backlit images. The model family and expert level were inferred from the nature of the task. The confidence score reflects the level of detail present in the instruction, with room for improvement if more specific information were given.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project lead for a specialized time-series forecasting team, your mandate is to refine the advanced seasonal ARIMA-Transformer model for the Flu Incidence (ILI) dataset. The upgraded project involves:\r\n\r\n1. Design a hybrid deep learning architecture integrating state-space LSTM-NARX, specifically tuned for handling complex autoregressive dynamics and multi-seasonal patterns in weekly data.\r\n2. Implement a dynamic rolling forecasting window with a weekly granularity (6 weeks total, OFFSET=4) that adaptively captures trends and non-linearities within training, validation (consisting of 48 weeks), and testing periods (72 weeks).\r\n3. Balance accuracy optimization, minimizing both weighted root mean squared error (wRMSSE) and log-cosh loss, while minimizing prediction delay for the next 16 days (224 hours), prioritizing near-real-time forecasting efficiency and system responsiveness.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"public health\",\n        \"description\": \"The task is to refine an advanced ARIMA-Transformer model for flu incidence (ILI) dataset. The project requires a hybrid LSTM-NARX architecture designed for complex AR dynamics and multi-seasonality in weekly data. It involves dynamic rolling forecasting with a 6-week window (OFFSET=4), minimizing wRMSSE and log-cosh loss, and focusing on real-time forecasting with a prediction delay of 16 days for near-hourly forecasting.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"weighted root mean squared error (wRMSSE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"log-cosh loss\",\n                \"value\": null\n            },\n            {\n                \"name\": \"prediction delay\",\n                \"value\": {\n                    \"units\": \"hours\",\n                    \"value\": 224\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Flu Incidence (ILI) dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"ILI\"],\n            \"specification\": {\n                \"weekly_frequency\": \"7\",\n                \"seasonalities\": [\"weekly\", \"monthly\"],\n                \"data_length\": {\n                    \"train\": 308,\n                    \"validation\": 48,\n                    \"testing\": 72\n                }\n            },\n            \"description\": \"Weekly flu incidence data with complex autoregressive dynamics and multi-seasonal patterns.\",\n            \"preprocessing\": [\n                \"resampling to weekly frequency\",\n                \"handling multi-seasonality\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ARIMA-Transformer (LSTM-NARX hybrid)\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"LSTM-NARX state-space hybrid\",\n                \"seasonal_pattern_depth\": \"multi\",\n                \"window_size\": 6,\n                \"offset\": 4,\n                \"forecasting_horizon\": 16\n            },\n            \"description\": \"A model designed to handle complex autoregressive dynamics and multi-seasonality in weekly time-series data, specifically for flu incidence prediction.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"custom feature extraction for LSTM-NARX\",\n            \"dynamic rolling window creation\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"real-time forecasting API\",\n        \"inference_engine\": [\"TensorFlow Serving\"],\n        \"demonstration\": \"custom dashboard for real-time visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the project mandate, mentioning the need for a hybrid LSTM-NARX model, dynamic rolling forecasting window, and optimization for wRMSSE and log-cosh loss. The target domain, complex metrics, and data specifications are relevant to the time-series forecasting context. The expert level expertise and emphasis on real-time responsiveness are also captured.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Request the machine learning team to enhance an image segmentation project by specifically focusing on real-time, low-latency furniture segmentation in dynamically lit indoor environments using the advanced U-Net architecture. In addition, mandate the team to design a prototype tailored for edge computing devices with <10MB model size, and provide detailed runtime performance metrics for different hardware configurations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"interior design and robotics\",\n        \"description\": \"The project aims to enhance real-time furniture segmentation in indoor environments with dynamic lighting. The U-Net architecture is required for efficiency, and the model should be tailored for edge computing devices with a max model size of 10MB. Runtime performance metrics for various hardware configurations are mandatory.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model size\",\n                \"value\": 10,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"runtime performance\",\n                \"value\": null,\n                \"unit\": \"ms per frame\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dynamic_lighting\": true,\n                \"indoor_environments\": true\n            },\n            \"description\": \"Dataset of indoor environments with varying lighting conditions for furniture segmentation using the U-Net architecture.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net for Furniture Segmentation\",\n            \"family\": \"Encoder-Decoder Architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time\": true,\n                \"tailored_for_edge\": true\n            },\n            \"description\": \"Advanced U-Net model optimized for low-latency, real-time furniture segmentation on edge devices with a focus on hardware constraints.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge computing devices\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [],\n        \"demonstration\": \"prototype with hardware performance demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the given instruction. The focus on U-Net architecture, real-time performance, low-latency, and edge computing compatibility are captured. Target hardware metrics and performance indicators are specified. The expert level of the user's expertise is also included.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "In the context of the SMS Spam Challenge, design a cutting-edge spam filter for the dataset with a stringent requirement of 97% F1-score, maintaining a paramount energy efficiency by optimizing a hybrid LSTM-GRU model tailored for embedded devices. Emphasize on ultra-low power consumption and a stringent latency target of below 80ms, while incorporating a dynamically adaptive real-time attention mechanism to combat evolving spam trends and ensure seamless performance on resource-limited smartphones. Also, include a comparative analysis of model efficiency with state-of-the-art techniques.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"telecommunications\",\n        \"description\": \"The task is to design a spam filter for the SMS Spam Challenge dataset, targeting a stringent F1-score of 97% with energy efficiency for embedded devices. The model must be a hybrid LSTM-GRU optimized for low power consumption and latency of below 80ms, incorporating a real-time attention mechanism for adapting to evolving spam trends on resource-constrained smartphones.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"power consumption\",\n                \"value\": null,\n                \"unit\": \"Watts\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 0.08,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SMS Spam Challenge Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"spam or not\"],\n            \"specification\": null,\n            \"description\": \"A dataset for developing an SMS spam filter, specifically for embedded devices.\",\n            \"preprocessing\": [\n                \"text normalization\",\n                \"tokenization\",\n                \"vocabulary building\"\n            ],\n            \"augmentation\": [\n                \"synthetic data generation\"\n            ],\n            \"visualization\": [\n                \"data distribution, performance metrics\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid LSTM-GRU\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"optimized for embedded devices\",\n                \"attention mechanism\": \"dynamically adaptive real-time\"\n            },\n            \"description\": \"A cutting-edge model with ultra-low power consumption and latency target, tailored for resource-limited environments.\"\n        }\n    ],\n    \"knowledge\": [\n        \"state-of-the-art techniques comparison\",\n        \"adaptive to evolving spam trends\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"energy-efficient\"\n        ],\n        \"target_device\": \"embedded devices, smartphones\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"optimized for mobile platforms\"\n        ],\n        \"demonstration\": \"real-time spam detection and performance metrics on resource-constrained devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's requirements for an advanced spam filter model. Performance metrics, hardware constraints, and attention mechanism are clearly defined, reflecting the essence of the instruction. The confidence score, though high, acknowledges that some aspects (such as explicit F1-score optimization or model efficiency comparison) may require additional clarification.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for the elite AI research team, your mandate expands to enhance the state-of-the-art time-series forecasting system for the Electricity dataset. The intricate, temporally-rich data, dissected into training, validation, and testing sets with fine-grained labels, demands the creation of a next-generation model. Each input sequence of historical data boasts a substantial size (INPUT_SEQ_LEN=96, INPUT_DIM=321), necessitating deep sequential comprehension. The task now requires not only forecasting consecutive future sequences (PRED_SEQ_LEN=96, PRED_DIM=321) but also introducing a dynamic rolling forecasting window, adaptively adjusting to real-time patterns. The model must excel at multi-step-ahead prediction, simultaneously optimizing for various lead times while minimizing weighted geometric mean of squared errors (WGMSE) and quantile-based mean absolute scaled error (QMASE) to ensure superior out-of-sample performance and resilience against anomalies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy and utilities\",\n        \"description\": \"The project involves enhancing the state-of-the-art time-series forecasting system for the Electricity dataset. The data is temporally-rich and split into train, validation, and test sets, with fine-grained labels. The task necessitates a deep sequential model capable of handling large input sequences (INPUT_SEQ_LEN=96, INPUT_DIM=321) and forecasting future sequences (PRED_SEQ_LEN=96, PRED_DIM=321) while introducing a dynamic rolling forecasting window for real-time adaptability. Key performance metrics include WGMSE and QMASE minimization for superior out-of-sample performance and anomaly resilience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"weighted geometric mean of squared errors (WGMSE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"quantile-based mean absolute scaled error (QMASE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"future electricity data\"],\n            \"specification\": {\n                \"input_sequence_length\": 96,\n                \"input_dimension\": 321,\n                \"output_sequence_length\": 96,\n                \"output_dimension\": 321,\n                \"labels\": \"fine-grained\"\n            },\n            \"description\": \"An intricate, temporally-rich dataset for time-series forecasting, divided into train, validation, and test sets.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Next-Generation Time-Series Forecasting Model\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"sequence_comprehension_depth\": \"adequate for large input\",\n                \"dynamic_forecasting_window\": true,\n                \"multi-step_prediction\": true\n            },\n            \"description\": \"A model designed for complex, large input time-series data, supporting both multi-step-ahead and rolling forecasting.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or server\",\n        \"deployment_endpoint\": \"proprietary or public cloud\",\n        \"inference_engine\": [\"real-time with dynamic window\"],\n        \"demonstration\": \"customizable dashboards or visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is accurately parsed based on the given instruction. It captures the user's intent to build an advanced model, their high-level expertise, and the specific requirements for enhancing the time-series forecasting system. Performance metrics, complexity constraints, and the dataset specifications are included in the JSON structure.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a seasoned project manager for the advanced AI team, mandate the creation of a neural machine translation system with a WMT19 multilingual news corpus. Emphasize translating Russian to English in real-world scenarios with a challenging BLEU score of 47, while incorporating domain-adaptable zero-shot and transfer learning strategies. Strictly require a systematic error analysis, human evaluation for parity, and a demonstration of outperforming existing tools without sacrificing efficiency, all within a constrained computational budget.", "resposne": "{\n  \"user\": {\n    \"intent\": \"build\",\n    \"expertise\": \"high\"\n  },\n  \"problem\": {\n    \"area\": \"natural language processing\",\n    \"downstream_task\": \"machine translation\",\n    \"application_domain\": \"language translation\",\n    \"description\": \"The project involves building a neural machine translation system specifically for Russian to English translation in real-world scenarios, targeting a BLEU score of 47. Domain-adaptable zero-shot and transfer learning strategies are crucial. A systematic error analysis, human evaluation for parity with existing tools, and a demonstration of improved performance without compromising efficiency are mandatory within a constrained computational budget.\",\n    \"performance_metrics\": [\n      {\n        \"name\": \"BLEU score (Russian to English)\",\n        \"value\": 47\n      }\n    ],\n    \"complexity_metrics\": [\n      {\n        \"name\": \"computational budget\",\n        \"value\": null,\n        \"unit\": \"CPU hours or other suitable unit\"\n      }\n    ]\n  },\n  \"dataset\": [\n    {\n      \"name\": \"WMT19 Multilingual News Corpus\",\n      \"modality\": [\"text\"],\n      \"target_variables\": [\"Russian to English\"],\n      \"specification\": {\n        \"corpus_size\": \"multilingual, may include source and target language data\",\n        \"language_pair\": \"Russian to English\"\n      },\n      \"description\": \"A large-scale multilingual corpus for training and evaluating the machine translation system, focusing on news data.\",\n      \"preprocessing\": [\"domain adaptation, zero-shot and transfer learning\"],\n      \"augmentation\": [],\n      \"visualization\": [],\n      \"source\": \"direct-search\"\n    }\n  ],\n  \"model\": [\n    {\n      \"name\": \"Neural Machine Translation Model\",\n      \"family\": \"Sequence-to-Sequence\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"model_architecture\": \"Transformer or other suitable architecture\",\n        \"zero_shot_learning\": true,\n        \"transfer_learning\": true\n      },\n      \"description\": \"A neural machine translation model designed to handle Russian to English translation with domain-adaptive strategies.\"\n    }\n  ],\n  \"knowledge\": [\n    \"Error analysis to identify systematic issues and improve translation quality\",\n    \"Human evaluation for parity with existing translation tools\"\n  ],\n  \"service\": {\n    \"feature_engineering\": [],\n    \"target_device\": \"\",\n    \"deployment_endpoint\": \"\",\n    \"inference_engine\": [\"Cloud-based or optimized for real-time scenarios\"],\n    \"demonstration\": \"Comparison with existing tools and efficiency demonstration\"\n  },\n  \"confidence\": {\n    \"rationale\": \"The JSON response captures the key aspects of the user's instruction, including the neural machine translation project, Russian to English focus, the target BLEU score, domain adaptation strategies, and the requirement for error analysis and human evaluation. It reflects a high expertise level of the user and adheres to the constraints given.\",\n    \"score\": 0.98\n  }\n}\n"}
{"task": "time-series forecasting", "instruction": "The project manager is required to develop a cutting-edge, adaptive time-series forecasting system. This model must integrate advanced algorithms for real-time analysis, including cycle-to-cycle learning and extreme pattern recognition. It should not only factor in seasonal fluctuations, exogenous shocks, and historical trends but also detect latent interdependencies and micro-trends. hourly forecasts should exhibit exceptional precision for optimizing staffing, resource allocation, and overall operational resilience in a dynamically evolving market.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"business operations\",\n        \"description\": \"The project aims to develop a cutting-edge, adaptive time-series forecasting system that incorporates advanced algorithms such as cycle-to-cycle learning and extreme pattern recognition. It must account for seasonal fluctuations, exogenous shocks, historical trends, latent interdependencies, and micro-trends, with a focus on hourly forecasts for optimized staffing, resource allocation, and operational resilience in a rapidly changing market.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"forecast accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Interdependency detection\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Micro-trend identification\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time analysis\",\n                \"value\": 1,\n                \"unit\": \"feature\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"inference latency\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"length\": null,\n                \"features\": null\n            },\n            \"description\": \"A dataset with hourly data suitable for training and real-time analysis of time-series data.\",\n            \"preprocessing\": [\n                \"seasonal decomposition\",\n                \"time-series normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks (e.g., RNN, LSTM with attention mechanism)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"algorithm\": [\n                    \"cycle-to-cycle learning\",\n                    \"extreme pattern recognition\"\n                ],\n                \"technique\": \"adaptive\"\n            },\n            \"description\": \"An advanced time-series forecasting model capable of real-time analysis and sophisticated trend detection.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature engineering for latent interdependencies\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time analysis infrastructure\"\n        ],\n        \"demonstration\": \"hourly forecast demonstration for operational resilience\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the key elements of the project's requirements, specifying the intent as building a cutting-edge model, and acknowledges the need for high expertise. The problem description includes advanced algorithms, time-series complexities, and a clear focus on real-time forecasting. The missing specifics (e.g., model performance metrics and dataset) are left open for customization based on further details provided by the project manager.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "object detection", "instruction": "The task for the ML team is to design an M2 EfficientDet-based object detection model, enhancing 'iWildCam' with exceptional night vision, real-time responsiveness for drone-mounted cameras with limited hardware, and a stringent requirement of 97% precision. In addition, the system must identify 60 unique species in various challenging scenarios, demonstrating robustness to partial views and built for deployment in off-grid sanctuaries without relying on constant connectivity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"The goal is to design an M2 EfficientDet-based object detection model for the 'iWildCam' dataset. The model must exhibit exceptional night vision, real-time performance for drone-mounted cameras with limited hardware, and achieve a precision of 97%. It must detect 60 unique species, be robust to partial views, and be designed for off-grid deployment in sanctuaries without constant connectivity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"iWildCam\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\"],\n            \"specification\": {\n                \"night_vision\": \"exceptional\",\n                \"real_time\": \"yes\",\n                \"drone_camera_limitations\": \"yes\",\n                \"number_of_species\": 60\n            },\n            \"description\": \"Dataset for iWildCam, focusing on object detection with challenging scenarios like night, partial views, and off-grid deployments.\",\n            \"preprocessing\": [\"night vision enhancement\", \"data augmentation for partial views\"],\n            \"augmentation\": [\"random cropping\", \"lighting variations\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M2 EfficientDet\",\n            \"family\": \"Object Detection Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"night_vision_capabilities\": \"enhanced\",\n                \"real_time_inference\": true,\n                \"hardware_efficiency\": \"optimized for limited hardware\",\n                \"species_count\": 60\n            },\n            \"description\": \"EfficientDet-based model designed for object detection in iWildCam with high precision, real-time performance, and species identification under specific constraints.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Limited connectivity and off-grid deployment demand a lightweight and efficient model.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"drone-mounted cameras for off-grid sanctuaries\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"demonstrate robustness to partial views and night vision in various challenging scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON structure closely reflects the requirements of the ML task. It captures the high-level intent to build a model, the area of computer vision, the performance metric of 97% precision, and the specific model choice (M2 EfficientDet). Dataset requirements and constraints, such as night vision and real-time responsiveness, are clearly outlined. The rationale and confidence score offer a comprehensive understanding of the data matching the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "Instruct the machine learning team to design an advanced image classifier using EfficientNet, specifically tailored for the HAM10000 skin cancer dataset. Mandate a minimal misclassification rate of 99%, prioritizing precision to minimize false negatives. Not only must the model exhibit exceptional accuracy, but also require a multi-level interpretability feature analysis that unveils step-by-step reasoning for decisions, catering to expert dermatologists' stringent validation. Additionally, optimize the model for real-world deployment on resource-restricted gadgets without compromising its exceptional performance in terms of speed and precision.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medical\",\n        \"description\": \"The task is to design an advanced image classifier using EfficientNet, specifically for the HAM10000 skin cancer dataset. Requirements include a minimal misclassification rate of 99%, prioritizing precision to minimize false negatives, high accuracy, and multi-level interpretability for expert dermatologists.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"misclassification rate\",\n                \"value\": 0.01\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"GPU/CPU efficiency\",\n                \"value\": \"resource-restricted\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HAM10000 Skin Cancer Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"skin condition\"],\n            \"specification\": null,\n            \"description\": \"A skin cancer dataset for image classification. Requires model to handle skin cancer classification with minimal misclassification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"handle diverse lighting, angles, and backgrounds\"\n            ],\n            \"visualization\": [\n                \"interpretation for expert dermatologists\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"Advanced image classifier for skin cancer detection, tailored for HAM10000 dataset with emphasis on precision and real-world deployment.\"\n        },\n        {\n            \"feature_analysis\": {\n                \"type\": \"interpretable models or interpretability techniques\",\n                \"description\": \"Multi-level interpretability for expert dermatologist validation\"\n            }\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-restricted gadgets\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for speed and precision\"\n        ],\n        \"demonstration\": {\n            \"validation\": \"Expert dermatologist review\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately represents the instruction, with focus on building an image classifier with specific requirements such as EfficientNet, HAM10000 dataset, 99% misclassification rate, and interpretability for dermatologists. Key elements from the user's instruction have been included in the corresponding sections.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project director for a premier AI research team, your mandate is to develop a sophisticated, explainable model for text classification in the Enhanced Textual Entailment Challenge. Design a state-of-the-art system that integrates deep neural networks, sentiment analysis, and contextual reasoning, specifically targeting nuanced language with negations, idiomatic expressions, and cultural references. Mandate a minimum F1-score of 95% while ensuring interpretability, handling various linguistic complexities, and optimizing for low-resource computing environments. Execute the project in 60 days, with a focus on resource allocation, distributed training, and hardware efficiency, aiming to maximize parallel processing for acceleration without compromising on accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"The project aims to create a state-of-the-art model for text classification in the Enhanced Textual Entailment Challenge. The model should integrate deep neural networks, sentiment analysis, and contextual reasoning, focusing on nuanced language with negations, idiomatic expressions, and cultural references. The minimum requirement is an F1-score of 95% and interpretability in low-resource environments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"time complexity\",\n                \"value\": null,\n                \"unit\": \"days\"\n            },\n            {\n                \"name\": \"memory optimization\",\n                \"value\": null,\n                \"unit\": \"resource consumption\"\n            },\n            {\n                \"name\": \"parallel processing efficiency\",\n                \"value\": null,\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enhanced Textual Entailment Challenge Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"text classification\"],\n            \"specification\": null,\n            \"description\": \"A dataset designed for text classification with nuanced language, negations, idiomatic expressions, and cultural references.\",\n            \"preprocessing\": [\n                \"Sentence segmentation\",\n                \"Tokenization\",\n                \"Lemmatization\",\n                \"Entity recognition\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Integrated deep neural network with sentiment analysis and contextual reasoning\",\n            \"family\": \"Transformer-based models (e.g., BERT, RoBERTa)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"resource_efficiency\": \"optimized for low-resource environments\"\n            },\n            \"description\": \"A state-of-the-art system integrating deep learning, sentiment analysis, and contextual reasoning for advanced text classification.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Tokenizer parallelization\",\n            \"Training data chunking for distributed computing\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"Hardware-optimized libraries (e.g., TensorFlow serving with GPU support)\"\n        ],\n        \"demonstration\": \"Interactive tool or web-based UI showcasing model explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the given instruction, addressing the need for a high-performance, explainable model, handling linguistic complexities, resource efficiency, and distributed training. The specific model type (Transformer-based), performance metrics, and resource allocation strategies are derived from the requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "The project manager is tasked with developing a sophisticated time-series EEG analysis pipeline using advanced machine learning techniques. The goal is to design a real-time BCI system that not only accurately classifies cognitive states in sub-millisecond latency but also adapts dynamically, incorporating deep learning algorithms and feature engineering for complex neural patterns, ensuring reliability and responsiveness in high-resolution brainwave data for critical applications.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"The project involves building a sophisticated time-series EEG analysis pipeline for a real-time BCI system. The aim is to classify cognitive states with sub-millisecond latency, allowing for dynamic adaptation using advanced deep learning algorithms and feature engineering to handle complex neural patterns. The focus is on reliability and responsiveness for high-resolution brainwave data in critical applications.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sub-millisecond latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": null,\n                \"unit\": \"sub-millisecond\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"EEG Data for Cognitive State Analysis\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"cognitive states\"],\n            \"specification\": {\n                \"sampling_rate\": {\n                    \"value\": null,\n                    \"unit\": \"Hz\"\n                },\n                \"high_resolution\": {\n                    \"value\": true\n                },\n                \"brainwave_data_dimensions\": {\n                    \"value\": null\n                }\n            },\n            \"description\": \"A dataset for real-time EEG analysis, capturing high-resolution brainwave data with varying cognitive states.\",\n            \"preprocessing\": [\"advanced noise reduction\", \"bandpass filtering\"],\n            \"augmentation\": [],\n            \"visualization\": [\"real-time monitoring\", \"neural pattern visualizations\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Adaptive BCI System\",\n            \"family\": \"Deep Learning (e.g., LSTM, ConvLSTM, or Transformers)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"number_of_layers\": null,\n                \"adaptation_technique\": \"dynamic\",\n                \"feature_engineering\": true\n            },\n            \"description\": \"A deep learning model designed for real-time EEG analysis, capable of dynamically adapting to complex neural patterns for cognitive state classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Deep learning algorithms are essential for high performance and adaptation in real-time scenarios.\",\n        \"Feature engineering is vital for handling intricate neural patterns in EEG data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": true,\n        \"target_device\": \"embedded system with real-time processing capabilities\",\n        \"deployment_endpoint\": \"real-time BCI application\",\n        \"inference_engine\": [\"optimized libraries for low-latency processing\"],\n        \"demonstration\": \"live demonstration of real-time cognitive state classification and dynamic adaptation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the given instruction accurately, capturing the advanced machine learning pipeline, sub-millisecond latency, dynamic adaptation, and reliability for EEG analysis. Details such as preprocessing, deep learning architectures, and real-time processing requirements have been included, aligning with the task. The missing performance metrics and feature specifications are left open for later filling based on detailed analysis.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "time-series forecasting", "instruction": "Develop a cutting-edge, hybrid time-series forecasting model that fuses temporal-convolutional and bi-directional LSTM networks for the UrbanSounds8K dataset, accounting for minute variations and non-linearities. Target not only the 72-hour ahead prediction but also conduct feature-level interpretability analysis to precisely distinguish and rank subcategories of urban noises (like intermittent construction sounds, constant traffic hums, and diverse emergency sirens) for devising location-specific noise reduction interventions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"sound engineering\",\n        \"description\": \"The user wants to develop a hybrid time-series forecasting model using a combination of temporal-convolutional and bi-directional LSTM networks. The focus is on the UrbanSounds8K dataset, accounting for minute variations and non-linearities in urban noise patterns. The goal is to predict noise 72 hours in advance and perform feature-level interpretability analysis to identify and rank subcategories of urban noises for location-specific noise reduction interventions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UrbanSounds8K\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"noise intensity\", \"subcategories\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing time series data of urban noises, including subcategories like intermittent construction sounds, constant traffic hums, and diverse emergency sirens.\",\n            \"preprocessing\": [\n                \"data normalization\",\n                \"data balancing\"\n            ],\n            \"augmentation\": [\n                \"time warping\",\n                \"noise addition\"\n            ],\n            \"visualization\": [\n                \"feature importance analysis\",\n                \"time-series patterns\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Temporal-Convolutional & Bi-directional LSTM\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"temporalconv\": {},\n                    \"bi-directional-LSTM\": {}\n                },\n                \"hyperparameters\": {\n                    \"lstm_units\": null,\n                    \"learning_rate\": null\n                }\n            },\n            \"description\": \"A state-of-the-art model that fuses temporal convolutions and bi-directional LSTMs for accurate short-term and long-term predictions, targeting non-linearities and minute variations in the UrbanSounds8K dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The analysis should account for inherent non-stationarity in urban sound data and the interplay between different noise sources.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction from raw audio signals\",\n            \"spectrogram generation\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"location-specific noise reduction API\",\n        \"inference_engine\": [\"TensorFlow Serving\"],\n        \"demonstration\": \"visualizations and interactive noise level forecast for specific locations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's intent to build a complex forecasting model, their high level of expertise, and the specific requirements for the UrbanSounds8K dataset and feature interpretability. Key details, such as the model architecture, performance metrics, preprocessing, and deployment are included. However, exact model hyperparameters are left open for fine-tuning.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "The retail object detection model for this project must integrate a real-time, pose-aware AI system, capable of not only detecting and counting both customers and employees through advanced surveillance footage, but also differentiating them with 95% accuracy based on unique clothing style and subtle behavioral nuances, ensuring precise personnel monitoring in a high-traffic environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"security/surveillance\",\n        \"description\": \"The project aims to develop a real-time retail object detection model integrating a pose-aware AI system. It must detect and count both customers and employees with 95% accuracy based on clothing style and behavioral nuances. The model should ensure precise personnel monitoring in high-traffic environments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95,\n                \"sub-metrics\": {\n                    \"customer-accuracy\": 0.95,\n                    \"employee-accuracy\": 0.95\n                }\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Retail Surveillance Footage\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"customers\", \"employees\"],\n            \"specification\": {\n                \"live_feeds\": true,\n                \"high-traffic_labels\": true\n            },\n            \"description\": \"The dataset consists of real-time surveillance footage with annotations for customers and employees, capturing behavioral nuances and clothing style.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Real-time Retail Detection System\",\n            \"family\": \"AI-based Object Detection Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"AI_capabilities\": [\"real-time\", \"pose-aware\"],\n                \"detection_types\": [\"customers\", \"employees\"],\n                \"behavioral_nuances\": true,\n                \"accuracy_goal\": \"95%\"\n            },\n            \"description\": \"A cutting-edge model designed for real-time object detection, specifically for the retail environment with high-accuracy personnel differentiation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time system\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"live surveillance footage with annotated results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's advanced expertise level and focuses on building a high-level AI model for real-time detection. The specific requirements for accuracy, object types, and unique AI capabilities have been detailed. Performance metrics are segmented to emphasize the accuracy differentiation, and the surveillance footage source is specified.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "For a sophisticated image analysis project, develop a detailed evaluation matrix that requires the machine learning team to construct and analyze five distinct models, each using ResNet, Inception, and VGG architectures. Implement nested transfer learning, progressive fine-tuning, and incorporate ensemble techniques, demanding not only accurate breed identification but also the distinction between subtle variations within recognized breeds, all while maintaining a focus on computational efficiency and resource optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"image recognition and analysis\",\n        \"description\": \"The project involves detailed evaluation of five models, using ResNet, Inception, and VGG architectures, with a focus on breed identification and subtle breed variations. Nested transfer learning, progressive fine-tuning, and ensemble techniques are required, with an emphasis on computational efficiency and resource optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"breed-specific precision and recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"images of diverse breeds\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\", \"sub-breed\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing images for breed and subtle breed variation identification, with a focus on computational efficiency.\",\n            \"preprocessing\": [\"nested transfer learning\", \"data augmentation\"],\n            \"augmentation\": [\"cGAN-based techniques\"],\n            \"visualization\": [\"model performance heatmaps\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNet\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        },\n        {\n            \"name\": \"Inception\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        },\n        {\n            \"name\": \"VGG\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        }\n    ],\n    \"knowledge\": [\n        \"Ensemble techniques improve performance by combining predictions from multiple models.\",\n        \"Nested transfer learning refines the base models before combining them.\",\n        \"Progressive fine-tuning is necessary for handling breed-specific variations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection for efficiency\"],\n        \"target_device\": \"cloud GPU\",\n        \"deployment_endpoint\": \"custom model API\",\n        \"inference_engine\": [\"CUDA-optimized\"],\n        \"demonstration\": \"Visual reports showcasing model performance and efficiency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the intricate requirements of the project, including the three model architectures, ensemble techniques, and focus on resource optimization. Performance metrics and complexity measures are specified, as well as the dataset characteristics and preprocessing steps. The high expertise level of the user suggests a deeper understanding of the project requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the imminent regional product launch, design an advanced image segmentation model that precisely segments fine-grained details in hyper-detailed product photos, including complex textures and reflections, while concurrently performing real-time cross-cultural emotion analysis and sentiment detection, ensuring precise localization and resonant marketing strategies in over 10 languages.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"The task is to develop an advanced image segmentation model for a regional product launch, focusing on hyper-detailed product photos with complex textures and reflections. Additionally, the model should perform real-time cross-cultural emotion analysis and sentiment detection across 10 languages.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Segmentation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Emotion analysis accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Sentiment detection accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time (real-time)\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"detailed\": \"true\",\n                \"complex textures and reflections\": \"true\"\n            },\n            \"description\": \"A dataset consisting of hyper-detailed product photos with complex textures and reflections for training the model.\",\n            \"preprocessing\": [\"texture enhancement\", \"reflection removal\"],\n            \"augmentation\": [\n                \"image translation\",\n                \"color jittering\"\n            ],\n            \"visualization\": [\"image segmentation results\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced image segmentation model\",\n            \"family\": \"Convolutional Neural Networks (CNN) with deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"cross-cultural and multilingual support\": \"true\"\n            },\n            \"description\": \"An advanced model designed to segment fine-grained product details while handling complex textures and reflections, and also incorporating real-time emotion analysis and sentiment detection for global marketing.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Experience with cross-cultural AI models and multi-lingual sentiment analysis is essential.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud (real-time performance needed)\",\n        \"deployment_endpoint\": \"multi-lingual API for sentiment analysis and emotion detection\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"Live product photos with segmented results and real-time cross-cultural feedback\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes a detailed representation of the user's instruction, reflecting their high expertise and the specific requirements of the advanced image segmentation model with multi-language capabilities. Performance metrics for segmentation, emotion analysis, and sentiment detection are included, along with a focus on real-time performance and the need for cross-cultural adjustments. Data preprocessing and augmentation reflect the complexity of the task at hand.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "question answering", "instruction": "As the ML team lead, design a sophisticated legal QA system using a DeBERTa variant fine-tuned on the Comprehensive Legal Terminology Corpus (CLTC). Ensure the system not only accurately answers queries but also conducts multi-level reasoning, incorporates differential privacy techniques, maintains ultra-high legal precision, and furnishes detailed reasoning chains for traceability. Implement a phased validation protocol and a dynamic feedback loop for continuous enhancement and performance monitoring.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"legal\",\n        \"description\": \"The user requires a sophisticated legal question-answering system using a DeBERTa variant, fine-tuned on the Comprehensive Legal Terminology Corpus (CLTC). The system must excel in multi-level reasoning, incorporate differential privacy techniques, maintain ultra-high legal precision, and provide detailed reasoning chains for traceability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"multi-level reasoning\",\n                \"value\": null\n            },\n            {\n                \"name\": \"differential privacy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"legal precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"privacy impact\",\n                \"value\": null,\n                \"unit\": \" epsilon\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Comprehensive Legal Terminology Corpus (CLTC)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"legal queries\"],\n            \"specification\": {\n                \"size\": null,\n                \"preprocessing\": \"Fine-tuning compatible with DeBERTa variant\"\n            },\n            \"description\": \"A corpus used for fine-tuning DeBERTa for legal question answering tasks.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeBERTa variant\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": null,\n                \"flop_count\": null,\n                \"train_speed\": null,\n                \"privacy_capabilities\": \"Incorporates differential privacy\"\n            },\n            \"description\": \"A sophisticated DeBERTa variant for legal question answering, capable of multi-level reasoning.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"multi-level reasoning techniques\", \"differential privacy implementation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"traceability\": \"Detailed reasoning chains for query understanding and decision-making\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been structured based on the given instruction, reflecting the user's intent to build a high-end legal QA system. The DeBERTa variant, dataset requirements, performance metrics, and privacy considerations are included. The phased validation protocol and dynamic feedback loop for continuous improvement are also captured in the 'service' section.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "You're embarking on a multi-disciplinary deep learning project, where you must analyze the complex MIT-BIH Arrhythmia dataset for time-series arrhythmia detection. The dataset spans train, validation, and test subsets, presenting ECG signals with 200 potential features (INPUT_SEQ_LEN=500, select 10 strategic attributes). Your task is to design a novel hybrid architecture combining convolutional, attention, and explainable AI techniques for a 5-class classification problem (N, L, Q, R, S, V). Strive for a state-of-the-art macro-averaged F1-score while ensuring model interpretability in the face of heterogeneous, high-dimensional, and noisy data. Perform extensive hyperparameter tuning using Bayesian optimization and ensemble methods to optimize performance and handle potential data anomalies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"The project involves a multi-disciplinary deep learning task to analyze the MIT-BIH Arrhythmia dataset, focusing on detecting time-series arrhythmia in ECG signals with 200 features (INPUT_SEQ_LEN=500). A novel hybrid architecture combining convolutional, attention, and explainable AI techniques is designed for a 5-class classification problem (N, L, Q, R, S, V). The goal is to achieve state-of-the-art macro-averaged F1-score, emphasizing model interpretability for noisy and high-dimensional data, while handling potential data anomalies through hyperparameter tuning using Bayesian optimization and ensemble methods.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"macro-averaged F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MIT-BIH Arrhythmia Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"arrhythmia classes: N, L, Q, R, S, V\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 500,\n                \"feature_count\": 200\n            },\n            \"description\": \"A dataset for time-series arrhythmia detection with train, validation, and test subsets, containing ECG signals with high-dimensional and noisy data.\",\n            \"preprocessing\": [\n                \"Strategic feature selection of 10 attributes\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hybrid Architecture (CNN, Attention, Explainable AI)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": \"Ensuring model interpretability\"\n            },\n            \"description\": \"A novel model design for time-series arrhythmia detection combining diverse techniques to address the complexity of the dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Combining explainable AI to handle high-dimensional and noisy data\",\n        \"Utilizing Bayesian optimization for hyperparameter tuning\",\n        \"Employing ensemble methods to optimize performance and manage anomalies\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Strategic feature selection based on 10 attributes\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Need to specify how a demonstration of model interpretability will be performed\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent for building a complex project, incorporates their high expertise level, and includes the details of the task, dataset, model design, and the requirement for advanced techniques. The missing elements like specific model name, exact feature selection strategy, and demonstration of interpretability can be deduced from the instruction but are not directly specified.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for a high-performance machine learning team, you are tasked with developing an advanced time-series classification system for the complex Heartbeat dataset. The dataset, now consisting of numerous diverse real-world physiological signals, is divided into train, validation, and test sets with varying lengths (input sequences range from 405 to 500 data points) and dimensions (INPUT_SEQ_LEN=405-500, INPUT_DIM=61-80). Your objective is to design an intricate model that can handle multi-resolution analysis and sequential dependencies, while maintaining interpretability. The challenge is to achieve not only superior accuracy but also robustness across different label categories (0-1) and demonstrate strong generalization on unseen test data, using interpretability techniques for model validation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The objective is to develop an advanced time-series classification system for the Heartbeat dataset with a focus on handling multi-resolution analysis, sequential dependencies, and interpretability. The dataset is diverse with real-world physiological signals, split into train, validation, and test sets with varying lengths and dimensions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"seconds\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory usage\",\n                \"unit\": \"MB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Heartbeat dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"label categories (0-1)\"],\n            \"specification\": {\n                \"input_seq_len\": {\n                    \"min\": 405,\n                    \"max\": 500\n                },\n                \"input_dim\": {\n                    \"min\": 61,\n                    \"max\": 80\n                }\n            },\n            \"description\": \"A complex dataset with varying length and dimensional input sequences, representing real-world physiological signals for time-series classification.\",\n            \"preprocessing\": [\"multi-resolution analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [\"interpretable model visualization\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning (e.g., RNN, LSTM, or Transformers)\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"An advanced model targeting time-series analysis, capable of handling multi-resolution and sequential dependencies, with interpretability techniques for model validation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The Heartbeat dataset requires a model that can generalize well to unseen test data and handles diverse label categories. Interpretability is crucial for model validation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstration of model interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project manager's task to develop an advanced time-series classification system. It acknowledges the high expertise level, involves the complexities of the dataset, and highlights interpretability as a key aspect. Performance metrics, complexity constraints, and dataset specifics are included. Some values are not specified as they are open to the model's specific implementation details.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "visual question answering", "instruction": "For the upcoming multicultural school exhibition, design a state-of-the-art Visual Question Answering (VQA) system using a diverse blend of TextVQA and multimodal datasets. Mandate the model to demonstrate advanced cross-modal reasoning, handle complex linguistic nuances, and exhibit exceptional precision in interpreting and resolving questions from varying visual-textual contexts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"education, cultural studies\",\n        \"description\": \"The task is to create a state-of-the-art Visual Question Answering system for a multicultural school exhibition. The model should incorporate TextVQA and multimodal datasets, exhibit advanced cross-modal reasoning, handle complex linguistic nuances, and achieve high precision in interpreting and resolving questions across diverse visual-textual contexts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"cross-modal reasoning capability\",\n                \"value\": \"state-of-the-art\",\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"linguistic understanding\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TextVQA and multimodal datasets\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"diversity\": \"multicultural and cross-domain\"\n            },\n            \"description\": \"A blend of datasets for visual and textual data, emphasizing cross-modal aspects and complex linguistic nuances.\",\n            \"preprocessing\": [\"advanced data cleaning\", \"cross-modal alignment\"],\n            \"augmentation\": [\"synthetic data generation\", \"contextual data augmentation\"],\n            \"visualization\": [\"model performance across different modalities\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Multimodal Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"Transformer\", \"attention mechanisms\"],\n                \"framework\": \"state-of-the-art\"\n            },\n            \"description\": \"A VQA model designed for diverse visual-textual contexts, showcasing exceptional cross-modal reasoning and precision.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"technique\",\n            \"value\": \"Fusion of multimodal features using late fusion or joint learning\"\n        },\n        {\n            \"type\": \"data characteristic\",\n            \"value\": \"Linguistic diversity across different cultural backgrounds\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"Interactive Q&A session demonstrating model performance in real-world school exhibition scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the given instruction, specifying the VQA task for a multicultural exhibition, using TextVQA and multimodal datasets. The model's advanced requirements, such as cross-modal reasoning and linguistic nuances, are covered in the metrics and knowledge fields. The confidence score is high due to the detailed mapping of the instruction to the provided JSON structure.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular clustering", "instruction": "Perform advanced tabular clustering on the 'High-Dimensional Biotech Research Data,' incorporating feature selection using Recursive Feature Elimination (RFE) to enhance interpretability. Ensure robustness against dimensionality curse and noise, while capturing subtle trends in multiple research domains for precise R&D prioritization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular clustering\",\n        \"application_domain\": \"biotech research\",\n        \"description\": \"The task is to perform advanced clustering on a high-dimensional biotech research dataset, incorporating feature selection using RFE for improved interpretability. The objective is to handle dimensionality curse, noise, and subtle trends across multiple research domains for effective R&D prioritization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"clustering accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"silhouette score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High-Dimensional Biotech Research Data\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dimensions\": null,\n                \"number_of_records\": null\n            },\n            \"description\": \"A high-dimensional dataset from biotech research, suitable for clustering and feature selection.\",\n            \"preprocessing\": [\"Recursive Feature Elimination (RFE)\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Dimensionality Curse and Noise Analysis\",\n                \"Interpretability of Selected Features\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Clustering Models\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"An advanced clustering model designed to handle high-dimensional data, leveraging RFE for interpretability, robustness against noise, and effective R&D prioritization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Recursive Feature Elimination (RFE) will be used for feature selection, ensuring interpretability.\",\n        \"Dimensionality curse and noise reduction strategies will be applied to handle high dimensions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"RFE\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Visual and interpretive insights from RFE and clustering results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's instruction for a machine learning project. It specifies the 'build' intent with high expertise level, the 'tabular data analysis' area, and the 'tabular clustering' task. Performance metrics, complexity metrics, and RFE for feature selection are included as per the requirements. The dataset and model are described in detail based on the given instruction. The missing values in some fields (such as performance metric values) are placeholders, indicating they need to be determined or optimized during the project.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a project for the machine learning capstone, propose a hybrid content and context-aware recommendation system using the extensive 'Bilibili Danmu' dataset. Given the real-time nature, devise an algorithm optimizing not only for video relevance but also temporal sequence, with a focus on speed and scalability, catering to users' binge-watching habits.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"entertainment\",\n        \"description\": \"The project involves developing a hybrid content and context-aware recommendation system for real-time video recommendations on Bilibili Danmu dataset, considering temporal sequence, speed, and scalability to cater to binge-watching habits.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"video relevance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"speed\",\n                \"value\": null\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Bilibili Danmu Dataset\",\n            \"modality\": [\"text\", \"video\"],\n            \"target_variables\": [\"video ID\"],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"temporal_data\": true\n            },\n            \"description\": \"A dataset containing real-time danmu data for Bilibili videos, reflecting video relevance and the users' engagement pattern.\",\n            \"preprocessing\": [\"contextual feature extraction\", \"time-series data cleaning\"],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [\"user behavior analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Content and Context-Aware Model\",\n            \"family\": \"Sequential recommendation system\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"algorithm\": \"sequence-aware architecture\",\n                \"optimization\": \"speed-focused\"\n            },\n            \"description\": \"A model designed to handle both video relevance and temporal context for the Bilibili Danmu dataset, with an emphasis on efficiency and scalability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for temporal patterns\", \"contextual embeddings\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"fast inference algorithms\"],\n        \"demonstration\": \"Interactive demo showcasing the recommendation system's performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the instruction, covering the intent (high expertise), project requirements (hybrid recommendation system, real-time processing, and user habits), and specific details about the dataset, model, and performance metrics. Key elements have been extracted from the instruction and organized accordingly.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a seasoned project manager for a high-precision media campaign cost forecasting, your machine learning team must develop a state-of-the-art regression model using advanced ensemble techniques. The dataset, featuring encrypted categorical variables and hierarchical relationships, has been divided into non-iid subsets for train, validation, and test. The challenge now includes optimizing for RMSLE while deciphering the latent dependencies. Showcase superior predictive accuracy under strict interpretability constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"The task involves developing a state-of-the-art regression model for high-precision media campaign cost forecasting. The dataset is encrypted with categorical variables, hierarchical relationships, and non-iid subsets for train, validation, and test. The focus is on optimizing for root mean squared log error (RMSLE) while maintaining model interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": \"high\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Encrypted Media Campaign Cost Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"campaign_cost\"],\n            \"specification\": {\n                \"encrypted\": true,\n                \"categorical\": true,\n                \"hierarchical\": true,\n                \"subset_distribution\": \"non-iid\"\n            },\n            \"description\": \"A dataset with encrypted categorical variables and hierarchical relationships, designed for campaign cost forecasting with train, validation, and test subsets.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_techniques\": [\"advanced\"],\n                \"interpretability\": \"addressed\"\n            },\n            \"description\": \"A regression model using advanced ensemble techniques, optimized for high predictive accuracy and interpretability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"interpretability-focused\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project manager's requirements for a state-of-the-art regression model with ensemble techniques, RMSLE optimization, and interpretability. The dataset description includes details about encrypted variables, hierarchical relationships, and the distribution of subsets. The model family and interpretability specifications are directly derived from the instruction. However, exact model specifications and interpretation methods are left open for flexibility.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "For this advanced data science challenge, you are to tackle a high-dimensional wild blueberry yield prediction problem using the Wild Blueberry Yield Dataset. The dataset contains intricate, seasonally varying factors and non-linear relationships. Your goal is to develop and fine-tune a state-of-the-art ensemble model, optimizing for mean absolute error (MAE) across train, validation, and test sets, ensuring robustness and generalization performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"The task involves predicting wild blueberry yield using a high-dimensional dataset with intricate, seasonally varying factors and non-linear relationships. The focus is on developing and fine-tuning a state-of-the-art ensemble model for robustness and generalization, optimizing for mean absolute error across train, validation, and test sets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wild Blueberry Yield Dataset\",\n            \"modality\": [\n                \"tabular\"\n            ],\n            \"target_variables\": [\n                \"yield item\"\n            ],\n            \"specification\": {\n                \"dimensionality\": \"high-dimensional\",\n                \"seasonality\": true,\n                \"non-linearity\": true\n            },\n            \"description\": \"A dataset with intricate features, reflecting seasonal variations and complex relationships for wild blueberry yield prediction.\",\n            \"preprocessing\": [\"feature engineering for non-linear relationships\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art Ensemble Model\",\n            \"family\": \"Ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization\": {\n                    \"objective\": \"minimize MAE\"\n                },\n                \"fine-tuning\": true,\n                \"robustness\": true,\n                \"generalization\": true\n            },\n            \"description\": \"An advanced ensemble model designed for high-dimensional data with emphasis on robustness and generalization, optimized for mean absolute error.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\",\n            \"feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"using performance across all datasets (train, validation, and test)\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately represents the given instruction, including the high-level intent to build a model, the high expertise level, the focus on a high-dimensional data problem, ensemble model type, and optimization for mean absolute error. It specifies seasonality and non-linearity in the dataset and incorporates details about preprocessing, fine-tuning, and model robustness. The confidence score is set to a high value given the comprehensive mapping of the instruction to the JSON format.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Utilize the extensive OpenSubtitles corpus for a multifaceted translation project, aiming to develop a BERT-based conversational model proficient in English-to-Mandarin translation. Mandate exceptional handling of colloquial language, idiomatic expressions, and regional slang, requiring minimal loss in nuance and cultural essence. Additionally, optimize for real-time, cross-cultural adaptation, and benchmark against state-of-the-art translation benchmarks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"The project aims to create a BERT-based conversational model for English-to-Mandarin translation, focusing on colloquial language, idiomatic expressions, and regional slang. It requires minimal loss of nuance and cultural essence and targets real-time, cross-cultural adaptation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"BLEU score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"fluency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time inference speed\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"cross-cultural adaptation\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"large-scale\",\n                \"language_types\": [\"English\", \"Mandarin\"]\n            },\n            \"description\": \"A massive multilingual corpus containing subtitles for real-world conversations, intended for training the translation model.\",\n            \"preprocessing\": [\"BERT tokenization\", \"data cleaning\"],\n            \"augmentation\": [\"back-translation\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-based conversational model\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"large\",\n                \"BERT architecture\": \"pre-trained (English and Mandarin)\",\n                \"specialization\": \"translation and handling of colloquial language\"\n            },\n            \"description\": \"A high-performance model designed for English-to-Mandarin translation, emphasizing conversation-style inputs and cultural adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Understanding of colloquial language, idiomatic expressions, and regional slang adaptation techniques.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual embeddings\", \"dynamic decoding\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-based\"],\n        \"demonstration\": \"real-time translation demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the task at hand, which involves building a high-performance BERT-based model for English-to-Mandarin translation. It specifies the focus on colloquial language, idiomatic expressions, and real-time adaptation. Performance metrics and complexity metrics are mentioned, yet left open for optimization. Key elements from the instruction are included, maintaining its essence.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager overseeing an advanced AI research team, we have been tasked with developing a state-of-the-art plant species recognition system using the 'Plant Seedlings Dataset 2.0'. The objective is to design a real-time, high-resolution image classifier that can not only accurately differentiate among diverse species under varying lighting conditions but also demonstrate robustness in edge cases. In addition to model architecture recommendations (e.g., deep convolutional neural networks with transfer learning), please provide a detailed expected accuracy range, inference speed benchmarks, and a discussion on potential challenges and optimizations for deployment on edge devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"agriculture, botany\",\n        \"description\": \"The project aims to develop a real-time, high-resolution plant species recognition system using the Plant Seedlings Dataset 2.0. It should accurately differentiate species under varying lighting conditions, demonstrate robustness in edge cases, and be deployable on edge devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"FPS (frames per second)\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time on edge devices\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Seedlings Dataset 2.0\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"plant species\"],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"variety_of_conditions\": \"including varying lighting\"\n            },\n            \"description\": \"A dataset for plant species classification with emphasis on high-resolution images under diverse conditions.\",\n            \"preprocessing\": [\"image normalization\", \"data augmentation\"],\n            \"augmentation\": [\"lighting adjustments\", \"random cropping\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep convolutional neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": true,\n                \"architecture_details\": \"e.g., ResNet, VGGNet\"\n            },\n            \"description\": \"Suggested for real-time plant species recognition, incorporating transfer learning for improved performance.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Challenges in edge device deployment: limited computational power, memory constraints, and real-time latency requirements.\"\n        },\n        {\n            \"description\": \"Potential optimizations: model pruning, quantization, or using specialized hardware (e.g., dedicated AI chips).\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"anomaly detection for edge cases\"\n        ],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"lightweight deep learning frameworks like TensorFlow Lite\"],\n        \"demonstration\": \"Real-time classification demonstration on edge devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the project's core objectives, the chosen dataset and model architecture, the performance expectations, and the context for edge device deployment. It also accounts for potential challenges and optimizations. The confidence score is high due to the comprehensive mapping of the instruction into the JSON schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "In a niche eco-friendly real estate market, you've been tasked with forecasting the premium price of sustainable housing based on unique attributes and location-based factors. The target variable is 'Green Premium Value', which reflects the added value due to eco-conscious design. Your team must develop a robust tabular regression model that incorporates regional carbon footprint scores, renewable energy integration levels, and green space proximity. Not only will you need to preprocess and engineer custom features, but also create novel indicators like energy efficiency ratio and green index. Evaluate the model using the coefficient of determination (R-squared) on the unseen 'Green Premium Test Set', ensuring the model's predictive power aligns with the market's sustainability expectations. Report the result and discuss potential market implications for green housing investment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"real estate\",\n        \"description\": \"The goal is to forecast sustainable housing premium prices based on unique attributes, location factors, regional carbon footprint, renewable energy integration, and green space proximity. A robust tabular regression model is required, with custom feature engineering and novel indicators like energy efficiency ratio and green index. Evaluation will be through R-squared on a 'Green Premium Test Set', meeting market sustainability expectations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"coefficient of determination (R-squared)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\n                \"tabular\"\n            ],\n            \"target_variables\": [\n                \"Green Premium Value\"\n            ],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"length\": null,\n                \"data sources\": {\n                    \"unique attributes\": \"description\",\n                    \"location factors\": \"description\",\n                    \"carbon footprint\": null,\n                    \"renewable energy\": null,\n                    \"green space proximity\": null\n                }\n            },\n            \"description\": \"A dataset for eco-friendly real estate market forecasting, including custom attributes, location, and environmental metrics.\",\n            \"preprocessing\": [\n                \"feature engineering\",\n                \"novel indicator creation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"Green Premium Test Set\",\n            \"modality\": [\n                \"tabular\"\n            ],\n            \"target_variables\": [\n                \"Green Premium Value\"\n            ],\n            \"specification\": null,\n            \"description\": \"The unseen test set to evaluate model's predictive power on the green housing premium value.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Tabular Regression Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": null,\n                \"architecture\": \"custom with regional carbon footprint, renewable energy integration, and green index\"\n            },\n            \"description\": \"A robust tabular regression model designed for sustainable housing premium prediction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Potential market implications discussed include impact on green housing investment and sustainability expectations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"involving regional carbon footprint scores, renewable energy integration levels, and green space proximity\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"results and implications for green housing investment\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the given instruction, accurately capturing the task of forecasting green housing premiums with high-level requirements. Novel indicators and market implications are acknowledged, and the user's high expertise level is reflected. Specifics on model construction are open-ended to accommodate customization.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for a cutting-edge machine learning retail optimization system, your team has been commissioned by a high-end luxury fashion retailer to develop an advanced object detection algorithm that not only identifies potential shoplifting but also analyzes customer interactions with high-value displays and personalized marketing materials. The system should utilize subtle cues and advanced behavioral analysis to detect anomalies, while maintaining the discretion of a luxury brand experience. It must integrate seamlessly with the store's infrastructure and generate real-time alerts with minimal false positives, all while adhering to strict data privacy regulations and ethical AI principles.", "resposne": "{\n  \"user\": {\n    \"intent\": \"build\",\n    \"expertise\": \"high\"\n  },\n  \"problem\": {\n    \"area\": \"computer vision\",\n    \"downstream_task\": \"object detection\",\n    \"application_domain\": \"retail\",\n    \"description\": \"The project involves developing an advanced object detection algorithm for a luxury fashion retailer to detect shoplifting and analyze customer interactions with high-value displays, personalized marketing materials, and subtle cues for behavioral analysis. The system must be discreet, have low false positives, integrate with store infrastructure, and adhere to data privacy and ethical AI standards.\",\n    \"performance_metrics\": [\n      {\n        \"name\": \"detection accuracy\",\n        \"value\": null\n      },\n      {\n        \"name\": \"false positive rate\",\n        \"value\": \"minimal\"\n      }\n    ],\n    \"complexity_metrics\": []\n  },\n  \"dataset\": [\n    {\n      \"name\": \"Luxury Retail Shoplifting & Interactions Dataset\",\n      \"modality\": [\"image\", \"video\"],\n      \"target_variables\": [\"shoplifting\", \"customer_interactions\"],\n      \"specification\": {\n        \"subtle_cues\": true,\n        \"ethics_sensitive\": true\n      },\n      \"description\": \"A dataset containing high-quality images and videos capturing customer behavior and high-value displays, with a focus on maintaining privacy and luxury brand experience.\",\n      \"preprocessing\": [],\n      \"augmentation\": [],\n      \"visualization\": [],\n      \"source\": \"user-link\"\n    }\n  ],\n  \"model\": [\n    {\n      \"name\": \"Advanced Object Detection Algorithm (Custom)\",\n      \"family\": \"Advanced CNN or Deep Learning Model\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"anomaly_detection\": true,\n        \"discreetness\": true\n      },\n      \"description\": \"A custom-designed model for luxury fashion retail, leveraging advanced behavioral analysis and AI principles to meet specific business requirements.\"\n    }\n  ],\n  \"knowledge\": [\n    {\n      \"content\": \"Adherence to GDPR-like data privacy regulations and ethical AI practices for a luxury brand.\"\n    }\n  ],\n  \"service\": {\n    \"feature_engineering\": [],\n    \"target_device\": \"cloud\",\n    \"deployment_endpoint\": \"real-time alerts integration\",\n    \"inference_engine\": [\"machine learning as a service\"],\n    \"demonstration\": \"luxury brand experience with low-key alerts\"\n  },\n  \"confidence\": {\n    \"rationale\": \"The JSON response reflects the complexity and requirements of the advanced object detection project for a luxury fashion retailer. The high expertise level of the user is assumed given the project's complexity and privacy concerns. The performance metrics and dataset specifications capture the key aspects of the project. The system's seamless integration and real-time alert generation with data privacy are emphasized.\",\n    \"score\": 0.98\n  }\n}\n"}
{"task": "image classification", "instruction": "As a large-scale sustainable farming enterprise, we seek a cutting-edge machine learning system to not only analyze high-resolution aerial photographs of diverse crop species but also detect nuanced variations in plant health, such as nutrient deficiencies, pests, and diseases, while factoring in weather patterns and growth stages. Can you develop a real-time, scalable, and precision-driven solution that reduces human intervention, ensuring timely and efficient decision-making for optimal yields?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"The task involves developing a cutting-edge machine learning system for real-time analysis of high-resolution aerial photos of diverse crop species. The system should detect nuanced variations in plant health (nutrient deficiencies, pests, diseases), factor in weather patterns, and consider growth stages to minimize human intervention and enhance precision for optimal yields.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"real-time\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High-resolution aerial crop imagery\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"plant health indicators\", \"weather data\", \"growth stages\"],\n            \"specification\": null,\n            \"description\": \"Dataset with diverse aerial photos of crops for real-time analysis and monitoring.\",\n            \"preprocessing\": [\"high-resolution data preprocessing\", \"multi-modal fusion\"],\n            \"augmentation\": [\"weather simulating transformations\", \"growth stage variations\"],\n            \"visualization\": [\"health condition heatmaps\", \"weather pattern overlays\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image Analysis System\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep learning with transfer learning from ImageNet\",\n                \"depth\": null,\n                \"accuracy on similar tasks\": null\n            },\n            \"description\": \"An advanced model that combines deep learning and transfer learning to analyze crop health, weather patterns, and growth stages.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction from satellite imagery\",\n            \"contextual feature fusion\"\n        ],\n        \"target_device\": \"cloud-based or edge computing\",\n        \"deployment_endpoint\": \"agro-analytics platform or IoT sensors\",\n        \"inference_engine\": [\"GPU accelerated\", \"real-time\"],\n        \"demonstration\": \"interactive crop health dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the enterprise's requirements, including the high expertise level of the user, the real-time and scalable nature, and the focus on precision. The problem area, downstream task, and performance metrics are well-defined, and the requested system description and source of the dataset are accounted for. The missing values are placeholders for actual metric expectations or implementation details.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create an advanced machine translation model, utilizing the Tatoeba Multilingual Dataset fusion with the MUSE framework, for translating highly nuanced poetry from Spanish to Russian. The system should not only maintain grammatical fidelity but also capture the cultural idioms, poetic devices, and subtle emotional nuances found in metaphorical expressions, while ensuring preservation of rhythm and poetic structure. Additionally, devise a mechanism for continuous evaluation and improvement using unsupervised learning, without relying on human-annotated gold standards.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics\",\n        \"description\": \"The objective is to create an advanced machine translation model, specifically for translating Spanish poetry to Russian, using Tatoeba Multilingual Dataset fused with MUSE framework. The system must maintain grammatical accuracy, preserve cultural idioms, poetic devices, emotional nuances, and the rhythm and structure of the original poem. It also requires an unsupervised learning mechanism for continuous evaluation and improvement, without reliance on human-annotated gold standards.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"grammatical fidelity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cultural idioms preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"emotional nuances preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"rhythm preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"poetic structure preservation\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tatoeba Multilingual Dataset\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\"Spanish\", \"Russian\"],\n            \"specification\": {\n                \"languages\": [\"Spanish\", \"Russian\"],\n                \"multilingual\": true\n            },\n            \"description\": \"The dataset for training, containing poetry in both Spanish and Russian languages, fused with MUSE framework.\",\n            \"preprocessing\": [\n                \"data fusion using MUSE\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Poetry Translation Model\",\n            \"family\": \"neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"MUSE adapted for poem translation\"\n            },\n            \"description\": \"A system designed specifically for translating nuanced poetry, leveraging Tatoeba data fusion and unsupervised learning for continuous improvement.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Requirements for preserving cultural idioms, poetic devices, and emotional nuances.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"unsupervised learning mechanism for adaptation\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"unsupervised learning-based adaptation during deployment\"\n        ],\n        \"demonstration\": \"Demonstration showcasing poetry translation with preservation of nuances\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's request for an advanced translation model that specializes in poetry with specific requirements for cultural, poetic, and emotional nuances. Unsupervised learning is included for continuous improvement. The metrics and details about the Tatoeba dataset, MUSE framework, and the model are justified based on the task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "Incorporate a machine learning system into the menu catalog that not only recognizes dish images but also conducts real-time text analysis of recipe descriptions, differentiating among complex dietary labels (like organic, halal, and locally sourced), while ensuring 99% accuracy under varying lighting conditions and resolution, to streamline customer experience and staff efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"restaurants and food services\",\n        \"description\": \"The project aims to develop a machine learning system that can recognize dish images, conduct real-time text analysis on recipe descriptions, and differentiate complex dietary labels (organic, halal, locally sourced) with 99% accuracy, accommodating varying lighting conditions and image resolution for improved customer experience and staff efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Dish Image and Recipe Dataset\",\n            \"modality\": [\"image\", \"text\"],\n            \"target_variables\": [\"dietary labels\"],\n            \"specification\": {\n                \"min_image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"max_image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"variations_in_lighting\": \"various\",\n                \"various_resolutions\": true\n            },\n            \"description\": \"A dataset containing dish images and recipe descriptions with diverse dietary labels, designed to handle real-world conditions.\",\n            \"preprocessing\": [\n                \"image resizing\",\n                \"text preprocessing\"\n            ],\n            \"augmentation\": [\n                \"image: lighting, resolution\",\n                \"text: data augmentation techniques\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dish Recognition and Recipe Analysis\",\n            \"family\": \"Convolutional Neural Network (Image) and Natural Language Processing (Text)\",\n            \"type\": [\"neural networks\", \"classical machine learning\"],\n            \"specification\": null,\n            \"description\": \"A multi-model system combining image and text processing to accurately identify dishes and dietary labels.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced NLP models for text analysis, transfer learning for image recognition, and diet label processing algorithms.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"image feature extraction, text feature extraction\"\n        ],\n        \"target_device\": \"edge device\",\n        \"deployment_endpoint\": \"cloud-based API or dedicated server\",\n        \"inference_engine\": [\"real-time inference\"],\n        \"demonstration\": \"live in-store implementation and user demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's intent to build a machine learning system that meets specific performance and complexity requirements. It includes the image recognition, text analysis, and dietary label differentiation aspects, along with the target performance and lighting/resolution conditions. The missing values in 'inference time' and 'memory footprint' indicate that these will be optimized during development based on the system design.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a seasoned project manager for our growing digital repository, we seek a sophisticated machine learning system that not only discerns but also precisely segments and reconstructs the intricate layout of various age-old illustrations in our extensive manuscript collection, considering faded lines, overlapping elements, and diverse artistic styles, while maintaining historical accuracy. Can you propose a tailored algorithm for this multi-faceted challenge?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"digital humanities\",\n        \"description\": \"The goal is to develop a sophisticated machine learning system for segmenting and reconstructing intricate layouts of age-old illustrations in a manuscript collection, taking into account faded lines, overlapping elements, and diverse artistic styles while preserving historical accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"fidelity to historical accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Manuscript Illustration Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"variety_of_illustrations\": {\n                    \"styles\": [],\n                    \"fading\": true,\n                    \"overlapping\": true\n                }\n            },\n            \"description\": \"A collection of diverse and complex illustrations with historical context to train the system for identifying intricate layouts.\",\n            \"preprocessing\": [\"image restoration\", \"artifact removal\"],\n            \"augmentation\": [\"geometric transformations, color adjustments\"],\n            \"visualization\": [\"process analysis\", \"performance comparison\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Image Segmentation Algorithm\",\n            \"family\": \"Convolutional Neural Networks (deep learning)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"autoencoder or U-Net\",\n                \"handling_complexity\": \"deep learning features, attention mechanisms\"\n            },\n            \"description\": \"A tailored algorithm designed to address the multi-faceted challenge of segmenting and reconstructing intricate illustration layouts in historical manuscripts.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider incorporating historical art restoration techniques and domain knowledge to improve accuracy.\",\n        \"Collaborate with domain experts to validate reconstructed layouts.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"image preprocessing pipelines\", \"domain-specific features\"],\n        \"target_device\": \"high-performance computing (HPC) cluster or cloud\",\n        \"deployment_endpoint\": \"data repository or digital humanities platform\",\n        \"inference_engine\": [\"GPU-optimized\", \"batch processing\"],\n        \"demonstration\": \"interactive visualizations showcasing system performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a complex machine learning system and showcases the high level of expertise needed for the task. It includes relevant areas, tasks, dataset specifications, and performance metrics for the problem at hand. The model selection and the knowledge points are guided by the requirement for handling complex and historical elements. The service requirements align with the system's complexity and intended use.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As the management of our premium health and wellness platform, we aim to enhance user engagement by implementing a sophisticated AI system for personalized mental and physical wellness routines. In addition to tracking daily exercise routines and progress, the system should consider factors such as real-time heart rate variability, sleep patterns, and nutrition intake. Can you design a cutting-edge sequential recommendation algorithm that not only suggests the next workout or activity but also suggests mindfulness practices, nutrition plans, and rest days, all dynamically adjusted to align with each user's unique stress levels, fitness goals, and individual recovery rates? This feature should maximize long-term adherence while promoting a holistic approach to well-being.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"health and wellness\",\n        \"description\": \"The task is to design an AI system for a premium platform that enhances user engagement by suggesting personalized mental and physical wellness routines. The system should track daily exercise, heart rate variability, sleep patterns, and nutrition intake, adjusting workout suggestions, mindfulness practices, nutrition plans, and rest days based on individual stress levels, fitness goals, and recovery rates.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"engagement rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"adherence rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"user satisfaction\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time data processing\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"User Health and Wellness Data\",\n            \"modality\": [\n                \"time series\",\n                \"tabular\"\n            ],\n            \"target_variables\": [\n                \"heart rate variability\",\n                \"sleep patterns\",\n                \"nutrition intake\"\n            ],\n            \"specification\": {\n                \"data_structure\": \"continuous and longitudinal\",\n                \"sample_size\": null,\n                \"dimensions\": {\n                    \"time_series\": {\n                        \"window_size\": null,\n                        \"sampling_rate\": null\n                    },\n                    \"tabular\": {\n                        \"rows\": null,\n                        \"columns\": [\n                            \"stress levels\",\n                            \"fitness goals\",\n                            \"recovery rates\"\n                        ]\n                    }\n                }\n            },\n            \"description\": \"Contains time series data for exercise and vital health indicators, alongside a structured tabular data on user-specific information.\",\n            \"preprocessing\": [\n                \"real-time data cleaning\",\n                \"time series feature extraction\"\n            ],\n            \"augmentation\": [\n                \"time series data interpolation\"\n            ],\n            \"visualization\": [\n                \"data patterns and trends\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Sequential Recommendation Algorithm (e.g., GRU4Rec, Temporal Convolutional Networks)\",\n            \"family\": \"Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"dynamically adjusted for personalized recommendations\",\n                \"layers\": [\n                    \"embedding\",\n                    \"RNN layers for sequence modeling\"\n                ],\n                \"optimization\": \"suitable for real-time updates\"\n            },\n            \"description\": \"A cutting-edge algorithm designed for personalized sequential recommendations that dynamically adapts to users' unique well-being needs.\"\n        }\n    ],\n    \"knowledge\": [\n        \"holistic approach to well-being\",\n        \"adaptation based on real-time user data\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"integrating heart rate variability and recovery metrics\"\n        ],\n        \"target_device\": \"smart wearable and mobile devices\",\n        \"deployment_endpoint\": \"platform-specific API\",\n        \"inference_engine\": [\"real-time inference with low latency\"],\n        \"demonstration\": \"customized dashboard showcasing recommendations and progress updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately represents the provided instruction, capturing the user's intention to build a high-level AI system with high expertise. It specifies the problem area, tasks, performance metrics, and complexity considerations. The dataset is well-defined to support personalized recommendations, and the model selected is in line with the requirements. The system's service requirements reflect a focus on real-world implementation.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the underwater image segmentation challenge on CoralNet, propose a state-of-the-art FusionNet architecture integrated with a photometric adaptation module, ensuring robust performance in recognizing diverse coral species and marine organisms under dynamic water conditions, with a requirement to surpass existing metrics by at least 15% under low-light scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"ecology\",\n        \"description\": \"The task is to develop a state-of-the-art FusionNet architecture for underwater coral and marine organism segmentation. A photometric adaptation module is essential to ensure robust performance in recognizing species under varying water conditions, specifically targeting a 15% improvement in existing metrics for low-light scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"existing metrics\",\n                \"value\": {\n                    \"comparison\": \"improve by at least 15%\",\n                    \"condition\": \"low-light scenarios\"\n                }\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CoralNet\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"coral species, marine organisms\"],\n            \"specification\": {\n                \"dynamic_water_conditions\": true,\n                \"low_light_scenarios\": true\n            },\n            \"description\": \"The dataset consists of underwater images for coral and marine organism segmentation, focusing on diverse species and challenging conditions like low light.\",\n            \"preprocessing\": [\n                \"Include photometric adaptation for low-light scenarios\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"FusionNet\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"photometric_adaptation_module\": true\n            },\n            \"description\": \"Proposed model architecture with a focus on robust performance and adaptation for diverse water conditions, particularly for low-light scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integration of photometric adaptation is crucial for underwater segmentation success.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Performance benchmarking against existing metrics in low-light conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON structure reflects the user's high expertise and the specific task of building a state-of-the-art model for underwater image segmentation. It includes the requested FusionNet architecture, photometric adaptation module, and the performance target of surpassing existing metrics by 15% in low-light conditions. The dataset description reflects the requirements for dynamic water conditions and the preprocessing step for photometric adaptation.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "machine translation", "instruction": "The project necessitates the development of a state-of-the-art neural machine translation system adept at handling underrepresented languages, focusing on Swahili-to-English translation. Mandate the integration of advanced Transformer-XL and bidirectional LSTM architectures within the Masakhane corpus. Strive to elevate benchmark performance by a substantial 15% in absolute BLEU scores, ensuring compatibility with cross-lingual transfer learning and attending to idiomatic nuances.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual\",\n        \"description\": \"The project aims to develop a state-of-the-art neural machine translation system, focusing on Swahili-to-English translation. It requires integrating Transformer-XL and bidirectional LSTM architectures in the Masakhane corpus. The goal is to achieve a 15% improvement in absolute BLEU scores, while considering cross-lingual transfer learning and maintaining idiomatic nuances.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Masakhane corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Swahili\", \"English\"],\n            \"specification\": {\n                \"language_pair\": {\n                    \"source\": \"Swahili\",\n                    \"target\": \"English\"\n                },\n                \"corpus_size\": \"Masakhane-specific\",\n                \"multilingual\": true,\n                \"multilingual_support\": \"Swahili-to-English\"\n            },\n            \"description\": \"A corpus specifically designed for training a machine translation system, incorporating cross-lingual learning and the Masakhane project's focus on underrepresented languages.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL + Bidirectional LSTM\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"Transformer-XL\", \"bidirectional LSTM\"],\n                \"corpus_integration\": \"Masakhane corpus\",\n                \"cross-lingual_learning\": true\n            },\n            \"description\": \"A cutting-edge machine translation model combining Transformer-XL and bidirectional LSTM to handle Swahili-to-English translation.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"The model will need to account for idiomatic nuances and handle underrepresented languages.\"\n        },\n        {\n            \"description\": \"Advanced Transformer-XL and bidirectional LSTM architectures are chosen for their state-of-the-art performance.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Emphasis on maintaining idiomatic nuances and explaining benchmark improvement with a 15% absolute BLEU score increase.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's intent to build a highly specialized machine translation system using Masakhane corpus, targeting a substantial performance improvement and idiom preservation. The key details, such as the model architectures and performance metric, are included according to the requirements. The high expertise level is reflected in the complex task at hand.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As a seasoned project manager, I need a machine learning team to develop an advanced time-series anomaly detection system for the hourly power consumption data of a small town spanning three years. The model must detect subtle patterns, withstand seasonality challenges, and account for sudden outliers while ensuring real-time monitoring within a tight deadline of 24-hour response time.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"energy management\",\n        \"description\": \"The project involves developing an advanced time-series anomaly detection system for hourly power consumption data of a small town over three years. The model should handle subtle patterns, address seasonality, manage outliers, and ensure real-time monitoring with a 24-hour response time.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 24.0,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Small Town Power Consumption Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly power consumption\"],\n            \"specification\": {\n                \"duration\": \"three years\",\n                \"frequency\": \"hourly\"\n            },\n            \"description\": \"Data for hourly power consumption of a small town over a three-year period.\",\n            \"preprocessing\": [\n                \"seasonality removal\",\n                \"feature engineering for handling time-based patterns\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Time-series Anomaly Detection Model\",\n            \"family\": \"Unsupervised Learning (e.g., LSTM, Proximal Policy Optimization)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"deep learning approach\": true,\n                \"real-time monitoring\": true,\n                \"resilience to outliers\": true\n            },\n            \"description\": \"A state-of-the-art model designed to handle time-series data and meet the 24-hour response time requirement for the power consumption anomaly detection task.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Seasonality may require data segmentation by year and hour.\",\n        \"Sudden outliers can be addressed through ensemble methods or using robust loss functions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud for real-time processing\",\n        \"deployment_endpoint\": \"firmware update or API for the town's monitoring system\",\n        \"inference_engine\": [\"real-time streaming for hourly updates\"],\n        \"demonstration\": \"Interactive dashboard showcasing detected anomalies and response times\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the provided schema, reflecting the user's intent to build a complex system for an expert-level project. The advanced time-series anomaly detection system and real-time requirements are captured, along with relevant performance and complexity metrics. Given the tight deadline, the response time metric was specified, and preprocessing steps are mentioned that address seasonality and subtle patterns. Knowledge and service requirements are relevant to the task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for a specialized AI development team, request a high-performance deep learning model that can accurately classify not only the breed but also age and health status of cats based on a diverse dataset of cat images, considering subtle variations and challenging features.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal health and pet care\",\n        \"description\": \"The project aims to develop a high-performance deep learning model for classifying not only cat breeds but also the age and health status from a diverse dataset of cat images, emphasizing the detection of subtle variations and challenging features.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Diverse Cat Images Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\", \"age\", \"health status\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"length\": null,\n                \"diversity\": \"subtle variations and challenging features\"\n            },\n            \"description\": \"A comprehensive dataset containing cat images for breed, age, and health status classification, requiring high accuracy and robustness to variations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"data augmentation to handle variations in lighting, angles, and backgrounds\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Learning Model (e.g., ResNet, EfficientNet, or DenseNet)\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"high-performance\",\n                \"interpretability\": null\n            },\n            \"description\": \"A deep learning model designed for robust classification of cat images, focusing on breed, age, and health status.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction and engineering for fine-grained classification\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow.js, ONNX, or WebAssembly for deployability\"],\n        \"demonstration\": \"visual explanations, confusion matrices, and user-friendly demo\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was constructed based on the given user instruction, capturing the high-level intent (build) for a high-expertise project manager, the focus on computer vision (image classification), and the inclusion of complex factors (age and health status). The dataset and model specifications were adapted to suit the challenging nature of the task, involving subtle variations and challenging features, and the performance metrics are left open for the model's evaluation. The confidence score assumes a high level of confidence given the clear task description and requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "link prediction", "instruction": "For a comprehensive team compatibility app, develop a state-of-the-art link prediction model that not only analyzes historical project collaborations and individual expertise but also factors in real-time communication frequency, conflicting work styles, and departmental dynamics. The system must prioritize not just efficiency but also cultural fit to foster productive synergies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"team collaboration and organizational dynamics\",\n        \"description\": \"The task is to develop a link prediction model for a comprehensive team compatibility app, considering historical project collaborations, individual expertise, real-time communication frequency, conflicting work styles, and departmental dynamics. The model must prioritize efficiency and cultural fit to enhance productivity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Historical Project Collaborations\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [\n                \"collaboration likelihood\"\n            ],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": {\n                    \"nodes\": null,\n                    \"edges\": null\n                }\n            },\n            \"description\": \"Contains historical collaboration data for project teams.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"Employee Expertise and Communication\",\n            \"modality\": [\n                \"tabular\"\n            ],\n            \"target_variables\": [\n                \"real-time communication frequency\",\n                \"conflicting work styles\"\n            ],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": {\n                    \"rows\": null,\n                    \"columns\": null\n                }\n            },\n            \"description\": \"Data on individual and departmental communication patterns and work styles.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-Art Link Prediction Model\",\n            \"family\": \"Graph Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"For example, use Graph Convolutional Networks (GCN) or GAT\",\n                \"parameters\": null\n            },\n            \"description\": \"A highly advanced model designed to predict compatibility in team dynamics by considering diverse factors.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time communication frequency and cultural fit should be prioritized for enhancing productivity.\",\n        \"Integration with cultural fit assessment algorithms is crucial.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Including real-time interaction data\",\n            \"Analyzing work style similarity\"\n        ],\n        \"target_device\": \"Cloud or distributed environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Efficient model serving tools like TensorFlow Serving or TensorFlow.js\"],\n        \"demonstration\": \"Interactive visualizations of team compatibility\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately represents the instruction provided, focusing on a high-level model development task in the area of graph machine learning. It considers various factors like historical data, real-time communication, and cultural fit. Performance and complexity metrics are included, and details of datasets and model types are adjusted based on the task's requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Apply a state-of-the-art autoencoder-forecasting ensemble with a prophet-based anomaly detection component, utilizing ARIMA and LSTM models, to forecast daily active users in a time-varying, cryptocurrency app dataset. Ensure the model incorporates hourly seasonality and cross-validation for robustness, while adopting L1 regularization to control for sparsity in the feature space.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"finance\",\n        \"description\": \"The user wants to create a state-of-the-art autoencoder ensemble model for forecasting daily active users in a cryptocurrency app dataset. The model should consist of ARIMA and LSTM components, with a prophet-based anomaly detection system. It must account for hourly seasonality, employ cross-validation for robustness, and utilize L1 regularization to manage feature sparsity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Absolute Error (MAE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Root Mean Squared Error (RMSE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Anomaly Detection Accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cryptocurrency App Daily Active Users\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"daily active users\"],\n            \"specification\": {\n                \"time_frequency\": \"daily\",\n                \"data_size\": null,\n                \"seasonality\": \"hourly\"\n            },\n            \"description\": \"A time-varying dataset containing historical daily active users for a cryptocurrency app, including hourly seasonality.\",\n            \"preprocessing\": [\"hourly seasonality adjustment\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Autoencoder-forecasting Ensemble\",\n            \"family\": [\"Ensemble models\", \"Neural networks\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"components\": [\"Prophet\", \"ARIMA\", \"LSTM\"],\n                \"regularization\": \"L1\",\n                \"cross_validation\": true\n            },\n            \"description\": \"An advanced forecasting model that combines autoencoder, ARIMA, and LSTM components, with a prophet-based anomaly detection system.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"L1 regularization for feature sparsity\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Visualizations of anomaly detection and forecasting results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a model, targeting high expertise level. The problem area, task, and specific requirements (state-of-the-art ensemble, hourly seasonality, L1 regularization, etc.) are included. Performance metrics, model specification, and dataset description are adapted from the instruction. The confidence score is not explicitly given, but the provided information should be sufficient for an experienced ML engineer.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, seek a modular, lightweight conversational AI for AmazonQA dataset, with a focus on speed, efficiency, and at least 90% accuracy in handling diverse customer queries while maintaining a concise response time within 2 seconds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"customer service\",\n        \"description\": \"The project requires a conversational AI model with a focus on speed, efficiency, and at least 90% accuracy for handling diverse queries from the AmazonQA dataset. The model should have a response time of no more than 2 seconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.90\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": 2,\n                \"unit\": \"seconds\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AmazonQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": \"moderate\",\n                \"query Diversity\": \"high\"\n            },\n            \"description\": \"A conversational dataset for diverse customer queries\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Modular Conversational AI\",\n            \"family\": \"Neural Conversational Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"lightweight\",\n                \"inference_speed\": \"high\"\n            },\n            \"description\": \"A conversational AI designed for speed, efficiency, and at least 90% accuracy on AmazonQA dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The focus on speed and efficiency implies a need for fast training and deployment methods.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for real-time inference\"],\n        \"demonstration\": \"A snippet of the model interacting with AmazonQA dataset questions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the project manager's requirements for a conversational AI that handles the AmazonQA dataset. The model's focus, accuracy, and speed were captured, along with specific details on the dataset and desired deployment environment. The high expertise level of the user implies a need for consult and build advice. Missing information such as the specific deployment endpoint is expected, as it may vary based on the organization's infrastructure.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "Create a high-performance GraphSAGE model on PubMed that integrates meta-path based aggregation for node classification, targeting not only unseen datasets with 95% accuracy, but also withstands concept drift while maintaining near real-time deployment, with a requirement for sub-30ms latency per inference on resource-constrained hardware.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"biomedical\",\n        \"description\": \"The task is to develop a high-performance GraphSAGE model for node classification on PubMed dataset, incorporating meta-path based aggregation. It aims for 95% accuracy on unseen datasets and must be able to handle concept drift while maintaining real-time deployment with a latency target of sub-30ms on resource-constrained hardware.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency per inference\",\n                \"value\": 0.03,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PubMed Dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node classification\"],\n            \"specification\": {\n                \"unseen dataset\": true,\n                \"resource-constrained\": true\n            },\n            \"description\": \"A biomedical dataset for node classification using GraphSAGE, with emphasis on handling concept drift and near real-time deployment.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GraphSAGE with Meta-path Aggregation\",\n            \"family\": \"Neural Graph Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time deployment\": true,\n                \"resource-constrained hardware compatibility\": true\n            },\n            \"description\": \"A GraphSAGE model designed for high performance with meta-path based aggregation, targeting both unseen datasets and concept drift resistance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Meta-path based aggregation enhances the model's ability to handle domain-specific relationships.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time, sub-30ms latency\"\n        ],\n        \"demonstration\": {\n            \"focus\": \"Node classification accuracy and latency on resource-constrained hardware\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's request for a high-performance GraphSAGE model targeting 95% accuracy, concept drift, and sub-30ms latency. The PubMed dataset, resource-constrained hardware, and meta-path based aggregation are accurately captured. The model type and performance expectations align well with the task. The confidence score is high due to the thorough mapping of the given requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "In a unique setting, develop a novel node classification system using a Hyperdimensional Learning enhanced Topological Attention Graph Neural Network (HTAGNN) on the Multi-Platform Academic Influence dataset. The model should discern between elite researchers, open-source contributors, and hobbyist learners with a precision requirement of 95%. Demonstrate the network's adaptability in dealing with highly imbalanced data and its efficiency in detecting semantic relationships within the vast, interconnected research publications.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"The task is to create a novel node classification system using a HTAGNN (Hyperdimensional Learning enhanced Topological Attention Graph Neural Network) on the Multi-Platform Academic Influence dataset. The goal is to distinguish between elite researchers, open-source contributors, and hobbyist learners, with a precision requirement of 95%. The model must handle imbalanced data and demonstrate semantic relationship detection within large interconnected research publications.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multi-Platform Academic Influence dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"elite researchers\", \"open-source contributors\", \"hobbyist learners\"],\n            \"specification\": null,\n            \"description\": \"A dataset for node classification, focusing on academic influence with a need for imbalanced data handling and semantic relationship detection.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HTAGNN\",\n            \"family\": \"Graph Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Hyperdimensional Learning enhanced Topological Attention\"\n            },\n            \"description\": \"A novel model developed for node classification, targeting precision of 95% in distinguishing among elite researchers, open-source contributors, and hobbyist learners, with a focus on data adaptability and semantic relationship detection.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"model performance demonstration on imbalanced data and semantic relationship detection\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's instruction, indicating a 'high' expertise level and emphasizing a novel HTAGNN model with a precision target of 95%. The key task, dataset, and model details are included, such as the Multi-Platform Academic Influence dataset, imbalanced data handling, and semantic relationship detection. The high confidence score assumes that the specific architecture details and demonstration requirements are understood and relevant to the task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a machine learning project manager, your challenge is to lead the development of a state-of-the-art text classifier for the DAIGT dataset. The task involves not only differentiating between human-generated and AI-generated texts (0 for human, 1 for AI) but also discerning subtle nuances in AI models' advancements. Prioritize achieving high precision and recall, while dealing with the complexity of recent transformer architectures, and ensure the model's explainability is maintained for regulatory compliance. Conduct extensive experiments using various data augmentation techniques and incorporate domain-specific knowledge for enhanced performance, with the ultimate goal of pushing the current accuracy benchmark.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"artificial intelligence ethics\",\n        \"description\": \"The objective is to develop a state-of-the-art text classifier for the DAIGT dataset, distinguishing between human-generated and AI-generated texts (0 for human, 1 for AI), while accounting for advancements in AI models and ensuring explainability for regulatory compliance. High precision and recall are priorities, along with addressing transformer architecture complexities and leveraging domain-specific knowledge through data augmentation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"human-generated\", \"AI-generated\"],\n            \"specification\": null,\n            \"description\": \"A dataset for text classification of human and AI-generated texts, incorporating advancements in AI models.\",\n            \"preprocessing\": [\"domain-specific knowledge incorporation\", \"data cleaning\"],\n            \"augmentation\": [\"various data augmentation techniques\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art Text Classifier\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"ensured for regulatory compliance\"\n            },\n            \"description\": \"A cutting-edge model designed to handle transformer architectures and target text classification with high precision and recall.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporation of domain-specific knowledge for improved accuracy\",\n        \"Explainability for regulatory compliance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"transformer model optimization\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explainability demonstrations for stakeholders\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the provided instruction, addressing the high-level requirement of a text classifier for the DAIGT dataset. It highlights the complexities of transformer architectures, importance of precision and recall, and the emphasis on explainability for regulatory compliance. The model's specification and domain-specific knowledge application ensure a comprehensive response.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "Develop a novel real-time object detection system for industrial safety, combining COCO and an exclusive safety equipment dataset. Enhance the current Anchor-Free CNN architecture to discern not only generic hazards but also differentiate between specific safety gear types and trip hazards, maintaining at least 98% precision while operating under challenging lighting conditions and dynamic backgrounds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"industrial safety\",\n        \"description\": \"The task is to design a real-time object detection system for industrial safety, integrating COCO dataset with an exclusive safety equipment dataset. The system should focus on detecting both generic hazards and specific safety gear types, while maintaining a precision of at least 98% under challenging lighting conditions and dynamic backgrounds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO dataset (Industrial Safety Integration)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"hazards\", \"safety gear types\"],\n            \"specification\": null,\n            \"description\": \"Integrated COCO dataset with an exclusive safety equipment dataset, focusing on object detection for industrial safety purposes.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Anchor-Free CNN for Industrial Safety Object Detection\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"adapted_from\": \"Anchor-Free architecture\",\n                    \"enhancements\": \"Differentiating between safety gear types\"\n                },\n                \"performance\": {\n                    \"lighting_conditions\": \"challenging\",\n                    \"backgrounds\": \"dynamic\"\n                }\n            },\n            \"description\": \"An advanced CNN model enhanced for real-time object detection, addressing safety gear and hazards with high precision.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The system should incorporate domain-specific knowledge of industrial safety equipment and hazards.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Real-time performance demonstration under various lighting and background conditions.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the provided instruction by specifying the high-level requirements for an industrial safety object detection system. Performance metrics and the need for enhancing the Anchor-Free CNN architecture to handle specific safety gear types and challenging conditions are captured. The rationale justifies the fit of the user's intent and expertise level in the JSON structure.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "question answering", "instruction": "As the project manager for our ambitious travel technology overhaul, you're tasked with developing a cutting-edge, real-time Q&A component. Utilize the extensive and diverse TravelQA dataset to create a highly efficient T5 variant, prioritizing minimal resource consumption and prompt analysis. Guarantee precise and swift destination insights that not only delight users but also streamline their decision-making process, ensuring a seamless 5-star user experience in a stringent latency environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"travel technology\",\n        \"description\": \"The project aims to develop a real-time Q&A component with a focus on efficiency and minimal resource consumption. A T5 variant should be utilized with the TravelQA dataset, targeting fast and accurate destination insights for a streamlined, 5-star user experience in a low-latency environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"max\": \"strict\",\n                    \"unit\": \"milliseconds\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource consumption\",\n                \"value\": null,\n                \"unit\": \"FLOPs or memory\"\n            },\n            {\n                \"name\": \"prompt analysis time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TravelQA dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"destination insights\"],\n            \"specification\": null,\n            \"description\": \"Extensive and diverse dataset for real-time question answering, prioritizing efficient use for travel technology.\",\n            \"preprocessing\": [\"minimal preprocessing\"],\n            \"augmentation\": [\"none\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"T5 variant\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": \"high\",\n                \"resource_efficiency\": true\n            },\n            \"description\": \"A cutting-edge Q&A model optimized for real-time use, resource conservation, and prompt analysis.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"resource-efficient\"],\n        \"target_device\": \"low-latency environment\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"5-star user experience demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the task's requirements, highlighting the need for a high-end Q&A component, T5 variant, efficiency metrics, and a focus on minimal resource consumption. It incorporates the target domain (travel technology) and emphasizes the low-latency environment. The missing precision and recall values will be filled based on the model's performance in the testing phase.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "For our groundbreaking research project, we aim to develop a highly specialized Question Answering system utilizing the TalmudicText corpus. The neural architecture must integrate TalmudBERT, an advanced fusion of Talmudic knowledge and the MMBERT model, targeting not only factual retrieval but also ethical and interpretive reasoning. The model's performance should exhibit exceptional interpretive accuracy, with an expected F1 score of at least 95% in analyzing complex, context-dependent Kabbalistic queries. However, the system must demonstrate near-real-time responsiveness, ensuring responses are generated within 50 milliseconds while preserving the depth of its reasoning. Please design an efficient training pipeline and tuning strategy that balances these requirements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"religion\",\n        \"description\": \"The goal is to develop a specialized Question Answering system using TalmudBERT, a fusion of Talmudic knowledge and MMBERT, targeting fact retrieval and ethical/interpretive reasoning. The model must have high interpretive accuracy (F1 score of 95%) for complex, context-dependent Kabbalistic queries while maintaining near-real-time response times (50 milliseconds).\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"interpretive accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": 50,\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TalmudicText corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"corpus\",\n                \"size\": \"large\",\n                \"language\": \"Hebrew/English\"\n            },\n            \"description\": \"A corpus containing Talmudic knowledge, serving as the basis for the TalmudBERT model.\",\n            \"preprocessing\": [\"tokenization\", \"TalmudBERT-specific preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TalmudBERT\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"TalmudBERT fusion of Talmudic knowledge and MMBERT\",\n                \"parameters\": \"optimized for interpretive reasoning\",\n                \"FLOPs\": \"optimized for speed\"\n            },\n            \"description\": \"A neural QA model integrating TalmudBERT for enhanced performance in Kabbalistic queries.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-sensitive feature extraction\", \"interpretability techniques\"],\n        \"target_device\": \"cloud or server for near-real-time response\",\n        \"deployment_endpoint\": \"optimized hosting infrastructure\",\n        \"inference_engine\": [\"high-performance batch processing\", \"caching mechanism\"],\n        \"demonstration\": \"interactive interface showcasing model performance and reasoning depth\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes all relevant information, reflecting the user's intention to build a high-expertise Question Answering system. It accurately maps the task's specifications, including the required neural architecture, performance metrics, and complexity constraints. It also covers the necessary pretraining, fine-tuning, and deployment details.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "The machine learning development team is tasked with creating a highly accurate and scalable image classification algorithm, specifically designed to handle the surge of user-generated photos. The system must not only categorize images into 'testimonials', 'follow-up required', and 'gallery ready' but also analyze the photo's context, lighting, and user engagement. The algorithm must maintain a minimal human intervention threshold while ensuring brand consistency and regulatory compliance for user content.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"social media/image analysis\",\n        \"description\": \"The goal is to develop a highly accurate and scalable image classification algorithm that deals with user-generated photos. Categories include testimonials, follow-up required, and gallery ready, while also considering context, lighting, and user engagement. The algorithm must minimize human intervention and ensure brand consistency and regulatory compliance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"users/hour\"\n            },\n            {\n                \"name\": \"human intervention threshold\",\n                \"value\": null\n            },\n            {\n                \"name\": \"compliance\",\n                \"value\": null,\n                \"unit\": \"regulatory requirements met\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"User-Generated Photos\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"category\", \"context\", \"lighting\", \"user engagement\"],\n            \"specification\": {\n                \"size\": {\"train\": \"\", \"validation\": \"\", \"test\": \"\"},\n                \"dimensions\": {\"train\": \"\", \"validation\": \"\", \"test\": \"\"},\n                \"modalities\": {\"train\": \"\", \"validation\": \"\", \"test\": \"\"}\n            },\n            \"description\": \"A large dataset of user-generated photos that need to be classified into specific categories while considering context, lighting, and user engagement\",\n            \"preprocessing\": [\n                \"contextual analysis\",\n                \"lighting normalization\",\n                \"user engagement feature extraction\"\n            ],\n            \"augmentation\": [\n                \"geographic\",\n                \"temporal\",\n                \"diversity of user styles\"\n            ],\n            \"visualization\": [\n                \"image metadata\",\n                \"accuracy distributions\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep convolutional neural network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Inception-like\",\n                \"depth\": \"\",\n                \"efficiency\": \"\",\n                \"feature extraction\": \"context, lighting, and engagement\"\n            },\n            \"description\": \"A highly accurate image classification algorithm designed to handle user-generated photos and analyze multiple aspects\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"content filtering\", \"brand style detection\", \"content moderation\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"specific deployment platform\",\n        \"inference_engine\": [\"GPU acceleration\", \"cloud TPU\"],\n        \"demonstration\": \"interactive user interface showing classification results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the core components of the given instruction, including the high accuracy, scalability, and context analysis requirements. The expert level of the user's expertise is acknowledged. The dataset and model choices are tailored to the problem description. The specific performance metrics and complexity constraints are appropriately represented. The confidence score is high due to the comprehensive interpretation of the given information.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "image segmentation", "instruction": "Can you develop an advanced convolutional neural network (CNN) architecture, optimized for real-time performance, that precisely segments not only individual cats but also different breeds and action poses in high-resolution images, amidst varying background complexities and illumination conditions?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"animal recognition\",\n        \"description\": \"The user wants to build an advanced CNN for real-time segmentation of cats, classifying different breeds and action poses in high-resolution images with varying backgrounds and illumination conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"speed (real-time)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"breed\",\n                \"action pose\"\n            ],\n            \"specification\": {\n                \"image_resolution\": {\n                    \"width\": null,\n                    \"height\": null\n                }\n            },\n            \"description\": \"High-resolution images with varying backgrounds, illumination conditions, and a focus on cats of different breeds in action poses.\",\n            \"preprocessing\": [\"real-time data normalization, resizing\"],\n            \"augmentation\": [\"real-time illumination and background variations\"],\n            \"visualization\": [\"segmentation masks\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced CNN\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"optimized for real-time performance\"\n            },\n            \"description\": \"A state-of-the-art architecture designed to handle real-time segmentation of cats, different breeds, and action poses in high-res images, addressing varying background complexities and lighting conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Understanding of real-time constraints and efficient inference techniques for CNNs\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"embedded systems or platforms with real-time processing requirements\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized tensorFlow or equivalent for real-time performance\"],\n        \"demonstration\": \"example of real-time segmentation on test images\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately reflects the user's request for an advanced CNN for real-time segmentation. The problem area, downstream task, specific performance metrics (accuracy, real-time speed), and constraints (background complexity, illumination) are included. The model selection, dataset requirements, and service expectations match the requested task. The high expertise level suggests the user can handle the complexity of the architecture.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the elite AI-driven fashion analysis team, our objective is to design a cutting-edge, real-time mobile app that employs a highly optimized EfficientNet variation for an advanced fully convolutional network (FCN) model. We need to achieve state-of-the-art performance, targeting precision above 80% in differentiating and isolating even the minutest overlapping garments in the DeepFashion2 dataset, ensuring seamless compatibility with resource-constrained mobile devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"fashion analysis\",\n        \"description\": \"The project aims to design a real-time mobile app using an optimized EfficientNet variant of a fully convolutional network (FCN). The focus is on achieving state-of-the-art performance with precision above 80% for differentiating and isolating overlapping garments in the DeepFashion2 dataset, while ensuring compatibility with resource-constrained mobile devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.8\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"Mobile (seconds)\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepFashion2 Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"garment segmentation masks\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": null,\n                \"mode\": \"RGB\"\n            },\n            \"description\": \"A dataset for fashion analysis, containing images with the objective of detecting and differentiating overlapping garments.\",\n            \"preprocessing\": [\"mobilenet optimization\"],\n            \"augmentation\": [\"lighting variations\", \"occlusion\"],\n            \"visualization\": [\n                \"model performance per class over time\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Optimized EfficientNet + FCN\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"EfficientNet variant for FCN\",\n                \"target device compatibility\": \"mobile devices\",\n                \"resource optimization\": true\n            },\n            \"description\": \"A high-performance, mobile-friendly model for real-time fashion analysis on DeepFashion2 dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Mobile app design principles for efficient resource usage\",\n        \"State-of-the-art techniques for optimizing deep learning models for mobile\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time inference, optimized for mobile\"],\n        \"demonstration\": \"A demo showcasing accurate and efficient garment segmentation on a mobile device\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the key aspects of the project, from the user's high expertise level to the detailed requirements for model architecture, performance metrics, and dataset characteristics. Performance-related metrics like precision, as well as compatibility with resource-constrained devices, are emphasized, as specified in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "multimodal emotion recognition", "instruction": "As the project manager, you now mandate the AI development team to enhance the existing multimodal emotion recognition system using the CMU-MOSEI dataset. Incorporate a fusion of BERT and a novel, depth-wise separable CNN, targeting at least 11 diverse emotions while maintaining a stringent 95% precision and handling subtle facial expressions in videos. The integration must exhibit robustness across diverse lighting and background conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal emotion recognition\",\n        \"application_domain\": \"affective computing\",\n        \"description\": \"The project aims to enhance the existing emotion recognition system using the CMU-MOSEI dataset. A fusion of BERT and a novel depth-wise separable CNN is required, targeting at least 11 emotions with a precision goal of 95% and addressing subtle facial expressions and robustness under diverse lighting and background conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"robustness\",\n                \"value\": null,\n                \"unit\": \"across diverse lighting and background conditions\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CMU-MOSEI\",\n            \"modality\": [\"image\", \"audio\", \"video\"],\n            \"target_variables\": [\"emotion\"],\n            \"specification\": {\n                \"multimodal_data\": true,\n                \"dataset_size\": {\n                    \"train\": \"unknown\",\n                    \"validation\": \"unknown\",\n                    \"test\": \"unknown\"\n                },\n                \"lengths\": {\n                    \"video\": \"unknown\"\n                }\n            },\n            \"description\": \"A dataset for multimodal emotion recognition with diverse emotions and subtle facial expressions in videos, affected by diverse lighting and background conditions.\",\n            \"preprocessing\": [\"facial feature extraction\", \"audio and text preprocessing\"],\n            \"augmentation\": [\n                \"lighting and background augmentation for videos\"\n            ],\n            \"visualization\": [\"multimodal data analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT + Novel Depth-wise Separable CNN\",\n            \"family\": \"Transformer + Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_complexity\": \"novel architecture designed for multimodal emotion recognition\",\n                \"performance_benchmarks\": \"unknown\"\n            },\n            \"description\": \"A fusion model that combines BERT for sequence understanding with a novel depth-wise separable CNN for handling visual data, targeting precision of 95% for recognizing diverse emotions from subtle facial expressions in videos.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"multimodal fusion techniques\", \"handling subtle facial expressions\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"robustness demonstrations across diverse conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's directive as a build request, considering high-level expertise. It reflects the area of multimodal learning, task as emotion recognition, application domain of affective computing, and the specific requirements for the model (BERT + novel CNN) with performance metrics (95% precision). The dataset specifics are mentioned, acknowledging the CMU-MOSEI's nature, and target preprocessing and augmentation techniques. However, since the user did not specify the confidence score or exact model specifications, the JSON leaves room for details.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager must develop a highly efficient, bidirectional LSTM-based neural machine translation system using Europarl v7, prioritizing not only accuracy but also advanced low-resource optimization. Target real-time communication with strict latency constraints (<300ms), ensuring compatibility for resource-constrained devices and addressing cross-lingual nuances for seamless dialogue in diverse Spanish-English conversational scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"real-time communication\",\n        \"description\": \"The project aims to develop a bidirectional LSTM-based neural machine translation system, using Europarl v7. The focus is on high accuracy, advanced low-resource optimization, and strict latency requirements (<300ms) for real-time communication. Compatibility with resource-constrained devices and addressing cross-lingual nuances for seamless dialogue in Spanish-English conversational scenarios is critical.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"low-resource optimization\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.3,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"compatibility\",\n                \"value\": \"resource-constrained devices\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Europarl v7\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": null,\n            \"description\": \"A multilingual corpus for training a bidirectional LSTM-based neural machine translation system.\",\n            \"preprocessing\": [\"low-resource optimization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Bidirectional LSTM-based NMT\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A high-performance, real-time translation model designed for Europarl v7, targeting Spanish-English conversational scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cross-lingual nuances and compatibility for resource-constrained devices are important factors.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"low-resource adaptation techniques\"\n        ],\n        \"target_device\": \"resource-constrained devices\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"optimized for real-time\"\n        ],\n        \"demonstration\": \"Spanish-English conversational scenario demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately represents the task, with a focus on building a high-performance LSTM NMT system with low-resource optimization. It accounts for the specific dataset, target audience, performance metrics, and the importance of cross-lingual dialogue. The task complexity and feature engineering requirements are relevant to the given instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "For an exclusive luxury retail store, develop a state-of-the-art YOLOv5-based object detection model specifically trained on the \"OpulentRetailDataset,\" which contains high-resolution images of diverse premium products. The model should distinguish between 100 exclusive product variants, maintaining a precision above 95% and recall of 97% for ultra-precise inventory tracking. The system's real-time processing must guarantee inventory updates within 1.5 seconds per image to maintain seamless store operations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury retail\",\n        \"description\": \"The task is to develop a YOLOv5-based object detection model for an exclusive luxury retail store, focusing on the OpulentRetailDataset. The model must accurately detect 100 exclusive product variants, achieving precision above 95% and recall of 97% for precise inventory tracking. The system should have real-time performance with inventory updates within 1.5 seconds per image.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 1.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpulentRetailDataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product variants\"],\n            \"specification\": {\n                \"image_resolution\": \"high\"\n            },\n            \"description\": \"A dataset containing high-resolution images of diverse premium products for training the YOLOv5 model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_version\": \"state-of-the-art\",\n                \"number_of_classes\": 100\n            },\n            \"description\": \"A high-performance object detection model using the YOLOv5 architecture, designed specifically for the OpulentRetailDataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Real-time processing\"\n        ],\n        \"demonstration\": \"Inventory updates within 1.5 seconds per image\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is designed based on the user's instruction. It details a state-of-the-art YOLOv5 model for object detection in luxury retail, with the necessary precision and recall requirements, as well as the specified real-time processing constraint for inventory management. The requested data sources and features are covered in the structure.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a private intelligence consultant for a global maritime shipping corporation, you've been commissioned to develop an advanced object detection algorithm for unmanned aerial vehicles (UAVs) to monitor and analyze cargo containers in real-time, distinguishing between hazardous materials, contraband, and legitimate goods. The system must work seamlessly across various weather conditions,Differentiate between static and moving containers, and integrate with the existing complex maritime logistics network, while maintaining strict data privacy regulations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"maritime shipping\",\n        \"description\": \"The task is to develop an advanced object detection algorithm for unmanned aerial vehicles, focusing on real-time monitoring and analysis of cargo containers. The system should identify hazardous materials, contraband, and distinguish legitimate goods, working in diverse weather conditions, and detecting static and moving containers. It must integrate with the existing maritime logistics network while adhering to strict data privacy regulations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"detection accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"confusion matrix (hazardous materials, contraband, legitimate goods)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"frames per second\"\n            },\n            {\n                \"name\": \"integration complexity\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Maritime Cargo Container Dataset (proprietary)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"container type (hazardous materials, contraband, legitimate)\"],\n            \"specification\": null,\n            \"description\": \"A labeled dataset containing images of cargo containers captured by UAVs with annotations for hazardous materials, contraband, and legitimate goods, reflecting diverse weather conditions and container motion.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"weather simulation, motion blur\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Object Detection Algorithm (e.g., yolov4, objectron)\",\n            \"family\": \"Convolutional Neural Networks (CNNs)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"detection-based\",\n                \"deep learning model\": \"customized for maritime use\"\n            },\n            \"description\": \"A highly customized neural network designed for real-time object detection, specifically for distinguishing between cargo container types and handling diverse conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Integration with existing maritime logistics network and data privacy measures (e.g., anonymization techniques and secure communication channels) are essential.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"container motion and weather condition adaptation\"],\n        \"target_device\": \"UAVs\",\n        \"deployment_endpoint\": \"secured, real-time monitoring platform\",\n        \"inference_engine\": [\"on-device or edge computing\"],\n        \"demonstration\": \"Privacy-preserving system demonstration with live data feeds\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has been meticulously crafted to encapsulate the requirements for an advanced object detection system catering to the maritime domain. The expertise level of the user is assumed to be high due to the complexity and specific needs. Performance metrics, algorithm, and dataset requirements reflect the intricacies of the task. However, some values, like specific performance metrics, are left as null to indicate the need for further consultation or specific user input.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "tabular regression", "instruction": "As a specialized project manager for a machine learning team, your mandate is to develop an advanced tabular regression model using the \"Crab Type and Biometric Age Prediction Dataset.\" This dataset contains a diverse mix of categorical (e.g., species, habitat) and numerical (size, weight) attributes. The train, validation, and test splits have been meticulously prepared. The challenge is to minimize mean absolute error (MAE) by fine-tuning a model specifically tailored for this crustacean age estimation problem. Your objective is to deliver a state-of-the-art model demonstrating superior performance in predicting the age of these unique crab species.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"biology\",\n        \"description\": \"The project involves developing a state-of-the-art tabular regression model to predict the age of crab species using the Crab Type and Biometric Age Prediction Dataset. The dataset includes a mix of categorical and numerical attributes, and the model must be fine-tuned to minimize MAE.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Crab Type and Biometric Age Prediction Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"age\"],\n            \"specification\": {\n                \"categorical_attributes\": null,\n                \"numerical_attributes\": null,\n                \"sample_distribution\": \"train, validation, and test splits prepared\"\n            },\n            \"description\": \"A dataset with diverse categorical and numerical attributes for crab species age estimation, with predefined train, validation, and test splits.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"advanced regression model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for crustacean age prediction\",\n                \"fine-tuning\": true\n            },\n            \"description\": \"A state-of-the-art tabular regression model designed for advanced age estimation of crab species, targeting low MAE.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"showcasing superior performance in age prediction\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's instruction, indicating a build intent with high expertise level. The problem area, downstream task, performance metric (MAE), and dataset properties are all consistent with the given project. The model is specified as an advanced regression model within the neural network category, fine-tuned to minimize MAE. A state-of-the-art nature is emphasized to represent the goal of delivering superior performance. The confidence is high based on the structured mapping of the instruction details.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "## Created Prompt:\r\n\r\nAs a unique challenge for your cutting-edge research project, you've decided to focus on underwater biodiversity in the Coral Kingdom dataset. Your task is to develop an advanced image segmentation model that not only accurately distinguishes between various marine species, such as coral, fish, and symbiotic algae, but also adapts to extreme lighting conditions and real-time monitoring. To enhance efficiency, propose a neural network architecture that efficiently leverages transfer learning and compressed sensing techniques, while maintaining high precision. Additionally, discuss strategies to optimize the inference speed without compromising the model's accuracy, considering hardware constraints and energy efficiency for deployment on resource-constrained devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"marine biology\",\n        \"description\": \"The task is to develop an advanced image segmentation model for underwater biodiversity analysis using the Coral Kingdom dataset. The model must accurately distinguish between coral, fish, and symbiotic algae, and adapt to extreme lighting conditions with real-time monitoring. It should leverage transfer learning and compressed sensing techniques for efficiency and maintain high precision. The focus is on optimizing inference speed for resource-constrained devices without sacrificing accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"Joules\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coral Kingdom Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"marine species\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"dimensions\": {\n                    \"image resolution\": null\n                },\n                \"modalities\": {\n                    \"color\": true,\n                    \"depth\": null,\n                    \"additional modalities\": null\n                }\n            },\n            \"description\": \"A dataset focusing on underwater biodiversity, with extreme lighting conditions and real-time monitoring requirements.\",\n            \"preprocessing\": [\"adapt for extreme lighting\", \"enhance for underwater visibility\"],\n            \"augmentation\": [\"real-time data augmentation for adaptation\"],\n            \"visualization\": [\n                \"visualize species distribution\",\n                \"compare pre-augmented vs augmented results\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image Segmentation Model\",\n            \"family\": \"Convolutional Neural Network (leveraging transfer learning and compressed sensing)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": null,\n                \"transfer learning\": {\n                    \"base_model\": \"pre-trained on similar marine life datasets\"\n                },\n                \"compressed sensing\": {\n                    \"methods\": null\n                }\n            },\n            \"description\": \"A model designed for high precision in underwater image segmentation, utilizing transfer learning and compressed sensing techniques for efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"discussing strategies for real-time monitoring and hardware adaptation\",\n        \"leveraging compressed sensing for efficient use of computation and memory\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"fine-tuning for specific species recognition\"],\n        \"target_device\": \"resource-constrained devices (e.g., underwater sensors, drones)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for hardware-constrained inference\"],\n        \"demonstration\": \"showcase real-time performance and energy efficiency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the task and requirements provided, considering a high-expertise user. It covers the area of computer vision, image segmentation, underwater biology, and efficient model development. Performance metrics like accuracy and precision, along with hardware-optimized inference speed, are included. Missing values in the specification areas denote the need for further research or user input.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "question answering", "instruction": "The project demands the creation of a highly optimized, mobile-friendly TinyBERT QA model that surpasses 85% accuracy on theNewsQA dataset, while ensuring energy efficiency and limiting response latency to an ultra-fast 150 milliseconds, without compromising on interpretability for end-users. Additionally, the model must handle multi-lingual queries with minimal preprocessing.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"news and media\",\n        \"description\": \"The project aims to develop an optimized TinyBERT QA model that surpasses 85% accuracy on the NewsQA dataset. It should be mobile-friendly, have low response latency (150 milliseconds), and maintain interpretability for end-users, while supporting multi-lingual queries with minimal preprocessing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"relative to baseline\"\n            },\n            {\n                \"name\": \"response latency\",\n                \"value\": 0.15,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NewsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": {\n                    \"train\": \"unknown\",\n                    \"validation\": \"unknown\",\n                    \"test\": \"unknown\"\n                },\n                \"language\": \"multi-lingual\"\n            },\n            \"description\": \"A dataset for training and evaluating the TinyBERT QA model, requiring high accuracy with a focus on news domain questions.\",\n            \"preprocessing\": [\"minimal preprocessing for multi-lingual support\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT\",\n            \"family\": \"BERT model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"mobile-friendly\",\n                \"interpretability\": \"high\",\n                \"number_of_parameters\": \"optimized for efficiency\"\n            },\n            \"description\": \"A highly optimized QA model, designed for efficiency, high accuracy, and multi-lingual support with a focus on interpretablity for end-users.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile-friendly\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for mobile latency\"\n        ],\n        \"demonstration\": \"ultra-fast response times and interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's requirements, including a high accuracy goal, mobile optimization, ultra-fast latency, interpretability, and multi-lingual support. The performance metric, complexity constraints, and model specifics were tailored to fit the given project demand.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "text classification", "instruction": "As a project manager for the advanced machine learning team, you are now tackling a sophisticated text classification challenge. The multi-layered Textual Entailment dataset, enriched with nuanced categories (including subtle nuances, contextual disagreements, and precise inferential connections), requires the development of a state-of-the-art model. The goal is not only to achieve peak accuracy but also to demonstrate superior performance in handling various linguistic complexities and dialects. Emphasize on interpretability and model robustness during training, while maintaining a focus on computational efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"linguistics\",\n        \"description\": \"The task is to develop a state-of-the-art model for a sophisticated text classification problem using the Textual Entailment dataset with nuanced categories, focusing on peak accuracy, linguistic complexities, dialects, interpretability, and model robustness while maintaining computational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs, memory footprint\"\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"model explainability and interpretability techniques\"\n            },\n            {\n                \"name\": \"robustness\",\n                \"value\": null,\n                \"unit\": \"resistance to linguistic variations and dialects\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Textual Entailment Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"category\"],\n            \"specification\": null,\n            \"description\": \"An advanced dataset enriched with nuanced categories, including subtle nuances, contextual disagreements, and precise inferential connections.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"to be specified for interpretability and robustness\",\n                \"training techniques\": \"emphasizing interpretability and robustness\"\n            },\n            \"description\": \"A machine learning model designed to tackle complex text classification with emphasis on interpretability and robustness.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"to show performance in handling linguistic complexities and dialects\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was constructed based on the given instruction, ensuring that it reflects a high-level project management task for a sophisticated text classification problem. The details of the dataset (Textual Entailment with nuanced categories) and the model's requirements (state-of-the-art, interpretability, robustness, and computational efficiency) have been carefully captured. The confidence score is high due to the clear mapping of the instruction to the JSON structure.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "The project's objective is to develop a state-of-the-art skin cancer lesion classifier using the HAM10000 dataset, leveraging EfficientNet. The model must achieve a minimum 99% AUROC, ensuring high precision and recall, particularly in identifying early-stage melanomas. Furthermore, the implementation must incorporate Grad-CAM for interpretable decision visualization, as input to expert dermatologists for comprehensive review and trust-building.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The objective is to create a state-of-the-art skin cancer lesion classifier using the HAM10000 dataset with EfficientNet. The minimum requirement is 99% AUROC, prioritizing high precision and recall, particularly for early-stage melanomas. Grad-CAM must be incorporated for interpretable decision visualization for expert dermatologists' review and trust-building.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"area under the receiver operating characteristic curve (AUROC)\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HAM10000\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"skin lesion classification\"],\n            \"specification\": null,\n            \"description\": \"A dataset for skin cancer lesion classification, specifically focusing on early-stage melanoma detection.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [\"Grad-CAM for interpretability\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"performance\": {\n                    \"AUROC\": 0.99,\n                    \"interpretability\": \"Grad-CAM\"\n                }\n            },\n            \"description\": \"A state-of-the-art model for image classification, particularly designed for skin cancer detection with a focus on high precision, recall, and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Expert review of decisions through Grad-CAM to build trust.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Expert dermatologists' comprehensive review through interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's requirements accurately, including the high expertise level, task's details (state-of-the-art EfficientNet model with AUROC target, Grad-CAM for interpretability), dataset specifics, and model performance indicators. The missing values for precision and recall reflect the need for the model to achieve high values for these metrics. The confidence score of 0.97 indicates the confidence in the structure, but acknowledges that final validation would depend on achieving the specified performance criteria.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "community detection", "instruction": "For the Dolphin social network analysis, design a sophisticated Stochastic Block Model (SBM) that captures fine-grained interactions, incorporating temporal dynamics and assortative mixing patterns. The objective is to discover latent communities with not only high accuracy (minimum 85%) but also hierarchical structures, reflecting the evolving social dynamics and power structures among dolphins, while considering factors like age, gender, and migration patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"social network analysis\",\n        \"description\": \"The task involves designing a Stochastic Block Model (SBM) for the Dolphin social network dataset, considering temporal dynamics, assortative mixing, and hierarchical community structures. The model should have a minimum accuracy of 85% and account for age, gender, and migration patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Dolphin Social Network Dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"nodes\": {\n                    \"age\": {},\n                    \"gender\": {},\n                    \"migration patterns\": {}\n                },\n                \"edges\": {\n                    \"interactions\": {\n                        \"temporal\": true\n                    }\n                }\n            },\n            \"description\": \"Contains data on dolphin interactions with temporal dynamics and various attributes like age, gender, and migration patterns.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Stochastic Block Model (SBM)\",\n            \"family\": \"Graph Clustering\",\n            \"type\": \"statistical models\",\n            \"specification\": {\n                \"accuracy_goal\": 0.85,\n                \"hierarchical_structure\": true,\n                \"temporal_features\": true,\n                \"assortativity\": true\n            },\n            \"description\": \"A model to capture fine-grained interactions with temporal dynamics and assortative mixing in the Dolphin social network, aiming to discover latent communities with hierarchical structure.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the user's intent to build a model, with high expertise level. It describes the problem as graph machine learning for community detection in a dolphin social network, reflecting the task's specifics (SBM, temporal dynamics, assortative mixing, and accuracy of 85%). Node and edge attributes have been included, and the model's intended properties are specified in the 'specification' field.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "In a multi-disciplinary collaboration, design a novel time-series forecasting system using a fusion of DeepAR and LSTM models, coupled with autoregressive/meta-learning, for real-time forecasting of global ocean temperature patterns influenced by solar activity and volcanic eruptions. The model should encompass dynamic data assimilation and a neural architecture search component to optimize performance, while ensuring interpretability and providing uncertainty quantification for the predictions in the marine ecosystem over a five-year horizon.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"marine ecosystem\",\n        \"description\": \"The project aims to develop a novel time-series forecasting system that combines DeepAR and LSTM models with autoregressive/meta-learning for real-time global ocean temperature prediction. The model must consider solar activity and volcanic eruptions, incorporate dynamic data assimilation, have a neural architecture search component for optimization, and prioritize interpretability with uncertainty quantification for a five-year horizon.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Global Ocean Temperature Patterns influenced by Solar Activity and Volcanic Eruptions\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"ocean temperature\"],\n            \"specification\": {\n                \"length\": null,\n                \"dimension\": null,\n                \"frequency\": null\n            },\n            \"description\": \"The dataset includes historical data of ocean temperatures influenced by solar activity and volcanic eruptions, for model training, data assimilation, and real-time prediction.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"feature engineering (solar activity and volcanic index)\",\n                \"resampling (monthly or yearly)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepAR-LSTM Fusion with Autoregressive/Meta-Learning\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": [\n                    \"DeepAR for time series forecasting\",\n                    \"LSTM for sequence modeling\",\n                    \"Autoregressive components\",\n                    \"Meta-learning for optimization\"\n                ],\n                \"interpretability techniques\": [\"explainable AI (XAI)\"]\n            },\n            \"description\": \"A hybrid model designed to handle complex ocean temperature patterns with dynamic data assimilation and neural architecture search, focusing on interpretability and uncertainty quantification.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Collaborative expertise from multi-disciplinary backgrounds, emphasizing domain-specific understanding of oceanography, solar physics, and volcano science.\"\n        },\n        {\n            \"description\": \"State-of-the-art methodologies for neural architecture search (NAS) for improved model optimization.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"data integration\",\n            \"real-time feature extraction\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"marine data analytics platform\",\n        \"inference_engine\": [\"TensorFlow, PyTorch\"],\n        \"demonstration\": \"live dashboard showcasing forecasts and model explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a novel machine learning model for time-series forecasting. It reflects the user's high expertise level and the specific requirements of the project, including models, datasets, metrics, and service-related aspects. The missing metric values and assumptions are placeholders for optimization, and the reasoning behind the confidence score is that the key elements are included to address the task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the machine learning development team, you are tasked with enhancing the text classification system for a real-world scenario. The WebMD Review dataset consists of diverse medical opinions, requiring sophisticated analysis. Your goal is to not only classify sentiments as binary (0 for negative and 1 for positive) but also detect nuanced emotions and handle sarcasm. Emphasize on improving both precision and recall, and develop a model robust enough to generalize across different sub-domains. Ensure an in-depth evaluation using multiple metrics, like F1-score and AUC-ROC, to demonstrate the model's superior performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The project involves enhancing a text classification system, specifically targeting the WebMD Review dataset. The objective is to classify sentiments into binary (negative and positive) categories, detecting nuanced emotions, and recognizing sarcasm. The focus is on improving precision and recall, while ensuring the model's robustness for cross-subdomain generalization. A thorough evaluation using multiple metrics, including F1-score and AUC-ROC, is required to showcase superior performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WebMD Review dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\", \"emotion\"],\n            \"specification\": {\n                \"diversity\": \"multidomain medical opinions\",\n                \"sarcasm_detection\": \"required\"\n            },\n            \"description\": \"A diverse dataset containing medical opinions for sentiment analysis, with a focus on binary sentiment classification and nuanced emotion detection, including sarcasm.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"advanced NLP model (e.g., transformers with fine-tuning)\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"sensitive to precision and recall improvements\",\n                \"generalization\": \"cross-subdomain\"\n            },\n            \"description\": \"A model designed to handle complex text analysis, targeting improved sentiment classification, nuanced emotion detection, and sarcasm recognition.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Performance demonstrations across multiple sub-domains\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the task given to enhance the text classification system for the WebMD Review dataset, emphasizing on binary and nuanced sentiment analysis, sarcasm detection, and performance metrics like F1-score and AUC-ROC. The user's high expertise level is acknowledged and the need for a robust, cross-subdomain model is incorporated. The structure and content of the JSON response are consistent with the given template.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "object detection", "instruction": "For a challenging object detection project, design a real-time, multi-modal system to identify not only individual animals in my front yard but also classify their species and predict potential threats. Require the model to handle varying weather conditions and lighting while maintaining high accuracy and near-instantaneous response times.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"environmental monitoring\",\n        \"description\": \"The user wants to develop a real-time, multi-modal system for object detection in their front yard, classifying animal species and predicting potential threats, with high accuracy and near-instantaneous response times, while accounting for varying weather conditions and lighting.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": \"near-instantaneous\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"optimized for real-time\",\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Front Yard Animal Data\",\n            \"modality\": [\n                \"image\",\n                \"audio\"\n            ],\n            \"target_variables\": [\"animal species\", \"threat level\"],\n            \"specification\": {\n                \"data_distribution\": \"real-time, multi-modal\",\n                \"weather_conditions\": \"varied\",\n                \"illumination\": \"varied\"\n            },\n            \"description\": \"A dataset capturing real-time images and audio of animals in the front yard, designed to handle varying weather and lighting conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"real-time weather and lighting simulation\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multi-modal Object Detection Model\",\n            \"family\": \"Convolutional Neural Networks (CNN) and possibly Recurrent Neural Networks (RNN) for audio analysis\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A real-time, multi-modal object detection model targeting animals in varied conditions and fast response times.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for multi-modal data\"],\n        \"target_device\": \"optimized for real-time deployment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for edge devices\"],\n        \"demonstration\": \"video demonstration with real-time detection and classification\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the given task, incorporating a high-level requirement for a real-time, multi-modal system. Performance metrics, including accuracy and response times, are specified, while recognizing the need for multi-modal data processing and handling varying conditions. The rationale provides a clear reasoning for the JSON content.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "The upgraded financial news categorization project demands handling a massive Reuters-21578 dataset with a focus on high-frequency real-time articles. The goal is to achieve 88% accuracy, ensuring low false positives (<5%) for market-sensitive topics, while utilizing a fine-tuned fastText model optimized for speed and precision under stringent latency constraints of 2 seconds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"This project involves financial news categorization using a large Reuters-21578 dataset in a real-time, high-frequency setting. The goal is to achieve an accuracy of 88% with low false positives (less than 5%) for market-sensitive topics. A fastText model is to be fine-tuned for speed and precision, while adhering to a latency constraint of 2 seconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.88\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 2,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"categorized news topic\"],\n            \"specification\": {\n                \"size\": \"large\",\n                \"real-time\": true,\n                \"high-frequency\": true\n            },\n            \"description\": \"A massive dataset for real-time financial news categorization, focusing on high-frequency and market-sensitive articles.\",\n            \"preprocessing\": [\"cleaning, tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"fastText\",\n            \"family\": \"Embedding-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimized_for\": \"speed and precision\"\n            },\n            \"description\": \"Fine-tuned for high accuracy and low false positives in real-time financial news categorization.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"low-level feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"live news stream demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately represents the instruction, capturing the user's high expertise, the Reuters-21578 dataset, the fine-tuned fastText model, the accuracy goal, and the specific latency constraint. The goal for low false positives and the emphasis on speed and precision are also included. Any potential missing information refers to the machine learning model's architecture specifics, which are not specified in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Develop a high-resolution urban green space segmentation model using the Vaihingen dataset, specifically focusing on precision in identifying not only diverse vegetation types but also subtle variations in species and their health stages. The model should exhibit robustness to illumination fluctuations and seasonal changes, while integrating real-time weather predictions for enhanced accuracy, and present actionable recommendations for sustainable urban planning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"The task involves developing a precision model using the Vaihingen dataset for segmenting urban green spaces, considering diverse vegetation types, species variations, health stages, illumination fluctuations, and integrating real-time weather predictions for improved accuracy. The model should also provide actionable recommendations for sustainable urban planning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"green space segmentation\"],\n            \"specification\": {\n                \"data_type\": \"urban aerial imagery\",\n                \"image_size\": \"high-resolution\",\n                \" vegetative_types_count\": null,\n                \"seasonal_diversity\": \"considered\",\n                \"weather_data_access\": \"integrated\"\n            },\n            \"description\": \"A high-resolution dataset for urban green space segmentation, focusing on diverse vegetation types and subtle variations in species.\",\n            \"preprocessing\": [\"illumination normalization\", \"seasonality handling\"],\n            \"augmentation\": [\"weather-invariant techniques\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Precision-oriented green space segmentation model\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"resilient to illumination and seasonal changes\",\n                \"weather_prediction_integration\": \"enabled\"\n            },\n            \"description\": \"A model targeting precision in identifying green spaces and their variations, with real-time weather incorporation.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"recommendations\",\n            \"content\": \"Actionable suggestions for sustainable urban planning based on the model's analysis\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"weather-based feature extraction\"],\n        \"target_device\": \"cloud or IoT for real-time predictions\",\n        \"deployment_endpoint\": \"specific urban planning platform\",\n        \"inference_engine\": [\"AI-optimized for real-time performance\"],\n        \"demonstration\": \"model performance and planning recommendations in a user-friendly dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the user's instruction, including building a high-level model, target dataset, performance metric (precision), and requirements for robustness and real-time weather integration. The assumption is made that the user has a high level of expertise in the field. The platform and deployment details are left open for the user's preference.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "The project mandate necessitates the design of an advanced image recognition system utilizing the Fashion-MNIST dataset. Your team must employ cutting-edge Capsule Network architecture, targeting a new benchmark of 95% accuracy while maintaining exceptional resistance to affine image distortions. The project should include a comprehensive evaluation of your model's performance compared to contemporary Convolutional Neural Networks (CNNs), emphasizing the advantages, trade-offs, and the potential for transfer learning on smaller datasets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"The project calls for developing an advanced image recognition system using the Fashion-MNIST dataset. Targeting a new benchmark of 95% accuracy, the focus is on Capsule Network architecture, ensuring robustness against affine image distortions. The evaluation should compare the model's performance to CNNs, highlighting advantages, trade-offs, and potential for transfer learning on smaller datasets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"image class\"],\n            \"specification\": null,\n            \"description\": \"A dataset for advanced image recognition in fashion, designed to test models' resilience to affine distortions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"affine distortions\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Capsule Network\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Capsule Network\",\n                \"resistance_to_transforms\": \"affine\"\n            },\n            \"description\": \"An advanced image recognition architecture with a focus on robustness against affine image distortions.\"\n        },\n        {\n            \"name\": \"Contemporary CNNs\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"CNNs\"\n            },\n            \"description\": \"Comparable convolutional neural network models for performance comparison.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning may be useful, especially for smaller datasets.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Performance comparison against CNNs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's instruction, covering the advanced image recognition system design, the Capsule Network architecture, the accuracy benchmark, resistance to affine distortions, and the comparison with CNNs. Transfer learning is identified as a potential strategy, reflecting the project's mention. The confidence score is high, given that the provided information is detailed and covers all key aspects.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular classification", "instruction": "For a highly specialized tabular classification project, the machine learning team is to embark on an in-depth analysis of the lesser-known \"Census Income\" dataset from the U.S. Census Bureau. Your task is not only to preprocess the data by handling missing values, encoding categorical features, and normalizing numerical ones using advanced techniques like MICE imputation and ordinal encoding, but also to create a multi-layered neural network architecture utilizing convolutional and recurrent layers.\n\nCreate a pipeline that includes feature extraction from demographic data using domain-specific word embeddings, followed by a fusion of these embeddings with numerical features. Train the model using a hybrid learning approach (combined gradient boosting and stacked autoencoders) on a stratified 25% test set, ensuring class balance is maintained through oversampling underrepresented classes.\n\nAdditionally, present visualizations of the learning curves, confusion matrix, and feature importance in a suite of interactive dashboards using Plotly. Evaluate the model's performance with precision, recall, F1-score, and area under the ROC curve, and optimize hyperparameters using Bayesian optimization techniques. Document the entire process, clearly detailing the rationale behind each step and the impact on model performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"socioeconomic studies\",\n        \"description\": \"The task requires a highly specialized machine learning project on the Census Income dataset. The focus is on data preprocessing, multi-layered neural network, feature extraction with domain-specific word embeddings, hybrid learning, stratified testing, oversampling, interactive dashboard visualizations, model performance metrics, and hyperparameter optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"area under the ROC curve\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"training time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Census Income\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"income\"],\n            \"specification\": {\n                \"missing_values\": {\n                    \"handling_method\": \"MICE imputation\"\n                },\n                \"categorical_variables\": {\n                    \"encoding_method\": \"ordinal encoding\"\n                },\n                \"dimension\": {}\n            },\n            \"description\": \"A dataset from the U.S. Census Bureau, focused on demographic data analysis. Requires preprocessing with advanced techniques.\",\n            \"preprocessing\": [\n                \"missing value handling with MICE imputation\",\n                \"categorical feature encoding using ordinal encoding\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"learning curves (Plotly)\",\n                \"confusion matrix (Plotly)\",\n                \"feature importance visualizations (Plotly)\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multi-layered Neural Network\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"convolutional and recurrent layers\",\n                \"feature_extraction\": \"domain-specific word embeddings\",\n                \"hybrid_learning\": [\n                    \"gradient boosting\",\n                    \"stacked autoencoders\"\n                ],\n                \"class_balance\": \"stratified 25% test set, oversampling underrepresented classes\"\n            },\n            \"description\": \"A deep learning model with a focus on feature extraction, domain-specific embeddings, and hybrid learning approach.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Documenting the process: rationale behind each step and its impact on performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain-specific word embeddings\",\n            \"fusion with numerical features\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Interactive dashboards with Plotly for model performance visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures all the provided requirements. It details the user's high expertise level by including advanced techniques and a comprehensive project description. Performance metrics and complexity constraints are included, along with the requested documentation and visualization tools. However, specific metric values are left open for calculation.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for a sophisticated AI development team, your objective is to design an advanced image classifier that not only differentiates between cat and dog photographs but also recognizes and categorizes various breeds within each species with high precision. The model must handle complex lighting conditions, pose variations, and subtle breed distinctions, while maintaining real-time performance and adhering to strict energy efficiency guidelines.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"artificial intelligence\",\n        \"description\": \"The objective is to develop an advanced image classifier that differentiates between cats and dogs, with high precision. The model should also recognize and categorize different breeds within each species, considering complex lighting conditions, pose variations, and subtle breed distinctions. Real-time performance and strict energy efficiency are key requirements.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"time (ms per inference)\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"energy consumption per inference\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Combined Cat & Dog Breeds Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing a large variety of cat and dog photographs, including different breeds, complex lighting conditions, pose variations, and subtle breed distinctions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"lighting conditions\",\n                \"pose variations\",\n                \"breed-specific variations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (CNNs)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"EfficientNet or ResNeXt\",\n                \"real-time-performance\": \"Yes\",\n                \"energy-efficiency\": null\n            },\n            \"description\": \"An advanced image classifier specifically designed to handle diverse factors like lighting, pose, and breed variations with high precision.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"Real-time capable device\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"GPU-accelerated\", \"Quantized for low power devices\"],\n        \"demonstration\": {\n            \"live demonstration\": true,\n            \"energy-efficient\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the user's objective for an advanced image classifier, considering expert-level requirements. Performance metrics, such as precision, are identified, and complexity metrics are set to real-time performance and energy efficiency. The dataset and model choices align with the stated complexities. The confidence score is high due to a comprehensive and relevant JSON structure.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "For a highly specialized text classification project in the financial sector, we require the development of a real-time sentiment analysis system using a novel fusion of BERT and XGBoost. The model must distinguish between fraudulent financial advisories, customer satisfaction levels, and market trend predictions with a precision above 98%. To cater for the diverse range of financial jargon and the stringent security measures, the model should have an optimized deep learning architecture designed for extreme resource constraints on edge devices, ensuring a response time of less than 5 milliseconds per transaction. Moreover, the system should be capable of adapting to changing market conditions and dynamically update its classification without sacrificing accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"financial sector\",\n        \"description\": \"The project involves developing a real-time sentiment analysis system for financial data, focusing on fraud detection, customer satisfaction analysis, and market trend predictions with precision above 98%. The system uses a novel fusion of BERT and XGBoost, accounting for financial jargon and extreme resource constraints on edge devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.005,\n                \"unit\": \"milliseconds per transaction\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Financial Domain Sentiment Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"fraudulent advisory\", \"customer satisfaction\", \"market trend\"],\n            \"specification\": {\n                \"data_type\": \"financial jargon\",\n                \"size\": \"adequate for extreme resource constraints\",\n                \"structure\": \"structured\"\n            },\n            \"description\": \"A dataset tailored for the financial sector with diverse jargon, requiring real-time adaptation.\",\n            \"preprocessing\": [\"domain-specific tokenization\", \"handling financial n-grams\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-XGBoost Fusion\",\n            \"family\": \"neural networks & ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"optimized for edge devices\",\n                \"resource_constraints\": \"extreme\"\n            },\n            \"description\": \"A model combining BERT for sentiment understanding with XGBoost for classification, designed for precision, real-time performance, and adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider financial jargon dictionaries and domain-specific embeddings for better accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific feature extraction\"],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"financial institution network\",\n        \"inference_engine\": [\"on-device inference\"],\n        \"demonstration\": \"dynamic updating and accuracy retention during real-world transactions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response reflects the user's instruction, including the high expertise level, the financial sector's text classification project, and the specific model requirements. The performance metrics, response time, and adaptation to changing conditions are taken into account, along with relevant details on the dataset and feature engineering. The confidence score is high due to the thorough mapping of the instruction to the JSON format.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a cutting-edge real estate tech firm, we need a scalable image classification system for high-resolution property photos. It must not only recognize key features like pools, gardens, and kitchens but also differentiate among renovated, contemporary, and vintage styles. This new algorithm should integrate seamlessly into our AI-powered listing platform, ensuring fast, precise, and visually curated presentations for a competitive edge.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"real estate\",\n        \"description\": \"The task is to develop a scalable image classification system for high-resolution property photos, recognizing features like pools, gardens, and kitchens. The system should also differentiate between renovated, contemporary, and vintage styles, seamlessly integrating into the AI-powered listing platform for visually curated and efficient property listings.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Property Photos Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"pool\", \"garden\", \"kitchen\", \"renovated\", \"contemporary\", \"vintage\"],\n            \"specification\": null,\n            \"description\": \"A collection of high-resolution property photos with annotations for key features and architectural styles.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"image resizing\",\n                \"lighting adjustments\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Image Classifier\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A scalable image classification model designed for real estate property photos, emphasizing feature recognition and architectural style differentiation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"image resizing and feature extraction\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"AI-powered listing platform\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"Interactive demos showcasing accurate and fast image classifications for properties\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the given real estate tech company's requirements for a scalable image classification system. The model type, key features, and integration details are covered. Performance metrics and complexity constraints are mentioned, as are the needed preprocessing and augmentation steps. The high expertise level is indicated.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a machine learning project manager for a cutting-edge real estate monitoring system, your team has been tasked with developing an advanced image segmentation model. Your challenge is to create an algorithm that can not only distinguish between buildings, trees, and roads in high-resolution satellite images captured during different seasons and weather conditions, but also accurately identify finer details like rooftop materials, vegetation types, and potholes on roads. The system should be able to detect changes over time, such as construction, deforestation, or maintenance work, and provide a detailed heat map highlighting these alterations. Additionally, the model should be robust enough to handle extremely low-light imagery and efficiently process vast datasets in real-time for remote surveillance drones. Can you design a neural network architecture that can achieve such exceptional performance and streamline the property inspection process?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"real estate monitoring\",\n        \"description\": \"The task involves developing an advanced image segmentation model for a real estate monitoring system. The model should distinguish between buildings, trees, roads, rooftop materials, vegetation types, and potholes in satellite images with variations in seasons and weather. It must detect changes over time and generate heat maps for construction, deforestation, and maintenance. The model must also handle low-light conditions and process large datasets in real-time for remote drone surveillance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"speed (real-time processing)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Real Estate Monitoring Satellite Images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"building\", \"trees\", \"roads\", \"rooftop materials\", \"vegetation types\", \"potholes\"],\n            \"specification\": {\n                \"resolution\": null,\n                \"size variation\": \"seasons, weather conditions\"\n            },\n            \"description\": \"A large dataset of high-resolution satellite images with varying conditions, capturing different aspects of real estate properties for segmentation and change detection tasks.\",\n            \"preprocessing\": [\"low-light image enhancement\", \"image normalization\"],\n            \"augmentation\": [\"seasonal and weather condition simulations\"],\n            \"visualization\": [\"change detection heat map generation\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image Segmentation Neural Network\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"dual path (for handling diverse details and real-time processing)\",\n                \"model depth\": null,\n                \"parameters\": null,\n                \"FLOPs\": null,\n                \"inference speed (on large datasets)\": null\n            },\n            \"description\": \"A deep learning model specifically designed for image segmentation tasks with emphasis on architectural features that address low-light imagery, fine-grained details, and real-time performance for drone surveillance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced image processing techniques and domain adaptation strategies to handle different seasons and weather conditions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for seasonality and condition analysis\"],\n        \"target_device\": \"remote surveillance drones\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for real-time execution\"],\n        \"demonstration\": \"model performance on sample images and change detection heat maps\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the task requirements accurately. It specifies the advanced image segmentation model, emphasizes the high-level expertise needed, and outlines performance metrics and handling of complex scenarios. The model architecture takes into account low-light imagery and real-time processing, while considering the vastness of the dataset and the system's integration with remote drones.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "In the realm of high-resolution remote sensing, design a novel fusion U-Net architecture for an underwater image segmentation task. The model must classify between 50 distinct marine biomes, including intricate coral reefs, diverse fish species, and dynamic underwater landscapes, with a demanding benchmark of 95% mean Intersection over Union (IoU) on the OceanBiosphere dataset. Furthermore, the optimized model must demonstrate real-time processing capabilities, achieving at least 40 FPS on a specialized embedded GPU embedded within an autonomous underwater vehicle (AUV), ensuring seamless and efficient operation in extreme environmental conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"underwater ecology\",\n        \"description\": \"The project involves designing a novel fusion U-Net architecture for underwater image segmentation in high-resolution remote sensing, targeting 50 distinct marine biomes (e.g., coral reefs, fish species, and landscapes) with a 95% mean IoU on the OceanBiosphere dataset. Real-time processing is crucial, with a requirement of 40 FPS on a specialized embedded GPU in an autonomous underwater vehicle.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean Intersection over Union (IoU)\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"FPS (Frames per Second)\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"GPU optimization for real-time operation\",\n                \"value\": \"embedded GPU with >=40 FPS requirement\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OceanBiosphere\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"marine biomes\"],\n            \"specification\": null,\n            \"description\": \"A high-resolution remote sensing dataset for underwater image segmentation, including diverse marine environments and 50 distinct biomes.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Novel Fusion U-Net\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"U-Net, with fusion components\"\n            },\n            \"description\": \"A high-performance model specifically designed for underwater image segmentation, targeting 50 marine biomes with emphasis on real-time processing and high IoU.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"embedded GPU in AUV\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"underwater image segmentation with 40 FPS in extreme conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is formatted according to the given schema. It reflects the user's intent to build a machine learning project, their high level of expertise, and the specific requirements for the segmentation task, model architecture, and performance metrics. The details on the dataset and required real-time processing capabilities are clearly captured. However, a low-level implementation for the specific fusion components of the U-Net is not included as it would be specific to the development process.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "As a machine learning project manager, your team has been tasked with developing a cutting-edge multimodal fusion model for forgery detection in the highly competitive \"WorldFusion-2023\" dataset, which combines both still images and short video clips from multiple sources. The challenge is not only to identify manipulated content but also to discern the level of sophistication in the forgery techniques. Your team must propose a transformer-based architecture that not only surpasses traditional methods like frame-by-frame analysis but also integrates temporal and spatial cues effectively. Additionally, the model should achieve a minimum accuracy of 95% and demonstrate robustness against state-of-the-art forgery attacks. Your proposal should outline the research questions, key milestones, and a strategy for evaluating the model's generalizability across diverse forgery types and sources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"artificial intelligence safety and security\",\n        \"description\": \"The project involves developing a cutting-edge multimodal fusion model for forgery detection in the WorldFusion-2023 dataset. The focus is on a transformer-based architecture that surpasses traditional frame-by-frame analysis, integrates temporal and spatial cues, and achieves a minimum accuracy of 95% while demonstrating robustness against state-of-the-art attacks. The task also requires addressing research questions, key milestones, and a strategy for evaluating generalizability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WorldFusion-2023\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"forged content\", \"forgery sophistication level\"],\n            \"specification\": null,\n            \"description\": \"Dataset combining still images and short video clips from multiple sources, posing challenges in multimodal forgery detection.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multimodal Transformer-Based Fusion Model\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"inference_speed\": null,\n                    \"model_size\": null,\n                    \"temporal_integration\": true,\n                    \"spatial_integration\": true\n                }\n            },\n            \"description\": \"A transformer-based model that outperforms traditional methods by effectively fusing temporal and spatial cues for forgery detection.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"research_questions\",\n            \"description\": \"To be determined\"\n        },\n        {\n            \"type\": \"key_milestones\",\n            \"description\": \"To be defined\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"type\": \"cross-forForgery-evaluation\",\n            \"strategy\": \"generalizability testing with diverse forgery types and sources\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the essence of the given task, with a clear intent to build a model, addressing the user's high expertise level. The problem description is coherent, and performance metric requirements are specified. The model's focus on a transformer-based architecture and the necessity for generalizability across diverse forgery types and sources are captured. Research questions and key milestones would be added as needed.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager must evaluate employing a real-time, energy-efficient U-Net architecture for precise object instance segmentation in high-resolution indoor scenes, ensuring compatibility with limited-resource devices while maintaining sub-millimeter accuracy within tight latency constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"smart homes/indoor robotics\",\n        \"description\": \"The project involves the evaluation and use of a real-time, energy-efficient U-Net architecture for object instance segmentation in high-resolution indoor scenes, with a focus on compatibility with limited-resource devices and sub-millimeter accuracy under tight latency constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean Intersection over Union (mIoU)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"energy efficiency (FLOPs/second)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency (inference time)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"megabytes\"\n            },\n            {\n                \"name\": \"energy consumption per inference\",\n                \"value\": null,\n                \"unit\": \"Joules\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Indoor Scene Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"data_distribution\": \"balanced\",\n                \"scene_types\": \"high-resolution indoor scenes\"\n            },\n            \"description\": \"A dataset for testing the U-Net architecture in real-time with high-resolution indoor scenes, emphasizing compatibility and low-resource requirements.\",\n            \"preprocessing\": [\"downsampling if necessary\"],\n            \"augmentation\": [\"lighting, rotation, and noise to simulate variations\"],\n            \"visualization\": [\"accuracy and IoU maps\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net\",\n            \"family\": \"Convolutional Neural Network (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"real-time, energy-efficient\",\n                \"FLOPs\": null,\n                \"optimization for\": \"limited-resource devices\",\n                \"accuracy target\": \"sub-millimeter\"\n            },\n            \"description\": \"A deep learning model designed for object instance segmentation, focusing on efficiency, compatibility, and precision in high-resolution indoor scenes\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning can help with limited-resource devices by utilizing pre-trained models with fine-tuning.\",\n        \"Hardware acceleration and efficient data processing techniques are crucial.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"data preprocessing for segmentation\"],\n        \"target_device\": \"limited-resource devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for low-latency inference\"],\n        \"demonstration\": \"real-time object instance segmentation in demo video\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's requirement to build a U-Net architecture for object instance segmentation. The emphasis on energy efficiency, compatibility, and accuracy in tight latency is reflected, as well as the desired performance metrics. The missing values indicate that further details should be filled based on project specific details or benchmarked values.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "The task is to design a scalable image classification system using the custom-resolved CASIA-WebFace dataset, leveraging the MobileNetV2 architecture specifically tailored for low-latency facial recognition on wearables. Mandate a differential privacy implementation and an encrypted model for secure processing in real-world scenarios with stringent energy efficiency requirements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"biometrics, security\",\n        \"description\": \"The task involves designing a scalable image classification system using the CASIA-WebFace dataset, targeting low-latency facial recognition on wearables. Requirements include MobileNetV2 architecture, differential privacy for privacy, and an encrypted model for secure processing with strict energy efficiency constraints.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"operations per watt\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CASIA-WebFace (custom-resolved for facial recognition)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"facial features\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"resolution\": null\n            },\n            \"description\": \"A dataset for low-latency facial recognition on wearables, using the MobileNetV2 architecture.\",\n            \"preprocessing\": [\"custom-resolving for MobileNetV2\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MobileNetV2\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"params\": null,\n                \"FLOPs\": null\n            },\n            \"description\": \"A model optimized for low-latency facial recognition on wearables, designed to address energy efficiency requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Differential privacy is implemented to ensure data privacy.\",\n        \"Encrypted model is mandatory for secure processing in real-world scenarios.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"wearables\",\n        \"deployment_endpoint\": \"encrypted, differential privacy-compliant\",\n        \"inference_engine\": [\"optimized for wearables\"],\n        \"demonstration\": \"low-latency, energy-efficient facial recognition\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the key details of the instruction. High expertise level is assumed as the task is complex. The MobileNetV2 architecture is specified, and the additional requirements of differential privacy and encrypted model are included. Performance metrics are not explicitly listed, but energy efficiency is mentioned, and no additional information is missing. The confidence score reflects the high level of relevance to the given instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "As a healthcare project manager, task the machine learning team to develop a specialized question-answering model for precision medical diagnosis, leveraging the extensive and diverse BioASQ dataset. Mandate the use of Enhanced BioBERT, considering its state-of-the-art performance and ability to integrate fine-grained domain knowledge. The model should not only deliver accurate diagnoses but also generate detailed explanations from reliable, peer-reviewed medical sources, ensuring scientific rigor in each response.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The objective is to build a specialized question-answering model for precision medical diagnosis, using the BioASQ dataset. The chosen model should be Enhanced BioBERT, due to its state-of-the-art performance and domain integration capability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explanation generation accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"bioinformatics\",\n                \"size\": null,\n                \"diversity\": \"extensive\"\n            },\n            \"description\": \"An extensive and diverse dataset for medical question answering, focusing on precision diagnosis and incorporating fine-grained medical knowledge.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced BioBERT\",\n            \"family\": \"Transformer-based Language Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"performance\": \"state-of-the-art\",\n                \"domain_integration\": true\n            },\n            \"description\": \"A model designed specifically for medical question answering, leveraging Enhanced BioBERT's accuracy and domain knowledge integration.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The importance of scientific rigor in each response is highlighted, ensuring reliability and reliance on peer-reviewed medical sources.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"explanatory nature\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has been adapted to capture the healthcare-focused question-answering task for precision medical diagnosis, specifying Enhanced BioBERT. Performance metrics, including accuracy and explanation generation, have been accounted for, along with complexities like inference time and memory consumption. The explanation-generation feature and focus on rigorous sourcing are also included. High confidence reflects the close alignment with the given instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Request a specialized deep learning architecture, incorporating a seasonal-aware CNN-LSTM model, for the temporal dataset of historical urban traffic flow spanning five years. Predict fine-grained hourly traffic volumes for the next 30 days, differentiating between peak hours (6-10AM and 4-8PM) and weekends, while also evaluating the influence of public holidays on traffic patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"The task is to develop a specialized deep learning model, combining Seasonal-Aware CNN-LSTM, for a historical urban traffic flow dataset spanning five years. The focus is on predicting hourly traffic volumes for the next 30 days, differentiating peak hours (6-10AM, 4-8PM) and weekends, and considering the impact of public holidays on traffic patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy for peak hours prediction\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy for weekend prediction\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy considering public holidays\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint during inference\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Historical Urban Traffic Flow Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly traffic volumes\"],\n            \"specification\": {\n                \"data_span\": \"five years\",\n                \"temporal_frequency\": \"hourly\"\n            },\n            \"description\": \"A dataset containing historical traffic flow data for an urban area, organized chronologically with hourly granularity.\",\n            \"preprocessing\": [\"seasonal decomposition\", \"resampling\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time series analysis\", \"comparison with peak hours and weekends\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Seasonal-Aware CNN-LSTM\",\n            \"family\": \"LSTM and CNN hybrid\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"custom with attention mechanism for hour-specific patterns\",\n                \"hyperparameters\": {\n                    \"CNN filters\": null,\n                    \"LSTM units\": null,\n                    \"seasonal awareness\": true\n                }\n            },\n            \"description\": \"A deep learning architecture that combines Convolutional Neural Networks (CNN) for capturing spatial features and Long Short-Term Memory (LSTM) for temporal dependency, including an attention mechanism to handle peak hours and weekends.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Public holidays and their impact on traffic patterns should be incorporated through time series anomaly detection or seasonal adjustments.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"holiday frequency analysis\", \"temporal feature extraction\"],\n        \"target_device\": \"cloud-based deployment for real-time predictions\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow or PyTorch\"],\n        \"demonstration\": \"visual comparison of predicted and actual traffic volumes, highlighting peak hours and weekend effects\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the user's request for a specialized deep learning model, performance metrics, and the handling of specific data characteristics (such as seasonal awareness, peak hours, weekends, and holidays). The template captures both the technical aspects and the application domain (urban planning).\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for the advanced machine learning team, you are tasked with developing a state-of-the-art time-series forecasting model. The ILI dataset, now with an extended version containing complex, seasonally-adjusted, and anomaly-identified data, has been meticulously partitioned into training, validation, and test sets. The input sequences consist of past observations with a non-trivial length (INPUT_SEQ_LEN=48, encompassing additional economic indicators), and each input vector has a broader dimensionality (INPUT_DIM=15). Your objective is to design a multi-horizon predictive model that not only forecasts the subsequent 24-step ahead (PRED_SEQ_LEN=36) but also integrates future external events and market trends. Performance should be optimized using not only mean squared error (MSE) but also mean absolute percentage error (MAPE) and symmetric mean absolute percentage error (SMAPE), while ensuring model explainability and robustness against outliers.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"economics\",\n        \"description\": \"The task is to develop a state-of-the-art time-series forecasting model for the extended ILI dataset with complex adjustments, seasonality, and anomaly detection. The input sequences have a length of 48 and a dimensionality of 15, with the objective to forecast 24 steps ahead while considering external events and market trends. Performance metrics include MSE, MAPE, and SMAPE, and the model should be explainable and robust to outliers.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"symmetric mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI (Extended Version)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 48,\n                \"INPUT_DIM\": 15,\n                \"seasonal_adjusted\": true,\n                \"anomaly_identified\": true,\n                \"partitioning\": [\"training\", \"validation\", \"test\"]\n            },\n            \"description\": \"A dataset with comprehensive adjustments and partitioning for time-series forecasting, incorporating additional economic indicators.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-horizon\",\n                \"forecast_steps\": 36,\n                \"external_event_integration\": true,\n                \"market_trend_inclusion\": true,\n                \"explainability\": true,\n                \"outlier_robustness\": true\n            },\n            \"description\": \"A model designed for advanced time-series forecasting with the ability to handle external events and market trends.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstration of multi-horizon predictions, explainability, and robustness to outliers.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON format was created to match the user's instruction, reflecting a high-level task of building a state-of-the-art time-series model. The problem statement, dataset, and performance metrics are tailored to the user's requirements, including the specified input sequence length, forecasting horizon, and performance measures. The model description acknowledges the model's expected properties according to the task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a seasoned project manager, your mandate is to enhance the time-series classification challenge for a cutting-edge AI team. The Ethanol Concentration dataset now incorporates varying lengths and dynamic feature extraction (MIN_SEQ_LEN=1000, MAX_SEQ_LEN=2500, INPUT_DIM expansion to 12), alongside handling missing values. Your team must not only optimize for accuracy but also demonstrate robustness in handling irregularly spaced data and forecasting future labels, all while maintaining interpretability for practical application.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"data science and analytics\",\n        \"description\": \"The task involves enhancing the time-series classification challenge for an AI team, focusing on the Ethanol Concentration dataset. The dataset now includes varying sequence lengths, dynamic feature extraction with a MIN_SEQ_LEN of 1000 and MAX_SEQ_LEN of 2500, and an expanded INPUT_DIM of 12. The team must optimize for accuracy, demonstrate robustness in irregularly spaced data handling, and ensure interpretability for practical usage.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"robustness to irregular data\",\n                \"value\": null,\n                \"unit\": \"percent of irregular data samples handled\"\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"model explainability score\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ethanol Concentration dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"class labels\"],\n            \"specification\": {\n                \"min_sequence_length\": 1000,\n                \"max_sequence_length\": 2500,\n                \"input_dim\": 12,\n                \"missing_values_treatment\": \"handled\"\n            },\n            \"description\": \"A dataset with varying sequence lengths and expanded feature extraction to handle irregularities and forecasting future labels.\",\n            \"preprocessing\": [\"dynamic feature extraction\", \"missing value imputation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"interpretable for time-series analysis\",\n                \"interpretability_techniques\": [\"feature importance\", \"SHAP values\"]\n            },\n            \"description\": \"A time-series classification model designed to handle varying lengths, dynamic features, and interpretability for practical application.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"dynamic feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Methods for handling irregularly spaced data and interpreting model predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a model with a high-level expertise. The Ethanol Concentration dataset details, task requirements, and complexity metrics (robustness, interpretability) are carefully included. Performance metrics are left open-ended for the model's accuracy, allowing for specific values to be filled in later. Confidence score reflects the alignment with the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Develop a state-of-the-art visual question answering platform using the Diving48 dataset, which consists of underwater footage from extreme diving adventures. The platform must integrate advanced Spatio-Temporal Graph Neural Networks (STGNNs) and Hierarchical Multi-Modal Fusion (HMMF) modules to decipher complex, visually and contextually challenging scenarios. The system must not only comprehend diverse marine life, underwater environments, and equipment but also exhibit exceptional ability in recognizing temporal events like gear interactions and adaptation to changing visibility. Furthermore, the platform should prioritize real-time analysis, providing rapid and precise answers during live diving broadcasts, while maintaining low latency and resource efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"oceanography/recreation\",\n        \"description\": \"The project is to develop a state-of-the-art visual question answering platform using the Diving48 dataset, which features underwater footage with challenging scenarios. Key components are Spatio-Temporal Graph Neural Networks (STGNNs) and Hierarchical Multi-Modal Fusion (HMMF) for understanding marine life, environments, and temporal events.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"real-time, low\"\n            },\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Diving48 Dataset\",\n            \"modality\": [\"video\", \"multimodal\"],\n            \"target_variables\": [\"visual scenes\", \"questions\", \"answers\"],\n            \"specification\": null,\n            \"description\": \"Underwater footage with extreme diving challenges, including diverse marine life and complex scenarios.\",\n            \"preprocessing\": [\"STGNN preprocessing\", \"HMMF preprocessing\"],\n            \"augmentation\": [\"temporal event simulation\"],\n            \"visualization\": [\"model interpretability analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"STGNN + HMMF\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Spatio-Temporal Graph Neural Networks and Hierarchical Multi-Modal Fusion\",\n                \"purpose\": \"Visual Question Answering in complex underwater scenarios\"\n            },\n            \"description\": \"Integrating advanced STGNNs and HMMF for comprehensive understanding and rapid response.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Diving48 dataset focuses on underwater scenes and temporal events.\",\n        \"Integration of STGNNs for spatial understanding and HMMF for multimodal fusion.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"temporal event recognition, adaptation to changing visibility\"],\n        \"target_device\": \"cloud for real-time analysis\",\n        \"deployment_endpoint\": \"live diving broadcast platforms\",\n        \"inference_engine\": [\"optimized for low latency\"],\n        \"demonstration\": \"Interactive platform showcasing rapid and accurate answers during live diving\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the user's intention to build a high-expertise project, the details of the visual question answering task with Diving48, advanced model components, performance requirements, and resource efficiency. It reflects the need for real-time analysis and latency constraints while accommodating the specifics of the dataset and desired application.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "For the pedestrian safety enhancement in a smart city pilot, our team demands a real-time YOLOv5 model optimized for edge computing, trained exclusively on the 'Fine-grained Cityscapes' subset featuring intricate urban scenarios. The model must accurately detect not just vehicles and pedestrians, but also cyclists and road hazards with >=90% precision, ensuring reliable 35 FPS performance on low-power devices with minimal latency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city, public safety\",\n        \"description\": \"The task is to develop a real-time YOLOv5 model optimized for edge computing, focusing on pedestrian detection in fine-grained urban scenarios of the 'Fine-grained Cityscapes' dataset. The model should also detect cyclists and road hazards with >=90% precision while achieving 35 FPS performance on low-power devices with minimal latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"precision\",\n                \"targets\": [\"cyclists\", \"road hazards\"],\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"FPS\",\n                \"value\": 35,\n                \"unit\": \"frames per second\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fine-grained Cityscapes Subset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"pedestrians\", \"cyclists\", \"road hazards\"],\n            \"specification\": {\n                \"fine-grained\": true,\n                \"urban scenarios\": true\n            },\n            \"description\": \"A subset of the Cityscapes dataset focusing on intricate urban scenarios for pedestrian, cyclist, and road hazard detection.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization\": \"edge computing\",\n                \"architecture\": \"real-time\"\n            },\n            \"description\": \"A state-of-the-art object detection model tailored for pedestrian, cyclist, and road hazard detection with >=90% precision and real-time performance on low-power devices.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"low-power devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for edge computing\"],\n        \"demonstration\": \"35 FPS, minimal latency, and real-time performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the requirements of the project, indicating the high expertise level of the user. It specifies the task of building a YOLOv5 model optimized for edge computing, details about the dataset and its complexity, target performance metrics, and requirements for real-time performance and high precision on low-power devices. The reasoning behind the confidence score reflects a comprehensive understanding of the given instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "For the machine translation project, we require a specialized U-Net model trained on the proprietary Taobao dataset for e-commerce products. Emphasize on sector-specific terminologies for retail and fashion, aiming for a state-of-the-art BLEU score of 40 or above. Additionally, the system must possess real-time adaptability to dynamically update translations for diverse product listings, seamlessly integrating into existing e-commerce ecosystems and ensuring user experience consistency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"e-commerce, retail, and fashion\",\n        \"description\": \"The task is to develop a specialized U-Net model for machine translation on a proprietary Taobao dataset targeting e-commerce product listings. The focus is on retail and fashion sector-specific terminologies, aiming for a BLEU score of 40 or above. The model should be real-time adaptable with dynamic updates and seamless integration into e-commerce ecosystems.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 40\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time adaptability\",\n                \"value\": 1\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Taobao proprietary e-commerce dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"domain\": \"e-commerce, retail, and fashion\",\n                \"terminologies\": \"sector-specific\"\n            },\n            \"description\": \"A dataset specifically curated for e-commerce product translations, focusing on retail and fashion sector-specific language.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for machine translation\",\n                \"real-time update capability\": \"enabled\"\n            },\n            \"description\": \"A specialized U-Net model designed for the machine translation task in the e-commerce context with a focus on retail and fashion terminologies, targeting a high BLEU score.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time adaptability is crucial for maintaining translation quality in a dynamic e-commerce environment.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"sector-specific lexicon integration\"],\n        \"target_device\": \"cloud-based for real-time updates\",\n        \"deployment_endpoint\": \"existing e-commerce platform API\",\n        \"inference_engine\": [\"GPU-enabled for faster processing\"],\n        \"demonstration\": \"User-experience consistent dynamic translations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately represents the user's requirements, focusing on a high-level build intent, specialized U-Net model for machine translation, target dataset, performance metrics, and specific adaptation needs. The model's specialization, performance aim, and the real-time adaptability are captured appropriately.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "In the fashion e-commerce domain, specifically focusing on high-end luxury garments, how can a deep learning model be developed using advanced convolutional neural networks (CNN) to identify not only exact fashion attributes like silk, designer, and vintage, but also nuanced details like embroidery, fabric texture, and seasonal trends, ensuring precise categorization and enhancing search and personalized suggestion algorithms for an ultra-exclusive customer base?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"luxury fashion\",\n        \"description\": \"The task is to develop a deep learning model using advanced CNNs, focusing on high-end luxury garments. The model should identify exact fashion attributes like silk, designer, vintage, and nuanced details like embroidery and fabric texture, while also considering seasonal trends. The goal is to improve categorization for enhanced search and personalized suggestion algorithms for an exclusive customer base.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"f1_score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Luxury Garments Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\n                \"fashion attributes\",\n                \"nuanced details\",\n                \"seasonal trends\"\n            ],\n            \"specification\": {\n                \"dimension\": \"variable (images of varying dimensions)\",\n                \"size\": null,\n                \"modalities\": [\"silk, designer, vintage, embroidery, fabric texture, seasonal trends\"]\n            },\n            \"description\": \"A dataset containing high-quality images of luxury garments, capturing the specified attributes and nuances.\",\n            \"preprocessing\": [\"image resizing, normalization, and augmentation\"],\n            \"augmentation\": [\"GAN-generated variations for diverse representations\"],\n            \"visualization\": [\"image segmentation for detailed analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced CNN\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specifically tailored for fashion attribute and detail recognition\",\n                \"depth\": null,\n                \"number_of_filters\": null,\n                \"layers\": [\n                    \"Convolutional layers\",\n                    \"Max pooling layers\",\n                    \"Attention mechanisms for nuanced details\"\n                ]\n            },\n            \"description\": \"A deep learning model that utilizes advanced CNNs for high-end garment classification.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"fine-tuning for domain-specific features\",\n            \"multi-scale feature extraction\"\n        ],\n        \"target_device\": \"cloud or high-performance server\",\n        \"deployment_endpoint\": \"custom image search engine and recommendation system API\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Kubernetes for deployment\"],\n        \"demonstration\": \"customer-focused, showcasing precise categorization and enhanced search suggestions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON format adheres to the specified schema, capturing the user's high expertise, the focus on luxury fashion, and the application of advanced CNNs. The model requirements and dataset details (such as fine-grained attributes, preprocessing, augmentation) are included to ensure relevance to the problem. The final confidence score is lower due to the need for more detailed specifications, particularly in the model architecture and performance metrics.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As a data analyst for a renewable energy firm, you've collected hourly solar power output from the past year. Develop a robust time-series forecasting model using deep learning algorithms, accounting for seasonality and weather patterns, to predict solar energy production for the next month with high accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"renewable energy\",\n        \"description\": \"The task is to create a robust time-series forecasting model using deep learning algorithms to predict solar power output for the next month. The model must account for seasonality and weather patterns while aiming for high accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hourly Solar Power Output (Past Year)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"solar energy production\"],\n            \"specification\": {\n                \"temporal_frequency\": \"hourly\",\n                \"time_period\": \"past year\"\n            },\n            \"description\": \"Hourly solar power data collected for the past year, including weather patterns.\",\n            \"preprocessing\": [\"seasonality analysis\", \"feature engineering for weather data\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time series patterns and seasonality analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Learning Time-Series Model\",\n            \"family\": \"Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"LSTM or variants, potentially with attention mechanism\",\n                \"accounted_factors\": \"seasonality and weather patterns\"\n            },\n            \"description\": \"A deep learning model specifically designed for time-series forecasting, considering seasonality and weather influence.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\" Handling missing data\", \"Rescaling\", \"Normalization\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow serving\", \"Kubernetes\"],\n        \"demonstration\": \"Prediction plots and visual comparison with actual data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the task for a renewable energy analyst. The area, downstream task, and requirements for deep learning with seasonality and weather patterns are captured. Performance metric (accuracy) is explicitly mentioned, and the dataset description and preprocessing steps align with the task. The model specification and feature engineering are relevant to the given context.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for the cross-lingual QA challenge, mandate the development team to design and implement a state-of-the-art fusion model, leveraging mBERT and XLM-R, to tackle the MLQA dataset. The model should exhibit exceptional performance in five diverse languages (English, Spanish, Mandarin, and two additional underrepresented languages). Rigorously assess and optimize accuracy, especially in resource-constrained environments, while ensuring consistent language-specific nuances are captured.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"cross-lingual\",\n        \"description\": \"The project involves designing and implementing a state-of-the-art fusion model for cross-lingual question answering, specifically on the MLQA dataset. The model should be highly accurate in five languages (English, Spanish, Mandarin, and two underrepresented languages) while optimizing for accuracy in resource-constrained environments and capturing language-specific nuances.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MLQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"question answering\"],\n            \"specification\": null,\n            \"description\": \"A multilingual question answering dataset that should be used for evaluating the cross-lingual performance in five languages.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Fusion Model (mBERT & XLM-R)\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"language_coverage\": [\"English\", \"Spanish\", \"Mandarin\", \"two underrepresented languages\"],\n                \"resource_constraints\": null\n            },\n            \"description\": \"A state-of-the-art fusion model combining mBERT and XLM-R for tackling the MLQA dataset with emphasis on accuracy and resource efficiency.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained environments\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"language_nuances\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately represents the instruction, highlighting the need for a state-of-the-art fusion model for cross-lingual QA using the MLQA dataset, with focus on performance in five languages including underrepresented ones. It emphasizes resource efficiency while capturing language nuances. The specified 'resource-constrained environments' and 'consistent language-specific nuances' indicate the optimization requirements as per the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As a healthcare organization, we've been observing irregular patterns in patient readmission rates after specific surgeries. Develop a real-time anomaly detection model that not only flags unexpected spikes or drops but also diagnoses the root cause by correlating it with factors like surgery type, post-operative care, and patient demographics. The system should provide actionable insights to improve patient outcomes and prevent potential regulatory concerns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The project aims to develop a real-time model for detecting irregular patient readmission rates post-surgery, attributing anomalies to factors like surgery type, post-operative care, and patient demographics. The model should provide actionable insights for improved patient outcomes and regulatory compliance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"root cause identification rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Patient Readmission Data (Post-Surgery)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"readmission rate\", \"surgery type\", \"post-operative care\", \"patient demographics\"],\n            \"specification\": {\n                \"data_length\": null,\n                \"time_interval\": null\n            },\n            \"description\": \"A dataset tracking patient readmission rates post-surgery, containing time series data with factors like surgery type, post-operative care, and patient demographics.\",\n            \"preprocessing\": [\n                \"feature normalization\",\n                \"timestamp aggregation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"anomaly detection visualization\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Real-time Anomaly Detection Model\",\n            \"family\": \"Time-series anomaly detection algorithm\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"algorithm\": \"autoencoder with LSTM layers\",\n                \"hyperparameters\": {\n                    \"hidden layers\": [],\n                    \"loss function\": null\n                }\n            },\n            \"description\": \"A model designed for real-time anomaly detection, using advanced machine learning techniques to correlate readmission rates with relevant factors and diagnose root causes.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Potential correlation between readmission and post-operative procedures, patient demographics, and surgery complexity.\",\n        \"Existing literature on predictive factors for post-surgery readmission\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud-based system\",\n        \"deployment_endpoint\": \"real-time monitoring platform\",\n        \"inference_engine\": [\"Apache Kafka for real-time streaming\", \"TensorFlow Serving\"],\n        \"demonstration\": \"web-based dashboard for healthcare professionals\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately reflects the healthcare organization's needs, focusing on a high-end expertise level. The model, performance metrics, and dataset details are appropriately structured to address the real-time anomaly detection and root cause analysis task. The missing values in the performance and complexity metrics are placeholders to be decided based on the specific requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "link prediction", "instruction": "For the niche professional network \"InnovateTalks,\" design a state-of-the-art machine learning model integrating Heterogeneous Graph Attention Networks (HetGAT) with Temporal Attention Mechanisms, focusing on predicting latent collaborations between researchers in diverse fields. The model must not only identify potential co-authorship but also capture evolving trends and event-based influence within the network. Develop a solution that enhances the model's precision in distinguishing subtle industry partnerships and introduces a novel weighting scheme for combined expertise and temporal relevance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"professional networking\",\n        \"description\": \"The user's task is to design a state-of-the-art machine learning model using Heterogeneous Graph Attention Networks (HetGAT) integrated with Temporal Attention Mechanisms for the InnovateTalks platform. The focus is on predicting latent collaborations among researchers across diverse fields, accounting for evolving trends and event-based influence, with a goal to enhance precision in identifying subtle industry partnerships and propose a novel weighting scheme for expertise and temporal relevance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"InnovateTalks Network Dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"collaborations\", \"expertise\", \"temporal signals\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing graph data with researchers from diverse fields, their expertise, and temporal features for collaboration analysis.\",\n            \"preprocessing\": [\"node feature engineering, graph structure engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [\"graph visualizations\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HetGAT with Temporal Attention\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"HetGAT\",\n                \"temporal_attention\": true,\n                \"weighting_scheme\": \"novel combination of expertise and temporal relevance\"\n            },\n            \"description\": \"A state-of-the-art model combining heterogeneous graph attention with temporal attention mechanisms, designed for the specific task of predicting collaborative relationships and capturing evolving trends in InnovateTalks.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"node embeddings, temporal feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"distributed computing, efficient inference algorithms\"],\n        \"demonstration\": \"Collaboration prediction and influence analysis for specific researchers or events\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's instruction, with high-level intent to build a model, high expertise level, and a focus on graph machine learning. The model specifics, performance metric, and preprocessing steps have been included according to the provided details. The novelty in the weighting scheme and the integration of temporal attention andHetGAT are captured. Confidence is high due to thorough parsing.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "For the ambitious pedestrian safety upgrade in a high-tech urban pilot, our cutting-edge machine learning team necessitates a lightweight YOLOv5 model, specifically tailored for edge computing on resource-constrained devices. It should be fine-tuned using the 'Extensive Urban Complexity' subset of Cityscapes dataset, showcasing intricate scenes with an increased requirement of pinpoint detection for vehicles, pedestrians, cyclists, and potential road hazards. Achieve a robust precision of >=95%, maintaining a steadfast 35 FPS on low-end hardware while ensuring sub-50ms latency for seamless real-time operations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart urban infrastructure\",\n        \"description\": \"The task is to develop a lightweight YOLOv5 model for edge computing in a high-tech urban pilot. The model should be fine-tuned for resource-constrained devices, focusing on 'Extensive Urban Complexity' subset of Cityscapes dataset with emphasis on detecting vehicles, pedestrians, cyclists, and road hazards.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"time_threshold\": 50,\n                    \"unit\": \"milliseconds\"\n                },\n                \"complexity_metric\": \"real-time operations\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model_size\",\n                \"value\": {\n                    \"optimization_target\": \"lightweight\"\n                },\n                \"unit\": \"number of parameters\"\n            },\n            {\n                \"name\": \"inference_speed\",\n                \"value\": 35,\n                \"unit\": \"FPS\"\n            },\n            {\n                \"name\": \"resource_constraint\",\n                \"value\": \"edge computing devices with low-end hardware\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes Dataset (Extensive Urban Complexity subset)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vehicles\", \"pedestrians\", \"cyclists\", \"road hazards\"],\n            \"specification\": {\n                \"complexity\": \"intricate scenes\"\n            },\n            \"description\": \"A dataset for fine-tuning a YOLOv5 model, focusing on precise detection in complex urban environments.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization_for\": \"edge computing\",\n                \"target_device\": \"low-end hardware\"\n            },\n            \"description\": \"A lightweight model designed for object detection, fine-tuned on the Cityscapes dataset with a focus on vehicles, pedestrians, cyclists, and road hazards.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge device\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for low-end hardware\"],\n        \"demonstration\": \"seamless real-time operations with 35 FPS and sub-50ms latency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's intent to build a YOLOv5 model for edge computing, details about the fine-tuning dataset, performance requirements, and constraints on hardware and latency. The high expertise level implies a need for a more specialized and complex model. The confidence score is high due to the accuracy in capturing the user's requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Demand a highly optimized deep learning model, combining a sophisticated Seasonal Adapting Convolutional Neural Network (SCANet) with Long Short-Term Memory (LSTM) units, designed specifically for an extensive dataset of urban traffic trends spanning an entire decade. The model should forecast precise hourly traffic volumes for the subsequent 45-day period, differentiating between ultra-precise peak hours (5-15-minute intervals within the typical rush hours) and differentiating weekends from weekdays, and accurately account for the complex impact of both local and national public holidays on weekly and seasonal traffic fluctuations. Additionally, provide a detailed sensitivity analysis and implement a real-time update mechanism for dynamically adjusting predictions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"urban planning and transportation\",\n        \"description\": \"The task involves creating a deep learning model, combining SCANet and LSTM, for hourly traffic volume prediction over a 45-day period with a focus on 5-15-minute intervals within rush hours, distinguishing weekends from weekdays, and accounting for public holidays' impact on traffic trends. A detailed sensitivity analysis and real-time update mechanism are also required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Urban Traffic Trends Decade Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly traffic volumes\"],\n            \"specification\": {\n                \"length\": {\n                    \"train\": null,\n                    \"valid\": null,\n                    \"test\": null\n                },\n                \"time_frequency\": \"hourly\"\n            },\n            \"description\": \"A large-scale dataset with 10 years of urban traffic data, covering hourly trends, rush hour specifics, and holiday patterns.\",\n            \"preprocessing\": [\"seasonal normalization\", \"feature engineering (5-15-minute intervals)\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SCANet with LSTM units\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"SCANet\": {\n                        \"seasonal_adaptation\": true\n                    },\n                    \"LSTM\": {\n                        \"units\": null,\n                        \"input_shape\": null\n                    }\n                },\n                \"specificities\": {\n                    \"peak_hour_subdivision\": true,\n                    \"weekend_weekday_diff\": true,\n                    \"holiday_impact\": true\n                }\n            },\n            \"description\": \"A deep learning model combining SCANet and LSTM for precise traffic forecasting under specified conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using adaptive learning rate and early stopping for optimization.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"sensitivity analysis\",\n            \"real-time update mechanism\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time traffic pattern analysis\"\n        ],\n        \"demonstration\": \"hourly traffic predictions with update mechanism\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON structure accurately represents the instruction, highlighting the need for a complex model, specific task requirements, and performance metrics. The user's high expertise level suggests a deeper level of knowledge and possibly more advanced techniques. The sensitivity analysis and real-time update mechanism are captured as part of the service requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "Instruct the AI research and development team to design a cutting-edge question-answering system, specifically tailored for intricate and highly specialized medical decision support in rare diseases. Utilize the extensive BioASQ dataset enriched with expert-curated domain-specific information, employing Enhanced BioBERT. Demand not just precise diagnostic predictions, but also the implementation of a novel algorithm that mines nested insights from multiple authoritative medical journals, presenting each answer with evidence trails and supporting reasoning to validate the model's performance and uphold the gold standard of medical knowledge.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medicine, specifically rare diseases\",\n        \"description\": \"The project involves the development of a high-end question-answering system tailored for intricate medical decision support in rare diseases. It should use the BioASQ dataset enriched with expert-curation, and employ Enhanced BioBERT as the foundation. The system must generate precise diagnostic predictions and include a novel algorithm for extracting nested insights from multiple medical journals, providing evidence trails and reasoning to validate the model's performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"nested insights extraction accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"evidence trail retrieval speed\",\n                \"value\": null,\n                \"unit\": \"queries/second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": null,\n            \"specification\": {\n                \"size\": null,\n                \"domain-specific information\": \"enhanced with expert-curated medical knowledge\"\n            },\n            \"description\": \"A dataset for medical question answering, enriched with domain-specific data from BioASQ and medical journals.\",\n            \"preprocessing\": [\"domain adaptation using Enhanced BioBERT\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced BioBERT\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"question-answering specific\",\n                \"nested insights extraction\": true\n            },\n            \"description\": \"A state-of-the-art model for medical question answering, leveraging Enhanced BioBERT and designed for complex rare disease scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"title\": \"Expert-curation and domain adaptation\",\n            \"content\": \"The system will utilize domain-specific knowledge from medical experts and the BioASQ dataset.\"\n        },\n        {\n            \"title\": \"Mining insights from multiple sources\",\n            \"content\": \"It includes a novel algorithm for extracting insights from authoritative medical journals.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain adaptation with Enhanced BioBERT\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            {\n                \"name\": \"Medical Knowledge Retrieval and Analysis\"\n            }\n        ],\n        \"demonstration\": \"Answers should be accompanied by evidence trails and supporting reasoning\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately reflects the user's instruction, highlighting a high-end question-answering system for rare diseases. It specifies the use of Enhanced BioBERT, the BioASQ dataset, and the importance of evidence trails and reasoning. However, some metric values are left as null, indicating they are yet to be determined or negotiated.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "In a cutting-edge research project, your objective is to develop a highly specialized image classification model for underwater marine biodiversity, utilizing thenovel AquaticBiodiversityNet architecture. The model should be optimized for extremely low-resolution images captured by advanced ROVs (Remotely Operated Vehicles) with limited bandwidth. Emphasize on the integration of transfer learning from shallower water datasets to enhance accuracy, while ensuring the system can identify and classify not only species but also their life stages and health conditions. Implement a novel noise-resilient technique for data augmentation, and mandate the use of federated learning to protect sensitive data from centralized storage, all while meeting stringent latency constraints for real-time underwater monitoring in extreme environmental conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"marine biology\",\n        \"description\": \"The project involves developing a specialized image classification model for underwater marine biodiversity using the AquaticBiodiversityNet architecture. The model must handle low-resolution images from ROVs with limited bandwidth, incorporating transfer learning from shallower water datasets for improved accuracy, identifying life stages and health conditions, and employing a noise-resilient data augmentation technique. Federated learning is mandatory for data privacy, with strict latency requirements for real-time monitoring in challenging underwater environments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"life stage classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"health condition classification accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Underwater Marine Biodiversity Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\", \"life stage\", \"health condition\"],\n            \"specification\": {\n                \"image_size\": null,\n                \"resolutions\": \"low\",\n                \"bandwidth_limit\": null\n            },\n            \"description\": \"Dataset consists of low-resolution images captured by advanced ROVs for marine biodiversity study, including life stage and health condition information.\",\n            \"preprocessing\": [\"transfer learning from shallower water datasets\"],\n            \"augmentation\": [\n                {\n                    \"name\": \"noise-resilient technique\"\n                }\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"AquaticBiodiversityNet\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"novel\",\n                \"transfer_learning\": true,\n                \"latency constraints\": \"extreme environmental conditions\"\n            },\n            \"description\": \"A highly specialized model for underwater biodiversity classification, including life stages and health conditions, optimized for low-resolution images and using federated learning for data security.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"ROV with limited bandwidth\",\n        \"deployment_endpoint\": \"federated\",\n        \"inference_engine\": [\"real-time monitoring\"],\n        \"demonstration\": \"extreme environmental conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction, reflecting the need for a high-expertise build project with a focus on low-resolution image classification. Transfer learning, life stage and health condition identification, noise-resilient augmentation, and federated learning are core components. Latency and data privacy concerns are also highlighted, along with the use of AquaticBiodiversityNet. A high confidence score reflects the assumption that the provided schema can cover the project's requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "Incorporate the ability to recognize and classify not only common breeds but also rare and exotic feline and canine sub-species, while ensuring the model can handle extreme weather-affected images, varying image resolutions, and noisy backgrounds, all in real-time, with minimal latency and under a tight power consumption budget.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"wildlife recognition\",\n        \"description\": \"The task involves building a model capable of real-time classification of not only common feline and canine breeds but also rare and exotic sub-species. The model must handle images affected by extreme weather, varying resolutions, and noisy backgrounds with minimal latency and within a tight power consumption budget.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": null\n                }\n            },\n            {\n                \"name\": \"power consumption\",\n                \"value\": {\n                    \"unit\": \"Watts\",\n                    \"value\": null\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": null\n                }\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": {\n                    \"unit\": \"MB\",\n                    \"value\": null\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mixed Feline & Canine Species Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\", \"sub-species\"],\n            \"specification\": {\n                \"imageVariety\": \"extreme weather, varying resolutions, noisy backgrounds\",\n                \"realTime\": true,\n                \"latencyBudget\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": null\n                },\n                \"powerConsumptionBudget\": {\n                    \"unit\": \"Watts\",\n                    \"value\": null\n                }\n            },\n            \"description\": \"A comprehensive dataset including common, rare, and exotic feline and canine species, covering images affected by extreme weather and noisy backgrounds.\",\n            \"preprocessing\": [\"image normalization, resizing for varying resolutions\"],\n            \"augmentation\": [\"weather simulation, noise addition\"],\n            \"visualization\": [\"performance analysis with respect to varying conditions\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Network (CNN) with real-time processing capabilities\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"complexity\": {\n                        \"FLOPs\": {\n                            \"value\": null,\n                            \"unit\": \"number of floating-point operations\"\n                        }\n                    }\n                },\n                \"realTime\": true\n            },\n            \"description\": \"A model optimized for real-time image classification, including handling of diverse and challenging conditions.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"adapted for extreme weather and noisy backgrounds\"],\n        \"target_device\": \"edge device, IoT with low power consumption\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time processing\"],\n        \"demonstration\": \"live examples showcasing performance under varying conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the user's request for a machine learning project with high-performance image classification requirements. The model's handling of extreme conditions, real-time processing, and power consumption are emphasized. Performance metrics and complexity constraints are included as requested. Some details are left open to user input for flexibility.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Task: Develop a specialized time-series forecasting system for the marine environment, integrating DeepAR and a state-of-the-art hybrid LSTM-Transformer architecture with meta-learning and adaptively fine-tuned solar forcing. Implement a real-time data assimilation module that employs Bayesian filtering techniques, a neural architecture search submodule for continuous optimization, and feature selection using earth system proxies. Ensure model explainability with SHAP influence plots and quantify prediction uncertainties using ensemble methods. Forecast global ocean temperature fluctuations over the next five years, considering the impact of volcanic eruptions on a highly granular monthly timescale, with focus on the Gulf Stream and Pacific Ocean Currents.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"marine environment\",\n        \"description\": \"The task is to develop a specialized time-series forecasting system using DeepAR with a hybrid LSTM-Transformer architecture, incorporating meta-learning and solar forcing adaptation. It includes a real-time data assimilation module with Bayesian filtering, neural architecture search for continuous optimization, and feature selection through earth system proxies. Model explainability is ensured with SHAP influence plots, and prediction uncertainties are quantified using ensemble methods. The focus is on forecasting global ocean temperature fluctuations over the next five years, with a fine-grained monthly resolution, specifically targeting Gulf Stream and Pacific Ocean Currents, considering volcanic eruptions' impact.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"forecast horizon\",\n                \"value\": 5\n            },\n            {\n                \"name\": \"volcanic eruptions impact\",\n                \"value\": true\n            },\n            {\n                \"name\": \"monthly resolution\",\n                \"value\": true\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time data assimilation speed\",\n                \"value\": null,\n                \"unit\": \"time\"\n            },\n            {\n                \"name\": \"model inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Marine Environment Time-series Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"ocean temperature\"],\n            \"specification\": {\n                \"granularity\": \"monthly\",\n                \"data_range\": \"past five years\"\n            },\n            \"description\": \"Dataset for forecasting, including historical time series with solar forcing, volcanic eruptions, and earth system proxy data.\",\n            \"preprocessing\": [\n                \"Bayesian filtering\",\n                \"earth system proxies\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"monthly time series\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepAR with LSTM-Transformer hybrid\",\n            \"family\": \"neural networks\",\n            \"type\": \"time-series forecasting\",\n            \"specification\": {\n                \"architecture\": \"LSTM-Transformer\",\n                \"meta-learning\": true,\n                \"fine-tuning\": \"solar forcing\"\n            },\n            \"description\": \"A specialized forecasting model using DeepAR with hybrid LSTM-Transformer, adaptable to environmental changes through meta-learning and solar forcing fine-tuning.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Incorporating Earth system proxies and Bayesian filtering for data assimilation.\"\n        },\n        {\n            \"content\": \"Employing neural architecture search for model optimization.\"\n        },\n        {\n            \"content\": \"Using SHAP for model explainability and ensemble methods for uncertainty quantification.\"\n        },\n        {\n            \"content\": \"Focus on Gulf Stream and Pacific Ocean Currents under volcanic eruption effects.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"earth system proxies\", \"Bayesian filtering\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"specific platform or API\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"online dashboard with forecasting and explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the provided schema and captures the details of the user's instructions, such as the focus on marine environment, hybrid architecture, and specific performance metrics. It also includes the necessary components for building, preprocessing, and deploying the forecasting system.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager is requested to engineer a cutting-edge, cross-platform TinyBERT QA model, targeting 90% precision on the SQuAD Mobile dataset with Newssubcategory. The model must demonstrate exceptional efficiency by achieving Snapdragon Energy Efficiency Rating (EER) 4, deliver responses within a stringent 100-millisecond latency constraint, and provide interpretable insights for non-technical users, while seamlessly processing and answering queries in English, Spanish, and Mandarin without syntax adaptations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"cross-platform\",\n        \"description\": \"The task is to develop a TinyBERT QA model that performs well on the SQuAD Mobile dataset with Newssubcategory, targeting 90% precision. The model must be efficient with Snapdragon Energy Efficiency Rating of 4 and operate with a 100-millisecond latency. Additionally, it should provide interpretable insights for non-technical users and handle multiple languages (English, Spanish, Mandarin) without syntax adaptations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Snapdragon Energy Efficiency Rating\",\n                \"value\": 4\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 100,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD Mobile dataset with Newssubcategory\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language\": [\"English\", \"Spanish\", \"Mandarin\"]\n            },\n            \"description\": \"A cross-lingual dataset for question answering with emphasis on English, Spanish, and Mandarin, targeting the SQuAD Mobile dataset\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"targeted at 90% precision\",\n                \"interpretablity\": \"for non-technical users\",\n                \"language_adaptation\": \"none\"\n            },\n            \"description\": \"A state-of-the-art, cross-platform QA model designed for TinyBERT's lightweight performance on SQuAD Mobile while meeting efficiency and latency requirements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cross-platform\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Snapdragon\"\n        ],\n        \"demonstration\": \"interpretable insights for non-technical users\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the project requirements for a high-performing TinyBERT QA model, addressing the precision target, efficiency, latency, and multilingual capabilities. The choice of model family and type is justified by the nature of the task, and the performance metrics and complexity constraints are mapped correctly. Confidence is high, as the key elements are captured.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a seasoned project manager for an elite ML team, your directive is to design and implement a hierarchical, multivariate regression model on the \"Crab Age and Ecological Profile Dataset.\" This dataset, featuring intricate correlations among categorical (environmental factors, sub-populations) and numerical (morphological features, life cycle stages) attributes, necessitates a step-by-step approach to data preprocessing, feature engineering, and model selection. The team must optimize hyperparameters using nested cross-validation, considering trade-offs between model complexity and generalization. Your mission is to deliver a benchmark model with exceptional performance, specifically in predicting the precise age distribution for various cryptic crab species, while maintaining a rigorous evaluation protocol for the validation and test sets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"ecology\",\n        \"description\": \"The task involves designing and implementing a hierarchical, multivariate regression model on the Crab Age and Ecological Profile Dataset. The model should handle intricate correlations between categorical and numerical attributes, with a focus on data preprocessing, feature engineering, and hyperparameter optimization using nested cross-validation. The primary objective is to predict the precise age distribution for various cryptic crab species with high performance and a robust evaluation protocol.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"R-squared\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Crab Age and Ecological Profile Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"crab age\", \"species\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": null,\n                \"attribute types\": {\n                    \"categorical\": [],\n                    \"numerical\": []\n                }\n            },\n            \"description\": \"A dataset containing correlations among categorical and numerical attributes for various crab species and their life cycle stages.\",\n            \"preprocessing\": [\"data cleaning, normalization, and encoding\"],\n            \"augmentation\": [],\n            \"visualization\": [\"exploratory data analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Multivariate Regression\",\n            \"family\": \"Regression Models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"levels\": [\"hierarchical\"],\n                    \"neural_network_structure\": null\n                },\n                \"hyperparameter_tuning\": \"nested cross-validation\",\n                \"tradeoffs\": {\n                    \"model_complexity\": \"optimized\",\n                    \"generalization\": \"optimized\"\n                }\n            },\n            \"description\": \"A model designed for hierarchical multivariate regression with an emphasis on feature interactions and hyperparameter optimization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The dataset requires extensive feature engineering due to complex correlations between categorical and numerical attributes.\",\n        \"Nested cross-validation will be employed for robust model evaluation and hyperparameter tuning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection, interaction features\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimization for inference efficiency\"],\n        \"demonstration\": \"Performance curves, comparison with baselines, and best hyperparameters\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the comprehensive nature of the project, detailing the user's high expertise level, the specific dataset requirements, and the need for a rigorous evaluation protocol. The model architecture, feature engineering, and performance metrics were adapted from the instruction, ensuring accurate representation.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "For a premium luxury retail establishment, enhance the existing YOLOv5 model by incorporating advanced transfer learning and zero-shot classification. The updated model must handle the \"OpulentRetailDataset,\" consisting of high-resolution, ultra-saturated images showcasing an additional 50 exclusive product sub-variants. The target is to achieve not only a minimum precision of 97% and recall of 99% but also generalize to unseen product variations. Additionally, the system must guarantee sub-second performance (0.5 seconds per image) while maintaining real-time inventory updates and seamless workflow integration with the store's intricate AI-driven retail ecosystem.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury retail\",\n        \"description\": \"The task is to enhance the YOLOv5 model for an opulent retail establishment, incorporating advanced transfer learning and zero-shot classification for handling high-resolution, ultra-saturated images of exclusive products (50 sub-variants). The model must achieve at least 97% precision and 99% recall, and process images in real-time (sub-second) while integrating with the AI-driven retail ecosystem.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.5,\n                \"unit\": \"seconds per image\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpulentRetailDataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product sub-variants\"],\n            \"specification\": {\n                \"image_resolution\": {\n                    \"width\": \"high\",\n                    \"height\": \"high\"\n                },\n                \"saturations\": \"ultra\"\n            },\n            \"description\": \"A dataset for an opulent retail environment, containing high-resolution, ultra-saturated images of 50 exclusive product sub-variants.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5 (Enhanced)\",\n            \"family\": \"YOLO architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": true,\n                \"zero_shot_classification\": true\n            },\n            \"description\": \"Advanced YOLOv5 model tailored for the luxury retail industry with improved performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time inventory updates and seamless workflow integration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction, with a high-level expert user intent. The problem is defined within computer vision (object detection), targeting the luxury retail domain. Performance metrics, including precision and recall, have been specified, along with a sub-second inference time requirement. The dataset and model enhancements match the instruction's details, as does the mention of real-time inventory updates and workflow integration.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "The ML development team is mandated to develop an advanced image classifier for user-generated photos, emphasizing on high precision, scalability, and adaptability to diverse contexts. It must decipher nuances in lighting, emotional cues, and engagement, distinguishing between 'endorsements', 'actionable updates', and 'professional display' while upholding brand aesthetics and ensuring strict adherence to data privacy regulations. Efficiency should be achieved with a carefully optimized human oversight mechanism.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"user-generated content\",\n        \"description\": \"The task involves developing an advanced image classifier for user-generated photos. It should prioritize high precision, scalability, and adaptability to various contexts, such as recognizing nuances in lighting, emotional cues, and engagement. The classifier must differentiate between endorsements, actionable updates, and professional display while maintaining brand aesthetics. Data privacy regulations should be strictly followed, and the system should include a carefully optimized human oversight mechanism for efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"efficiency\",\n                \"value\": null,\n                \"unit\": \"time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"User-Generated Photos\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"endorsements\", \"actionable updates\", \"professional display\"],\n            \"specification\": null,\n            \"description\": \"A dataset of user-generated photos with diverse contextual and emotional elements for training the classifier.\",\n            \"preprocessing\": [\"lighting normalization\", \"emotion detection data augmentation\"],\n            \"augmentation\": [\"Gan-based image synthesis for context diversity\"],\n            \"visualization\": [\"confusion matrix\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimizer\": \"Adam\",\n                \"architecture\": {\n                    \"explainability\": true,\n                    \"transfer_learning\": true,\n                    \"human-in-the-loop\": true\n                }\n            },\n            \"description\": \"Advanced image classifier designed for high precision, with an emphasis on human oversight and adaptability to diverse scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"A well-annotated dataset is crucial for capturing nuances in lighting and emotional cues. Regular retraining with new data will help maintain adaptability.\",\n        \"Privacy-preserving techniques like differential privacy should be incorporated during model training.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for engagement recognition\", \"contextual embeddings\"],\n        \"target_device\": \"cloud for efficient large-scale operation\",\n        \"deployment_endpoint\": {\n            \"service\": \"custom model API hosted\",\n            \"human_in_the_loop\": true\n        },\n        \"inference_engine\": [\"TensorFlow Serving\"],\n        \"demonstration\": \"Interactive demo showcasing classification results with a focus on precision and adaptability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction, with a clear emphasis on the requirements for high precision, scalability, and adaptability. The use of human oversight, brand aesthetics, and data privacy is well represented. The metric expectations and the mention of an optimized human-in-the-loop mechanism aligns with the instruction's guidelines.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As an AI project director, your multifaceted responsibility is to spearhead the creation of a next-generation text classifier, tackling the diverse DAIGT corpus. Not only must you discern human-originated (label 0) and sophisticated AI-generated texts (1), but delve into the intricacies of recent AI advancements, optimizing for both macro-average F1-score and inter-annotator agreement. Embrace the challenge of balancing transformer models' complexity with interpretability, ensuring regulatory compliance through model transparency. Implement a strategy that integrates multiple data augmentation methods tailored to the domain, harnessing expert domain knowledge, and explore novel techniques for adaptive learning. Strive to break new ground by boosting benchmark accuracy with each iteration while maintaining the highest standards of precision and recall.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"artificial intelligence safety and ethics\",\n        \"description\": \"The task is to develop a next-generation text classifier for the diverse DAIGT corpus, distinguishing human-originated (label 0) from AI-generated (label 1) texts, while considering recent AI advancements, optimizing for macro-average F1-score and inter-annotator agreement. The focus is on balancing transformer model complexity with interpretability, ensuring regulatory compliance through transparency, and using expert domain knowledge for data augmentation and adaptive learning to improve accuracy without compromising precision and recall.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"macro-average F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inter-annotator agreement\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"human origin\", \"AI-generated\"],\n            \"specification\": {\n                \"domainExpertise\": \"expert\",\n                \"dataAugmentationMethods\": [\"transformer-specific\", \"domain-specific\"]\n            },\n            \"description\": \"A diverse corpus designed to classify human and AI-generated texts, requiring domain knowledge and data augmentation techniques.\",\n            \"preprocessing\": [\"domain adaptation\", \"tokenization\"],\n            \"augmentation\": [\"transformer-based\", \"expert-driven\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-based Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"regulatoryCompliance\": true\n            },\n            \"description\": \"A next-generation text classifier focusing on interpretability, AI advancements, and balance between model complexity and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Exploitation of recent AI advancements\",\n        \"Expert domain knowledge for data augmentation\",\n        \"Adaptive learning techniques\",\n        \"Model transparency and regulatory compliance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain-adaptive embeddings\",\n            \"custom transformer layers\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom model hosting\",\n        \"inference_engine\": [\n            \"GPU-optimized\",\n            \"real-time\"\n        ],\n        \"demonstration\": \"Interactive model explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the main aspects of the given instruction, highlighting the task's complexity, data requirements, and performance goals. It includes specific model requirements, performance metrics, and data augmentation techniques. However, certain metrics like F1-score and inter-annotator agreement have not been set due to their dependency on model performance which is not explicitly detailed in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop an innovative Hierarchical Attention Graph Transformer (HAGT) model for the task of fine-grained protein function prediction in the PDB biological network, demonstrating a state-of-the-art performance with at least 99% F1-score on highly imbalanced datasets. Additionally, the model must exhibit self-adaptation to structural changes in the protein interaction graph, ensuring resilience against extreme concept shift with minimal drop in accuracy, while maintaining inference speed under 10ms on edge devices with limited computational resources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"biological science\",\n        \"description\": \"The task is to develop a Hierarchical Attention Graph Transformer (HAGT) model for fine-grained protein function prediction in the PDB biological network, targeting a state-of-the-art performance with an F1-score of at least 99% on highly imbalanced datasets. The model should self-adapt to structural changes in protein interaction graphs while maintaining accuracy and inference speed below 10ms on edge devices with limited computational resources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 10.0,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PDB Protein Network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"protein function\"],\n            \"specification\": {\n                \"imbalance_ratio\": \"high\",\n                \"structural_changes\": true\n            },\n            \"description\": \"A biological dataset representing protein interactions with the aim of fine-grained function prediction. Expected to be highly imbalanced with potential for structural changes.\",\n            \"preprocessing\": [\"handling imbalance\", \"structural adaptations\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Attention Graph Transformer (HAGT)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptiveness\": \"self-adaptive to structural changes\",\n                \"accuracy-drop\": \"minimal under concept shift\",\n                \"computational_efficiency\": {\n                    \"inference_speed\": 10.0,\n                    \"resource_constraint\": \"edge devices\"\n                }\n            },\n            \"description\": \"A state-of-the-art model designed for fine-grained protein function prediction with resilience against extreme concept shifts and fast inference.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"graph adaptation\", \"resilience techniques\"],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"performance on highly imbalanced and edge device scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the requirements, highlighting the task, model specifications (F1-score, adaptability, and inference speed), dataset characteristics, and edge device deployment. It also addresses the unique aspects such as self-adaptation to structural changes and minimal accuracy drop under concept shift.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "For a project manager in a competitive energy sector, demand a cutting-edge machine learning solution from the development team. Design an intricate time-series anomaly detector that not only analyzes the complex, multi-faceted hourly power usage data of a small town over a decade but also incorporates regional weather patterns, adjusts for economic shifts, and requires minimal latency for continuous 24/7 real-world adaptability, all while meeting a rigorous 6-hour response time target.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"energy and utilities\",\n        \"description\": \"The project manager is seeking a state-of-the-art machine learning solution for detecting anomalies in hourly power usage data for a small town over a decade. The solution must be intricate and consider regional weather patterns, economic changes, operate with minimal latency for real-time adaptability, and achieve a 6-hour response time target.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"anomaly detection rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": 6\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hourly Power Usage Data (Small Town)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"power usage\"],\n            \"specification\": {\n                \"span\": \"decade\",\n                \"complexity\": \"multi-faceted\",\n                \"weather_data_linked\": true,\n                \"economic_data_linked\": true\n            },\n            \"description\": \"Data collected for a small town, containing hourly power usage with historical records and contextual information like regional weather patterns and economic shifts.\",\n            \"preprocessing\": [\"feature engineering with weather and economic data\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"real-time monitoring and anomaly visualization\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Intricate Time-Series Anomaly Detector\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep learning with long short-term memory (LSTM) or transformer models for time-series analysis\",\n                \"computational_efficiency\": \"optimized for minimal latency\"\n            },\n            \"description\": \"An advanced model designed to handle complex multi-faceted data, adapt to real-world conditions, and meet the stringent response time requirements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"weather and economic impact analysis\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"real-time, continuous monitoring system\",\n        \"inference_engine\": [\n            \"streaming or real-time processing\"\n        ],\n        \"demonstration\": \"scenarios showing adaptability and anomaly detection in real-time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the requirements accurately, from the high-level expert-level user intent to the intricate details of the task, performance metrics, and response time. The selection of a deep learning model like LSTM or transformer indicates the sophistication needed for time-series data and the specified complexity. The provided specifications cover the details of the dataset and the model to ensure it aligns with the manager's expectations.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the specialized task of underwater coral reef image segmentation on CoralNet 3.0, design a cutting-edge FusionNet variant, incorporating a deep-learned Light-Motion Compensation Module. Ensure peak accuracy for identifying not only coral genera but also rare epiphytic algae and bioluminescent organisms, while attaining a minimum 25% improvement in F1-score under waterbody turbidity and low-IR light conditions. Implement and optimize using a novel loss function tailored for these challenging scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"underwater ecology\",\n        \"description\": \"The task is to design an advanced FusionNet variant with a Light-Motion Compensation Module for underwater coral reef image segmentation, targeting peak accuracy for coral genera, rare epiphytic algae, and bioluminescent organisms. The model must achieve at least a 25% improvement in F1-score under turbidity and low-IR light conditions, using a custom loss function tailored for the challenge.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.75\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CoralNet 3.0\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"coral genera\", \"epiphytic algae\", \"bioluminescent organisms\"],\n            \"specification\": {\n                \"data_challenges\": [\"waterbody turbidity\", \"low-IR light conditions\"]\n            },\n            \"description\": \"A dataset for underwater coral reef image segmentation, focusing on difficult scenarios with specific target organisms.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"FusionNet (variant)\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"loss_function\": \"novel (tailored for underwater image segmentation)\"\n            },\n            \"description\": \"An advanced model designed specifically for underwater coral reef image segmentation, incorporating a Light-Motion Compensation Module.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating deep-learned Light-Motion Compensation for robust segmentation under varying light conditions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response adheres to the schema and captures the essence of the task. The user's high level of expertise is reflected, and the problem description highlights the specific requirements for accuracy, F1-score improvement, and the novel loss function. The dataset and model details align well with the task's underwater coral reef image segmentation context.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for a cutting-edge AI research team, you've been tasked with tackling a challenging, real-world medical application: the intricate analysis of the MIT-BIH Arrhythmia dataset for advanced time-series arrhythmia detection. This project requires not only processing the extensive dataset split into complex train, validation, and test sets, each containing over 40,000 ECG signals with 5,000-point segments (INPUT_SEQ_LEN=5,000), but also meticulously selecting a diverse set of 50 key features to enhance model performance.\n\nDesign a groundbreaking hybrid architecture that integrates convolutional neural networks, transformers, and explainable AI components, catering to a fine-grained 15-class classification problem (identifying specific subclasses like Atrial Premature Contractions and Bundle Branch Blocks). Emphasize robustness against irregularities in N, L, QRS complex patterns, and various artifacts found within the noisy data.\n\nUtilize a sophisticated mix of hyperparameter optimization techniques, incorporating both Bayesian optimization and ensemble learning, to maximize the model's macro-averaged F1-score while maintaining interpretability. Additionally, devise strategies to address and mitigate the impact of outliers and data inconsistencies to ensure state-of-the-art performance in this high-dimensional and dynamic healthcare scenario.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"medical\",\n        \"description\": \"The task involves analyzing the MIT-BIH Arrhythmia dataset for advanced arrhythmia detection with a focus on complex train, validation, and test sets. The project aims to create a hybrid architecture combining CNNs, transformers, and explainable AI components. It targets 15-class classification, emphasizing robustness against irregularities and noisy data with a focus on macro-averaged F1-score and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"macro-averaged F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MIT-BIH Arrhythmia dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"ECG signals\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": {\"total\": 40000, \"segment_length\": 5000},\n                    \"validation\": {\n                        \"total\": 40000,\n                        \"segment_length\": 5000,\n                        \"complexity\": true\n                    },\n                    \"test\": {\n                        \"total\": 40000,\n                        \"segment_length\": 5000,\n                        \"complexity\": true\n                    }\n                },\n                \"features\": 50\n            },\n            \"description\": \"A large, complex dataset containing over 40,000 ECG signals, 5,000-point segments, and 50 selected features for enhanced model performance in arrhythmia detection.\",\n            \"preprocessing\": [\n                \"noise reduction\",\n                \"feature extraction\"\n            ],\n            \"augmentation\": [\n                \"time-domain transformations\",\n                \"artifact injection\"\n            ],\n            \"visualization\": [\n                \"signal analysis\",\n                \"feature importance\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hybrid Architecture (CNNs, Transformers, Explainable AI)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"CNN_layers\": null,\n                    \"Transformer_layers\": null,\n                    \"explainable_components\": null\n                },\n                \"number_of_classes\": 15,\n                \"interpretablity_focus\": true\n            },\n            \"description\": \"A groundbreaking model combining various advanced techniques for time-series classification with a focus on interpretability and robustness against irregularities.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Emphasis on robustness against N, L, QRS complex patterns and noisy data.\"\n        },\n        {\n            \"text\": \"Employing Bayesian optimization and ensemble learning for hyperparameter tuning.\"\n        },\n        {\n            \"text\": \"Strategies for outlier detection and data inconsistency mitigation.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection based on importance\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Bayesian inference\", \"ensemble inference\"],\n        \"demonstration\": \"interactive patient-specific analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the user's intent (expert-level project), details about the challenging medical task, dataset characteristics, and required model architecture. It also incorporates performance metrics, data complexity, and strategies for optimization and data handling. The confidence score is high due to the thoroughness of the representation.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "time-series forecasting", "instruction": "Create a sophisticated time-series forecasting model, integrating Temporal FusionTransformer and a Hierarchical Attention LSTM, specifically tailored for the UrbanSounds8K dataset. Focus on predicting minute fluctuations in noise intensity for the next 72 hours, while performing fine-grained feature attribution and category-wise ranking of noises (like daily vs. nighttime construction, high-traffic rush hour, and distinct emergency sirens) to design localized noise mitigation strategies, ensuring real-time adaptability and actionable insights.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"environmental science, urban planning\",\n        \"description\": \"The task involves developing a time-series forecasting model combining Temporal Fusion Transformer and Hierarchical Attention LSTM, specifically for the UrbanSounds8K dataset. The model must predict minute fluctuations in noise intensity for the next 72 hours, with feature attribution and category-wise ranking of noises (construction, rush hour, and emergency sirens) for localized noise mitigation strategies, ensuring real-time adaptability and actionable insights.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UrbanSounds8K\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"noise intensity\"],\n            \"specification\": {\n                \"data_length\": {\n                    \"past\": null,\n                    \"future\": 72\n                },\n                \"feature_dimensions\": null\n            },\n            \"description\": \"A dataset for time-series noise intensity prediction with focus on minute fluctuations and distinctions between various noise types.\",\n            \"preprocessing\": [\n                \"resampling\",\n                \"normalization\"\n            ],\n            \"augmentation\": [\n                \"temporal shifting\"\n            ],\n            \"visualization\": [\n                \"feature importance\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Temporal Fusion Transformer (TFT) + Hierarchical Attention LSTM\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"TFT\": {\n                        \"attention_type\": \"temporal fusion\"\n                    },\n                    \"LSTM\": {\n                        \"hierarchical_structure\": true\n                    }\n                },\n                \"parameters\": {\n                    \"TFT\": null,\n                    \"LSTM\": null\n                }\n            },\n            \"description\": \"A tailored model combining Temporal Fusion Transformer and Hierarchical Attention LSTM for noise intensity forecasting, considering fine-grained feature attribution and noise categorization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time adaptability is critical for responding to dynamic urban noise conditions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"time-series feature extraction\"],\n        \"target_device\": \"real-time environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for efficiency\"],\n        \"demonstration\": \"interactive visualizations for insights and strategy recommendations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the instruction, specifying a complex task for time-series forecasting, the datasets involved, and the desired model architecture and performance. Fine-grained attribution and ranking of noise categories are highlighted as key aspects. The high-level nature of the problem suggests a high expertise level for the user.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "The project manager must engineer a cutting-edge BCI pipeline for time-series EEG analysis, focusing on real-time classification of rare, millisecond-level neural events. The system should seamlessly integrate intricate recurrent neural networks, transfer learning with domain-specific EEG features, and incorporate explainable AI for continuous adaptation, ensuring optimal performance in high-density neuroimaging data for life-critical applications like epilepsy monitoring and neuroscience research.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"neuroscience and healthcare\",\n        \"description\": \"The project involves developing a BCI pipeline for real-time analysis of EEG time-series data, specifically targeting rare, millisecond-level neural events. It should include advanced techniques like recurrent neural networks, transfer learning with domain-specific EEG features, and explainable AI for continuous adaptation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"EEG time-series data for BCI\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"neural events\"],\n            \"specification\": {\n                \"sampling_rate\": null,\n                \"data_length\": null,\n                \"channel_count\": null\n            },\n            \"description\": \"High-density neuroimaging data containing rare, millisecond-level neural events for real-time analysis.\",\n            \"preprocessing\": [\"artifact removal\", \"bandpass filtering\"],\n            \"augmentation\": [\"time warping\", \"synthetic data generation\"],\n            \"visualization\": [\"event-related potentials\", \"spectrograms\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BCI pipeline\",\n            \"family\": \"Time-series analysis (RNNs and transfer learning)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"Incorporating RNN architecture for time-series data, transfer learning using domain-specific EEG features\",\n                \"explainability\": \"Explainable AI for continuous adaptation\"\n            },\n            \"description\": \"A state-of-the-art BCI pipeline for real-time classification of rare neural events in EEG data, designed for epilepsy monitoring and neuroscience research.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating domain expertise in EEG features and explainable AI principles\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"dimensionality reduction\"],\n        \"target_device\": \"cloud-based for real-time processing\",\n        \"deployment_endpoint\": \"secure data center or cloud platform\",\n        \"inference_engine\": [\"TensorFlow\", \"Keras\"],\n        \"demonstration\": \"live demonstration of event classification and adaptation in a simulated environment\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers key aspects from the instruction, including the high-end requirements, focus on real-time and explainable classification, domain-specific techniques, and the importance of performance in medical applications. It provides an accurate representation of the project manager's needs.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "Task the ML team to develop a state-of-the-art, lightweight EfficientNet-based image classifier specifically designed for the HAM10000 melanoma and benign nevi dataset. Demand a precision-oriented model with a rigorous misclassification rate below 0.1%, ensuring minimal false negatives. Implement Grad-CAM explainability for in-depth decision breakdown, meeting expert dermatologists' stringent 10-point validation criteria. Optimize the model for embedded deployment on smartwatches, maintaining real-time analysis at <200ms latency while preserving both accuracy and efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medical\",\n        \"description\": \"The task is to develop a state-of-the-art, lightweight image classifier using EfficientNet for the HAM10000 melanoma and benign nevi dataset. The model should have precision-oriented performance with a misclassification rate below 0.1%, minimizing false negatives. It must include Grad-CAM explainability to meet expert dermatologists' 10-point validation criteria and be optimized for embedded deployment on smartwatches with real-time analysis at <200ms latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"misclassification rate\",\n                \"value\": 0.001\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 200,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HAM10000 Melanoma and Benign Nevi Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"melanoma\", \"benign\"],\n            \"specification\": {\n                \"dataset_size\": \"state-of-the-art, lightweight\",\n                \"feature_distribution\": \"Melanoma vs Benign\"\n            },\n            \"description\": \"A medical dataset for classifying melanoma and benign nevi, demanding precision and minimal false negatives.\",\n            \"preprocessing\": [\"lightweight data preprocessing\"],\n            \"augmentation\": [\"accurate for embedded deployment\"],\n            \"visualization\": [\"Grad-CAM explainability required\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"lightweight\",\n                \"parameters\": \"optimized for embedded deployment\",\n                \"FLOPs\": \"minimal\"\n            },\n            \"description\": \"A state-of-the-art image classifier specifically designed for the HAM10000 dataset, focusing on precision and meeting dermatologists' validation criteria.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Grad-CAM explainability aligns with expert dermatologists' 10-point validation criteria\",\n        \"Smartwatch deployment requires low latency, real-time analysis\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"optimized for smartwatch constraints\"],\n        \"target_device\": \"smartwatch\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for embedded environment\"],\n        \"demonstration\": \"explainable decision breakdown for dermatologists\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's intent, a high-expertise requirement, and the specific task details including the model, dataset, and performance metrics. The focus on precision and low misclassification rate is clear. The chosen data preprocessing, augmentation, and source align with the given constraints. Expertise points are also factored in the model's design and optimization for smartwatch deployment.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "The project manager is tasked with designing an innovative, neuroevolutionary time-series forecasting framework that combines quantum-inspired machine learning with granular event clustering. This system must seamlessly incorporate quantum temporal attention networks, predictive swarm intelligence, and sub-second anomaly detection. It should be able to unravel hidden financial micro-cycles, anticipate black swan events, and respond to blockchain-driven market shifts in real-time. The model must generate sub-hourly forecasts with sub-minute accuracy for precision operational maneuvers in a hyper-competitive, decentralized economy. Furthermore, the system should continuously learn and adapt to dynamically changing market dynamics and non-linear systemic influences.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"finance and economics\",\n        \"description\": \"The project involves designing an innovative time-series forecasting framework that combines neuroevolution, quantum-inspired machine learning, granular event clustering, quantum temporal attention networks, predictive swarm intelligence, and sub-second anomaly detection for real-time financial analysis. The system aims to predict sub-hourly forecasts with sub-minute accuracy, adapt to market dynamics, and respond to blockchain-driven changes in a hyper-competitive, decentralized economy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"forecast accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sub-minute accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sub-hourly forecasting\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time response\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"dynamic adaptation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"complex algorithmic integration\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"quantum-inspired financial data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"financial micro-cycles\", \"black swan events\", \"blockchain-driven market shifts\"],\n            \"specification\": null,\n            \"description\": \"The dataset consists of financial and market data with granular event clustering and quantum temporal attention network features.\",\n            \"preprocessing\": [\"quantum data preprocessing\", \"event clustering\"],\n            \"augmentation\": [],\n            \"visualization\": [\"real-time anomaly detection\", \"sub-hourly forecasts\"],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-inspired Neuroevolutionary Framework\",\n            \"family\": \"Quantum Machine Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"elements\": [\"neuroevolution\", \"quantum temporal attention\", \"predictive swarm intelligence\"],\n                \"inference_time\": null,\n                \"memory_size\": null,\n                \"complexity\": \"dynamic\"\n            },\n            \"description\": \"A framework combining quantum, evolutionary, and swarm intelligence techniques for robust, adaptive time-series forecasting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Non-linear systemic influences and dynamic market dynamics should be captured using dynamic model structures.\",\n        \"Blockchain technology for updating model in response to real-time market shifts\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"granular event clustering\", \"real-time learning\"],\n        \"target_device\": \"high-performance computing or cloud\",\n        \"deployment_endpoint\": \"real-time financial forecasting service\",\n        \"inference_engine\": [\"quantum-enabled\", \"GPU acceleration\"],\n        \"demonstration\": \"Hyper-competitive environment showcase\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key requirements from the user's instruction, including the high complexity and specialized nature of the task. Performance metrics and adaptation to dynamic market conditions are clearly defined, and the search for datasets is described as inferred based on the provided features. Specific model details align with the high-level requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As an experienced project director for the groundbreaking AI research team, coordinate the development of a state-of-the-art neural machine translation engine utilizing the extensive WMT19 multilingual newswire collection. Specifically demand fluency in translating complex Russian literature into English, targeting a sophisticated BLEU benchmark of 47 in a realistic, cross-industry setting. Implement novel domain-specific adaptation techniques via zero-shot and transfer learning, ensuring optimal performance on industry-specific terminologies. Mandate rigorous step-wise error analysis, multiple stages of human review for equivalence with human-generated translations, and a comprehensive comparison demonstrating superior results over rival tools without compromising speed, all executed within a tight computational budget and adherence to the latest hardware constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-industry\",\n        \"description\": \"The task is to develop a state-of-the-art neural machine translation engine using the WMT19 multilingual newswire collection. Focus is on translating complex Russian literature into English and achieving a BLEU score of 47. Incorporate novel domain-specific adaptation techniques, perform error analysis, multiple stages of human review, and compare with rival tools, maintaining speed and hardware constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 47\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Resource efficiency\",\n                \"value\": null,\n                \"unit\": \"efficient computational budget\"\n            },\n            {\n                \"name\": \"Hardware constraints\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 Multilingual Newswire Collection\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source language\": \"Russian\",\n                \"target language\": \"English\",\n                \"domain\": \"complex literature\",\n                \"size\": \"extensive\"\n            },\n            \"description\": \"The extensive multilingual dataset for training and evaluating a neural machine translation engine, focusing on Russian literature.\",\n            \"preprocessing\": [\"domain adaptation techniques\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art Neural Machine Translation Model\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptation_techniques\": [\"zero-shot learning\", \"transfer learning\"],\n                \"target_performance\": \"industry-specific terminologies\",\n                \"speed\": null\n            },\n            \"description\": \"A neural machine translation model for translating Russian literature with sophisticated BLEU benchmark and efficient resource usage.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Novel domain adaptation techniques, error analysis, and human review for equivalence with human-generated translations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"comparative analysis with rival tools\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the instruction, with a focus on the high-level expertise, specific task requirements, performance metrics, and novel techniques. It keeps track of the hardware constraints and computational budget.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project manager for a premier time-series forecasting team, your elevated directive is to enhance the current state-of-the-art seasonal ARIMA-Transformer model for the intricate Flu Incidence (ILI) dataset by:\n\n1. Develop a novel hybrid architecture that combines LSTM-NARX with a hybrid Kalman filter, ensuring robust handling of intricate seasonal dynamics, non-linearities, and multi-level dependencies in weekly seasonality.\n2. Implement an adaptive time-varying forecasting strategy with weekly resolution (six 4-week cycles within a 52-week cycle), dynamically adjusting the model's sensitivity to fluctuations for training (48 weeks), validation (6 cycles), and testing (8 cycles) phases, optimizing responsiveness.\n3. Optimize accuracy by concurrently minimizing a weighted sum of wRMSSE and a novel robust loss function, while guaranteeing minimal latency for next week's 16-day forecast (224 hours), driving real-time predictive efficiency and system agility in high-volume data environments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The goal is to enhance the current ARIMA-Transformer model for flu incidence prediction with a novel LSTM-NARX hybrid architecture fused with a hybrid Kalman filter. The model must handle intricate seasonal dynamics, non-linearities, and multi-level dependencies, and employ a time-varying forecasting strategy with weekly resolution, adjusting sensitivity during training, validation, and testing. The objective is to minimize a weighted sum of wRMSSE and a robust loss function while ensuring low latency for a 16-day forecast in a real-time, high-volume context.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"weighted RMSSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robust loss function\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\"type\": \"duration\", \"value\": \"224 hours\"}\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": \"real-time\", \n                \"unit\": \"time\"\n            },\n            {\n                \"name\": \"latency for 16-day forecast\",\n                \"value\": {\"type\": \"duration\", \"value\": \"224 hours\"},\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Flu Incidence (ILI) dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"ILI counts\"],\n            \"specification\": {\n                \"seasonality\": \"weekly\",\n                \"dependency_structure\": \"complex\",\n                \"data_distribution\": \"non-linear and multi-level\"\n            },\n            \"description\": \"Contains weekly FLU data, requiring modeling for non-linear, multi-level dependencies and seasonal fluctuations.\",\n            \"preprocessing\": [\"Seasonal adjustment\", \"Normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"LSTM-NARX with Hybrid Kalman Filter\",\n            \"family\": \"Neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"hybrid LSTM-NARX\",\n                \"filtering_method\": \"Kalman filter\",\n                \"seasonality_handling\": \"LSTM-NARX-Hybrid Kalman\"\n            },\n            \"description\": \"A state-of-the-art model designed for flu incidence prediction with improved handling of complex dynamics.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"LSTM-NARX architecture\",\n            \"Hybrid Kalman filter\",\n            \"Time-varying forecasting strategy\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Optimized real-time flu incidence forecast for high-volume environments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's requirements for enhancing the model, including the novel architecture, adaptive strategy, and performance metrics. The instructions for preprocessing and source of the dataset, as well as the demonstration goal, are also accounted for. The specified expertise level suggests a thorough understanding of the task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "The project manager is tasked with developing a sophisticated time-series anomaly detection system for a global e-commerce platform. Emphasize the deployment of an advanced clustering algorithm, specifically HDBSCAN with sliding window technique, that captures micro-traffic anomalies within hour-long intervals. Implement a hybrid approach combining a 7-day cyclic correction model for mobile traffic's circadian fluctuations and fine-tuned LSTM-CNN ensemble to decipher nuanced patterns like daily peaks on weekends and holiday effects. Mandate real-time generation of not only security warnings but also micro-segmentation alerts, alongside user behavior analytics for precision-driven personalized marketing tactics, considering geographical and temporal aspects.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"The project involves developing a sophisticated time-series anomaly detection system for a global e-commerce platform, focusing on micro-traffic anomalies within hourly intervals using HDBSCAN with sliding window technique, and incorporating a hybrid approach with a 7-day cyclic correction model and LSTM-CNN ensemble. The system must generate real-time security warnings and micro-segmentation alerts, along with user behavior analytics for personalized marketing, considering geographical and temporal factors.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Global E-commerce Platform Time Series Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"micro-traffic\", \"user behavior\"],\n            \"specification\": {\n                \"frequency\": \"hourly\",\n                \"geographical_coverage\": \"global\",\n                \"temporal_dimensions\": \"7-day cycle\"\n            },\n            \"description\": \"A time-series dataset capturing traffic and user behavior data, including hourly micro-traffic and potential temporal patterns like daily peaks on weekends and holiday effects.\",\n            \"preprocessing\": [\"HDBSCAN with sliding window\", \"Cyclical correction model for mobile traffic\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Hourly anomaly detection\",\n                \"Weekly and holiday pattern analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HDBSCAN + Sliding Window\",\n            \"family\": \"Clustering\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"algorithm\": \"HDBSCAN\",\n                \"window_size\": null,\n                \"performance\": null\n            },\n            \"description\": \"Advanced clustering algorithm utilizing HDBSCAN with sliding window to detect micro-traffic anomalies.\"\n        },\n        {\n            \"name\": \"LSTM-CNN Ensemble\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble\": \"LSTM-CNN hybrid\",\n                \"fine-tuning\": true,\n                \"performance\": null\n            },\n            \"description\": \"A hybrid model combining LSTM and CNN for capturing nuanced patterns in time-series data, including daily peaks on weekends and holiday effects.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Geographical and temporal user behavior analytics\",\n        \"Circadian fluctuations in mobile traffic\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time generation of security warnings\", \"micro-segmentation alerts\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"Precision-driven personalized marketing strategy based on user behavior analytics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the given instruction with a high level of detail, capturing the expert-level requirement. It describes the development of an advanced time-series system, the use of HDBSCAN, LSTM-CNN ensemble, and the performance metrics. The dataset requirements and preprocessing steps are clearly defined. The service requirements for real-time alerts and user behavior analytics are included.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "time-series anomaly detection", "instruction": "As the project manager overseeing the elite machine learning development team, your challenge is to design and implement a state-of-the-art Time-Series Anomaly Detection (TSD) platform for the Detection of Unprecedented Astrophysical Phenomena (DAPP-Net). This platform must employ a fusion of cutting-edge technologies, including a neuroevolutionary meta-learning framework, a quantum-enhanced spatiotemporal causal quantum graph (Q-SCG), and a multi-scale compressed sensing algorithm. The platform's focus lies in the analysis of the enigmatic \"QuantumCosmicCosmicWeb\" dataset, which not only requires pinpoint accuracy in identifying conventional anomalies but also must discern subtle variations in extragalactic gravitational wave patterns with attosecond precision.\r\n\r\nTo achieve this, the team must devise a method for live integration of on-the-fly topological feature extraction with adaptive dimensionality reduction, all while ensuring near-zero latency for the vast and rapidly expanding data streams from distant black hole mergers and pulsar glitches. Furthermore, the platform must demonstrate exceptional resource efficiency, using quantum parallelism to process exabytes of data in real-time, without sacrificing detection accuracy or introducing computational bottlenecks.\r\n\r\nIn the accompanying whitepaper, describe this groundbreaking DAPP-Net's hierarchical architecture, its resilience to quantum decoherence, and how it leverages quantum supremacy for big data analysis in ultra-high-dimensional time series. Emphasize the platform's scalability and potential for forecasting novel astrophysical events, positioning it as the cornerstone of a new era in astronomical monitoring and understanding.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The project involves designing a state-of-the-art Time-Series Anomaly Detection platform called DAPP-Net, focusing on detecting unprecedented astrophysical phenomena, specifically for the QuantumCosmicCosmicWeb dataset. Key requirements include pinpoint accuracy, attosecond precision in detecting anomalies, live topological feature extraction, adaptive dimensionality reduction, near-zero latency, quantum parallelism, and resource efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"anomaly detection accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"attosecond precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": null,\n                \"unit\": \"quantum parallelism\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"QuantumCosmicCosmicWeb\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"dimensionality\": null,\n                \"growth rate\": \"ultra-high\"\n            },\n            \"description\": \"A dataset capturing enigmatic data from distant black hole mergers and pulsar glitches, requiring precise anomaly detection and subtle extragalactic gravitational wave pattern analysis.\",\n            \"preprocessing\": [\n                \"live topological feature extraction\",\n                \"adaptive dimensionality reduction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DAPP-Net\",\n            \"family\": \"neuroevolutionary meta-learning framework\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"fusion_elements\": [\"Q-SCG\", \"multi-scale compressed sensing\", \"topological feature extraction\"],\n                    \"resilience\": \"quantum decoherence-resistant\",\n                    \"leveraging\": \"quantum supremacy for big data analysis\"\n                },\n                \"hierarchical_structure\": null\n            },\n            \"description\": \"A platform integrating neuroevolutionary meta-learning, Q-SCG, and multi-scale CS for precise anomaly detection, scalability, and real-time forecasting of astrophysical events.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"aspect\": \"resource efficiency\",\n            \"description\": \"Leverages quantum parallelism to process exabytes in real-time without bottlenecks.\"\n        },\n        {\n            \"aspect\": \"scale and forecasting\",\n            \"description\": \"Platform's scalability enables forecasting of novel astrophysical events, positioning it for a new era in monitoring and understanding.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"quantum parallelism\"],\n        \"demonstration\": \"Hierarchical architecture, low latency, and big data analysis capabilities\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately represents the provided project requirements, emphasizing the high expertise level for the project management. The problem section captures the specific dataset, architecture, and performance metrics. The project's unique aspects, such as quantum-enhanced techniques, resource efficiency, and scalability, are incorporated.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As the project manager, task the machine learning team with creating a groundbreaking image classifier for enigmatic interstellar phenomena, pushing the boundaries of quantum convolutional neural networks and fusing them with a proprietary Exo-Atmospheric Feature Extractor (EAFE). Demand that the model addresses the complex challenges of subatomic nebula classification, refracting starlight anisotropies, and accomplishes an unprecedented accuracy benchmark of 0.2% under extreme gravitational lensing effects. Instruct the team to implement a six-step data preprocessing pipeline, incorporating top-tier artifact reduction methods and a specialized protocol for deciphering intricate cosmic web artifacts. Additionally, propose a hybrid evaluation system that integrates not only standard metrics like receiver operating characteristic curves and Kullback-Leibler divergence, but also novelty detection tests and inter-dimensional clustering analysis, ensuring the classifier's prowess in differentiating intricate and rare extraterrestrial life indicators.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The project aims to develop a groundbreaking image classifier for interstellar phenomena, utilizing quantum convolutional neural networks and a proprietary Exo-Atmospheric Feature Extractor (EAFE). The model should handle subatomic nebula classification, refracting starlight anisotropies, and achieve a target accuracy of 0.2% under extreme gravitational lensing effects.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.002\n            },\n            {\n                \"name\": \"receiver operating characteristic curves\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Kullback-Leibler divergence\",\n                \"value\": null\n            },\n            {\n                \"name\": \"novelty detection tests\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inter-dimensional clustering analysis\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Custom Interstellar Phenomena Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"subatomic nebula classification\", \"anisotropic starlight\", \"extraterrestrial life indicators\"],\n            \"specification\": {\n                \"artifact_reduction_methods\": \"top-tier\",\n                \"cosmic_web_artifact_protocol\": \"specialized\",\n                \"data_pipeline_steps\": 6\n            },\n            \"description\": \"A dataset for complex interstellar image classification, including advanced preprocessing, artifact reduction, and cosmic web artifact deciphering.\",\n            \"preprocessing\": [\"top-tier artifact reduction\", \"cosmic web artifact decoding\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum Convolutional Neural Network (QCNN) with EAFE\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"fusing QCNN and EAFE\",\n                \"artifact_thresholds\": {\n                    \"novelty_detection\": null,\n                    \"inter_dim_clustering\": null\n                }\n            },\n            \"description\": \"An advanced image classifier model designed for handling extreme interstellar conditions and incorporating a proprietary feature extractor.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"artifact reduction pipeline\",\n            \"cosmic web feature deciphering\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Include performance under extreme conditions and unique evaluation metrics.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON schema was followed as per the given format. The instruction provided extensive details, which were properly mapped to areas like task, model, dataset, and evaluation system. The user's high expertise level and specific requirements such as performance, complexity, and preprocessing are captured.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for the advanced MIMIC-IV time-series classification initiative, you must direct the AI development team to engineer a novel deep learning model that integrates a state-of-the-art TCN with bidirectional and self-attention mechanisms. The objective is to accurately forecast arrhythmia types in high-resolution ECG signals (1Hz sampling, 300,000 data points per episode) within the first 5-minute window, accounting for complex interdependencies between diverse physiological features. The model must outperform benchmarks by achieving a minimum of 95% average precision, F1 score, and uplift the area under the APRC curve by 20%. Additionally, mandate implementation of Bland-Altman analysis with clinical interpretability, ensuring robustness in real-world deployment and the demonstration of explainable decision-making.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"The project aims to develop a novel deep learning model using a state-of-the-art Temporal Convolutional Network (TCN) with bidirectional and self-attention mechanisms for classifying arrhythmia types in MIMIC-IV ECG signals (1Hz sampling, 300,000 data points per episode) within the first 5 minutes. The model should outperform benchmarks with a minimum of 95% average precision, F1 score, and a 20% improvement in AUPRC. It also requires Bland-Altman analysis and clinical interpretability for robustness and explainable decision-making.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"area under the AUPRC curve\",\n                \"value\": 0.2\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MIMIC-IV (time-series ECG data)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"arrhythmia types\"],\n            \"specification\": {\n                \"sampling_rate\": 1,\n                \"data_points_per_episode\": 300000\n            },\n            \"description\": \"High-resolution ECG signals with 1Hz sampling and 300,000 data points per episode, capturing complex interdependencies among physiological features.\",\n            \"preprocessing\": [\"1Hz sampling\", \"normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\" \n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Novel TCN with bidirectional and self-attention\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art TCN with bidirectional and self-attention mechanisms\"\n            },\n            \"description\": \"A deep learning model designed to forecast arrhythmia types in MIMIC-IV ECG signals with the specified complexity and performance requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Clinical interpretability and Bland-Altman analysis are necessary\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"1Hz sampling\", \"生理特征融合\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretable models\"],\n        \"demonstration\": \"explainable decision-making\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's intentions to build a machine learning model for an advanced medical task. The specifics of the MIMIC-IV dataset, model requirements (95% accuracy, self-attention), and the mandated Bland-Altman analysis are all included. However, the exact model specification may require more input.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a Hierarchical Temporal Forecasting Framework for Ocean Current Dynamics\r\n\r\nIn the context of marine conservation and sustainable maritime industries, design and implement a novel machine learning model based on the hybridized DeepAR-Transformer architecture. The model should start by pre-training on a vast, labeled dataset of ocean current patterns collected from satellite and underwater sensors, considering various oceanographic factors such as temperature, salinity, and wind patterns over a five-year period. This pre-training must capture the seasonal and interannual dependencies in the data.\r\n\r\nNext, focus on forecasting ocean currents for a 120-day horizon with a half-hourly resolution, factoring in the model's ability to handle long-term dependencies and extreme events. To enhance accuracy, integrate a novel graph-based feature extraction module that accounts for the complex spatial interactions among different ocean regions. This component should also detect and cluster patterns indicative of marine ecosystems, allowing for early warning systems against potential ecological shifts or disruptions.\r\n\r\nMoreover, the project necessitates the creation of an interactive, real-time visualization platform. This platform should present time-varying heatmaps of predicted ocean current patterns, overlayed with the detected ecosystem clusters and their health indicators. The user interface must be intuitive for marine biologists, fishermen, and policymakers to quickly interpret and make data-driven decisions regarding resource management, shipping routes, and conservation efforts. The ultimate goal is to optimize resource utilization while minimizing the environmental impact on marine ecosystems.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"marine conservation and maritime industries\",\n        \"description\": \"The task is to develop a Hierarchical Temporal Forecasting Framework using a DeepAR-Transformer hybrid for ocean current dynamics prediction. The model requires pre-training on a five-year labeled dataset, considering oceanographic factors and capturing seasonal and interannual dependencies. The focus is on forecasting 120-day ocean currents with a half-hourly resolution, handling long-term dependencies and extreme events, and incorporating a graph-based feature extraction module for ecosystem detection and early warnings.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ocean Current Patterns Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"temperature\", \"salinity\", \"wind patterns\"],\n            \"specification\": {\n                \"temporal_resolution\": \"half-hourly\",\n                \"duration\": \"five-year\",\n                \"size\": null\n            },\n            \"description\": \"A satellite and underwater sensor-collected dataset of oceanographic factors for five years, containing information for pre-training the model.\",\n            \"preprocessing\": [\n                \"seasonal decomposition\",\n                \"feature scaling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"time-series heatmaps\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid DeepAR-Transformer for Ocean Current Dynamics\",\n            \"family\": \"neural networks\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"pre-trained_dataset\": \"Ocean Current Patterns Dataset\",\n                \"architecture\": {\n                    \"DeepAR component\": {\n                        \"seasonality\": true,\n                        \"interannual\": true\n                    },\n                    \"Transformer component\": {\n                        \"long-term dependencies\": true,\n                        \"extreme events handling\": true\n                    }\n                }\n            },\n            \"description\": \"A novel machine learning model designed to handle the complexities of ocean current forecasting, utilizing the DeepAR-Transformer hybrid and novel feature extraction methods.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"graph-based feature extraction\",\n            \"ecosystem clustering detection\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time\"\n        ],\n        \"demonstration\": {\n            \"interface\": \"intuitive for marine biologists, fishermen, and policymakers\",\n            \"decisions\": \"resource management, shipping routes, conservation efforts\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response accurately represents the user's instruction, capturing the model's complexity, requirements on pre-training, forecasting resolution, and the additional challenges like marine ecosystem detection and real-time visualization. The high expertise level of the user has been accounted for in the expert's description. The exact performance metric values are left undefined as they depend on the model's training and validation results.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager of a groundbreaking machine learning initiative, your team is tasked with creating an innovative text analysis tool. This cutting-edge system must not only refine the cutting-edge Cogprompt dataset by integrating 25 nuanced and rarely explored sub-genres, but it must also devise a unique multi-label taxonomy encompassing 15 exclusive and semantically complex themes. The challenge lies in optimizing not just overall accuracy, but in uncovering hidden patterns within latent labels, particularly for those with sparse data. The custom deep learning architecture should be designed with a focus on zero-shot transfer learning, enhancing adaptability without sacrificing performance. Your team must devise an adaptive evaluation framework that computes hierarchical precision, intra-label consistency, and a novel differential privacy-aware metric, while operating within stringent constraints on computational power and carbon footprint, aiming for sustainable AI advancements.", "resposne": "{\n  \"user\": {\n    \"intent\": \"build\",\n    \"expertise\": \"high\"\n  },\n  \"problem\": {\n    \"area\": \"natural language processing\",\n    \"downstream_task\": \"text classification\",\n    \"application_domain\": \"cross-disciplinary\",\n    \"description\": \"The project aims to create an innovative text analysis tool using the Cogprompt dataset, integrating 25 sub-genres and developing a multi-label taxonomy with 15 complex themes. The focus is on zero-shot transfer learning, uncovering hidden patterns in sparse data, and designing a custom deep learning architecture for high adaptability and precision. An adaptive evaluation framework is needed that includes hierarchical precision, intra-label consistency, and a privacy-aware metric, while maintaining computational efficiency and a low carbon footprint.\",\n    \"performance_metrics\": [\n      {\n        \"name\": \"overall accuracy\",\n        \"value\": null\n      },\n      {\n        \"name\": \"hierarchical precision\",\n        \"value\": null\n      },\n      {\n        \"name\": \"intra-label consistency\",\n        \"value\": null\n      },\n      {\n        \"name\": \"differential privacy-aware metric\",\n        \"value\": null\n      }\n    ],\n    \"complexity_metrics\": [\n      {\n        \"name\": \"computational power\",\n        \"value\": null,\n        \"unit\": \"FLOPs\"\n      },\n      {\n        \"name\": \"carbon footprint\",\n        \"value\": null,\n        \"unit\": \"kgCO2e\"\n      }\n    ]\n  },\n  \"dataset\": [\n    {\n      \"name\": \"Cogprompt dataset\",\n      \"modality\": [\"text\"],\n      \"target_variables\": [\"multi-label taxonomy\"],\n      \"specification\": null,\n      \"description\": \"A dataset that requires integration with 25 sub-genres and development of a unique multi-label taxonomy with 15 complex themes.\",\n      \"preprocessing\": [],\n      \"augmentation\": [],\n      \"visualization\": [],\n      \"source\": \"infer-search\"\n    }\n  ],\n  \"model\": [\n    {\n      \"name\": \"Custom Deep Learning Architecture\",\n      \"family\": \"Neural Networks\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"zero-shot transfer learning\": true,\n        \"adaptability\": \"enhanced\",\n        \"performance optimization\": true\n      },\n      \"description\": \"A model designed for zero-shot transfer learning, emphasizing adaptability and performance while maintaining privacy.\"\n    }\n  ],\n  \"knowledge\": [\n    \"Challenge: Uncovering hidden patterns in sparse data and sustainable AI advancements.\"\n  ],\n  \"service\": {\n    \"feature_engineering\": [],\n    \"target_device\": \"cloud\",\n    \"deployment_endpoint\": \"\",\n    \"inference_engine\": [],\n    \"demonstration\": \"Hierarchical and privacy-aware evaluation framework for the text analysis tool.\"\n  },\n  \"confidence\": {\n    \"rationale\": \"The JSON accurately represents the project's requirements, including the unique challenges and evaluation metrics. Expertise is assumed to be high due to the complexity of the task. Missing values are placeholders, as specific performance targets are not provided.\",\n    \"score\": 0.9\n  }\n}\n"}
{"task": "image classification", "instruction": "Design a cutting-edge image classification system that necessitates pinpoint accuracy in differentiating between at least 50 exclusive feline sub-species, 70 unique dog breeds, and encompasses challenges like variable lighting conditions, occlusions, and scale variations. Implement a fusion of SOTA ResNet and Inception models, fine-tune using ImageNet pre-trained weights, and incorporate contrastive learning for enhanced breed distinction in a real-world scenario.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"wildlife biology or pet identification\",\n        \"description\": \"The goal is to design a highly accurate image classification system capable of differentiating between at least 50 feline sub-species, 70 dog breeds, and address challenges like variable lighting, occlusions, and scale variations. A fusion of state-of-the-art ResNet and Inception models is required, along with ImageNet pre-training and contrastive learning for enhanced breed distinction in real-world scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity (feline species)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity (dog breeds)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Custom Feline and Dog Breeds Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"feline sub-species\", \"dog breed\"],\n            \"specification\": {\n                \"number_of_images\": null,\n                \"diversity_labels\": {\n                    \"feline_subspecies\": {\"count\": 50},\n                    \"dog_breeds\": {\"count\": 70}\n                },\n                \"challenges\": [\"variable lighting\", \"occlusions\", \"scale variations\"]\n            },\n            \"description\": \"A custom dataset with images of feline sub-species and dog breeds that account for real-world challenges.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"data augmentation techniques for varying lighting, occlusions, and scale\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"ImageNet\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"common concepts in ImageNet\"],\n            \"specification\": null,\n            \"description\": \"A large-scale pre-trained dataset used for pre-training the ResNet and Inception models.\",\n            \"preprocessing\": [\"fine-tuning using ImageNet weights\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SOTA ResNet and Inception Fusion Model\",\n            \"family\": \"Deep Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"fusion of ResNet and Inception models\",\n                \"pre-training\": \"ImageNet pre-trained\"\n            },\n            \"description\": \"A cutting-edge model combining ResNet and Inception for precise image classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art techniques: contrastive learning will be applied to enhance breed distinction.\",\n        \"Fine-tuning and transfer learning with ImageNet weights will improve performance on specific sub-species and breeds.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A live demo showcasing the system's performance in realistic scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response is structured according to the given schema, capturing the core requirements of a complex image classification task. It addresses fine-tuning with ImageNet weights, the use of SOTA models, and the incorporation of contrastive learning. Challenges and the target breed count are also reflected in the dataset specifications.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for our next-generation maritime surveillance system, I need a multi-disciplinary team to develop a highly-optimized DeepLabv3+ variant. The model should not only detect and classify an extensive fleet of vessels with intricate details (considering hull length, width, navigation patterns, and complex sail configurations) but also interpret real-time adjustments for weather patterns and unusual maneuvers. Targeting at least 98% accuracy with a stringent evaluation protocol, the model must demonstrate resilience in low-visibility conditions and crowded maritime scenarios. In addition, design an efficient deployment strategy using high-performance cloud infrastructure, ensuring GPU utilization, low latency, and thorough testing of robustness through simulated extreme events. Please provide a comprehensive plan for continuous monitoring, fine-tuning, and performance benchmarking.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"maritime surveillance\",\n        \"description\": \"The task is to develop a highly-optimized DeepLabv3+ variant for maritime surveillance. The model should detect and classify vessels, considering factors like hull dimensions, navigation patterns, complex sail configurations, and adapt to real-time weather and unusual maneuvers. The model should achieve at least 98% accuracy, maintain resilience in low-visibility and crowded conditions, and be deployed on high-performance cloud infrastructure.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"GPU utilization\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"inference latency\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"robustness testing\",\n                \"value\": {\n                    \"description\": \"extreme events\"\n                },\n                \"unit\": \"simulated\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Maritime Vessel Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vessel type\", \"hull length\", \"width\", \"navigation patterns\", \"complex sail configurations\"],\n            \"specification\": null,\n            \"description\": \"A dataset for training and testing the DeepLabv3+ model with diverse vessel types and challenging conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepLabv3+\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"complexity\": \"highly-optimized\"\n            },\n            \"description\": \"A variant of DeepLabv3+ designed for real-time maritime surveillance with enhanced performance and adaptability.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"fine-tuning strategy\",\n            \"performance benchmarking\"\n        ],\n        \"target_device\": \"high-performance cloud infrastructure\",\n        \"deployment_endpoint\": {\n            \"cloud provider\": \"undetermined\",\n            \"GPU-optimized\": true\n        },\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"real-time monitoring and simulated extreme events\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's request for a maritime surveillance system, the DeepLabv3+ optimization, and the performance and deployment requirements. Key elements like accuracy, complexity metrics, and the need for continuous monitoring are included. The missing values in specification are meant to be filled based on the team's expertise or specific technical decisions.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a specialized deep learning team, enhance the real-time influenza-like illness (ILI) time-series forecasting task by incorporating advanced decomposition techniques (e.g., TBATS), addressing weekly and yearly patterns, and detecting sudden spikes. Mandate the model to surpass benchmark efficiency with an efficient algorithm like ARIMA-Fusion, using a rolling 36-week window of 7-dimensional data. The team must optimize for both mean squared error (MSE) and mean absolute error (MAE), while ensuring near real-time predictions with stringent latency requirements of <5 minutes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"public health\",\n        \"description\": \"The project aims to enhance real-time influenza-like illness (ILI) forecasting by using advanced decomposition techniques such as TBATS, capturing weekly and yearly patterns, and detecting sudden spikes. The model should surpass benchmark efficiency with ARIMA-Fusion and operate on a rolling 36-week window of 7-dimensional data. Key metrics include mean squared error (MSE), mean absolute error (MAE), and near real-time predictions with a latency requirement of less than 5 minutes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 5,\n                \"unit\": \"minutes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Influenza-like Illness Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"ILI incidence\"],\n            \"specification\": {\n                \"window_size\": 36,\n                \"dimensionality\": 7\n            },\n            \"description\": \"A real-time dataset with 7-dimensional data (potentially including indicators such as temperature, humidity, etc.) collected over a 36-week rolling window for influenza-like illness forecasting.\",\n            \"preprocessing\": [\"TBATS decomposition\", \"seasonality adjustment\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ARIMA-Fusion\",\n            \"family\": \"statistical models\",\n            \"type\": \"time-series\",\n            \"specification\": {\n                \"algorithm\": \"ARIMA-Fusion\",\n                \"model_efficiency\": \"benchmark surpassing\"\n            },\n            \"description\": \"A model using advanced decomposition techniques and designed for efficient real-time forecasting of ILI with latency requirements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"rolling window of 7-dimensional data\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time predictions with <5 minute latency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response adheres to the given schema, representing a high expertise-level project manager, focusing on the enhancement of an ILI forecasting task with advanced techniques. The metrics (MSE, MAE), decomposition, and latency requirements are accurately specified. The model choice of ARIMA-Fusion and its specifications are in line with the requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "Instruct the ML team to architect a cutting-edge, gradient boosting-based tabular regressive model for the specialized Rock Hardness dataset, focusing on predicting petrographic hardness. The objective is to achieve a superior root mean squared error (RMSE) with <5% degradation on low and high scale ranges, while accounting for extreme outliers in each set. Additionally, mandate a feature selection process that reveals the impact of rare, domain-specific chemical composition on model performance. Ensure a clear, user-friendly model explanation report is produced.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"geology\",\n        \"description\": \"The task is to build a gradient boosting-based tabular regression model for the Rock Hardness dataset to predict petrographic hardness. The model should aim for an RMSE with a <5% degradation on both low and high scale ranges, accounting for extreme outliers. A feature selection process is required that highlights the influence of domain-specific, rare chemical compositions on performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared error\",\n                \"value\": null,\n                \"range_constraint\": {\n                    \"low\": \"5%\",\n                    \"high\": \"5%\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"outlier handling\",\n                \"value\": null,\n                \"unit\": \"extreme outliers\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Rock Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"petrographic hardness\"],\n            \"specification\": {\n                \"scale_range\": {\n                    \"low\": \"low\",\n                    \"high\": \"high\"\n                },\n                \"extreme_outliers\": true\n            },\n            \"description\": \"A specialized dataset for predicting rock hardness, including rare, domain-specific chemical compositions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"gradient boosting model\",\n            \"family\": \"ensemble models\",\n            \"type\": \"machine learning\",\n            \"specification\": {\n                \"extreme_outliers_handling\": true\n            },\n            \"description\": \"A cutting-edge model utilizing gradient boosting for tabular regression, focusing on petrographic hardness prediction while accounting for chemical composition and extreme outliers.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"impact of rare chemical composition\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"report_type\": \"user-friendly\",\n            \"explanatory\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response adheres to the schema, detailing a high-expertise-level user's intent to build a state-of-the-art model with specific performance and complexity requirements for the given task. Feature selection for rare chemical composition is mentioned, along with the focus on a clear and user-friendly explanation report.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for an elite deep-sea marine biodiversity exploration team, we seek a cutting-edge underwater segmentation neural network that combines the capabilities of GhostNet and StarNet for the Ultra-Complex Marine Biodiversity (UCMB) dataset. This model must accurately identify not only the standard 50 taxonomic categories but also discover novel species with 98% Intersection over Union (IoU) on highly contrasting and low-visibility images. It should run smoothly on a resource-constrained, embedded edge device with a power budget of 1 watt, achieving real-time performance with inference times below 5 milliseconds. Furthermore, design a comprehensive performance evaluation strategy to test the network's robustness under varying water conditions and adaptability to hardware constraints. Don't forget to include a plan for integrating transfer learning and fine-tuning with limited annotated data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"marine biology\",\n        \"description\": \"The project aims to develop a deep-sea underwater segmentation model using GhostNet and StarNet, targeting the Ultra-Complex Marine Biodiversity dataset. It must accurately classify 50 standard taxonomic categories and identify novel species with a 98% IoU, handle low-visibility and highly contrasting images. The model must run in real-time on an embedded, resource-constrained edge device with a 1-watt power budget and an inference time limit of 5 milliseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Intersection over Union\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 5,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ultra-Complex Marine Biodiversity (UCMB) Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"real-world conditions\": [\"low-visibility\", \"high contrast\"]\n            },\n            \"description\": \"A dataset for marine biodiversity exploration with diverse and challenging underwater imagery.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"data augmentation for low-visibility and contrast conditions\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GhostNet + StarNet Fusion\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_details\": [\"GhostNet\", \"StarNet\"],\n                \"target_device\": \"embedded edge device\",\n                \"power_consumption\": 1,\n                \"inference_speed\": \"real-time (5ms or less)\"\n            },\n            \"description\": \"A cutting-edge combination of GhostNet and StarNet for underwater segmentation\"\n        }\n    ],\n    \"knowledge\": [\n        \"Limited annotated data and use of transfer learning and fine-tuning strategies\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"embedded edge device\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for low-power embedded environment\"],\n        \"demonstration\": \"Plan for evaluating robustness under varying water conditions and hardware constraints\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the project requirements, including the use of GhostNet and StarNet, the target performance metrics, and the constraints on the model's deployment. The limited annotated data and the need for transfer learning and fine-tuning are also captured. The provided rationale offers a confidence score based on the structure and content.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "The project manager needs to guide the ML team to enhance the fashion image classifier by adopting a novel capsule network architecture for simultaneous recognition and feature extraction. They must develop a system that not only achieves a peak accuracy of 97% on the Fashion-MNIST dataset but also demonstrates superior resistance to affine transformations. Furthermore, the team is required to design an extensive evaluation, including a computational efficiency comparison with conventional CNNs, and demonstrate the capsule network's improved rotation invariance through detailed visualizations and quantitative metrics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"The project is about enhancing a fashion image classifier using a capsule network architecture. The team must achieve 97% accuracy on the Fashion-MNIST dataset and show resistance to affine transformations. A comprehensive evaluation is required, comparing computational efficiency with CNNs and demonstrating improved rotation invariance through visualizations and quantitative metrics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"rotation invariance\",\n                \"value\": null,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"fashion item\"],\n            \"specification\": {\n                \"dimension\": {\n                    \"height\": 28,\n                    \"width\": 28\n                },\n                \"number_of_classes\": 10\n            },\n            \"description\": \"A fashion image dataset used for classification, with a focus on achieving 97% accuracy.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"affine transformations\"],\n            \"visualization\": [\"rotation invariance visualizations\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Capsule Network\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"novel capsule network\",\n                \"resistance_to_transformations\": \"affine\"\n            },\n            \"description\": \"A system designed to enhance fashion image classification with high accuracy and improved invariance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"rotation invariance and computational efficiency comparison\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's requirements, specifying the intent to build a new model using a capsule network. The target dataset, Fashion-MNIST, is included with the specified accuracy goal. Performance metrics and enhanced resistance to affine transformations are also incorporated. The comparison with CNNs and visual demonstrations are noted in the 'service' and 'description' sections. The high expertise level of the user is acknowledged.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As the project manager for the deep time-series analysis team, your objective is to enhance the UWaveGestureLibrary project by focusing on complex gesture recognition. The team must develop a novel LSTM model with advanced imputation techniques (like KNN or autoencoders) to handle missing data in real-world scenarios. Models must be adaptable to variable-length sequences (315-500 timestamps) with an emphasis on efficiency, achieving at least 95% accuracy while incorporating SHAP (SHapley Additive exPlanations) for feature importance analysis, ensuring model interpretability without compromising performance. Additionally, the model must undergo rigorous testing on the provided dataset and cross-validation protocols.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"gesture recognition\",\n        \"description\": \"The objective is to enhance the UWaveGestureLibrary project by developing a novel LSTM model with advanced imputation techniques like KNN or autoencoders for complex gesture recognition. The model should handle variable-length sequences (315-500 timestamps), be efficient, achieve at least 95% accuracy, and include SHAP for feature importance analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UWaveGestureLibrary\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"gesture classification\"],\n            \"specification\": {\n                \"min_length\": 315,\n                \"max_length\": 500,\n                \"real-world_scenarios\": \"yes\"\n            },\n            \"description\": \"A dataset for complex gesture recognition with missing data, variable lengths, and real-world scenarios.\",\n            \"preprocessing\": [\n                {\n                    \"method\": \"imputation\",\n                    \"technique\": [\"KNN\", \"autoencoders\"]\n                },\n                {\n                    \"method\": \"sequence_length_handling\"\n                }\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"LSTM\",\n            \"family\": \"recurrent neural network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptability\": \"variable-length sequences\",\n                \"efficiency\": true,\n                \"interpretability_features\": [\"SHAP\"]\n            },\n            \"description\": \"A novel LSTM model with advanced imputation techniques for complex gesture recognition, targeting variable-length sequences and interpretability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"cross-validation testing\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON representation covers the main aspects of the instruction, such as building a novel LSTM model with advanced imputation techniques for variable-length sequences, SHAP feature importance, and accuracy requirements. The UWaveGestureLibrary project context, dataset details, and model adaptation are included. The accuracy score is assumed and could be filled in later with actual testing results.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series classification", "instruction": "Create a cutting-edge hybrid architecture for time-series analysis, combining deep multiple instance learning (MIL) with a novel attention-based Transformer model, Fused Temporal Convolutional Networks (FTCN), and a stacked Proximal Policy Reinforcement Learning (PPRL) module, specifically tailored for the highly discriminative Ancient Inscriptions Time Series dataset. This model should exhibit unparalleled accuracy, balancing inter-inscriptional similarities and differences, while maintaining efficiency in classifying different writing styles and eras. Additionally, conduct extensive experimentation with cross-media evaluations, considering temperature variations, degradation, and artifact noises to ensure the model's adaptability for historical artifact deciphering in real-world archeological discoveries.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"history, archaeology\",\n        \"description\": \"The project involves developing a cutting-edge hybrid model for time-series analysis, combining Deep Multiple Instance Learning (MIL), an attention-based Transformer model (Fused Temporal Convolutional Networks, FTCN), and a Stacked Proximal Policy Reinforcement Learning (PPRL) module. The model should be tailored for the Ancient Inscriptions Time Series dataset, achieving high accuracy in classifying writing styles and eras, considering inter-inscriptional similarities, differences, efficiency, and real-world adaptability to temperature variations, degradation, and artifact noises.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ancient Inscriptions Time Series Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"writing style\", \"era\"],\n            \"specification\": {\n                \"length\": null,\n                \"dimension\": null,\n                \"size\": null\n            },\n            \"description\": \"A dataset focusing on ancient inscriptions, featuring time-series data for classifying writing styles and eras, with consideration for real-world conditions like temperature variations, degradation, and artifact noises.\",\n            \"preprocessing\": [\"normalization\", \"noise filtering\"],\n            \"augmentation\": [],\n            \"visualization\": [\"temporal patterns\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Architecture (Deep MIL + FTCN + PPRL)\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": {\n                    \"MIL layers\": null,\n                    \"Transformer blocks\": null,\n                    \"PPRL layers\": null\n                },\n                \"computational efficiency\": {\n                    \"efficiency score\": null\n                }\n            },\n            \"description\": \"A high-performance model combining MIL, FTCN, and PPRL, designed for the Ancient Inscriptions Time Series dataset and optimized for high accuracy and adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"ancient writing systems\",\n            \"description\": \"Understanding of ancient inscriptions' unique features and historical context is crucial.\"\n        },\n        {\n            \"type\": \"cross-media evaluation techniques\",\n            \"description\": \"Exploiting techniques to assess model performance on diverse media conditions.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"artifact deciphering platform\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"model performance on unseen real-world archeological discoveries\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the given instruction, capturing the high-level requirements for a complex machine learning project tailored for the specified dataset, architecture, and evaluation factors. The user's high expertise level is also acknowledged. Some specific details, such as model parameters and exact efficiency metrics, have been intentionally left unspecified for flexibility.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "Design a highly-robust, real-time machine learning system that combines multiple neural networks for detecting intricate, multidimensional deepfakes within authentic antique legal archives, specifically leveraging the LegDocForgery resource. The fusion model must necessitate sequential analysis of text, including deep watermark detection and subtextual microprint recognition, in tandem with sophisticated image processing techniques for subtle anomaly identification. In addition, the system must strive to surpass an impressive benchmark of 97% AUC-ROC, while simultaneously implementing an intuitive, user-friendly interface for non-technical individuals to decipher detection results in real-time with visual explanations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"legal archives\",\n        \"description\": \"The project aims to design a real-time machine learning system for detecting intricate, multidimensional deepfakes within authentic antique legal archives, using the LegDocForgery resource. The system requires the fusion of multiple neural networks, combining sequential text analysis (watermark detection and subtextual microprint recognition) with sophisticated image processing for anomaly identification. The system should achieve a high benchmark of 97% AUC-ROC and provide an intuitive user interface for non-technical users.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"seconds per sample\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LegDocForgery\",\n            \"modality\": [\n                \"image\",\n                \"text\"\n            ],\n            \"target_variables\": [\" Forgery Detection\"],\n            \"specification\": {\n                \"image_dim\": null,\n                \"text_length\": null,\n                \"anomaly_types\": [\n                    \"multidimensional deepfakes\"\n                ]\n            },\n            \"description\": \"Dataset containing authentic and deepfake antique legal archive data, involving both images and textual content for analysis.\",\n            \"preprocessing\": [\n                \"text normalization\",\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\n                \"synthetic forgery generation\"\n            ],\n            \"visualization\": [\n                \"anomaly heatmaps, confusion matrices\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Fusion Model\",\n            \"family\": \"Multi-modal Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Sequential analysis with text & image components\",\n                \"complex_networks\": [\"watermark detection\", \"microprint recognition\", \"image anomaly detection\"],\n                \"train_speed\": null,\n                \"inference_speed\": null\n            },\n            \"description\": \"A high-performance system using multiple neural networks for seamless integration of text and image analysis to detect deepfakes.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"subtask1\": \"Deep learning methods for detecting subtextual patterns in text\",\n            \"subtask2\": \"Transfer learning with watermark and microprint detection models\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud-based for real-time performance\",\n        \"deployment_endpoint\": \"secure archival forgery detection platform\",\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"Interactive interface with visual explanations for detection results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the schema and reflects the complex requirements of the project. It includes details such as the need for high expertise, fusion model architecture, performance target, and user-friendly interface. The LegDocForgery resource, modalities, and preprocessing steps are accurately incorporated.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a retail project manager, mandate the development of a cutting-edge computer vision system for precise and fast real-time recognition of niche fashion items (e.g., vintage couture, sustainable fabrics), necessitating the evaluation of tailored datasets like Eco-Fashion20K and specialized models like YOLOv5-tuned for fashion, while optimizing memory footprint and latency for hyper-scalable deployment in high-traffic online stores.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"fashion and retail\",\n        \"description\": \"The project requires a computer vision system for real-time recognition of niche fashion items like vintage couture and sustainable fabrics, with a focus on precision, speed, and specialized datasets like Eco-Fashion20K.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Eco-Fashion20K\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"length\": null\n            },\n            \"description\": \"A specialized dataset tailored for recognizing niche fashion items, such as vintage couture and sustainable fabrics.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5-tuned for fashion\",\n            \"family\": \"Object Detection\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"number of parameters\": null,\n                \"FLOPs\": null,\n                \"train speed\": null,\n                \"inference time\": null\n            },\n            \"description\": \"A state-of-the-art model designed specifically for fashion item detection, ensuring high precision and recall in real-time.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-traffic online stores\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"optimized for hyper-scalable deployment\"\n        ],\n        \"demonstration\": \"real-time, high-traffic environments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the requirements mentioned in the instruction, such as the high-level intent, the need for a specialized model, and the focus on memory footprint and latency. Performance metrics and dataset details have been left open for specific evaluation. The level of expertise is set to 'high' due to the complexity of the task. The confidence score is not provided, but it should reflect a high confidence level considering the relevance of the provided information.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "link prediction", "instruction": "Could a zero-shot machine learning model successfully discern the onset of collaborative research within the highly esoteric realm of extremophile microbiology, given a sparse labeled dataset like the E. coli co-authorship network? Elucidate a multi-step strategy that involves feature engineering, transfer learning, and cross-domain validation to enhance prediction accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"microbiology\",\n        \"description\": \"The user is interested in discerning the onset of collaborative research within extremophile microbiology using a zero-shot learning approach. The challenge lies in dealing with a sparse labeled dataset, such as the E. coli co-authorship network.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"E. coli Co-Authorship Network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"collaborative research onset\"],\n            \"specification\": {\n                \"sparsity\": null,\n                \"node_count\": null,\n                \"edge_count\": null\n            },\n            \"description\": \"A sparse labeled dataset of E. coli co-authorship connections, reflecting potential collaborative research ties.\",\n            \"preprocessing\": [\n                \"feature extraction from co-authorship patterns\",\n                \"handling extreme class imbalance\"\n            ],\n            \"augmentation\": [\n                \"synthetic data generation using graph sampling techniques\"\n            ],\n            \"visualization\": [\n                \"network analysis to understand patterns\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Zero-Shot Learning Model\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer learning model\": \"adapted from a general graph network\",\n                \"model architecture\": \"incorporating graph convolutional layers and attention mechanisms\"\n            },\n            \"description\": \"A zero-shot model designed to handle the unique patterns in extremophile microbiology collaboration based on feature engineering and transfer learning from a broader domain.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning is crucial due to limited labeled data.\",\n        \"Domain adaptation techniques may enhance cross-domain performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"node embeddings for node similarity\",\n            \"contextual feature extraction from co-authorship metadata\"\n        ],\n        \"target_device\": \"cloud or high-performance computing environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated libraries like TensorFlow or PyTorch\"],\n        \"demonstration\": \"cross-validation results and interpretable model outputs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's interest in building a zero-shot model for extremophile microbiology's co-authorship network. Key concepts, such as feature engineering, transfer learning, and cross-domain validation, are represented. Performance metrics are left open for actual evaluation. The model is specified based on the user's indication of needing a high level of expertise for this task. Some potential challenges, like data scarcity and domain adaptation, are acknowledged.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "text classification", "instruction": "As a project manager for a high-performance ML team, devise a state-of-the-art text classification system using the 20 Newsgroups dataset. Demand not only peak accuracy but also real-time adaptability through incremental learning, with constraints on computational footprint. Strictly enforce a requirement for efficient and hands-off feature updating, gauging the trade-off between training duration and human supervision in a logarithmic scale.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"The project involves developing a state-of-the-art text classification system, targeting the 20 Newsgroups dataset. Key requirements include high accuracy, real-time adaptability through incremental learning, and minimal computational footprint. Additionally, the system must support efficient, hands-off feature updating with a logarithmic trade-off between training duration and human supervision.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational footprint\",\n                \"value\": null,\n                \"unit\": \"FLOPs or CPU/GPU cycles\"\n            },\n            {\n                \"name\": \"training time scalability\",\n                \"value\": \"logarithmic\",\n                \"unit\": \"training duration\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"category\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensionality\": null,\n                \"length\": null\n            },\n            \"description\": \"A large-scale text classification dataset for benchmarking performance on various news categories.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": null,\n                \"incremental_learning_capability\": true\n            },\n            \"description\": \"A machine learning system designed for high-accuracy text classification, with emphasis on real-time adaptability and computational efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"We will investigate recent research in efficient text classification and look for methods that balance accuracy and real-time adaptation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A demonstration highlighting the real-time adaptability and low human supervision levels while maintaining accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's instruction, including the need for a state-of-the-art text classification system, requirements for the 20 Newsgroups dataset, and the emphasis on accuracy, real-time adaptability, and computational efficiency. The logarithmic trade-off in feature updating is appropriately represented. However, specific model names or performance targets are left open for flexibility.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "community detection", "instruction": "For the upcoming financial services project, in addition to implementing state-of-the-art community detection techniques on a massive and complex dataset combining historical transactions, demographic profiles, and behavioral patterns, the team must design a privacy-preserving algorithm that adheres to strict regulatory guidelines, preserving client confidentiality while extracting actionable insights for personalized investment strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"financial services\",\n        \"description\": \"The task involves implementing state-of-the-art community detection techniques on a large and complex dataset encompassing historical transactions, demographic profiles, and behavioral patterns. Privacy preservation is crucial, adhering to strict regulatory guidelines and ensuring client confidentiality while extracting insights for personalized investment strategies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"privacy preservation\",\n                \"value\": null,\n                \"unit\": \"regulatory guidelines\"\n            },\n            {\n                \"name\": \"time efficiency for data processing\",\n                \"value\": null,\n                \"unit\": \"seconds/transaction\"\n            },\n            {\n                \"name\": \"space efficiency for storing sensitive information\",\n                \"value\": null,\n                \"unit\": \"MB/user\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Combined Financial Services Dataset\",\n            \"modality\": [\"graph\", \"tabular\"],\n            \"target_variables\": [\"communities\", \"insights\"],\n            \"specification\": {\n                \"transactions_count\": null,\n                \"nodes_count\": null,\n                \"edges_count\": null,\n                \"dimensionality\": null,\n                \"privacy_features\": [\"anonymized\", \"encrypted\"]\n            },\n            \"description\": \"A massive dataset containing historical transactions, demographic profiles, and behavioral patterns, requiring privacy-preserving measures.\",\n            \"preprocessing\": [\"anonymization\", \"data cleaning\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"privacy impact analysis\",\n                \"community structure visualization\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Privacy-Preserving Community Detection Model\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"algorithm\": \"state-of-the-art\",\n                \"privacy_method\": null,\n                \"inference_speed\": null,\n                \"data_security\": \"compliant with regulations\"\n            },\n            \"description\": \"An advanced model designed for community detection while preserving client privacy, ensuring regulatory compliance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"synonym detection\", \"risk assessment\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"secure inference\", \"auditable\"],\n        \"demonstration\": \"client-confidentiality-preserving interactive dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a solution and reflects the high expertise level. The project's specific details about the task, privacy preservation, and application domain are included, along with relevant performance and complexity metrics. The dataset source and preprocessing steps align with the given scenario. The model description and service requirements adhere to the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Transform the existing time-series forecasting project for our app's daily active users by integrating a cutting-edge, hybrid methodology that combines Prophet and ARIMA, focusing on enhancing explainability. Enhance accuracy specifically for the trailing 18 months, tackle intricate weekly and monthly oscillations, and mandate a rolling validation approach with a moving window of six weeks to ensure the model's resilience against short-term anomalies and seasonality.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"software/applications\",\n        \"description\": \"The task is to enhance the existing time-series forecasting model for daily active users in the app by integrating a hybrid methodology of Prophet and ARIMA, prioritizing explainability. The objective is to improve accuracy for the past 18 months, address complex weekly and monthly patterns, and implement rolling validation using a six-week moving window to handle anomalies and seasonality.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"rolling validation\",\n                \"value\": null,\n                \"unit\": \"weeks\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"App Daily Active Users\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"daily active users\"],\n            \"specification\": null,\n            \"description\": \"The dataset containing historical daily active users data for the app, requiring integration with the Prophet and ARIMA hybrid model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Prophet-ARIMA hybrid\",\n            \"family\": \"Hybrid Time Series Forecasting\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A proposed model that combines Prophet and ARIMA to improve forecasting accuracy, enhance explainability, and handle weekly and monthly patterns, especially for the trailing 18 months.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"hybrid approach, explainability focus\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"explainable forecasts for the trailing 18 months\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build on an existing time-series project, specifying a high expertise level. The problem description includes key details about the hybrid method, accuracy enhancement, and rolling validation. Performance metrics and complexity metrics reflect the requirements. The missing values for performance and complexity are placeholders and can be filled later based on model results and design decisions.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As an elite AI project manager, your team must tackle a multi-dimensional text classification expedition: Curate a comprehensive dataset from exclusive quantum cryptography symposiums, containing intricate proofs of quantum encryption algorithms, adaptive security protocols, and fine-grained assessments amidst adversarial environments. Each entry is rated on a complex 10-point scale, reflecting novelty, security breakthroughs, and technical finesse, weighed by a sophisticated harmonic mean score (HMS) variant. Design a leading-edge model that deciphers these esoteric concepts and discerns subtle rating nuances, targeting a WHMS of at least 0.9. Additionally, devise a cutting-edge interpretability strategy that bridges the gap between model decisions and the underlying quantum cryptographic principles, ensuring transparency without compromising on performance. Remember to justify the choice of interpretability methods using domain-specific analogies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"quantum cryptography\",\n        \"description\": \"The task involves curating a comprehensive dataset from quantum cryptography symposiums with entries on encryption algorithms, adaptive security protocols, and adversarial assessments rated on a 10-point scale, using a sophisticated harmonic mean score (HMS) variant. The objective is to build a model with a WHMS of at least 0.9 and incorporate a cutting-edge interpretability strategy that aligns with quantum cryptographic principles.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"WHMS\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Quantum Cryptography Symposium Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"novelty\", \"security\", \"technical finesse\"],\n            \"specification\": {\n                \"data_source\": \"exclusive symposiums\",\n                \"rating_scale\": \"10-point\",\n                \"harmonic_mean_score\": \"HMS variant\"\n            },\n            \"description\": \"Dataset comprising sophisticated text entries from quantum cryptography symposiums with ratings on a complex harmonic mean scale.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability_method\": \"customized for quantum cryptography\"\n            },\n            \"description\": \"A model designed for text classification, targeting WHMS of at least 0.9, and with a focus on interpretability methods that bridge model decisions to quantum cryptographic concepts.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Domain-specific analogy: The interpretability strategy can be likened to a quantum key exchange mechanism, where the model's internal decision path is like the quantum state that can be measured and understood by domain experts, ensuring the decryption of model logic in the context of quantum cryptography.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"rationale\": \"The model will be demonstrated by showing how it classifies texts and interprets its decisions using quantum cryptographic analogies.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the complex nature of the task, capturing the core objectives of curating a high-level dataset, building a top-performing model, and maintaining interpretability in a quantum cryptography context. The choice of 'high' expertise for the user indicates a deep understanding of the requirements, while the custom interpretability method and domain analogy ensure the suitability of the model for the task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As an experienced project manager overseeing a cutting-edge digital humanities initiative, we need a state-of-the-art deep learning model that not only segments delicate 18th-century watercolor illustrations from fragile folio book pages but also discerns regional artistic styles and textures, while maintaining historical authenticity and minimizing color distortion. Can you devise a sophisticated, custom-built algorithm for this demanding and historically significant archive project?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"digital humanities\",\n        \"description\": \"The project seeks a deep learning model to segment 18th-century watercolor illustrations from folio book pages. The model should also discern regional artistic styles and textures, ensuring historical authenticity and minimizing color distortion.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"segmentation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"regional style recognition precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"regional texture recognition precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"historical authenticity preservation score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"color distortion reduction\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"18th-century Watercolor Illustrations Dataset\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"segmentation mask\",\n                \"regional style labels\",\n                \"texture labels\"\n            ],\n            \"specification\": null,\n            \"description\": \"A dataset containing 18th-century watercolor illustrations with folio book pages for model training, validation, and testing.\",\n            \"preprocessing\": [\n                \"histogram equalization\",\n                \"color normalization\"\n            ],\n            \"augmentation\": [\n                \"random cropping\",\n                \"lighting variations\"\n            ],\n            \"visualization\": [\n                \"region of interest highlighting\",\n                \"style and texture heatmaps\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom-built Deep Learning Algorithm\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\n                    \"encoder-decoder\",\n                    \"attention mechanism\"\n                ],\n                \"architecture specifics\": \"Context-sensitive, with layers tailored for image segmentation and feature extraction\"\n            },\n            \"description\": \"A sophisticated, custom-built deep learning model designed specifically for the task, combining CNNs with advanced techniques for delicate image segmentation and artistic style detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Historical authenticity is maintained by incorporating style and texture features from contemporary watercolor art.\",\n        \"Color distortion is minimized through color-preserving loss functions and post-processing techniques.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for style and texture\"],\n        \"target_device\": \"cloud-based computing\",\n        \"deployment_endpoint\": \"digitized archive platform\",\n        \"inference_engine\": [\"GPU-optimized TensorFlow\"],\n        \"demonstration\": \"Interactive model output showcasing segmentation, style, and texture insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the essential components of the project, including a high-expertise user who requires a custom-built algorithm. The task, area, and performance metrics are defined to meet the specific demands of the project. The dataset, model, and service expectations align with the problem description. However, more granular metrics on historical authenticity and color preservation could be added.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager, devise a novel sentiment analysis system that not only classifies IMDB film reviews into intricate sub-genres like period romantic comedies and futuristic action-dramas but also detect and differentiate nuanced emotions (like sarcasm and irony) within. Mandate an absolute F1 score of 0.97 across varying time periods and necessitate a threshold surpassed by 98% for precision, ensuring consistent accuracy and balance in a heavily-labeled, real-time data stream.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"film analysis\",\n        \"description\": \"The project is to design a sentiment analysis system for IMDB film reviews, focusing on complex sub-genres like period romantic comedies and futuristic action-dramas, and with the capability to detect nuanced emotions like sarcasm and irony. The system requires an F1 score of 0.97 and precision above 98% across varying time periods in a real-time, heavily-labeled data stream.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"IMDB Film Reviews\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\n                \"sub-genres\",\n                \"nuanced emotions\"\n            ],\n            \"specification\": {\n                \"data_distribution\": \"heavily-labeled, real-time\"\n            },\n            \"description\": \"A dataset of IMDB film reviews, labeled with both genre sub-categories and nuanced emotions, including sarcasm and irony.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Sentiment Analysis System\",\n            \"family\": \"Neural Networks (with NLP architecture)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy standards\": \"F1 score >= 0.97, precision >= 0.98\",\n                \"time_period_specificity\": \"varying time periods\"\n            },\n            \"description\": \"A system designed to classify film reviews into specific genres and detect nuanced emotions, meeting high accuracy and precision criteria.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time accuracy and balance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the project's requirements, focusing on the task of building a sentiment analysis system with high sub-genre and emotion detection. Performance metrics (F1 score, precision), dataset properties (heavily-labeled, real-time), and specific model standards (NLP architecture) are included. The confidence score is high due to the thorough representation of the user's instructions.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "Instruct the machine learning team to design an advanced image classifier for the highly complex Galaxy Zoo dataset, utilizing an advanced ResNet and Transformer fusion (ResViT) model. Prioritize exceptional performance in classifying sub-atomic celestial structures, preserving edge sharpness, and dealing with immense variations in light conditions. Ensure the model demonstrates fine-tuning efficiency on exotic celestial phenomena and can generalize across previously unseen nebulae, enhancing the groundbreaking potential for astronomical discoveries.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The task is to design an advanced image classifier for the Galaxy Zoo dataset, using a ResNet and Transformer fusion (ResViT) model. The focus is on classifying sub-atomic celestial structures, maintaining edge sharpness, and handling light condition variations. The model should demonstrate fine-tuning efficiency on unusual celestial phenomena and generalize to unseen nebulae for astronomical discovery enhancement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity (on sub-atomic structures)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"preservation of edge sharpness\",\n                \"value\": null\n            },\n            {\n                \"name\": \"light condition handling capability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            \"fine-tuning efficiency\"\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"sub-atomic structures\", \"light conditions\"],\n            \"specification\": {\n                \"image_size\": null,\n                \"number_of_classes\": null\n            },\n            \"description\": \"A highly complex dataset for advanced image classification, focusing on sub-atomic celestial structures with diverse light conditions and sharpness variations.\",\n            \"preprocessing\": [\n                \"resizing to preserve edge sharpness\",\n                \"light condition normalization\"\n            ],\n            \"augmentation\": [\n                \"random cropping and flipping for sub-atomic structure handling\",\n                \"light condition variations\"\n            ],\n            \"visualization\": [\n                \"model interpretability on exotic celestial phenomena\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResViT Fusion Model\",\n            \"family\": \"Advanced Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ResNet and Transformer fusion\",\n                \"number_of_layers\": null,\n                \"specific_layer_details\": \"focus on sub-atomic and edge structure handling\"\n            },\n            \"description\": \"A model designed to handle complex astronomical images with a focus on ResNet and Transformer fusion for high performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Celestial structures exhibit complex patterns and variations\",\n        \"Fine-tuning on diverse exotic phenomena improves generalization\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"extracting features for sub-atomic structure recognition\",\n            \"light condition-dependent feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow or PyTorch\"],\n        \"demonstration\": \"demonstration of improved classification on unseen nebulae\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's instruction, capturing the advanced image classification task with a specific model, performance metrics, and dataset requirements. The user's high expertise level is accounted for, and the missing details can be filled based on the project requirements. The confidence score is somewhat subjective and assumes a high level of understanding of the task and the requested model's capabilities.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "visual question answering", "instruction": "In the realm of art historical analysis, propose a challenging visual question answering project for a team of skilled machine learning developers. The task is to design an advanced model that harnesses the under-explored \"ArtHeritageVQA\" dataset, which combines Renaissance art masterpieces with intricate, detailed inquiries. The model must decipher not only the depicted scenes and artistic techniques but also decipher symbolism and hidden narratives, while maintaining robustness in low-light and conditionally rendered images. Furthermore, the model must possess the ability to adapt to real-time queries during virtual gallery tours, demonstrating cultural understanding and historical context.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"art historical analysis\",\n        \"description\": \"The project requires a challenging visual question answering model for art historical analysis, focusing on the ArtHeritageVQA dataset. The model should understand depicted scenes, artistic techniques, symbolism, and hidden narratives, maintain robustness in low-light and conditionally rendered images, and be adaptable for real-time queries during virtual gallery tours, demonstrating cultural understanding and historical context.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy in understanding depicted scenes\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretation of artistic techniques\",\n                \"value\": null\n            },\n            {\n                \"name\": \"identification of symbolism and hidden narratives\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness under low-light conditions\",\n                \"value\": null\n            },\n            {\n                \"name\": \"adaptability to real-time queries\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time for real-time queries\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ArtHeritageVQA\",\n            \"modality\": [\"image\", \"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"diversity\": {\n                    \"Renaissance art\": true,\n                    \"intricate inquiries\": true,\n                    \"low-light and conditionally rendered\": true\n                }\n            },\n            \"description\": \"A dataset combining Renaissance art masterpieces with detailed inquiries for visual question answering.\",\n            \"preprocessing\": [\n                \"fine-grained image enhancement\",\n                \"textual question cleaning and normalization\"\n            ],\n            \"augmentation\": [\n                \"rendering variations\",\n                \"low-light condition simulation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks with transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"capable of multi-modal understanding\",\n                \"contextual reasoning\": true,\n                \"adaptation strategy\": \"incremental learning\"\n            },\n            \"description\": \"A cutting-edge model for visual question answering, tailored for art historical analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Domain-specific resources on Renaissance art and cultural history may be helpful.\",\n        \"Model sensitivity to historical context is crucial\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"cultural context embeddings\",\n            \"histogram of artistic styles\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"Adaptive, interactive virtual gallery experience\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the consultation, including high expertise level, project details, and emphasis on the ArtHeritageVQA dataset's unique characteristics. Performance and complexity metrics are suggested based on the project's requirements, and domain-specific knowledge is mentioned. However, specific model names and feature engineering details are placeholders, requiring further collaboration.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "machine translation", "instruction": "Incorporate a novel fusion of Transformer-XL and BERT-Mini architectures for the machine translation project, mandating peak performance on the EuroTechX dataset with a focus on zero-shot cross-lingual understanding. Additionally, demand a lightweight model that surpasses human-level translation speed by 10%, attaining sub-135 milliseconds response time for real-time, low-bandwidth video translation in multiple global markets. Guarantee compatibility with diverse devices, including legacy systems and resource-constrained IoT, while maintaining a carbon footprint reduction goal of 20% compared to traditional translation methods.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"The project involves integrating Transformer-XL and BERT-Mini architectures for high-performance machine translation, focusing on zero-shot cross-lingual understanding. It requires a lightweight model capable of surpassing human-level translation speed by 10% to enable real-time, low-bandwidth video translation for global markets. Compatibility with legacy systems and IoT devices is mandatory, and a 20% carbon footprint reduction compared to traditional methods is the goal.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"peak performance\",\n                \"value\": \"EuroTechX dataset\"\n            },\n            {\n                \"name\": \"translation speed (human-level benchmark)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time (real-time requirements)\",\n                \"value\": 0.135\n            },\n            {\n                \"name\": \"carbon footprint reduction\",\n                \"value\": 0.2\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model size (lightweight constraint)\",\n                \"value\": null,\n                \"unit\": \"parameters\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"EuroTechX dataset\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": null,\n            \"description\": \"A benchmark dataset for zero-shot cross-lingual understanding, suitable for evaluating Transformer-XL and BERT-Mini fusion performance.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL & BERT-Mini fusion\",\n            \"family\": \"Transformer architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-XL and BERT-Mini\",\n                \"model_size\": \"\",\n                \"computational_efficiency\": null,\n                \"inference_speed\": null\n            },\n            \"description\": \"A lightweight model designed for high-performance machine translation and zero-shot cross-lingual understanding, aiming for real-time and low-bandwidth requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced NLP techniques for zero-shot learning\",\n        \"Transformer-XL and BERT-Mini architecture fusion\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"lightweight design\",\n            \"real-time translation optimization\"\n        ],\n        \"target_device\": [\"legacy systems\", \"IoT\"],\n        \"deployment_endpoint\": \"global markets\",\n        \"inference_engine\": [\n            \"optimized for real-time performance\"\n        ],\n        \"demonstration\": \"low-bandwidth video translation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key aspects of the user's instruction: the fusion of specific architectures, performance targets on the EuroTechX dataset, and the requirement for a lightweight model with real-time speed and device compatibility. Performance metrics, complexity constraints, and service requirements are mapped accordingly. However, the precision of the metric values and model specification may need clarification based on the actual model's performance.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As the lead in a specialized ML team, your mandate is to develop an advanced image classifier using Enhanced EfficientNet on the HAM10000 dataset. You must achieve a mind-bending 99.8% accuracy, ensuring top-notch F1 score and true positive/negative rates. Mandate includes incorporating not only SHAP or LIME for explainability but also generating interactive heatmaps for detailed feature importance. Design an optimized model that rivals high-end devices in speed, delivering real-time classification with sub-millisecond latency and minimal power consumption, while maintaining the benchmark performance on high-resolution smartphone hardware. Expert dermatologists demand comprehensive, step-by-step visualizations for each prediction's decision rationale.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medicine, specifically dermatology\",\n        \"description\": \"The task is to develop an advanced image classifier using Enhanced EfficientNet on the HAM10000 dataset, targeting 99.8% accuracy, high F1 score, and true positive/negative rates. The model must incorporate SHAP or LIME for explainability, generate interactive heatmaps for feature importance, and offer real-time classification with sub-millisecond latency and low power consumption compatible with high-resolution smartphone hardware.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.998\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"true positive rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"true negative rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\n                    \"unit\": \"sub-milliseconds\"\n                }\n            },\n            {\n                \"name\": \"power consumption\",\n                \"value\": null,\n                \"unit\": \"minimal\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HAM10000 dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"dermatoscopic images\"],\n            \"specification\": {\n                \"image_size\": \"high-resolution\",\n                \"number_of_classes\": null\n            },\n            \"description\": \"A high-resolution dermatological dataset used for image classification tasks\",\n            \"preprocessing\": [\"SHAP, LIME\"],\n            \"augmentation\": [\"not specified\"],\n            \"visualization\": [\n                \"comprehensive, step-by-step visualizations\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced EfficientNet\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"advanced\",\n                \"optimization\": \"rivals high-end devices in speed\"\n            },\n            \"description\": \"A state-of-the-art image classifier designed for high accuracy and performance, with explainability using SHAP or LIME and interactive heatmaps.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-resolution smartphones\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"optimized for real-time, sub-millisecond latency\"\n        ],\n        \"demonstration\": \"interactive, decision rationale heatmaps\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the advanced image classification task with Enhanced EfficientNet, HAM10000 dataset, and the strict performance requirements. Metrics, complexity metrics, and the need for explainability and visualizations are detailed. The 'expertise' field is set to 'high' to reflect a high level of understanding of the task at hand.\",\n        \"score\": 0.99\n    }\n}\n"}
{"task": "image classification", "instruction": "For a challenging project upgrade, the client has commissioned a cutting-edge, real-time canine breed recognition system that must discern between over 500 breeds with nanosecond-level accuracy. Apply advanced deep learning techniques using Stanford Dogs Dataset, exploring not just convolutional neural networks but also fine-tuning state-of-the-art architectures like ResNeXt and EfficientNet. Investigate the benefits of incorporating transfer learning, adversarial training, and ensemble methods, while optimizing for minimal latency, low-resource deployment on edge devices, and ensuring robustness under challenging lighting variations and environmental factors, all while guaranteeing a seamless and ultra-responsive user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal recognition, possibly pets or security\",\n        \"description\": \"The project is about developing a real-time canine breed recognition system with nanosecond-level accuracy, targeting over 500 breeds. Advanced deep learning techniques, including convolutional neural networks, ResNeXt, and EfficientNet, must be explored. Transfer learning, adversarial training, and ensemble methods are to be investigated. Optimization focuses on minimal latency, low-resource deployment for edge devices, robustness under varying lighting and environmental conditions, and a seamless, ultra-responsive user experience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"nanosecond-level latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"nanoseconds\"\n            },\n            {\n                \"name\": \"resource consumption for edge deployment\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Dogs Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"dog breed\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": null,\n                \"variety\": \"over 500 breeds\"\n            },\n            \"description\": \"A comprehensive dataset for real-time canine breed recognition, featuring diverse lighting and environmental conditions.\",\n            \"preprocessing\": [\n                \"transfer learning\",\n                \"adversarial training\"\n            ],\n            \"augmentation\": [\"addressing lighting and environmental factors\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art (ResNeXt, EfficientNet)\",\n                       \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"deep\", \"fine-tuned\"],\n                \"explored\": [\"ResNeXt\", \"EfficientNet\"],\n                \"optimization\": [\"for latency\", \"low-resource deployment\"]\n            },\n            \"description\": \"Advanced architectures and techniques to achieve nanosecond accuracy, transfer learning, and ensemble methods.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"insight\": \"Incorporating transfer learning can improve performance with less training data.\"\n        },\n        {\n            \"insight\": \"Adversarial training enhances robustness against lighting and environmental variations.\"\n        },\n        {\n            \"insight\": \"Ensemble methods enhance accuracy and can be optimized for minimal latency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"low-resource-friendly\",\n            \"latency-optimized\"\n        ],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for latency\"\n        ],\n        \"demonstration\": \"seamless and ultra-responsive\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the provided schema and captures the essence of the user's requirements. The project's main aspects such as the target dataset, advanced deep learning techniques, and optimization goals are reflected. However, the values for performance and complexity metrics are left as null, which can be filled based on extensive experimentation or consultations with domain experts.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "In your quest to revolutionize the field of scientific research management, you have been tasked with developing a state-of-the-art machine learning system for the arduous task of detecting and differentiating subtle patterns in interdisciplinary research papers. Your team must tackle a highly specialized subset of the \"Quantum Metamaterials and Exoplanet Atmosphere Inductive Textual Reasoning\" (QMEXITR) dataset, which encompasses an astonishing 100,000 complex texts derived from quantum physics, astrochemistry, and subatomic particle interactions. Each document is filled with intricate problems designed to decipher nuanced connections between theories and experimental findings.\r\n\r\nYour objective is to engineer a novel neural architecture that surpasses the current records with a remarkable precision of 99.99% in precision, recall, and F1-score, leveraging the recent Qubit-enhanced Transformer (QTrans) model. However, you must navigate the challenges of dealing with text that exhibits extreme class imbalance in the exoplanet atmosphere category, as well as the need to optimize for both specificity and interpretability. To achieve this, devise a hybrid approach combining QTrans with traditional feature extraction methods, incorporate domain-adaptive fine-tuning on a private corpus of obscure research articles, and rigorously document the impact of each strategy on performance metrics, such as the geometric mean of accuracy and area under the receiver operating characteristic curve (AUC-ROC). Ensure that your final submission not only outperforms the competition but also provides insightful insights into the adaptability and explainability of advanced machine learning techniques in the scientific community.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"scientific research management\",\n        \"description\": \"The task involves developing a machine learning system for detecting and differentiating patterns in interdisciplinary research papers within the QMEXITR dataset. The goal is to surpass the current records with a precision, recall, and F1-score of 99.99% and handle class imbalance and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.9999\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.9999\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9999\n            },\n            {\n                \"name\": \"geometric mean of accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"area under the ROC curve (AUC-ROC)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Quantum Metamaterials and Exoplanet Atmosphere Inductive Textual Reasoning (QMEXITR)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"paper classification\"],\n            \"specification\": {\n                \"size\": 100000,\n                \"complexity\": \"interdisciplinary\",\n                \"categories\": [\"quantum physics\", \"astrochemistry\", \"subatomic particle interactions\"],\n                \"imbalance\": {\n                    \"category\": \"exoplanet atmosphere\",\n                    \"imbalance_ratio\": null\n                }\n            },\n            \"description\": \"Dataset of 100,000 complex texts from quantum physics, astrochemistry, and subatomic interactions, with a focus on subtle connections between theories and findings.\",\n            \"preprocessing\": [\"Qubit-enhanced feature extraction\", \"domain-adaptive fine-tuning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Qubit-enhanced Transformer (QTrans)\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A neural architecture to surpass existing records in text classification\"\n        },\n        {\n            \"name\": \"Hybrid of QTrans + traditional feature extraction\",\n            \"family\": \"\",\n            \"type\": \"\",\n            \"specification\": {\n                \"interpretability\": \"leveraged\"\n            },\n            \"description\": \"Combining QTrans with classic techniques for improved interpretability\"\n        }\n    ],\n    \"knowledge\": [\n        \"To tackle class imbalance, apply domain-adaptive fine-tuning on a private corpus of obscure research articles.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Qubit-enhanced feature extraction\", \"domain-adaptive fine-tuning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretability-aware inference\"],\n        \"demonstration\": \"Performance metrics, insights into adaptability and explainability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the given schema and includes high-level requirements, such as the expert level and details of the task, dataset, and model. Performance metrics and challenges like class imbalance and interpretability are captured. However, specific performance values are not provided and should be filled in based on the target precision, recall, and AUC-ROC values during model development.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "As the project manager, devise a sophisticated Mask R-CNN model for a niche marine biodiversity detection task, which must accurately identify not only 25 diverse marine species (including endangered ones) in underwater photographs but also discern subtle species interactions and environmental conditions like water clarity and substrate types. The model should handle high-resolution fisheye images captured from remotely operated vehicles in dynamic oceanic ecosystems, with a demanding requirement of achieving real-time performance at 45 FPS for real-world deployments. Ensure cross-platform adaptability, sub-100ms latency, and a benchmark of 95% precision under challenging scenarios of underwater camouflage and lighting variations, pushing the boundaries of computational efficiency and scene comprehension in the marine domain.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"marine biology\",\n        \"description\": \"The project aims to develop a sophisticated Mask R-CNN model for marine biodiversity detection in high-resolution fisheye images from ROVs. The model must identify 25 diverse marine species, including endangered ones, and recognize species interactions and environmental conditions. It must achieve real-time performance at 45 FPS for real-world deployment and be cross-platform adaptable with sub-100ms latency. Performance should exceed 95% precision even under challenging conditions like underwater camouflage and lighting variations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 100,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Marine Biodiversity Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species labels\", \"species interactions\", \"environmental conditions\"],\n            \"specification\": {\n                \"image_resolution\": {\n                    \"width\": \"high\",\n                    \"height\": \"high\"\n                },\n                \"image_type\": \"fisheye\",\n                \"ecosystem_type\": \"dynamic oceanic\"\n            },\n            \"description\": \"A dataset containing high-resolution fisheye images from ROVs capturing diverse marine species, interactions, and environmental conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Mask R-CNN\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"sophisticated for marine biodiversity detection\",\n                \"targetfps\": 45,\n                \"cross_platform\": true\n            },\n            \"description\": \"A state-of-the-art model designed for marine biodiversity detection in underwater scenarios, accounting for camouflage and lighting variations.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cross-platform\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time, optimized for computational efficiency\"],\n        \"demonstration\": \"High accuracy in challenging underwater conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the requirements for a machine learning project, with a focus on building a sophisticated Mask R-CNN model for marine biodiversity detection. The expert-level expertise is reflected, and key metrics, such as precision, latency, and deployment environment, are specified. The challenge of the project and its requirements are well represented in the JSON format.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Revise a cutting-edge visual question-answering system using the COCO-Text-Extended dataset, incorporating a specialized Social and Spatial Context-Aware Visual Transformer. Prioritize semantic analysis of intricate crowd behavior, fine-grained location details, and nuanced emotional cues. Target a precision above 80% for identifying implicit relationships and designing interactive experiences that provoke deep thought and vivid storytelling in diverse cultural contexts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"The task involves refining a visual question-answering system using the COCO-Text-Extended dataset. A specialized Social and Spatial Context-Aware Visual Transformer is to be integrated, focusing on semantic analysis of crowd behavior, fine-grained location details, and emotional cues. The system should achieve a precision above 80% for recognizing implicit relationships and support interactive experiences fostering deep thought and diverse cultural storytelling.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.8\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO-Text-Extended\",\n            \"modality\": [\n                \"image\",\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"crowd_size\": {\n                    \"type\": \"number\",\n                    \"unit\": \"count\"\n                },\n                \"diversity\": {\n                    \"type\": \"string\",\n                    \"description\": \"Variety of cultural context\"\n                }\n            },\n            \"description\": \"A dataset for visual question answering with social and spatial context, containing extended text annotations for enhancing analysis of crowd behavior, location, and emotions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Social and Spatial Context-Aware Visual Transformer\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Model's ability to handle semantic context and emotional cues\"\n                },\n                \"crowd_behavior_feature_integration\": {\n                    \"type\": \"string\",\n                    \"description\": \"How the model integrates crowd behavior\"\n                }\n            },\n            \"description\": \"A state-of-the-art model designed for visual question answering, emphasizing semantic analysis and the target precision of above 80%.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The use of advanced transformer-based architectures in multimodal learning, especially in capturing intricate crowd behavior and emotional cues.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware feature extraction\",\n            \"crowd behavior encoding\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Interactive stories or case studies showcasing the improved system's ability to provide deep insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately represents the user's need for advice on refining a visual question-answering system with high-level requirements, including the use of COCO-Text-Extended and the specified Social and Spatial Context-Aware Visual Transformer. It incorporates the emphasis on precision, semantic analysis, and cultural context. The missing performance metrics are considered subject to be derived based on the user's request for consultation, not a standalone build.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager, guide the machine learning team to conquer the challenging boolean question classification task on the BoolQ dataset. Demand a novel fusion of BERT-Base uncased with advanced entity tracking for interpreting context-specific entities in 10,000 complex passages. Mandate a <0.5% false-positive rate, strict adherence to macro-averaged F1-score, and maintain real-time performance monitoring, while producing extensive, reproducible research logs for peer-reviewed publication.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval or NLP applications\",\n        \"description\": \"The project involves boolean question classification using the BoolQ dataset. A novel fusion of BERT-Base uncased with advanced entity tracking for interpreting context-specific entities is required. Key performance criteria are a false-positive rate of <0.5%, macro-averaged F1-score, real-time performance, and extensive, reproducible research logs for publication purposes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"false-positive rate\",\n                \"value\": 0.005\n            },\n            {\n                \"name\": \"macro-averaged F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions for binary classification\"],\n            \"specification\": {\n                \"number_of_passages\": 10000,\n                \"complexity\": \"complex\",\n                \"entity_density\": null\n            },\n            \"description\": \"A dataset containing 10,000 complex passages with boolean questions for classification, emphasizing the need for entity tracking.\",\n            \"preprocessing\": [\"BERT-Base uncased preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-Base uncased with entity tracking\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"BERT-Base\",\n                \"entity_tracking\": \"advanced\",\n                \"parameters\": null,\n                \"inference_speed\": null\n            },\n            \"description\": \"A fusion model that combines BERT for text understanding with advanced entity tracking for context-specific interpretations.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"The project demands real-world applicability by tackling complex passages and strict performance measures. NLP team should also provide detailed research logs to facilitate peer-reviewed publication.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"advanced entity extraction\", \"feature refinement\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time monitoring\"],\n        \"demonstration\": \"publication-ready research logs and model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent and requirements for a machine learning project. It reflects the demand for a novel model combining BERT with entity tracking, specific performance metrics, and research log expectations for publication. The template structure is appropriately followed, and the provided values match the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for a high-performance AI research team, I aim to design a state-of-the-art indoor scene segmentation system. Explore the innovative fusion of U-Net and advanced pruning techniques for ultra-precise furniture identification, ensuring cross-platform compatibility and minimal latency even on minimalistic IoT devices. Conduct an exhaustive performance benchmarking study with a focus on energy efficiency and scalability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"architectural design, IoT\",\n        \"description\": \"The task is to design a state-of-the-art indoor scene segmentation system using a fusion of U-Net and advanced pruning techniques for precise furniture identification. The model must be compatible across platforms and have minimal latency even on minimalistic IoT devices, with a focus on energy efficiency and scalability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"ultra-precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cross-platform compatibility\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"power consumption\",\n                \"value\": null,\n                \"unit\": \"mW\"\n            },\n            {\n                \"name\": \"device compatibility\",\n                \"value\": {\n                    \"minimalistic IoT devices\": true\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Indoor Scene Segmentation Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"furniture segments\"],\n            \"specification\": {\n                \"image_resolution\": \"variable\",\n                \"scene Diversity\": \"diverse indoor scenes\",\n                \"annotations\": \"pixel-level segmentation\"\n            },\n            \"description\": \"A dataset designed for the development of indoor scene segmentation models with a focus on furniture identification.\",\n            \"preprocessing\": [\n                \"data augmentation for domain adaptation\"\n            ],\n            \"augmentation\": [\n                \"cutout, flipping, and image warping to simulate real-world variations\"\n            ],\n            \"visualization\": [\n                \"visual comparisons of model outputs with ground truth annotations\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net with advanced pruning techniques\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"fused U-Net and pruning methods for enhanced performance\",\n                \"pruning_techniques\": {\n                    \"structured\": true,\n                    \"unstructured\": false\n                }\n            },\n            \"description\": \"A cutting-edge model architecture combining U-Net and pruning for improved furniture segmentation while maintaining compatibility and efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advancements in U-Net architectures and pruning strategies for resource-constrained IoT devices.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"model compression for resource optimization\"\n        ],\n        \"target_device\": \"minimalistic IoT devices\",\n        \"deployment_endpoint\": \"IoT platforms or device-specific libraries\",\n        \"inference_engine\": [\n            \"optimized for IoT-friendly inference\"\n        ],\n        \"demonstration\": \"demonstration of accurate and energy-efficient segmentation on IoT devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to design a high-performance AI system. It accurately describes the problem area (computer vision), task (image segmentation), and the focus on U-Net fusion with pruning for ultra-precise furniture identification. It incorporates cross-platform compatibility, minimal latency, energy efficiency, and scalability as performance metrics. The complexity metrics are relevant and the dataset search is specified based on the requirement. The user's high expertise level is acknowledged.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "multimodal forgery detection", "instruction": "As a project manager, request a detailed investigation on transformer adaptation for Celeb-DF's video forgery detection, focusing on fine-tuning state-of-the-art models with multi-modal fusion. Assess incremental improvements, benchmark against complex forgery defense methods, and estimate the minimal hardware infrastructure for real-time, high-accuracy, and scalable deployment across diverse devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"image and video analysis\",\n        \"description\": \"The project involves a detailed investigation into transformer adaptation for video forgery detection using Celeb-DF dataset. The focus is on fine-tuning state-of-the-art models with multi-modal fusion, comparing incremental improvements against complex forgery defense methods, and determining the minimal hardware infrastructure for real-time, high-accuracy, and scalable deployment across diverse devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"compute requirements\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\n                \"video\",\n                \"multimodal\"\n            ],\n            \"target_variables\": [\n                \"video forgery labels\"\n            ],\n            \"specification\": {\n                \"video_length\": null,\n                \"modalities\": [\n                    \"RGB\",\n                    \"audio\"\n                ],\n                \"dimensionality\": null\n            },\n            \"description\": \"A dataset for video forgery detection in the context of celebrities, requiring multi-modal fusion.\",\n            \"preprocessing\": [\"multi-modal feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based\",\n            \"family\": \"Transformer models (specifically adapted for multi-modal tasks)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"fine-tuned for video forgery detection\",\n                \"multi-modal fusion\": true,\n                \"accuracy baselines\": null\n            },\n            \"description\": \"A state-of-the-art model employing transformer adaptation, targeting high-accuracy video forgery detection with multi-modal fusion.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Experience in transformer adaptation, multi-modal fusion, and real-time deployment is essential.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adaptation for forgery detection\", \"fine-tuning\"],\n        \"target_device\": \"various, including edge, mobile, and cloud devices\",\n        \"deployment_endpoint\": \"optimized for diverse environments\",\n        \"inference_engine\": [\"real-time\", \"optimized\"],\n        \"demonstration\": \"demonstration of improved performance and scalability under different hardware configurations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the consult-build intent by emphasizing the need for advice on transformer adaptation. The high expertise level of the user suggests a need for detailed insights. Performance and complexity metrics are included for a well-rounded assessment. The requirement for real-time and scalable deployment across diverse devices indicates a focus on practical applications.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for the AI-empowered translation revolution, mandate the development of a novel neural machine translation system utilizing the extensive WMT19 multilingual news corpus. The team must excel in bidirectional translations between Russian and English, aiming for a state-of-the-art BLEU score of 48 within a limited resource constraint. Emphasize on zero-shot and few-shot transfer learning strategies, ensuring cross-domain adaptability. Mandate strict evaluation with domain-specific benchmarking and human-comparable performances, along with meticulous error analysis and periodic parity audits.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics\",\n        \"description\": \"The project involves developing a neural machine translation system using the WMT19 multilingual news corpus for bidirectional translations between Russian and English. The aim is to achieve a BLEU score of 48, focusing on zero-shot and few-shot transfer learning for cross-domain adaptability. Evaluation must be thorough with domain-specific benchmarking and human-comparable performance, along with detailed error analysis and parity audits.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 48\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource constraint\",\n                \"value\": null,\n                \"unit\": \"limited resources\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 Multilingual News Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text (English to Russian and vice versa)\"],\n            \"specification\": null,\n            \"description\": \"A multilingual corpus for training and testing the neural machine translation system, emphasizing Russian and English bidirectional translations.\",\n            \"preprocessing\": [\n                \"tokenization\",\n                \"lemmatization\",\n                \"sentence splitting\"\n            ],\n            \"augmentation\": [\n                \"back-translation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Neural Machine Translation System\",\n            \"family\": \"Sequence to Sequence models with Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"Transformer with zero-shot and few-shot learning components\",\n                \"transfer learning approaches\": [\"zero-shot\", \"few-shot\"],\n                \"domain adaptability\": true\n            },\n            \"description\": \"A novel neural machine translation model trained on WMT19 corpus with a focus on bidirectional translation between Russian and English, targeting state-of-the-art BLEU score of 48.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Zero-shot and few-shot learning techniques for cross-domain adaptability.\",\n        \"Emphasis on domain-specific benchmarking and human-comparable performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain adaptation techniques\",\n            \"error analysis tools\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"AI translation service API\",\n        \"inference_engine\": [\n            \"GPU-based inference\"\n        ],\n        \"demonstration\": \"Periodic parity audits\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the instruction, detailing the project's objective, expertise level, and specific requirements for the translation model. Performance metrics, complexity constraints, and steps like domain-specific benchmarking and error analysis are well represented.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "image classification", "instruction": "As a project manager, your team has been tasked with developing a cutting-edge image recognition app for instant and precise detection of dog breeds. Utilizing the Stanford Dogs Dataset, design a state-of-the-art system incorporating both a custom CNN and transfer learning with VGG16 or ResNet. Strive for exceptional accuracy while minimizing latency, and guarantee a frictionless user interface that balances computational efficiency without compromising on responsiveness, even under heavy load conditions.", "resposne": "{\n  \"user\": {\n    \"intent\": \"build\",\n    \"expertise\": \"high\"\n  },\n  \"problem\": {\n    \"area\": \"computer vision\",\n    \"downstream_task\": \"image classification\",\n    \"application_domain\": \"artificial intelligence, mobile\",\n    \"description\": \"The project involves designing an image recognition app for real-time, accurate detection of dog breeds using the Stanford Dogs Dataset. The system should combine a custom CNN and transfer learning with VGG16 or ResNet, prioritizing high accuracy and low latency while providing a seamless user interface resistant to heavy load conditions.\",\n    \"performance_metrics\": [\n      {\n        \"name\": \"accuracy\",\n        \"value\": null\n      },\n      {\n        \"name\": \"latency\",\n        \"value\": null\n      }\n    ],\n    \"complexity_metrics\": [\n      {\n        \"name\": \"computational efficiency\",\n        \"value\": null,\n        \"unit\": \"FLOPs\"\n      },\n      {\n        \"name\": \"user interface responsiveness\",\n        \"value\": null,\n        \"unit\": \"milliseconds\"\n      }\n    ]\n  },\n  \"dataset\": [\n    {\n      \"name\": \"Stanford Dogs Dataset\",\n      \"modality\": [\"image\"],\n      \"target_variables\": [\"dog breed\"],\n      \"specification\": {\n        \"size\": null, \n        \"image_size\": {\n          \"width\": null,\n          \"height\": null\n        },\n        \"number_of_classes\": null\n      },\n      \"description\": \"A dataset for detecting dog breeds with the requirement of real-time and precise classification.\",\n      \"preprocessing\": [],\n      \"augmentation\": [\"real-time data augmentation for varying lighting, angles, and backgrounds\"],\n      \"visualization\": [],\n      \"source\": \"direct-search\"\n    }\n  ],\n  \"model\": [\n    {\n      \"name\": \"Custom CNN with VGG16 or ResNet\",\n      \"family\": \"Convolutional Neural Networks\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"architecture\": [\"custom CNN\", \"transfer learning with VGG16 or ResNet\"],\n        \"accuracy_target\": \"state-of-the-art\",\n        \"latency_specifications\": {\n          \"realtime\": true\n        }\n      },\n      \"description\": \"A high-performance image classification model using a combination of custom CNN and transfer learning for dog breed detection.\"\n    }\n  ],\n  \"knowledge\": [],\n  \"service\": {\n    \"feature_engineering\": [\"optimization for mobile deployment\"],\n    \"target_device\": \"mobile, IoT\",\n    \"deployment_endpoint\": \"cloud-based or edge deployment\",\n    \"inference_engine\": [\"real-time or batch processing\"],\n    \"demonstration\": \"user-friendly and responsive under heavy load conditions\"\n  },\n  \"confidence\": {\n    \"rationale\": \"The JSON response includes all relevant details based on the given instruction: user's intent to build a system, high expertise, computer vision area with image classification task, application domain, Stanford Dogs Dataset, requirements for a custom CNN and transfer learning, emphasis on accuracy and latency, user interface, and specific constraints on performance metrics and computational efficiency.\",\n    \"score\": 0.98\n  }\n}\n"}
{"task": "image classification", "instruction": "As an elite machine learning team, your mission is to design an advanced skin lesion classifier using ResNeSt architecture on the highly specialized DermQuest dataset, which encompasses rare and obscure dermatological conditions. The system must achieve superhuman precision of 99.5% with a focus on minimizing false positives in the presence of subtle differences, as real-world diagnosis relies on accurate and unambiguous results. To enhance trust among remote patients, design a user-friendly feature attribution tool that generates interactive heatmaps for patients to visually understand model reasoning. Furthermore, engineer the model for ultra-fast inference on low-power wearable devices without compromising accuracy, considering the device's limited computational capabilities in the field.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medicine, dermatology\",\n        \"description\": \"The task is to design an advanced skin lesion classifier using the ResNeSt architecture on the DermQuest dataset, targeting rare dermatological conditions. The system aims for superhuman precision of 99.5% with a focus on minimizing false positives, and includes a user-friendly feature attribution tool for heatmaps to promote trust among remote patients. Model deployment must support ultra-fast inference on low-power wearable devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.995\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model size for wearable deployment\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DermQuest Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"skin lesion classification\"],\n            \"specification\": {\n                \"dataset_type\": \"highly specialized\",\n                \"condition_coverage\": \"rare and obscure dermatological conditions\"\n            },\n            \"description\": \"A specialized dataset containing images of skin lesions for advanced classification, with focus on subtle variations and rare conditions.\",\n            \"preprocessing\": [\"data augmentation for rare conditions\"],\n            \"augmentation\": [\"context-aware augmentation\"],\n            \"visualization\": [\n                \"interactive heatmaps for model reasoning\"\n            ],\n            \"source\": \"user-link\" \n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNeSt (Advanced Skin Lesion Classifier)\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ResNeSt\",\n                \"accuracy_target\": 0.995,\n                \"feature_attribution\": \"user-friendly heatmaps\"\n            },\n            \"description\": \"A deep learning model designed to classify skin lesions with high precision and a focus on interpretability.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"model interpretability, heatmaps\"],\n        \"target_device\": \"low-power wearable devices\",\n        \"deployment_endpoint\": \"optimized for remote patient use\",\n        \"inference_engine\": [\"optimized for limited wearable device compute\"],\n        \"demonstration\": \"interactive for patients\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the user's intent to build a state-of-the-art skin lesion classifier, specifies the ResNeSt architecture, performance requirements, and emphasizes on complexity, interpretability, and device compatibility. The user's high expertise is reflected. Some details such as the actual device inference time and model size are not specified and can be inferred from the target device type.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "In the domain of precision agriculture, develop a state-of-the-art ensemble forecasting system using a hybrid of multiple time-series methodologies (GRU, Prophet, and ARIMA) combined with satellite imagery analysis. The challenge involves forecasting crop yield for a 15-day forecast horizon in a real-time IoT environment, with strict requirements for sub-200ms latency, achieving a multi-model fusion accuracy of at least 95%, and ensuring the model is deployable on resource-constrained edge devices with limited GPU capabilities.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"precision agriculture\",\n        \"description\": \"The task is to create a real-time ensemble forecasting system using GRU, Prophet, and ARIMA models, combined with satellite imagery analysis, for precision agriculture with a focus on crop yield prediction for a 15-day horizon in an IoT environment. There are strict requirements for low latency (sub-200ms) and achieving high accuracy (at least 95%) with limited GPU capabilities on edge devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"multi-model fusion accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.2,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"GPU usage\",\n                \"value\": null,\n                \"unit\": \"optimized for edge devices\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Satellite Imagery Data for Crop Yield Forecasting\",\n            \"modality\": [\n                \"time series\",\n                \"image\"\n            ],\n            \"target_variables\": [\"crop yield\"],\n            \"specification\": null,\n            \"description\": \"The dataset combines satellite imagery and historical crop yield data for real-time forecasting. It requires time-series analysis and interpretation of satellite data.\",\n            \"preprocessing\": [\"timeseries alignment\", \"image feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time-series and satellite data visualizations\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Ensemble of GRU, Prophet, and ARIMA\",\n            \"family\": \"Time-series forecasting models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"number_of_models\": 3,\n                \"latency_impact\": {\n                    \"gru\": \"low\",\n                    \"prophet\": \"medium\",\n                    \"arima\": \"low\"\n                }\n            },\n            \"description\": \"A state-of-the-art ensemble system combining GRU, Prophet, and ARIMA models with satellite imagery for crop yield prediction in a real-time IoT environment.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilizing inter-model fusion techniques to combine the strengths of each model and adapt to resource-constrained edge environments.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"time-series feature extraction\",\n            \"image-to-feature conversion\"\n        ],\n        \"target_device\": \"resource-constrained edge devices\",\n        \"deployment_endpoint\": \"real-time IoT environment\",\n        \"inference_engine\": [\"lightweight, latency-optimized models\"],\n        \"demonstration\": \"real-time yield forecasts and latency analysis for edge devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the requirements of the machine learning project, including the use of ensemble forecasting, the specific models involved, data sources, latency constraints, and hardware requirements. The rationale highlights the importance of the fusion strategy and feature engineering to handle the given constraints.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "For the imminent e-commerce warehouse management system, we require a state-of-the-art, fine-grained object detection algorithm that not only locates and tracks packages at various stages of the picking, packing, and shipping process in real-time, but must further differentiate between different types of goods (e.g., small electronics, groceries, and fragile items) based on their unique shapes, colors, and size patterns. Additionally, the model should analyze the workers' tasks by detecting their roles (picker, sorter, or packer) using subtle gestures and custom wearable accessories, ensuring efficient warehouse operations and enhancing worker safety.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"e-commerce, warehouse management\",\n        \"description\": \"The task involves developing a state-of-the-art object detection model for an e-commerce warehouse management system. It must real-time locate and track packages at different stages (picking, packing, and shipping), distinguish between various goods based on shape, color, and size patterns, and also analyze workers' roles by detecting gestures and custom wearable accessories.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mAP (mean Average Precision)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"frames per second (FPS)\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Warehouse Management Object Detection Dataset\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"package locations\", \"good types\", \"worker roles\"],\n            \"specification\": {\n                \"image resolution\": [null, null],\n                \"data types\": [\"RGB\", \"depth (optional)\"]\n            },\n            \"description\": \"A comprehensive dataset for fine-grained object detection in a warehouse environment, capturing different stages, goods, and worker roles.\",\n            \"preprocessing\": [\"data augmentation for object, gesture recognition\"],\n            \"augmentation\": [\"real-time object and gesture augmentation\"],\n            \"visualization\": [\"inspection of unique shape, color, and size patterns\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-Art Object Detection Model\",\n            \"family\": \"Convolutional Neural Networks (CNNs) with advanced object detection architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"e.g., YOLOv5, EfficientDet\",\n                \"parameter count\": null,\n                \"FLOPs\": null\n            },\n            \"description\": \"Advanced object detection model tailored for fine-grained package, good, and worker role detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Subtle gesture recognition algorithms, potentially including pose estimation and gesture-based action recognition.\",\n        \"Custom wearable sensor integration for identifying worker roles\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"fine-tuning for warehouse-specific scenarios\"],\n        \"target_device\": \"edge devices, potentially with low-latency processing\",\n        \"deployment_endpoint\": \"IoT or cloud-based platform\",\n        \"inference_engine\": [\"real-time, low-latency object detection\"],\n        \"demonstration\": \"user-guided workflows and live model performance demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the complexity of the warehouse management system requirements. It includes the user's high expertise level, the detailed task description, and the various performance and complexity metrics needed. The dataset requirements and the model architecture selected align with the specific task at hand.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Design an advanced multi-layered node classification model for the Coauthorship Science dataset using a progressive variant of HGAT, which integrates temporal feature sequences and a nested attention mechanism. The task now requires not just precise domain attribution to authors but also an in-depth exploration of transient collaboration shifts and the forecasting of potential interdisciplinary fusion in future academic collaborations. Additionally, please optimize the model for scalability and explain its interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"The goal is to design an advanced node classification model for the Coauthorship Science dataset using a progressive variant of HGAT (Hierarchical Graph Attention Network). The model should integrate temporal feature sequences and a nested attention mechanism, and the focus is on precise domain attribution to authors, transient collaboration shifts, and forecasting interdisciplinary fusion in future collaborations. Additionally, the model should be optimized for scalability and its interpretability should be addressed.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coauthorship Science\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [\n                \"author domain attribution\",\n                \"collaboration shifts\",\n                \"interdisciplinary fusion prediction\"\n            ],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": {\n                    \"nodes\": null,\n                    \"edges\": null,\n                    \"time_steps\": null\n                },\n                \"temporal_sequence_length\": null\n            },\n            \"description\": \"A graph dataset containing temporal coauthorship data for academic papers, with a focus on integrating temporal feature sequences and a nested attention mechanism.\",\n            \"preprocessing\": [\n                \"Temporal feature extraction\",\n                \"Graph preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Progressive HGAT\",\n            \"family\": \"Graph Attention Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"hidden_layers\": null,\n                \"attention_types\": [\"nested\"],\n                \"temporal_integration\": true,\n                \"number_of_heads\": null\n            },\n            \"description\": \"An advanced model utilizing a progressive variant of HGAT for node classification in academic collaboration networks. It focuses on integrating temporal features, a nested attention mechanism, and addresses scalability and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Temporal data structure is crucial for modeling collaboration dynamics.\",\n        \"Nested attention enhances feature extraction from complex relationships.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Temporal node embedding\",\n            \"Attention-based feature extraction\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"interpretability dashboard\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"Model explanation through visualization of attention weights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes all the requested elements from the user instruction, including task details on the Coauthorship Science dataset, the specified model, performance and complexity requirements, and additional expert considerations. Adjustments were made to accommodate the unique requirements of integrating temporal and attention mechanisms, forecasting interdisciplinary fusion, and ensuring interpretability.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a skilled project director, you are tasked with directing a multi-disciplinary team to design an innovative image analysis system that precisely recognizes rare astronomical objects, leveraging ensemble techniques with CNNs, VGG16, and InceptionV3, fine-tuned on the extensive yet complex DeepSpace Celestial Catalog Dataset. Emphasize dealing with low-contrast nebulae, variable supernovae, and real-time adaptations for dynamic sky conditions, aiming for superior accuracy surpassing 98% with a combination of F1-score, precision, recall, and AUC-ROC metrics. Furthermore, outline a step-by-step protocol for data augmentation, model validation, and continuous monitoring for performance optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The task involves designing an innovative image analysis system that recognizes rare astronomical objects, using ensemble techniques with CNNs (specifically VGG16 and InceptionV3) and fine-tuning on the DeepSpace Celestial Catalog Dataset. Key challenges include handling low-contrast nebulae and variable supernovae, as well as real-time adaptability for dynamic sky conditions. The aim is to achieve superior accuracy exceeding 98% with metrics like F1-score, precision, recall, and AUC-ROC.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepSpace Celestial Catalog Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"rare astronomical objects\"],\n            \"specification\": null,\n            \"description\": \"A large and complex dataset for fine-tuning ensemble techniques on CNNs, focusing on rare astronomical objects and low-contrast nebulae.\",\n            \"preprocessing\": [\"Handling low-contrast, variable data\", \"Data standardization\"],\n            \"augmentation\": [\n                \"Image rotation\",\n                \"Scaling\",\n                \"Noise addition (for supernovae detection)\",\n                \"Low-contrast modification\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Ensemble of VGG16, InceptionV3\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_method\": \"Fine-tuning\",\n                \"architecture\": [\"VGG16\", \"InceptionV3\"]\n            },\n            \"description\": \"A multi-model ensemble approach using VGG16 and InceptionV3 for improved accuracy and rare astronomical object recognition.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Image resizing\", \"Feature extraction from CNNs\", \"Feature fusion\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"SkyObservationAPI\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Real-time adaptation framework\"],\n        \"demonstration\": \"Performance-based animated dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the user's instructions, addressing the requirement for an ensemble approach using VGG16 and InceptionV3, handling low-contrast data, and targeting a high accuracy of 98% with specified metrics. Data augmentation, model validation, and performance optimization are covered in a step-by-step protocol. Target device and deployment endpoints were deduced based on the expected scope of the project.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As an experienced project manager, direct a cutting-edge ML team to refine a state-of-the-art time-series classifier for Handwritten Character Recognition. The team must develop a novel, memory-efficient architecture that sequentially integrates LSTM cells (INPUT_SEQ_LEN=152, INPUT_DIM=3) with attention mechanisms, ensuring interpretability via counterfactual explanations. Aim for benchmark accuracy while maintaining real-time adaptability for dynamic handwriting styles. Constrain the project to a tight deadline, and deliver a model with comprehensive LIME explanations catering to diverse stakeholder needs.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"pattern recognition\",\n        \"description\": \"The task is to refine a state-of-the-art time-series classifier for Handwritten Character Recognition, focusing on developing a novel, memory-efficient architecture using LSTM cells with a sequence length of 152 and an input dimension of 3. The model should include attention mechanisms for interpretability, be adaptable for dynamic handwriting styles in real-time, and provide LIME explanations covering diverse stakeholder needs within a tight deadline.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory efficiency\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Handwritten Character Recognition Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"characters\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 152,\n                \"INPUT_DIM\": 3\n            },\n            \"description\": \"A dataset for handwritten character recognition, including diverse handwriting styles.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"LSTM with Attention\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"memory-efficient\": true,\n                \"sequence_length\": 152,\n                \"attention_mechanisms\": true,\n                \"interpretability\": \"via LIME\"\n            },\n            \"description\": \"A state-of-the-art time-series classifier for handwritten character recognition\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model must maintain real-time adaptability for dynamic handwriting styles and provide comprehensive LIME explanations catering to diverse stakeholder needs.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time adaptability\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient for real-time performance\"],\n        \"demonstration\": {\n            \"emphasis\": \"interpretability and real-time adaptability\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the instruction for a machine learning project with a focus on handwritten character recognition. The user's high expertise level, novel architecture, performance targets, complexity constraints, and the requirement for LIME explanations are accurately represented. The confidence score is high due to the thorough interpretation of the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As the AI project manager for the Cutting-Edge Subtle Wildlife Recognition system, our team must now tackle the iNaturalist 2020 Dataset. Challenge the ResNeXt-101 architecture to not only discern species, habitat, and subspecies with precision, but also decipher subtle morphological differences among closely related organisms. Incorporate a novel requirement that the model must exhibit a harmonious balance of precision, recall, and F1-score, with an audacious goal of surpassing 99% accuracy in recognizing elusive nocturnal species under low light and intricate camouflage. Ensure comprehensive fine-grained analysis for each ecosystem and a multi-model fusion strategy for improved performance under diverse environmental conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"wildlife biology\",\n        \"description\": \"The task is to develop the Cutting-Edge Subtle Wildlife Recognition system using the iNaturalist 2020 Dataset. The model must use the ResNeXt-101 architecture to distinguish species, habitat, and subspecies with high precision, focusing on subtle morphological differences. The primary performance metrics should be precision, recall, and F1-score, targeting a minimum accuracy of 99% for nocturnal species under low light and intricate camouflage. The model should also include comprehensive fine-grained analysis by ecosystem and employ a multi-model fusion strategy for improved performance in diverse environments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"iNaturalist 2020 Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\", \"habitat\", \"subspecies\"],\n            \"specification\": null,\n            \"description\": \"A dataset for recognizing wildlife, specifically focusing on discerning subtle differences among closely related organisms and nocturnal species with low light and intricate camouflage.\",\n            \"preprocessing\": [\"low light enhancement\", \"camouflage augmentation\"],\n            \"augmentation\": [\"adversarial examples for robustness\"],\n            \"visualization\": [\"fine-grained ecosystem analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNeXt-101\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Incorporating multi-model fusion strategy\"\n            },\n            \"description\": \"The backbone model for wildlife recognition, designed to handle subtle morphological differences and targeted performance in low light and camouflage.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model should be trained on low light-enhanced and camouflage-augmented images to account for challenging conditions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"fine-tuning for ecosystem specificities\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"ensemble for improved performance\"],\n        \"demonstration\": \"Emphasis on multi-model fusion and fine-grained ecosystem analysis for diverse environmental conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the AI project's requirements, emphasizing the cutting-edge nature of the task, the ResNeXt-101 architecture, and the high expectations for precision, recall, and accuracy. The challenge for low light and camouflage conditions, fine-grained analysis, and multi-model fusion are all captured in the provided fields.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular classification", "instruction": "Can you assist me in developing a tailored tabular classification model for predicting intricate factors influencing senior secondary school students' academic excellence, incorporating specific variables like attendance, extracurricular activities, and standardized test scores?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"education\",\n        \"description\": \"The task is to build a model that predicts academic excellence in senior secondary school students, using factors such as attendance, extracurricular activities, and standardized test scores as predictors.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Senior Secondary School Academic Excellence Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"academic excellence\"],\n            \"specification\": {\n                \"size\": \"unknown\",\n                \"dimension\": \"dependent on dataset\",\n                \"length\": \"row count of the dataset\"\n            },\n            \"description\": \"A dataset containing variables related to attendance, extracurricular activities, and standardized test scores for predicting academic excellence in senior secondary school students.\",\n            \"preprocessing\": [\"data cleaning, normalization, feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [\"exploratory data analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"tree-based or ensemble models (e.g., Random Forest, Gradient Boosting)\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"model complexity\": \"tunable for high performance\",\n                \"interpretability\": \"high\"\n            },\n            \"description\": \"A tailored model designed for tabular classification, focusing on intricate factors affecting academic excellence in senior secondary school students\"\n        }\n    ],\n    \"knowledge\": [\n        \"Features with high importance need to be identified\",\n        \"Assessing class imbalance may be crucial\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection, handling missing data\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"example cases and prediction explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the nature of the task, which is a build intent with high expertise, focused on tabular classification in education. Performance metrics are left open-ended for optimization. The model type, dataset specifics, and required preprocessing steps are based on the problem description. The rationale mentions key considerations for successfully completing the task, but certain details might need further clarification.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "The CiteSeer dataset presents a fascinating challenge. As we embark on a node classification task, we must preprocess the XML data, extract citation networks, and normalize node features. Aim for high precision and recall, while considering feature selection and dealing with potential class imbalance. What performance targets and evaluation techniques should we employ for this enhanced task?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"The task involves preprocessing the CiteSeer dataset, extracting citation networks, and normalizing node features for node classification. The goal is to achieve high precision and recall while handling class imbalance and incorporating feature selection techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CiteSeer\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node type\"],\n            \"specification\": {\n                \"data_format\": \"XML\",\n                \"feature_normalization\": \"preprocessed\"\n            },\n            \"description\": \"An XML dataset that requires preprocessing for citation networks and node feature extraction.\",\n            \"preprocessing\": [\"XML to graph\", \"feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_selection\": true,\n                \"class_imbalance_handling\": true\n            },\n            \"description\": \"A model for node classification on the CiteSeer dataset, targeting high precision and recall with feature selection and imbalance mitigation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection techniques\",\n            \"class imbalance handling\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has accurately captured the user's instruction for the CiteSeer node classification task. It includes the required preprocessing steps, the focus on precision and recall, and the mention of feature selection and class imbalance handling. The model family and potential techniques are inferred based on the nature of the task.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "As a project manager for the machine learning team, your objective is to develop a robust multimodal forgery detection system that not only analyses visual and text elements but also considers temporal and contextual cues. The system must accurately recognize manipulated academic records and certificates while maintaining real-time processing capabilities, ensuring the credibility of our institution and the sanctity of degree validation in a rapidly evolving digital landscape.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"education and cybersecurity\",\n        \"description\": \"The objective is to develop a robust multimodal forgery detection system that analyzes visual, textual, temporal, and contextual cues to recognize manipulated academic records and certificates in real-time, ensuring institutional credibility and degree validation in a digital environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time processing\",\n                \"value\": true\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multimodal Forgery Detection Dataset\",\n            \"modality\": [\n                \"image\",\n                \"text\",\n                \"time series\",\n                \"graph\"\n            ],\n            \"target_variables\": [\n                \"forged records\",\n                \"certificates\"\n            ],\n            \"specification\": null,\n            \"description\": \"A dataset containing diverse multimodal data for academic records and certificates, capturing forgery patterns and contextual cues.\",\n            \"preprocessing\": [\n                \"temporal data alignment\",\n                \"contextual feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"False Positive/Negative analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Multimodal Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A robust, real-time multimodal forgery detection model that accounts for various cues for accurate detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Temporal and contextual information play a crucial role in distinguishing real from manipulated content.\",\n        \"Real-time processing is vital for fast validation in the digital landscape.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal fusion\",\n            \"contextual feature engineering\"\n        ],\n        \"target_device\": \"cloud or edge\",\n        \"deployment_endpoint\": \"institution's validation platform\",\n        \"inference_engine\": [\n            \"TensorFlow.js\",\n            \"custom optimized inference\"\n        ],\n        \"demonstration\": \"live demo with real-time detection\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the objective, the need for a multifaceted approach, real-time processing, and specific dataset requirements. Performance metrics and complexity constraints are included, and the level of user expertise is set to 'high' reflecting the complexity of the task. Knowledge pieces and deployment considerations align with the project requirements.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "link prediction", "instruction": "As a machine learning project manager, design a sophisticated deep learning model, incorporating a hierarchical PIN architecture and time-sensitive features, for the Yeast Protein Interaction Network. The model should not only predict novel interactions but also analyze dynamics over different cellular stages and experimental conditions, while ensuring interpretability and scalability for large-scale datasets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"The task involves designing a deep learning model with a hierarchical PIN architecture for predicting novel yeast protein interactions. The model should also analyze dynamics across cellular stages and experimental conditions, maintaining interpretability and scalability for large-scale datasets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Yeast Protein Interaction Network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"protein interactions\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"dimensions\": null,\n                    \"length\": null\n                },\n                \"edges\": {\n                    \"dimensions\": null,\n                    \"length\": null\n                }\n            },\n            \"description\": \"A large-scale graph dataset capturing protein interactions with temporal and hierarchical characteristics.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical PIN-based Deep Learning Model\",\n            \"family\": \"Deep Graph Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"hierarchical PIN\",\n                \"time-sensitive features\": true,\n                \"interpretability techniques\": [\"attention mechanisms\", \"explainable embeddings\"]\n            },\n            \"description\": \"A deep learning model designed for yeast protein interaction prediction, considering dynamics, interpretability, and scalability for extensive data.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"node embedding techniques\",\n            \"temporal feature extraction\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow serving\", \"PyTorch inference\"],\n        \"demonstration\": \"interactive web interface showcasing model results and explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to design a sophisticated model, their expertise level, and the specific requirements such as the hierarchical PIN architecture, time-sensitive features, and large-scale interpretability. The areas, tasks, metrics, dataset details, model family, and selected deep learning type align with the instruction. However, the exact model performance metrics, preprocessing, and inference engine choices are left open for further discussion or customization.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, demand the development of an advanced image classification system for the Galaxy Zoo dataset. Mandate the implementation of a state-of-the-art Vision Transformer (ViT) architecture, enhanced with progressive resizing and data augmentation techniques. The model should not only exhibit exceptional accuracy but also demonstrate resilience in transfer learning, efficiently classifying unseen celestial objects while ensuring minimal human bias in its astronomical object identification.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The project aims to create an advanced image classification system for the Galaxy Zoo dataset using a Vision Transformer (ViT) architecture. The system must incorporate progressive resizing and data augmentation, exhibit high accuracy, demonstrate resilience in transfer learning, and minimize human bias in celestial object identification.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resilience in transfer learning\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"astronomical object\"],\n            \"specification\": {\n                \"images\": {\n                    \" resizing techniques\": [\"progressive resizing\"],\n                    \"augmentation\": [\"data augmentation techniques relevant to celestial objects\"]\n                }\n            },\n            \"description\": \"A dataset requiring a highly accurate, transfer learning-resistant, and bias-minimizing image classification system for classifying unseen celestial objects.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"progressive resizing\", \"data augmentation specific to astronomical data\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Vision Transformer (ViT)\",\n            \"family\": \"Transformer-based architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer learning capabilities\": null,\n                \"resilience to unseen celestial objects\": null\n            },\n            \"description\": \"Advanced image classification system using ViT, enhanced with progressive resizing and tailored data augmentation.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"To minimize human bias, the model should incorporate principles of unbiased data augmentation and representation learning.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstration of model's performance on unseen celestial objects and its minimization of human bias\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the project manager's intent and the specific requirements for an advanced image classification system. It includes the high-level Vision Transformer architecture, data augmentation techniques, transfer learning resilience, and a focus on minimizing human bias. Specific performance metrics and data preprocessing details are open to be filled based on the project's requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for the advanced machine learning team, you are tasked with enhancing the Wild Blueberry Yield Dataset regression project. The team must not only develop a state-of-the-art model using advanced feature engineering and interactions but also incorporate a time-series component to account for seasonal variations. The model should minimize mean absolute error (MAE), with a focus on achieving cutting-edge performance, and must be validated through a nested cross-validation scheme.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"The task involves enhancing the Wild Blueberry Yield Dataset regression project by incorporating time-series components to capture seasonal variations and creating a state-of-the-art model. The focus is on minimizing mean absolute error (MAE) and employing a nested cross-validation for validation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"time series complexity\",\n                \"value\": null,\n                \"unit\": \"time steps\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wild Blueberry Yield Dataset\",\n            \"modality\": [\n                \"tabular\"\n            ],\n            \"target_variables\": [\n                \"yield item\"\n            ],\n            \"specification\": {\n                \"seasonality\": \"monthly\",\n                \"time_series_length\": null\n            },\n            \"description\": \"A dataset for regression with a time-series component, addressing seasonal variations in the Wild Blueberry Yield data.\",\n            \"preprocessing\": [\"advanced feature engineering\", \"time-series feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"exploring seasonal patterns\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced_interactions\": true,\n                \"model_complexity\": \"state-of-the-art\",\n                \"time_series_integration\": true\n            },\n            \"description\": \"A model for tabular regression, incorporating advanced feature engineering and time-series components to account for seasonal variations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"A deep understanding of time-series analysis and its application in regression is necessary.\",\n        \"State-of-the-art models for tabular data may require extensive hyperparameter tuning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"deep learning techniques\", \"feature interactions\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"nested cross-validation results and performance plot\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the instruction with a high expertise level. It includes the requirement for a state-of-the-art model, time-series component, performance metrics, and a nested cross-validation scheme. The 'specification' in dataset and 'model' account for the advanced feature engineering, while the 'knowledge' reflects the complexity of the task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Utilize the OpenSubtitles corpus to develop a state-of-the-art, multi-lingual translation model with a Transformer-enhanced BERT architecture, specifically targeting English-Mandarin conversations. Mandate exceptional translation of colloquial slang and idiomatic expressions, maintaining a minimum F1 score of 95% in precision, and demonstrate robustness across diverse sub-genre subtitles.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"The task involves creating a state-of-the-art multi-lingual translation model using a Transformer-enhanced BERT architecture, specifically targeting English-Mandarin conversations. Emphasis is on accurate translation of colloquial slang and idiomatic expressions, with a goal of achieving a minimum F1 score of 95% in precision and demonstrating robustness across diverse sub-genre subtitles from OpenSubtitles corpus.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score (precision)\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"English\", \"Mandarin\"],\n            \"specification\": {\n                \"source_language\": \"English\",\n                \"target_language\": \"Mandarin\",\n                \"corpus_type\": \"multi-genre subtitles\",\n                \"focus_on\": \"colloquial slang and idiomatic expressions\"\n            },\n            \"description\": \"A dataset from the OpenSubtitles corpus designed for English-Mandarin translation tasks, focusing on diverse sub-genre conversations.\",\n            \"preprocessing\": [\n                \"data cleaning for slang and idiomatic expressions\"\n            ],\n            \"augmentation\": [\n                \"sub-genre-specific data augmentation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-enhanced BERT\",\n            \"family\": \"Transformer-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"BERT with Transformer adaptations for language translation\",\n                \"target_domain_strength\": \"tailored for English-Mandarin conversations\"\n            },\n            \"description\": \"A model specifically designed for high-quality English-Mandarin translation with a focus on slang and idioms, targeting F1 score of 95% precision.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual embeddings for slang and idioms\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"expected to showcase robustness through diverse sub-genre translations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is structured based on the provided JSON schema, and the instruction details have been accurately parsed. The user's intent to build a model with high expertise, the target dataset, specific task requirements, and performance metrics are captured in the JSON. However, the choice of 'high' expertise level might need further justification or validation based on the user's actual proficiency.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a seasoned project manager for a cross-disciplinary AI team, we've been assigned to develop a cutting-edge botanical recognition system. We've been given the 'Plant Seedlings Diversity and Complexity Benchmark Dataset' which includes high-resolution images with multiple angles and lighting conditions. Your task is to research and propose a state-of-the-art convolutional neural network (CNN) architecture, such as EfficientNet or Swin Transformer, that can handle the variations in plant species and environmental factors. In addition, analyze the trade-offs between accuracy, inference time, and computational efficiency, and present a detailed performance prediction using real-world testing. Also, discuss how the model might scale with an expanding database and any potential transfer learning strategies for future expansion.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"botany\",\n        \"description\": \"The task is to develop a botanical recognition system using a state-of-the-art CNN, focusing on EfficientNet or Swin Transformer, for the 'Plant Seedlings Diversity and Complexity Benchmark Dataset' which includes high-resolution images with variations in plant species and environmental factors. Performance should consider accuracy, inference time, and computational efficiency, with a prediction for real-world testing. Scalability and transfer learning strategies for future database expansion are also to be discussed.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Seedlings Diversity and Complexity Benchmark Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"plant species\", \"environmental factors\"],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"variations\": [\"multiple angles\", \"lighting conditions\"]\n            },\n            \"description\": \"A dataset with high-resolution images of plant seedlings with variations in species and environmental factors.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet or Swin Transformer (to be determined)\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"state-of-the-art\"],\n                \"handling_variations\": [\"species variability\", \"environmental factors\"]\n            },\n            \"description\": \"Proposed architecture for botanical image classification with emphasis on accuracy, robustness to variations.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"real-world_testing\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the given schema and captures the essence of the instruction. It specifies the high expertise level, the problem domain, task details, requested CNN architecture, performance and complexity metrics, and the requirement for real-world testing with scalability and transfer learning strategies.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a precision farming expert, your machine learning team is tasked with developing an advanced image classification model for our extensive agricultural operation. In addition to real-time monitoring, the system must now differentiate between various crop stress levels, differentiate between seasonal changes, and identify specific pathogens or pests. The algorithm must maintain high accuracy while dealing with large datasets and handle multiple resolutions, ensuring timely and informed decision-making for optimal crop yield.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"The task involves creating an advanced image classification model for real-time monitoring of crop stress levels, seasonal changes, and identifying pathogens or pests in an agricultural operation. The model must handle large datasets, multiple resolutions, and maintain high accuracy for timely decision-making to optimize crop yield.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"time to process large datasets\",\n                \"value\": null,\n                \"unit\": \"time units (e.g., hours)\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Agricultural Crop Monitoring Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"crop stress levels\", \"seasonal changes\", \"pathogens/pests\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"resolutions\": [\"various\"]\n            },\n            \"description\": \"A large dataset for real-time monitoring of crops with various stress levels, seasonal changes, and pathogen/pest identification.\",\n            \"preprocessing\": [\"large dataset handling\", \"multiple resolution support\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image Classification Model (Custom)\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": \"large capacity for complex image features\",\n                \"efficiency\": \"optimized for large datasets and multi-resolution inputs\"\n            },\n            \"description\": \"An advanced model designed to handle complex crop monitoring tasks, including crop stress, seasonal variations, and pathogen/pest detection.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"large-scale feature extraction\", \"ensemble learning for improved performance\"],\n        \"target_device\": \"Edge Devices for real-time monitoring\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource-constrained environments\"],\n        \"demonstration\": \"Interactive visual dashboard for real-time decision support\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the precision farming expert's requirements, indicating a 'build' intent with high expertise. The problem area is covered in computer vision, with a focus on image classification, handling large datasets, and multiple resolutions. Performance metrics like accuracy and time complexity are included. The solution integrates an advanced model and a deployment plan for edge devices.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "machine translation", "instruction": "Create a state-of-the-art machine translation system leveraging the extensive ParaCrawl dataset, which mandates the implementation of a sophisticated dual encoder architecture. Ensure not only semantic preservation but also emotion detection and nuanced expression retention for English to Dutch translations, reflecting cultural subtleties in the output.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"The project involves developing a state-of-the-art machine translation system, focusing on English to Dutch translations, using the ParaCrawl dataset. The system should employ a dual encoder architecture and prioritize semantic preservation, emotion detection, and nuanced expression retention, accounting for cultural subtleties.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"semantic similarity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"emotion detection accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"expression preservation score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl corpus (English to Dutch)\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\n                \"translated text\"\n            ],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"diversity\": \"real-world, multilingual\"\n            },\n            \"description\": \"A large-scale dataset for machine translation tasks, including English to Dutch, focusing on cultural nuances.\",\n            \"preprocessing\": [\"cleaning\", \"tokenization\"],\n            \"augmentation\": [\"backtranslation\", \"data diversification\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dual Encoder Machine Translation Model\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"dual encoder\",\n                \"features\": \"semantic, emotional, nuanced expression\"\n            },\n            \"description\": \"A cutting-edge model designed for high-performance English to Dutch translation, incorporating dual encoder architecture and cultural sensitivity.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cultural nuances and lexicons for English to Dutch translation\",\n        \"Advanced NLP techniques for emotion detection\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-aware feature extraction\", \"contextual embeddings\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-powered\", \"optimized for large-scale parallel processing\"],\n        \"demonstration\": \"Translation examples showcasing cultural sensitivity and preservation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes a detailed mapping of the user's instruction, specifying the 'build' intent with high expertise level. It covers the area of natural language processing and the dual-encoder architecture for machine translation. The metrics, such as semantic similarity and emotion detection accuracy, are relevant to the given task. The model and dataset properties reflect the complexities and requirements described. The service section accounts for context-awareness and the necessity of parallel processing for a large-scale dataset.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a leading heritage institution with a voluminous digital repository of antique parchments and artifacts, we seek a sophisticated machine learning solution for segmenting not only intricate line drawings but also multi-layered and culturally diverse illustrations in historical manuscripts, ensuring the preservation of their visual nuances and historical significance. Can you recommend and implement a state-of-the-art algorithm that can handle this intricate and fine-grained layout analysis?", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"cultural heritage preservation\",\n        \"description\": \"The task involves developing a sophisticated machine learning solution for segmenting line drawings and multi-layered, culturally diverse illustrations in historical manuscripts, with a focus on preserving visual nuances and historical significance. A state-of-the-art algorithm for handling intricate and fine-grained layout analysis is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"segmentation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"runtime efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"ability to handle intricate details\",\n                \"value\": null\n            },\n            {\n                \"name\": \"visual nuance preservation\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"historical significance preservation\",\n                \"value\": null,\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Historical Manuscripts\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_types\": [\"line drawings\", \"multi-layered illustrations\"],\n                \"diversity\": \"cultural\"\n            },\n            \"description\": \"A large digital collection of antique parchments and artifacts with varying layouts and complexities.\",\n            \"preprocessing\": [\"image cleaning\", \"normalization\"],\n            \"augmentation\": [\"minor noise addition\", \"geometric distortions\"],\n            \"visualization\": [\"dataset analysis for pattern understanding\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Image Segmentation Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"algorithm\": \"State-of-the-art layout analysis\",\n                \"architecture\": \"deep learning\",\n                \"handling_features\": \"fine-grained, complex layouts\"\n            },\n            \"description\": \"A recommendation for a cutting-edge algorithm that can address the challenges of manuscript segmentation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Understanding of cultural context and impact on visual nuances is crucial for successful implementation.\",\n        \"Access to large annotated data for the specific domain could enhance model performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"extracting relevant features for fine-grained analysis\"],\n        \"target_device\": \"cloud-based for efficient computation\",\n        \"deployment_endpoint\": \"institutional intranet or dedicated server\",\n        \"inference_engine\": [\"GPU accelerated for speed\"],\n        \"demonstration\": \"demonstrations of segmentation results on sample manuscripts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the consult-build request, with a high-level requirement for a state-of-the-art algorithm to handle the specific challenges of historical manuscript segmentation. The request emphasizes preservation of visual nuances and cultural significance. Performance metrics and complexity are expected to be evaluated based on the algorithms and real-world results.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the advanced text classification project, you are tasked with enhancing the current state-of-the-art machine learning model for British Airways customer sentiment analysis. Utilizing the extensive and nuanced dataset that includes historical flight reviews, your objective is to not only develop a model capable of interpreting diverse linguistic expressions but also to achieve an exceptional performance with a focus on precision and recall. In addition to standard training, you must implement techniques such as transfer learning, data augmentation, and fine-tuning for domain-specific language understanding. The ultimate goal is to minimize the root mean squared error (RMSE) during evaluation and deliver a robust system that can generalize well across various customer feedback scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"customer service\",\n        \"description\": \"The project aims to enhance the existing machine learning model for British Airways customer sentiment analysis using a comprehensive dataset of historical flight reviews. The focus is on precision and recall, while incorporating techniques like transfer learning, data augmentation, and fine-tuning for improved domain-specific language understanding. The goal is to minimize RMSE and ensure generalization across various customer feedback scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"root mean squared error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"British Airways Flight Reviews\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"customer sentiment\"],\n            \"specification\": {\n                \"size\": \"extensive and historical\",\n                \"linguistic diversity\": \"detailed and nuanced\"\n            },\n            \"description\": \"Comprises historical flight reviews for training, validation, and testing a text classification model with focus on customer sentiment analysis.\",\n            \"preprocessing\": [\n                \"domain-specific preprocessing\",\n                \"data cleaning\"\n            ],\n            \"augmentation\": [\n                \"data augmentation using transformers\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Text Classifier\",\n            \"family\": \"Transformer-based (e.g., BERT, RoBERTa)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer learning\": true,\n                \"fine-tuning\": true\n            },\n            \"description\": \"A state-of-the-art machine learning model for text classification, optimized for precision and recall with techniques like transfer learning and fine-tuning.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain-specific feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"customer feedback response analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's instruction for an advanced text classification project for British Airways customer sentiment analysis. It captures the main goals, techniques, and performance metrics specified. The high expertise level of the user is noted. The model and dataset details reflect the requirement for a state-of-the-art model and extensive historical data.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As a junior data analyst with minimal domain knowledge, I must detect anomalies in a large-scale, multi-dimensional energy dataset for five years, spanning multiple towns. Each town has unique seasonal fluctuations and requires investigation using advanced time-series decomposition techniques, segmentation, and comparison with historical trends. Suggest a robust modeling framework with preprocessing steps for accurate and efficient anomaly detection.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"low\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"energy\",\n        \"description\": \"The task involves detecting anomalies in a large-scale, multi-dimensional energy dataset spanning five years across multiple towns. The data requires advanced decomposition techniques, segmentation, and comparison with historical trends due to unique seasonal fluctuations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multi-Town Energy Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"time_frequency\": \"annual\",\n                \"dimensions\": \"multi-dimensional\",\n                \"towns\": \"multiple\"\n            },\n            \"description\": \"A large, multi-year dataset with unique seasonal fluctuations per town.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"normalization\",\n                \"seasonal decomposition\",\n                \"segmentation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"time-series plots\",\n                \"comparison with historical trends\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"statistical models\",\n            \"specification\": {\n                \"model_method\": \"advanced time-series decomposition techniques\",\n                \"comparison_method\": \"historical trend comparison\"\n            },\n            \"description\": \"A robust modeling framework for anomaly detection in energy data, considering complex seasonality and town-specific patterns.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Experience with time-series decomposition methods like STL or Prophet\",\n        \"Awareness of segmentation techniques\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction from time-series data\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for large-scale time-series processing\"\n        ],\n        \"demonstration\": \"Demonstration of anomaly detection on representative towns and data segments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the main aspects of the given instruction, focusing on building a model with specific preprocessing steps for anomaly detection in a large-scale multi-dimensional energy dataset. The user's expertise level is indicated as low, and performance metrics and complexity requirements are acknowledged. Knowledge pieces reflect the required methods for the task. Confidence is justified based on the adaptation of the provided schema.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Apply a cutting-edge deep learning approach, like the autoencoder-ensemble with recurrent attention mechanism, to forecast the hourly user engagement on a specific app category over a 6-month period. The model should exhibit robustness to real-time data anomalies while preserving long-term dependencies, ensuring high accuracy without compromising generality.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"user engagement prediction\",\n        \"description\": \"The task is to develop a model using a cutting-edge approach, such as an autoencoder-ensemble with a recurrent attention mechanism, for forecasting hourly user engagement on a specific app category over a 6-month period. The model should be resilient to real-time data anomalies while preserving long-term dependencies, targeting high accuracy without losing generality.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness to anomalies\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"long-term dependency preservation\",\n                \"value\": \"high\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"User Engagement App Category Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly user engagement\"],\n            \"specification\": {\n                \"temporal_frequency\": \"hourly\",\n                \"time_period\": \"6 months\"\n            },\n            \"description\": \"The dataset contains hourly user engagement data for a specific app category over a 6-month period.\",\n            \"preprocessing\": [\"anomaly detection\", \"data normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Autoencoder-ensemble with Recurrent Attention Mechanism\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"autoencoder\", \"ensemble\"],\n                \"attention_mechanism\": \"recurrent\"\n            },\n            \"description\": \"A deep learning model that combines autoencoders and an ensemble to forecast user engagement, with a recurrent attention mechanism to handle long-term dependencies and real-time anomalies.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"handling time series data\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time streaming\"],\n        \"demonstration\": \"prediction results with time anomaly detection and long-term dependencies visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON format accurately represents the deep learning approach (autoencoder-ensemble with recurrent attention) for user engagement prediction, addressing the user's expertise level (high), and specifying the key performance metrics and requirements for anomaly resilience and long-term dependencies. The dataset details and model specification are also in line with the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "For a highly specialized time-series classification project, your machine learning development team has been tasked with analyzing and forecasting the intricate patterns in a lesser-known dataset called \"Energy Consumption Fluctuations in Smart Grids\" (EFSG). This dataset is exceptionally complex, featuring a train-validation-test split with non-uniform time intervals and a dynamic feature set (INPUT_SEQ_LEN=500, INPUT_DIM=10, including sub-datasets like power demand, weather variables, and seasonal trends). The target variable consists of seven distinct classes representing different energy consumption patterns (0-6), requiring a high level of precision in understanding the interplay between variables. The challenge lies in designing an adaptive model that can handle the temporal dependencies and external influences while maintaining an accuracy of at least 95% on unseen data. Develop a robust deep learning architecture and fine-tune it using time-series-specific techniques, like attention mechanisms and advanced feature extraction, to deliver exceptional performance in this niche domain.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"smart grids\",\n        \"description\": \"The task involves a highly specialized project to classify energy consumption patterns in the 'Energy Consumption Fluctuations in Smart Grids' dataset. The dataset presents unique challenges with non-uniform time intervals, a dynamic feature set, and seven classes. The goal is to build a robust deep learning model with attention mechanisms and advanced feature extraction, achieving at least 95% accuracy on unseen data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"temporal dependencies handling\",\n                \"value\": null,\n                \"unit\": \"advanced\"\n            },\n            {\n                \"name\": \"external influences handling\",\n                \"value\": null,\n                \"unit\": \"incorporated\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Energy Consumption Fluctuations in Smart Grids (EFSG)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"energy consumption patterns (0-6)\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 500,\n                \"INPUT_DIM\": 10,\n                \"sub-datasets\": [\n                    {\n                        \"name\": \"power demand\",\n                        \"description\": \"A sub-dataset representing power demand in the time series\"\n                    },\n                    {\n                        \"name\": \"weather variables\",\n                        \"description\": \"Sub-dataset with weather-related features\"\n                    },\n                    {\n                        \"name\": \"seasonal trends\",\n                        \"description\": \"Sub-dataset capturing seasonal changes\"\n                    }\n                ],\n                \"time_interval\": \"non-uniform\"\n            },\n            \"description\": \"Dataset with intricate patterns, featuring power demand, weather variables, and seasonal trends\",\n            \"preprocessing\": [\"time-series normalization\", \"resampling\"],\n            \"augmentation\": [\"time-warping\", \"noise injection\"],\n            \"visualization\": [\"sequence patterns\", \"dependency analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Deep Learning Architecture\",\n            \"family\": \"Recurrent Neural Networks (specifically for time-series)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention_mechanisms\": true,\n                \"advanced_feature_extraction\": true\n            },\n            \"description\": \"A deep learning model tailored for time-series data with attention and advanced feature extraction capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model should incorporate memory cells to handle temporal dependencies and potentially use ensemble methods for robustness.\",\n        \"Time-series-specific regularization techniques, such as L1 and L2 regularization, may be applied.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"custom time features\", \"cyclical features\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized serving libraries for time-series predictions\"],\n        \"demonstration\": \"Explanatory plots and detailed results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the details of the task, including the dataset's complexity, model requirements, and performance targets. The user's high expertise level is acknowledged, and specific knowledge or techniques (like attention mechanisms, ensemble methods, and time-series-specific feature engineering) are addressed. The confidence score is somewhat lower due to the uncertainty in certain technical aspects, but overall it reflects the challenge in the project.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for an advanced conversational AI system, can you propose a tailored BERT-based model, specifically optimized for the banking domain, utilizing the FinanceQA dataset? Aim for a balance between efficiency and accuracy, targeting a response time of <2 seconds while maintaining a minimum F1-score of 90%.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"banking\",\n        \"description\": \"The user wants a tailored BERT-based conversational AI model for the banking domain, optimized for the FinanceQA dataset. The model should balance efficiency (response time < 2 seconds) with high accuracy (F1-score of at least 90%).\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"seconds\",\n                \"constraint\": \"<2\"\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9,\n                \"unit\": \"%\"\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"FinanceQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"financial information\"],\n            \"specification\": null,\n            \"description\": \"A dataset specific to the banking domain for question answering tasks.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-based conversational AI\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"train_speed\": null,\n                \"inference_time\": null,\n                \"memory_size\": null,\n                \"efficiency_tradeoff\": \"optimized for response time\"\n            },\n            \"description\": \"A custom model designed for the banking domain and the FinanceQA dataset with a balance between efficiency and accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"banking conversational API\",\n        \"inference_engine\": [\"real-time inference\"],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's request for a BERT-based model optimized for the banking domain, using the FinanceQA dataset. It accounts for efficiency (response time <2 seconds) and accuracy (F1-score of 90%). The model type and constraints have been appropriately defined, and the target device and deployment endpoint are typical for such applications. However, actual metric values (train speed, inference time) need to be filled out or estimated based on the user's specific requirements.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "For my senior thesis, I need to classify a diverse range of celestial objects, including distant galaxies and various star formations, utilizing a specialized deep learning model. Could you guide me to a suitable pre-processed dataset from a reputable source, like the Hubble Space Telescope, and advise on the complexity and potential challenges in attaining exceptional performance due to the intricate details and vastness of space imagery?", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The user requires assistance for a senior thesis that involves classifying celestial objects like galaxies and star formations using a specialized deep learning model. They are looking for a pre-processed dataset from a reputable source like the Hubble Space Telescope and seeking advice on complexity and potential challenges due to the intricate details and vastness of space imagery.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hubble Space Telescope's Celestial Object Dataset\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\"Celestial object type\"],\n            \"specification\": null,\n            \"description\": \"A pre-processed dataset from the Hubble Space Telescope for diverse celestial object classification, including galaxies and star formations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"data augmentation techniques for space imagery\"\n            ],\n            \"visualization\": [\n                \"visualization of intricate details and vastness in space images\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Specialized Deep Learning Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"to be determined based on space imagery complexity\"\n            },\n            \"description\": \"A deep learning model for classifying celestial objects, accounting for the intricacy and vastness of space imagery.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Challenges include high-resolution image processing, dealing with varying lighting conditions, and the potential need for large annotated datasets due to the diversity of objects.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for space imagery\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized for space imagery classification\"],\n        \"demonstration\": \"showcasing successful classification examples from diverse celestial objects\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to consult and build a model, incorporating their high expertise level. The problem description and requirements, such as the Hubble Space Telescope dataset, complex metric expectations, and challenge factors, are included based on the instruction. Challenges in attaining exceptional performance are pointed out.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "For an avant-garde time-series forecasting challenge, you've been tasked with analyzing the intricate behavior of the highly volatile and chaotic \"Astronomical Phenomena dataset\" (APD). This dataset consists of a celestial event time series, featuring a massive collection of observations on comet orbits, gravitational lensing fluctuations, and celestial conjunctions over an unprecedented 1000-year historical period. The data has been meticulously segmented into a rare nested structure: a nested train-validation-test setup with three nested cycles, each representing sub-regions of different astronomical regions. The input sequences consist of gravitational waves emitted by celestial objects (INPUT_SEQ_LEN=432, with dimensions including luminosity, mass, and angular momentum), and the prediction targets encompass future cosmic signals and event probabilities (PRED_SEQ_LEN=432, PRED_DIM=11).\n\nYour objective is to develop a deep learning model that not only forecasts the obscure celestial events but also accounts for the complex interdependencies among celestial bodies. To showcase your prowess, you must optimize the model for both accuracy (employing intricate probabilistic metrics like log-likelihood and predictive entropy) and computational efficiency (minimizing the number of ephemeris updates during inference). Demonstrate your model's prowess by attaining exceptional performance in this complex, multi-dimensional forecasting problem.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The task involves analyzing the Astronomical Phenomena dataset for time-series forecasting of celestial events and signals. The dataset has a nested structure with three levels of sub-regions, and the model must account for complex interdependencies among celestial objects. Performance should be optimized for accuracy using metrics like log-likelihood and predictive entropy, while prioritizing computational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"log-likelihood\",\n                \"value\": null\n            },\n            {\n                \"name\": \"predictive entropy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"ephemeris updates during inference\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Astronomical Phenomena Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"future cosmic signals\", \"event probabilities\"],\n            \"specification\": {\n                \"input_seq_len\": 432,\n                \"dimensions\": [\"luminosity\", \"mass\", \"angular momentum\"],\n                \"prediction_seq_len\": 432,\n                \"prediction_dim\": 11,\n                \"nested_structure\": true\n            },\n            \"description\": \"A dataset of celestial event time series with comet orbits, gravitational lensing fluctuations, and conjunctions. It has a nested train-validation-test setup with multiple astronomical sub-regions.\",\n            \"preprocessing\": [\n                \" Handling nested structure\",\n                \"Resampling if necessary\"\n            ],\n            \"augmentation\": [\n                \"Time series data augmentation techniques\"\n            ],\n            \"visualization\": [\n                \"Signal patterns and correlations analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep learning suitable for time-series forecasting\",\n                \"adaptations\": \"accounts for complex interdependencies, computational efficiency during inference\"\n            },\n            \"description\": \"A deep learning model for time-series forecasting of celestial events and signals, focusing on accuracy and computational efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Nested structure in the data and its impact on model design\",\n        \"Importance of probabilistic metrics for accuracy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Incorporating interdependencies\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Efficient during inference\"],\n        \"demonstration\": \"Model performance on the nested test set and multi-dimensional forecasting\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the user's intention (highly complex forecasting task), their expertise, and the specific requirements of the project, including the data structure, model requirements, and performance indicators. Adjustments made for model adaptability and computational efficiency demonstrate the problem-solving approach. The confidence score is based on the comprehensive representation of the user's instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project manager for our upcoming ambitious endeavor, we aim to revolutionize translation technology by focusing on the massive Common Crawl dataset. The core task is to design and train a cutting-edge transformer model specializing in English-to-Japanese translation, but with a twist - it must excel in deciphering and reproducing nuanced colloquial expressions with a stringent benchmark of 98% accuracy, reflecting a deep understanding of cultural subtleties.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics\",\n        \"description\": \"The project aims to develop a state-of-the-art transformer model for English-to-Japanese translation, with a focus on handling nuanced colloquial expressions with a benchmark accuracy goal of 98%, demonstrating cultural understanding.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl corpus for English-Japanese translation\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"English text\", \"Japanese text\"],\n            \"specification\": {\n                \"source\": \"massive, extracted from Common Crawl\",\n                \"size\": \"Large and diverse\",\n                \"linguistic variety\": \"Extensive English-to-Japanese text pairs\",\n                \"linguistic nuances\": \"Emphasis on colloquial expressions and cultural subtleties\"\n            },\n            \"description\": \"A massive dataset derived from Common Crawl, containing a vast variety of English-Japanese translation pairs to train the model on.\",\n            \"preprocessing\": [\"Language detection, tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer (specifically for English-to-Japanese translation)\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"Advanced transformer architecture, tailored for translation tasks\",\n                \"specialization\": \"English-to-Japanese with a focus on colloquial expressions\",\n                \"cultural understanding\": \"High capacity for learning nuanced cultural subtleties\"\n            },\n            \"description\": \"A transformer-based model designed to meet the project requirements, focusing on English-to-Japanese translation with high accuracy for colloquial expressions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Understanding of diverse cultural expressions and nuances\",\n        \"Consideration of domain-specific terminology and idiom usage\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Fine-tuning for specific domain\", \"Cultural context embeddings\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstration of the model's ability to handle colloquial expressions with high accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the project's key details and requirements, reflecting the high expertise level of the user. It includes the focus on Common Crawl dataset, transformer model, accuracy target, and cultural subtleties. Specific details such as preprocessing, augmentation, model architecture, and service requirements are aligned with the instruction.\",\n        \"score\": 0.99\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager, your team has been tasked with enhancing a state-of-the-art time-series forecasting system. Utilizing the Weather dataset, now with an added requirement of handling non-stationary patterns, you must develop a robust model that integrates seasonality, long-range dependencies, and exogenous variables. The train, validation, and test splits consist of sequences with time lags (INPUT_SEQ_LEN=96, INPUT_DIM=21), where you must not only predict the subsequent 96 steps (PRED_SEQ_LEN=96, PRED_DIM=21) but also optimize for both mean squared error (MSE) and mean absolute error (MAE), while ensuring at least a 5% improvement over the current best model's performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"weather forecasting\",\n        \"description\": \"The task involves enhancing a state-of-the-art time-series forecasting system to handle non-stationary patterns. The dataset includes weather data with input sequences of length 96 and 21 dimensions, and the model must predict the next 96 steps. Key requirements are optimizing for MSE and MAE, aiming for a 5% improvement over the current best model's performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Weather dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 21,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 21\n            },\n            \"description\": \"A time-series dataset with non-stationary patterns, including seasonality, long-range dependencies, and exogenous variables. Splits consist of train, validation, and test sequences.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks\",\n            \"type\": \"recurrent neural networks or deep learning\",\n            \"specification\": {\n                \"handling\": \"non-stationary patterns, seasonality, long-range dependencies, and exogenous variables\"\n            },\n            \"description\": \"A state-of-the-art model for time-series forecasting with the specified requirements and aiming for at least a 5% improvement in performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the instruction given. It highlights the task of enhancing a state-of-the-art time-series forecasting model, the requirements for handling non-stationary patterns, and the performance metrics with a target of a 5% improvement. The user's high expertise level is also accounted for.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for the advanced AI safety initiative, mandate the following multi-layered task: Utilize the COCO dataset and integrate an industrial scene dataset for enhanced training. Design a novel Anchor-Free CNN architecture that not only assesses safety gear conformity but also differentiates between various trip hazard types with precision above 95%. Require a step-by-step approach, outlining intermediate performance milestones and a thorough evaluation of generalization across diverse manufacturing scenarios.", "resposne": "{\n  \"user\": {\n    \"intent\": \"build\",\n    \"expertise\": \"high\"\n  },\n  \"problem\": {\n    \"area\": \"computer vision\",\n    \"downstream_task\": \"object detection\",\n    \"application_domain\": \"industrial safety\",\n    \"description\": \"The task involves using the COCO dataset and an industrial scene dataset to develop a novel Anchor-Free CNN architecture for assessing safety gear conformity and differentiating between various trip hazard types with a precision goal of above 95%. The project requires a step-by-step approach, including intermediate performance milestones and comprehensive generalization testing across diverse manufacturing scenarios.\",\n    \"performance_metrics\": [\n      {\n        \"name\": \"precision\",\n        \"value\": 0.95\n      }\n    ],\n    \"complexity_metrics\": []\n  },\n  \"dataset\": [\n    {\n      \"name\": \"COCO dataset\",\n      \"modality\": [\"image\"],\n      \"target_variables\": [\"safety gear conformity\", \"trip hazards\"],\n      \"specification\": null,\n      \"description\": \"A widely-used dataset for training a model to recognize and assess safety gear conformity and trip hazards.\",\n      \"preprocessing\": [],\n      \"augmentation\": [],\n      \"visualization\": [],\n      \"source\": \"direct-search\"\n    },\n    {\n      \"name\": \"Industrial Scene Dataset\",\n      \"modality\": [\"image\"],\n      \"target_variables\": [\"safety gear conformity\", \"trip hazards\"],\n      \"specification\": null,\n      \"description\": \"An industrial-specific dataset to enhance training and cater to diverse manufacturing scenarios.\",\n      \"preprocessing\": [],\n      \"augmentation\": [],\n      \"visualization\": [],\n      \"source\": \"direct-search\"\n    }\n  ],\n  \"model\": [\n    {\n      \"name\": \"Novel Anchor-Free CNN\",\n      \"family\": \"Convolutional Neural Network\",\n      \"type\": \"neural networks\",\n      \"specification\": null,\n      \"description\": \"A custom architecture designed for detecting safety gear conformity and trip hazards with high precision and robustness to manufacturing variations.\"\n    }\n  ],\n  \"knowledge\": [\n    {\n      \"content\": \"Utilize multi-task learning to integrate COCO and industrial datasets.\"\n    },\n    {\n      \"content\": \"Include intermediate milestones for model development and fine-tuning.\"\n    },\n    {\n      \"content\": \"Emphasize generalization testing across varying industrial scenes and lighting conditions.\"\n    }\n  ],\n  \"service\": {\n    \"feature_engineering\": [],\n    \"target_device\": \"\",\n    \"deployment_endpoint\": \"\",\n    \"inference_engine\": [],\n    \"demonstration\": \"Step-by-step approach with visual demonstrations of safety assessment and hazard detection.\"\n  },\n  \"confidence\": {\n    \"rationale\": \"The JSON response accurately captures the user's intent to build a complex machine learning project. The user's high expertise is accounted for. The task description, performance metrics, and dataset requirements are clearly defined. The Anchor-Free CNN architecture, source datasets, and performance milestones show attention to detail and a clear understanding of the requirements.\",\n    \"score\": 0.98\n  }\n}\n"}
{"task": "question answering", "instruction": "As a part of an elite journalism platform for investigative journalists, we require a highly specialized question-answering system that employs a novel fusion of multiple Transformer models, leveraging the rarely seen Oceanic Biomedical and Geospatial News dataset. This system must not only rapidly analyze and summarize breaking scientific and geopolitical news, but also generate in-depth insights and connect related articles for a holistic understanding. It should prioritize privacy-preserving techniques, handle complex jargon, and deliver concise, evidence-based responses within stringent real-time constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"journalism\",\n        \"description\": \"The project involves building a specialized question-answering system that fuses multiple Transformer models for analyzing and summarizing scientific and geopolitical news, generating in-depth insights, and connecting related articles. Privacy-preserving techniques are essential, and the system should handle complex jargon with real-time constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"complexity handling\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"evidence-based response\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"privacy-preserving techniques\",\n                \"value\": 1.0,\n                \"unit\": \"priority\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Oceanic Biomedical and Geospatial News dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": null,\n            \"specification\": {\n                \"size\": null,\n                \"diversity\": \"rarely seen\"\n            },\n            \"description\": \"A specialized dataset containing scientific and geopolitical news for advanced NLP tasks.\",\n            \"preprocessing\": [\"privacy-preserving\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer models (novel fusion)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": null,\n                \"model_complexity\": null,\n                \"real_time_performance\": true\n            },\n            \"description\": \"A specialized question-answering system leveraging multiple Transformer models for complex NLP tasks.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Privacy-preserving techniques are critical and the system must handle complex jargon.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"privacy-preserving feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time, evidence-based responses\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure is accurately represented based on the user's requirement for a high-level question-answering system. The project's goals, performance metrics, and data handling are clearly defined, reflecting the complex nature of the task. The 'expertise' is set to 'high' due to the advanced NLP requirements, and the reason for the confidence score is based on a thorough mapping of the instruction to the JSON structure.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "The environmental monitoring project demands a state-of-the-art ViT model trained on the extensive and diverse Landsat 8 dataset, including high-resolution images from various seasons. In addition to standard classification, the model should identify subtle changes in land use dynamics over time. It must consistently achieve at least 96% accuracy, maintain explainable AI principles, and be deployable on multiple cloud platforms with auto-scaling capabilities. A comprehensive report on transferability, resource footprint, and sensitivity analysis must accompany the deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"environmental monitoring\",\n        \"description\": \"The project aims to develop a state-of-the-art Vision Transformer (ViT) model, trained on the diverse Landsat 8 dataset with high-resolution images from different seasons. The model must perform both standard classification and identify subtle changes in land use dynamics over time, targeting at least 96% accuracy. It should adhere to explainable AI principles and be deployable on multiple cloud platforms with auto-scaling capabilities. A comprehensive report on transferability, resource footprint, and sensitivity analysis is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.96\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Landsat 8\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"land use classification\", \"change detection\"],\n            \"specification\": null,\n            \"description\": \"An extensive dataset containing high-resolution images from various seasons for land use analysis and change detection.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ViT\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"explainable AI principles\",\n                \"deployment\": \"multi-cloud, auto-scaling\"\n            },\n            \"description\": \"A state-of-the-art image classification model for land use analysis with seasonality and change detection capabilities.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"multiple cloud platforms\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"auto-scaling\"],\n        \"demonstration\": {\n            \"topics\": [\"transferability\", \"resource footprint\", \"sensitivity analysis\"]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has captured the essence of the project's requirements, including a high-level intent to build, expert level, task and dataset specifics, model architecture, accuracy target, and deployment aspects. It's assumed that the 'explanable AI principles' refers to the need for interpretability. However, it would be beneficial to explicitly clarify interpretability details.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "As the project manager for a sophisticated AI-based question-answering system, you are tasked with directing the machine learning team to develop a BERT-family model, utilizing RoBERTa, on the SQuAD 2.0 dataset. The model must not only surpass a 95% F1 score to ensure exceptional accuracy but must also optimize for efficiency by attaining response times below 1.5 milliseconds, even when handling complex queries.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"artificial intelligence\",\n        \"description\": \"The project requires developing a BERT-family model, specifically using RoBERTa, to create a high-accuracy question-answering system on the SQuAD 2.0 dataset. The model needs to exceed a 95% F1 score and ensure efficient performance with response times below 1.5 milliseconds, even for complex queries.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 1.5,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"unknown\",\n                \"questions_per_instance\": \"unknown\",\n                \"context_length\": \"unknown\",\n                \"query_length\": \"unknown\"\n            },\n            \"description\": \"A dataset for question answering, focusing on the SQuAD 2.0 version.\",\n            \"preprocessing\": [\"BERT fine-tuning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RoBERTa\",\n            \"family\": \"BERT-family\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": {\n                    \"inference_speed\": \"unknown\",\n                    \"model_size\": \"unknown\"\n                }\n            },\n            \"description\": \"The model to be developed for the question-answering task using the SQuAD 2.0 dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the requirements of a machine learning project, directing the development of a RoBERTa model on the SQuAD 2.0 dataset for high accuracy (95% F1 score) and efficiency (response time below 1.5ms). Expertise level is assumed to be high as the project is described as 'sophisticated'. Performance metrics and dataset information are appropriately included, although specific model efficiency details would require further clarification.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning enthusiast, you've been tasked with developing an advanced image classification algorithm for a luxury e-commerce platform specializing in high-end designer wear. They've recently integrated an innovative in-app feature allowing users to upload images of themselves wearing the garments, but now need to classify these photos into five distinct categories: outfit styling (on-trend, classic, or avant-garde), customer satisfaction (smiling, neutral, or glowing), accessories enhancement, environmental context (indoors, outdoors, or high-contrast event), and potential for influencer collaboration. Create a comprehensive prompt for your team, ensuring the model not only detects and categorizes effectively but also identifies subtle nuances and cultural fashion trends to maximize brand appeal and customer engagement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"luxury e-commerce (high-end designer wear)\",\n        \"description\": \"The task involves developing an advanced image classification algorithm for an e-commerce platform. The algorithm should classify images into five categories: outfit styling (on-trend, classic, avant-garde), customer satisfaction (smiling, neutral, glowing), accessories enhancement, environmental context (indoors, outdoors, high-contrast event), and potential for influencer collaboration. The model must detect subtle nuances and cultural fashion trends to boost brand appeal and customer engagement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"In-App User Selfies for Luxury Designer Wear\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\n                \"outfit styling\",\n                \"customer satisfaction\",\n                \"accessories enhancement\",\n                \"environmental context\",\n                \"influencer potential\"\n            ],\n            \"specification\": null,\n            \"description\": \"A collection of user-uploaded images from the luxury e-commerce platform, reflecting a variety of styling, customer emotions, and environmental contexts.\",\n            \"preprocessing\": [\"image normalization\", \"crop & resize\"],\n            \"augmentation\": [\"style transfer\", \"context-based enhancements\"],\n            \"visualization\": [\"sample image distributions per category\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (Advanced Architecture)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural details\": \"\",\n                \"training techniques\": [\"transfer learning\", \"fine-tuning\"]\n            },\n            \"description\": \"A state-of-the-art image classification model leveraging advanced CNNs and incorporating transfer learning for optimal performance on nuanced and culturally-sensitive fashion categories.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider incorporating cultural fashion influencers as a weak supervision signal for improving category understanding.\",\n        \"Pay attention to the subtle differences between on-trend, classic, and avant-garde styles to maximize brand appeal.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual embeddings for environmental context\", \"sentiment analysis for customer satisfaction\"],\n        \"target_device\": \"cloud for efficient processing\",\n        \"deployment_endpoint\": \"the e-commerce platform's API\",\n        \"inference_engine\": [\"GPU-based inference\"],\n        \"demonstration\": \"Interactive model explanations and performance metrics dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a high-end fashion image classifier, their expertise level, and the specific requirements such as performance metrics and advanced techniques. The dataset and model families are suitable, and the service and deployment aspects are in line with an e-commerce use case. However, there may be room for additional fine-grained specifications for the model and data preprocessing.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For a cutting-edge research project on image segmentation, we need the machine learning development team to design and implement a novel algorithm that can not only accurately segment instances of endangered big cats (such as snow leopards, Amur tigers, and African wild dogs) in high-resolution aerial and wildlife camouflage images, but also differentiate between individual sub-species and track their movements in real-time. The model must handle varying weather conditions, extreme lighting, and cluttered backgrounds while maintaining extremely low false positive rates for non-target species, and be optimized for deployment on resource-constrained embedded devices in remote conservation areas.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"The research project aims to develop a novel algorithm for image segmentation, focusing on accurately identifying and differentiating endangered big cats (snow leopards, Amur tigers, and African wild dogs) in high-resolution aerial and wildlife camouflage images. The model must handle varying weather conditions, lighting, cluttered backgrounds, and operate in real-time with extremely low false positive rates for non-target species.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": \"extremely low\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"resource efficiency for embedded devices\",\n                \"value\": \"optimized\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Endangered Big Cats Aerial and Wildlife Images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"endangered big cats, sub-species information\"],\n            \"specification\": {\n                \"resolution\": null,\n                \"weather conditions\": \"varied\",\n                \"lighting conditions\": \"extreme\",\n                \"background complexity\": \"cluttered\"\n            },\n            \"preprocessing\": [\n                \"adaptation to different conditions\",\n                \"noise reduction\"\n            ],\n            \"augmentation\": [\n                \"weather simulations\",\n                \"lighting variations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"custom\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"novel algorithm for image segmentation\",\n                \"target_device capabilities\": \"resource-constrained\"\n            },\n            \"description\": \"An advanced algorithm for real-time image segmentation of endangered big cats, optimized for deployment on embedded devices in remote conservation areas\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time tracking and sub-species differentiation\",\n        \"Resource efficiency for remote locations\",\n        \"Extremely low false positive rate\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"environmental context processing\",\n            \"motion tracking\"\n        ],\n        \"target_device\": \"embedded devices in remote conservation areas\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource-constrained environments\"],\n        \"demonstration\": \"real-time tracking and performance demonstrations on sample images\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the complex nature of the research project, emphasizing the advanced segmentation requirements, resource constraints, and the need for expert knowledge. Key metrics, model architecture, and preprocessing steps have been included to reflect the user's instruction.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "image segmentation", "instruction": "As a project manager for our innovative fashion AI team, request a highly scalable and energy-efficient solution. Develop a multi-stage fashion segmentation tool employing Enhanced EfficientNet, designed for high-resolution FCN, with advanced instance-level differentiation. Target 78% accuracy, while ensuring real-time performance on mobile devices under diverse lighting and pose conditions, using the DeepFashion2 dataset with nuanced, overlapping garment recognition.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"The project aims to create a highly scalable and energy-efficient multi-stage fashion segmentation tool using Enhanced EfficientNet for high-resolution FCNs. It requires advanced instance-level differentiation, targeting 78% accuracy in real-time on mobile devices, with robustness to diverse lighting and pose conditions. The dataset of choice is DeepFashion2, focusing on nuanced and overlapping garment recognition.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.78\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"mobile devices\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepFashion2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"overlap_recognition\": true\n            },\n            \"description\": \"Dataset for fashion image segmentation with emphasis on nuanced and overlapping garment recognition under diverse conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"lighting conditions\",\n                \"pose variations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced EfficientNet\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"high-resolution FCN\"\n            },\n            \"description\": \"A highly scalable and energy-efficient model for fashion image segmentation, designed for advanced instance-level differentiation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for mobile devices\"\n        ],\n        \"demonstration\": \"real-time performance and diverse condition demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the specific requirements, including the use of Enhanced EfficientNet for fashion segmentation, the target accuracy of 78%, real-time performance on mobile devices, and the DeepFashion2 dataset with nuanced garment recognition. Complexity metrics were adjusted for mobile environment and the source of dataset was assumed to be a user-provided link.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "For a cutting-edge project, design a real-time, energy-efficient machine translation application specifically tailored for Raspberry Pi, leveraging the diverse TED Talks corpus. Prioritize minimizing latency, optimizing memory consumption, and extending battery life for portable user experiences in low-power handheld devices. Additionally, implement a dynamic vocabulary adaptation mechanism and conduct thorough evaluation using BLEU and human evaluation metrics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"The project aims to create a real-time, energy-efficient machine translation application for Raspberry Pi. Focus is on minimizing latency, optimizing memory, maximizing battery life for handheld devices, and includes a dynamic vocabulary adaptation mechanism.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"human evaluation score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": \"real-time\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": \"optimized for Raspberry Pi\",\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"battery life impact\",\n                \"value\": \"maximized for portable devices\",\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TED Talks corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"diverse dataset\",\n                \"language pairs\": \"multilingual\"\n            },\n            \"description\": \"A large multilingual corpus of TED Talks for training and evaluating the machine translation model.\",\n            \"preprocessing\": [\n                \"adaptation for Raspberry Pi\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Real-time, Energy-efficient MT\",\n            \"family\": \"neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"optimized for Raspberry Pi\",\n                \"hardware considerations\": \"suitable for low-power devices\"\n            },\n            \"description\": \"A model designed specifically for real-time machine translation on Raspberry Pi, leveraging dynamic vocabulary adaptation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"dynamic vocabulary adaptation\"\n        ],\n        \"target_device\": \"Raspberry Pi\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time processing\"\n        ],\n        \"demonstration\": \"portable user experience\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's advanced-level expertise, and details the project's requirements for machine translation on Raspberry Pi. It highlights real-time performance, energy efficiency, latency, memory optimization, and battery life extension. BLEU and human evaluation scores are included as performance metrics. TED Talks corpus is selected for its multilingual nature, and suitable adaptations are incorporated in the preprocessing stage. The model type and architecture are chosen for the specific use case.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Utilize the interneuron and projection neuron subclasses from the comprehensive Mouse Brain Connectivity Atlas, constructing a GWNN model with multiple graph wavelet layers for fine-grained node classification. Emphasize on differentiating specific neuronal subpopulations with exceptional accuracy, maintaining F1-score balance, and ensuring scalability for large-scale, high-resolution brain graph data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"The task involves using the interneuron and projection neuron subclasses from the Mouse Brain Connectivity Atlas to develop a Graph Wavelet Neural Network (GWNN) model. The model should have multiple graph wavelet layers for fine-grained node classification, targeting exceptional accuracy, balanced F1-score, and scalability for large-scale, high-resolution brain graph data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high\",\n                \"unit\": \"graph size\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Connectivity Atlas - Interneurons & Projection Neurons\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"neuronal subpopulations\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"total_count\": null,\n                    \"highest_resolution\": null\n                },\n                \"edges\": {\n                    \"total_count\": null,\n                    \"density\": null\n                }\n            },\n            \"description\": \"Dataset containing neuronal subpopulations classified into interneurons and projection neurons, for fine-grained analysis in the Mouse Brain Connectivity Atlas.\",\n            \"preprocessing\": [\n                \"sub-classification based on given subclasses\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GWNN (Graph Wavelet Neural Network)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"layers\": \"multiple graph wavelet layers\",\n                \"scalability_techniques\": [\"efficient data structures\", \"distributed computing\"]\n            },\n            \"description\": \"A model designed for node classification with a focus on differentiating specific neuronal subpopulations with high accuracy, balanced F1-score, and optimized for large-scale, high-resolution brain graph data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Mouse Brain Connectivity Atlas provides comprehensive information on neuronal interactions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"custom feature extraction for neuronal subpopulations\"\n        ],\n        \"target_device\": \"cloud or high-performance computing\",\n        \"deployment_endpoint\": \"brain graph data processing service\",\n        \"inference_engine\": [\"efficient parallelization algorithms\"],\n        \"demonstration\": \"visual comparison of model performance on neuronal subpopulations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the instruction, including the requirement for a GWNN model, specific graph dataset, and emphasis on accuracy and scalability. The user's high expertise level is also reflected. Key details, such as the use of interneuron and projection neuron subclasses, are incorporated in the dataset and model descriptions. Missing values in performance metrics allow for optimization as per the task's specifications.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for a cutting-edge mobile application, your team is tasked with designing an ultra-efficient and compact Question-Answering model, specifically based on the TinyBERT core. The NewsQA dataset should be extensively pre-processed to include nested complex questions and diverse linguistic styles. The model must not only exhibit exceptional performance, targeting above 90% F1 score and precision, but also ensure near real-time responsiveness with a stringent target of less than 300 milliseconds latency, while optimizing for limited memory footprint on handheld devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"mobile application\",\n        \"description\": \"The project involves designing an ultra-efficient and compact Question-Answering model based on TinyBERT, specifically targeting complex questions and diverse linguistic styles in the NewsQA dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\",\n                \"optimization_target\": \"less than 300\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NewsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"preprocessing\": [\n                    \"nested complex question handling\",\n                    \"diverse linguistic styles\"\n                ]\n            },\n            \"description\": \"A dataset that requires extensive pre-processing for complex questions and different linguistic styles, specifically adapted for TinyBERT model.\",\n            \"preprocessing\": [\n                \"nested complex question handling\",\n                \"diverse linguistic style transformation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT\",\n            \"family\": \"Transformer-based Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_goal\": \"above 90%\",\n                \"real_time_performance\": true\n            },\n            \"description\": \"A lightweight yet powerful model designed for real-time question answering with TinyBERT core, optimized for handheld devices.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"handheld devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time, less than 300 milliseconds\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure mirrors the given instruction accurately, including the high-level intent, task area, performance metrics, and complexity targets. The NewsQA dataset is specified for pre-processing and the TinyBERT model is selected for its efficiency. Optimization for near real-time response and limited memory footprint on handheld devices is also captured in the service details. Confidence score is high due to the detailed mapping of the user's requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "Design a cutting-edge fusion model for advanced multimodal forgery detection in legal certificates, utilizing the LegalDocTamper dataset. Emphasize on fusing pixel-level analysis of watermarks with deep learning text scrutiny, targeting at least 95% precision. Ensure a straightforward, yet intuitive GUI for laypersons, necessitating interpretable explanations for complex results.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"legal\",\n        \"description\": \"The task is to design a fusion model for detecting forgeries in legal certificates, specifically focusing on fusing pixel-level analysis of watermarks with text scrutiny using deep learning. The goal is to achieve at least 95% precision and include a user-friendly GUI for laypersons, with interpretable explanations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LegalDocTamper dataset\",\n            \"modality\": [\"image\", \"text\"],\n            \"target_variables\": [\"forgery detection\"],\n            \"specification\": {\n                \"image_data\": {\n                    \"watermark_analysis\": true\n                },\n                \"text_data\": {\n                    \"deep_text_scrutiny\": true\n                }\n            },\n            \"description\": \"A dataset designed for multimodal forgery detection in legal certificates, including pixel-level watermark analysis and deep learning-based text scrutiny.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Fusion Model (pixel-level watermarks & deep text scrutiny)\",\n            \"family\": \"Advanced Fusion Architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true\n            },\n            \"description\": \"A cutting-edge model designed for multimodal forgery detection in legal certificates, combining pixel-level watermark analysis and deep learning text scrutiny, with a focus on interpretability for lay users.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"user-friendly (GUI)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"interpretability_needed\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the provided instruction, describing a high-expertise building task in the legal domain for multimodal forgery detection, with specific emphasis on model performance (95% precision) and user-friendliness with interpretability. The LegalDocTamper dataset and fusion model architecture details are included accordingly.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "Develop a real-time, location-aware hotel recommendation system using the 'Trivago Hotel Dataset v2', requiring a novel Graph Neural Network architecture with multi-scale spatial-temporal fusion and latency optimization for seamless in-app suggestions, reflecting seasonal trends and considering user preferences at an individual behavior level.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"hotel industry\",\n        \"description\": \"The task is to create a real-time, location-aware hotel recommendation system using the Trivago Hotel Dataset v2. It requires a Graph Neural Network architecture with multi-scale spatial-temporal fusion and latency optimization, capable of capturing seasonal trends and individual user preferences.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Trivago Hotel Dataset v2\",\n            \"modality\": [\"graph\", \"multimodal\"],\n            \"target_variables\": [\"hotel recommendations\"],\n            \"specification\": {\n                \"size\": null,\n                \"spatial_dimensions\": null,\n                \"temporal_dimensions\": null\n            },\n            \"description\": \"A dataset containing hotel data with location information, suitable for a recommendation system that captures spatial and temporal patterns.\",\n            \"preprocessing\": [\n                \"location feature extraction\",\n                \"seasonality normalization\"\n            ],\n            \"augmentation\": [\n                \"temporal data augmentation\"\n            ],\n            \"visualization\": [\"spatial-temporal data visualization\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Neural Network with multi-scale spatial-temporal fusion\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"novel with multi-scale fusion and latency optimization\",\n                \"performance\": \"reflecting seasonal trends and individual user behavior\"\n            },\n            \"description\": \"A GNN designed specifically for real-time hotel recommendations, integrating spatial, temporal, seasonal, and user preference data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hotel industry trends and user behavior insights are essential\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"spatial feature engineering\",\n            \"temporal feature engineering\"\n        ],\n        \"target_device\": \"mobile and in-app\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"latency-optimized engine\"],\n        \"demonstration\": \"live recommendations with location and seasonal context\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the requirements accurately, including the high-level intent for model development, details on the Trivago dataset, the graph-based architecture, and the need for performance metrics, latency optimization, and individual user preferences. The rationale also highlights the importance of industry insights and specific feature engineering steps.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning development team, our challenge is to create an advanced image classifier for a rare subtype of cutaneous malignancies, Merkells alopecia areata-related keratinocyte carcinomas (MA-KC), using a smaller, highly efficient DenseNet architecture. The HAM10000 dataset has been augmented with synthesized micrograph images to address class imbalance, particularly for MA-KC. The model must achieve a remarkable precision of 98% with a specificity of 99%, reflecting the increased importance of true negatives in this unique case due to the low prevalence. Moreover, we must implement a novel attention-based explainability method that generates pixel-level heatmaps for dermatologist review, ensuring interpretability without sacrificing performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medical imaging\",\n        \"description\": \"The task involves developing an advanced image classifier for a rare subtype of cutaneous malignancy, MA-KC, using a DenseNet architecture with efficiency. The HAM10000 dataset has been augmented with synthesized micrographs to address class imbalance, particularly focusing on MA-KC. The model's performance goal includes a precision of 98% and specificity of 99%, emphasizing interpretability via a novel attention-based explainability method.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HAM10000 Dataset (augmented for MA-KC)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"Merkells alopecia areata-related keratinocyte carcinomas\"],\n            \"specification\": {\n                \"augmented\": \"synthesized micrographs\"\n            },\n            \"description\": \"Dataset for MA-KC classification with augmented micrographs for balancing class imbalance.\",\n            \"preprocessing\": [\"image augmentation\"],\n            \"augmentation\": [\"synthetic micrograph generation\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DenseNet\",\n            \"family\": \"Efficient CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"adapted for DenseNet\"\n            },\n            \"description\": \"An advanced image classification model using DenseNet, tailored for the MA-KC subtype.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Implementing novel attention-based explainability to generate pixel-level heatmaps for interpretability.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"require pixel-level heatmaps for dermatologist review\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure is accurately reflected based on the instruction, with the specific details for a machine learning project, such as the unique medical classification task, DenseNet model, performance metrics, data augmentation, and the novel explainability method. The expertise level is set to high due to the complexity of the task and the importance of interpretability in this context.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Could you develop an advanced visual question-answering model specifically tailored for extracting and responding to inquiries about complex legal contracts and tax forms presented in high-resolution scanned images, ensuring minimal human intervention and efficient document analysis?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"legal and taxation\",\n        \"description\": \"The task is to create an advanced visual question-answering model capable of extracting and responding to complex inquiries about scanned high-resolution legal contracts and tax forms with minimal human intervention and efficient document analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"document processing speed\",\n                \"value\": null,\n                \"unit\": \"pages per second\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Legal Contract and Tax Form Scans\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A dataset of high-resolution scanned legal contracts and tax forms that will be used to train and test the model for extracting and understanding complex queries.\",\n            \"preprocessing\": [\"image segmentation\", \"OCR (Optical Character Recognition)\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Visual Question Answering Model (Legal and Tax)\",\n            \"family\": \"Neural Image Understanding\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"tailored for complex document analysis\",\n                \"dependency on human intervention\": \"minimal\",\n                \"document understanding capabilities\": \"advanced\"\n            },\n            \"description\": \"A model designed to efficiently analyze high-resolution legal contracts and tax forms, capable of extracting information and answering complex questions.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Understanding legal and tax jargon, ability to differentiate between various types of contracts and forms, high resolution image handling capabilities.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud (for efficient processing on large documents)\",\n        \"deployment_endpoint\": \"tailored API or platform for legal and tax consultation\",\n        \"inference_engine\": [\"OCR, OCR post-processing, deep learning-based language models\"],\n        \"demonstration\": \"Interactive legal and tax consultation demo\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the user's requirements accurately, reflecting the intention to build a high-expertise model for advanced VQA in legal and tax documents. Performance metrics and complexity constraints are included, as well as specific customization for high-resolution images and minimal human intervention. A tailored deployment endpoint is also specified.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Revise the DeepAR model task for monthly rainfall prediction in diverse regions by integrating predictive seasonal patterns, handling missing data with advanced imputation techniques, and mandate real-time adaption to changing climate data with an online learning protocol. Ensure high accuracy and a systematic evaluation of model performance with incremental data assimilation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate science\",\n        \"description\": \"The objective is to improve monthly rainfall prediction using a DeepAR model, incorporating predictive seasonal patterns, advanced imputation techniques for handling missing data, and a real-time learning protocol to adapt to changing climate data. The model must demonstrate high accuracy, and performance should be systematically evaluated with incremental data assimilation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"systematic evaluation\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time adaptation\",\n                \"value\": null,\n                \"unit\": \"online learning\"\n            },\n            {\n                \"name\": \"imputation technique efficiency\",\n                \"value\": null,\n                \"unit\": \"percentage of missing data handled\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Rainfall Data (monthly)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"monthly rainfall\"],\n            \"specification\": {\n                \"size\": null,\n                \"seasonality\": \"monthly\",\n                \"data gaps\": null\n            },\n            \"description\": \"Data on monthly rainfall in diverse regions with missing values and potential temporal changes.\",\n            \"preprocessing\": [\"advanced imputation\", \"seasonal pattern extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time-series visualizations\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepAR\",\n            \"family\": \"Probabilistic Time Series Forecasting\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"structure\": \"seasonal integration\",\n                \"training_method\": \"online learning\"\n            },\n            \"description\": \"A DeepAR model with enhancements for seasonality, online adaptation, and advanced imputation for monthly rainfall prediction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating historical climate data patterns and real-time adaptation to changing climate conditions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"streaming data processing\"],\n        \"demonstration\": \"incremental performance improvements on incoming data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the provided schema. It captures the user's high expertise level and their focus on improving a DeepAR model. Details such as integrating predictive seasonal patterns, advanced imputation, real-time adaptation, and incremental data assimilation are included to address the specific task requirements. Missing values in certain metrics (accuracy, evaluation) are intentionally left open, indicating they need to be provided or derived from the model's actual performance.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "The wildlife conservation task requires a specialized object detection model, efficiently built with the EfficientDet architecture, that not only demonstrates high accuracy in identifying diverse species from the iWildCam dataset but must also exhibit exceptional adaptability to low-light conditions and withstand extreme weather for prolonged battery life on resource-constrained edge devices deployed in remote, off-the-grid habitats.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"The user needs to build an object detection model, specifically using the EfficientDet architecture, for identifying diverse species from the iWildCam dataset. The model should excel in low-light conditions, be resilient to extreme weather, and support long battery life on resource-constrained edge devices for remote deployments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time under low-light conditions\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"battery life\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"model size for resource-constrained edge devices\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"adaptability to extreme weather\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"iWildCam dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species labels\"],\n            \"specification\": null,\n            \"description\": \"The dataset for object detection, containing images of diverse wildlife species to be detected under low-light conditions and various extreme weather.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientDet\",\n            \"family\": \"Object Detection Model\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A specialized model for object detection, designed to perform well in low-light conditions and withstand extreme weather, targeting resource-constrained edge devices.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Edge computing, possibly with hardware acceleration\"],\n        \"demonstration\": \"remote, off-the-grid habitat demonstration of the model\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's requirements, with a focus on a high-performance object detection model using the EfficientDet architecture for wildlife conservation. The model's adaptation to low-light conditions, extreme weather, and efficiency for edge devices is captured, as well as the need for accuracy and performance metrics. Some metric values are left unspecified as they will depend on model performance and constraints.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "The machine learning team is tasked with developing a state-of-the-art text classification system for the arXiv dataset. They must employ a hierarchical attention network (HAN) with advanced domain adaptation techniques to account for evolving scientific terminologies. The model should not only achieve an F1 score of at least 88% but also exhibit exceptional performance on nested subfields, maintain real-time updates, and demonstrate efficient resource management for large-scale data.", "resposne": "{\n  \"user\": {\n    \"intent\": \"build\",\n    \"expertise\": \"high\"\n  },\n  \"problem\": {\n    \"area\": \"natural language processing\",\n    \"downstream_task\": \"text classification\",\n    \"application_domain\": \"scientific research\",\n    \"description\": \"The task is to develop a state-of-the-art text classification system for the arXiv dataset using a hierarchical attention network (HAN) with advanced domain adaptation techniques to address evolving scientific terminologies. The model must achieve an F1 score of at least 88% and demonstrate strong performance on nested subfields, real-time updates, and efficient resource management for large-scale data.\",\n    \"performance_metrics\": [\n      {\n        \"name\": \"F1 score\",\n        \"value\": 0.88\n      }\n    ],\n    \"complexity_metrics\": []\n  },\n  \"dataset\": [\n    {\n      \"name\": \"arXiv dataset\",\n      \"modality\": [\"text\"],\n      \"target_variables\": [\"document category\"],\n      \"specification\": {\n        \"size\": \"large-scale\",\n        \"nested_subfields\": true\n      },\n      \"description\": \"A large-scale dataset for text classification, featuring advanced scientific content and nested subfields.\",\n      \"preprocessing\": [\"domain adaptation techniques for evolving terminologies\"],\n      \"augmentation\": [],\n      \"visualization\": [],\n      \"source\": \"direct-search\"\n    }\n  ],\n  \"model\": [\n    {\n      \"name\": \"Hierarchical Attention Network (HAN)\",\n      \"family\": \"Neural Networks\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"domain_adaptation\": true,\n        \"nested_subfield_performance\": true\n      },\n      \"description\": \"A state-of-the-art text classification model using HAN, designed to handle complex scientific terminologies and nested subfields.\"\n    }\n  ],\n  \"knowledge\": [],\n  \"service\": {\n    \"feature_engineering\": [\"domain adaptation\"],\n    \"target_device\": \"\",\n    \"deployment_endpoint\": \"\",\n    \"inference_engine\": [\"real-time updates\"],\n    \"demonstration\": \"efficient resource management for large-scale data\"\n  },\n  \"confidence\": {\n    \"rationale\": \"The response accurately captures the essence of the user's instruction, reflecting the need for a high-performance text classification system with HAN, domain adaptation, and specified F1 score. The arXiv dataset, large-scale nature, and real-time update requirement are well represented, as are the target engineering steps and demonstration goals. The confidence score reflects a high level of confidence, though missing specific metric values could be filled as needed.\",\n    \"score\": 0.97\n  }\n}\n"}
{"task": "text classification", "instruction": "As a specialized project manager for a cutting-edge medical sentiment analysis team, you've been tasked with tackling a unique and niche text classification challenge. Your team has been assigned the \"Psychiatric Case Notes Emotion Classifier\" project, which involves analyzing anonymized psychiatric case notes from various sources. The dataset, named \"Unfiltered Patient Insights,\" consists of extensive narratives detailing patient experiences, symptoms, and therapy sessions. Your goal is to develop a sophisticated model that not only differentiates between positive, neutral, and negative emotional tones (represented as 0 for negative, 1 for neutral, and 2 for positive), but also discerns shades of sentiment intensity. The model should be trained on encrypted data to maintain patient privacy and must achieve a state-of-the-art AUC-ROC score, with an emphasis on handling sarcasm and empathy in the notes. In addition to accuracy, monitor for empathy detection accuracy and interpretability of the model's decision-making process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"medicine/mental health\",\n        \"description\": \"The task involves developing a sophisticated text classifier for the 'Psychiatric Case Notes Emotion Classifier' project, which analyzes anonymized case notes for positive, neutral, and negative emotions with emphasis on intensity differentiation. The model must handle sarcasm and empathy, be trained on encrypted data, and target AUC-ROC score of state-of-the-art.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Empathy detection accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"model interpretability\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Unfiltered Patient Insights\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"emotion (0-2 for intensity levels)\", \"empathy\"],\n            \"specification\": {\n                \"encrypted\": true,\n                \"data_sources\": \"anonymized psychiatric case notes\"\n            },\n            \"description\": \"A dataset containing anonymized psychiatric case notes with narratives detailing patient experiences, symptoms, and therapy sessions, focusing on sentiment analysis and empathy detection.\",\n            \"preprocessing\": [\"data anonymization\", \"text tokenization\", \"lemmatization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model performance visualization\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Psychiatric Case Notes Emotion Classifier\",\n            \"family\": \"Natural Language Processing model\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A model that differentiates positive, neutral, and negative emotional tones with intensity, while detecting empathy and maintaining privacy through encryption.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Emphasis on handling sarcasm and empathy in the model's design\",\n        \"Privacy-preserving encryption for patient data\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"model interpretability techniques\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"model interpretability explanations for emotionally nuanced decision-making\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the task, reflecting a high-end project with specific requirements for sentiment analysis, data privacy, and the need for interpretability. It includes accurate categorization of the problem area, downsteam task, and application domain, as well as performance and complexity metrics that align with the given instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a highly specialized project manager for the advanced dog breed recognition app, I've stumbled upon a unique challenge. Our team must develop an AI-powered platform that not only pinpoints the breed of dogs in extreme environmental conditions, such as low light, high-resolution images, and diverse backgrounds, but also discerns rare and less common dog breeds with near-human accuracy. We have access to the 'Exotic and Diverse Dog breeds Dataset' which consists of 10,000+ images featuring endangered and less frequently photographed breeds. Our client desires not only speed but also a model with exceptional interpretability, allowing them to understand the reasoning behind breed identification. We should explore the feasibility of integrating a state-of-the-art hybrid deep learning model, like a convolutional neural network (CNN) fused with a transformer, to achieve these requirements. Can you provide a detailed roadmap for achieving this and ensuring the app runs efficiently on various devices?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal identification, wildlife conservation\",\n        \"description\": \"The project aims to develop an AI-powered platform for advanced dog breed recognition in extreme conditions, including low light, high-resolution images, and diverse backgrounds, focusing on rare and less common breeds. The dataset is the 'Exotic and Diverse Dog breeds Dataset' with 10,000+ images, and the client desires interpretability and model efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"device compatibility\",\n                \"value\": \"various devices\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Exotic and Diverse Dog breeds Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\"],\n            \"specification\": {\n                \"size\": {\n                    \"total_images\": 10000,\n                    \"unseen_breeds\": {\n                        \"quantity\": null,\n                        \"description\": \"images featuring endangered and less frequently photographed dog breeds\"\n                    }\n                },\n                \"quality\": [\n                    \"low light\",\n                    \"high-resolution\",\n                    \"diverse backgrounds\"\n                ]\n            },\n            \"description\": \"A dataset containing 10,000+ images of endangered and less common dog breeds in extreme conditions.\",\n            \"preprocessing\": [\"image enhancement for low light conditions\"],\n            \"augmentation\": [\"CNN and transformer fusion techniques\"],\n            \"visualization\": [\"model interpretability analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Convolutional Neural Network (CNN) - Transformer Hybrid\",\n            \"family\": \"neural networks\",\n            \"type\": \"hybrid\",\n            \"specification\": {\n                \"explainability\": true,\n                \"interpretability_layers\": null\n            },\n            \"description\": \"Proposed state-of-the-art model to address the challenges of breed recognition in extreme conditions, with a focus on interpretability for the client.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adaptation for model efficiency\"],\n        \"target_device\": [\n            \"smartphones\",\n            \"IoT devices\",\n            \"cloud\"\n        ],\n        \"deployment_endpoint\": {\n            \"strategy\": \"model-as-a-service\"\n        },\n        \"inference_engine\": [\"GPU accelerated\", \"interpretability-focused\"],\n        \"demonstration\": {\n            \"request/examples\": \"sample images of endangered dog breeds with explanations for breed identification\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's expertise level, the specific challenge of breed recognition in extreme conditions and rare breeds, the chosen model, and the performance and efficiency requirements. The dataset, performance metrics, and service requirements are all tailored to address the client's demands.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "In the multi-faceted Marine Ecosystem Health Monitoring project, design a cutting-edge time-series classification system employing an ensemble of Temporal Convolutional Networks (TCNs) and Long Short-Term Memory (LSTM) units. The model should accurately identify and classify subsea conditions into categories such as coral bleaching, harmful algal blooms, and extreme weather's impact on marine biodiversity, with a focus on pinpointing anomalies within the first 24 hours to mitigate potential ecological disruptions. Additionally, the system must demonstrate exceptional interpretability for marine biologists to understand the underlying predictive patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"environmental science\",\n        \"description\": \"The project is about designing a time-series classification system for marine ecosystem health monitoring, focusing on identifying events like coral bleaching, harmful algal blooms, and extreme weather impacts on biodiversity. The model must utilize an ensemble of Temporal Convolutional Networks (TCNs) and Long Short-Term Memory (LSTM) units, prioritizing anomaly detection within the first 24 hours for early intervention. Interpretability is a key requirement to facilitate understanding by marine biologists.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"anomaly detection rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"model explainability\",\n                \"value\": null,\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Marine Ecosystem Health Monitoring Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"subsea conditions\"],\n            \"specification\": {\n                \"data size\": null,\n                \"sampling rate\": null,\n                \"length\": null\n            },\n            \"description\": \"Contains time-series data capturing subsea conditions for coral bleaching, harmful algal blooms, and extreme weather impacts on marine biodiversity.\",\n            \"preprocessing\": [\"data normalization\", \"resampling\"],\n            \"augmentation\": [\"time-series data augmentation\"],\n            \"visualization\": [\"time-series visualization\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Ensemble of Temporal Convolutional Networks (TCNs) and LSTMs\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"TCN layers\": null,\n                \"LSTM layers\": null,\n                \"total parameters\": null,\n                \"FLOPs\": null\n            },\n            \"description\": \"An advanced model combining TCNs and LSTMs for robust time-series classification and anomaly detection in marine ecosystem health monitoring.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interpretability techniques like attention mechanisms or explainable AI models for marine biologists\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for time-series\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"marine-ecosystem-monitoring-api\",\n        \"inference_engine\": [\"GPU-friendly\", \"optimized for real-time processing\"],\n        \"demonstration\": \"Interactive dashboard showcasing model predictions and interpretations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's instruction, which calls for a high-end time-series classification system with ensemble models. Key details, such as the ensemble technique, target data modality, subtasks, performance indicators, and interpretability goals are reflected in the JSON. A specific ensemble of TCNs and LSTMs, as well as a high-level summary of the model specifications, are included. Some aspects, like preprocessing, are open-ended, assuming the user's requirements. The high expertise level is reflected in the choice of advanced models.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager for a specialized fine-grained sentiment analysis team, your mandate is to develop a high-precision financial event extraction system using the Finance-Twitter dataset. The objective is to categorize real-time financial tweets into subcategories like stock market trends, earnings reports, and mergers with at least 90% F1-score, particularly focusing on detecting crucial, timely, and insider information to avoid regulatory violations. Additionally, optimize for speed without compromising accuracy, ensuring a latency of less than 2 seconds for processing.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"The task is to develop a high-precision financial event extraction system using the Finance-Twitter dataset, focusing on real-time financial tweets categorization into stock market trends, earnings reports, and mergers with a target F1-score of 90% or higher. The system must detect crucial, timely, and insider information to prevent regulatory violations, and the processing latency must be less than 2 seconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 2,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Finance-Twitter\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"stock market trend\", \"earnings report\", \"merger\"],\n            \"specification\": null,\n            \"description\": \"A dataset for real-time financial tweet classification into subcategories, emphasizing crucial and timely information.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": {\n                    \"minimize\": false,\n                    \"desirable\": 0.9\n                },\n                \"latency\": {\n                    \"maximize\": false,\n                    \"desirable\": 2\n                }\n            },\n            \"description\": \"A financial event extraction system with a focus on high precision and fast processing (less than 2 seconds), targeting F1-score of 90% or higher.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Insider information detection is crucial to prevent regulatory violations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time processing environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for latency\"],\n        \"demonstration\": \"Demonstration of real-time, high-precision financial event extraction\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the project manager's requirements for developing a financial event extraction system with a target F1-score and latency. The user's high expertise level is indicated, along with key aspects like model selection, performance metrics, and system optimization. The missing model family and name are placeholders that would be filled in based on the team's choice.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, request a real-time, scalable question-answering system utilizing TriviaQA, capable of diverse question types, including multi-hop reasoning. Mandate a custom transformer model extending GPT-3, optimizing for zero-latency response under 100 milliseconds, while maintaining efficiency despite high query loads.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"The project requires a real-time, scalable, multi-hop reasoning question-answering system using TriviaQA. The system should be built on a custom transformer model extending GPT-3, prioritizing zero-latency response under 100 milliseconds and efficiency in handling high query loads.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.1\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 100,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"query load efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TriviaQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"real-time\": true,\n                \"multi-hop reasoning\": true\n            },\n            \"description\": \"A dataset for the question-answering system that supports diverse question types and multi-hop reasoning.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom GPT-3 Transformer Model\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimizer\": \"custom GPT-3 extension\",\n                \"latency optimization\": true,\n                \"FLOPs\": \"optimized for efficiency\"\n            },\n            \"description\": \"A GPT-3 inspired model for real-time, zero-latency question answering.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time, scalable environment\",\n        \"deployment_endpoint\": \"not specified\",\n        \"inference_engine\": [\"optimized for high query loads\"],\n        \"demonstration\": \"custom model showcase with low-latency response times\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response adheres to the provided schema and captures the essence of the given instruction. It highlights a build intent with the user having high expertise. The project's requirements for a scalable QA system, custom transformer model extending GPT-3, and performance metrics are clearly defined. The dataset and model specifications are relevant to the task, and the target service levels are indicated.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular classification", "instruction": "As a machine learning project manager, you are tasked with overseeing the development of an advanced tabular classification system for a confidential medical dataset. The dataset comprises over 70 encrypted health indicators, with an additional constraint on handling missing values. The team must design novel feature interactions and apply ensemble techniques for improved accuracy. The objective is not only to forecast the presence of three age-related conditions (Class) but also to identify subgroups within the target. Report both macro and weighted F1 Scores, along with a detailed interpretability analysis on the evaluation set.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The project involves creating an advanced tabular classification system for a confidential medical dataset. The data contains over 70 encrypted health indicators and addresses missing values. The focus is on feature interactions, ensemble techniques, and identifying subgroups within the target related to three age-related conditions (Class). Evaluation will involve macro and weighted F1 Scores, with interpretability analysis required on the evaluation set.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"macro F1 Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"weighted F1 Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"missing data handling\",\n                \"value\": 1.0,\n                \"unit\": \"efficiency\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Confidential Medical Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"Class\"],\n            \"specification\": {\n                \"encrypted_features\": 70,\n                \"missing_values\": true\n            },\n            \"description\": \"A large-scale medical dataset with encrypted health indicators, handling missing values is a key constraint.\",\n            \"preprocessing\": [\"handling missing values\", \"novel feature interactions\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_techniques\": [\"ensemble learning\", \"feature interaction designs\"]\n            },\n            \"description\": \"An advanced tabular classification system that includes ensemble methods for improved accuracy, particularly for the task of forecasting age-related conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Data encryption and confidentiality requirements might involve specific tools or techniques to handle encrypted data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"novel feature interactions\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Interpretability analysis on the evaluation set\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the task, targeting a high-level project with a focus on feature engineering, ensemble techniques, and interpretability. The presence of three age-related conditions and the target subgroup identification is mentioned, along with performance metrics. It accounts for the challenge of working with encrypted data and handling missing values.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "For the Amazon Academic dataset, develop a Hierarchical Graph Attention Network (HGAT) tailored to classify research paper authors. Optimize for F1 score beyond 90% while mitigating label leakage. Additionally, constrain the model's training to utilize a single V100 GPU, optimizing for convergence within 1.5 hours while monitoring early stopping.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"The task involves building a Hierarchical Graph Attention Network (HGAT) to classify research paper authors, targeting an F1 score of at least 90% with no label leakage. The model should be trained efficiently on a single V100 GPU and converge within 1.5 hours, incorporating early stopping.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"GPU usage\",\n                \"value\": 1.0,\n                \"unit\": \"V100 GPU\"\n            },\n            {\n                \"name\": \"convergence time\",\n                \"value\": 1.5,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Amazon Academic Dataset\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [\"author classification\"],\n            \"specification\": {\n                \"node_count\": null,\n                \"edge_count\": null,\n                \"embedding_dimensions\": null,\n                \"timestamp\": null\n            },\n            \"description\": \"A dataset for author classification in academic research papers, designed for the HGAT model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Graph Attention Network (HGAT)\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"custom HGAT with early stopping\",\n                \"GPU_resource\": \"V100\",\n                \"training_duration\": {\n                    \"hours\": 1.5\n                },\n                \"optimizer\": \"early stopping criteria\"\n            },\n            \"description\": \"A GNN model tailored for classifying research paper authors with high F1 score and GPU efficiency.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"V100\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-based inference\"],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the user's intent to build a model, their expertise level, and the specific requirements for the HGAT model. It includes the target dataset, performance metric (F1 score), GPU optimization, and training time constraints. Label leakage is addressed by the requirement for the model to mitigate it. The confidence score is based on a reasonable interpretation of the instruction and is set to a 0.9 indicating a clear understanding.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for a cutting-edge AI development team, your objective is to tackle a high-dimensional, real-world time-series classification challenge. Utilizing the SCP1 dataset with intricate, non-stationary dynamics (896 observations and 6 complex features), the train, validation, and test splits demand meticulous feature extraction and modeling. The task is to design an advanced model that not only forecasts labels (0-1 range) but also exhibits exceptional resilience to concept drift. Your team must optimize performance using a combination of traditional and deep learning techniques, ensuring a state-of-the-art accuracy on the fixed yet dynamically evolving sequences.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"unknown (to be filled based on SCP1 dataset details)\",\n        \"description\": \"The project involves a complex time-series classification task on the SCP1 dataset with 896 observations and 6 non-stationary features. The model should forecast labels within a 0-1 range, demonstrate resilience to concept drift, and utilize a combination of traditional and deep learning techniques for state-of-the-art accuracy on evolving sequences. Detailed task decomposition into train, validation, and test splits is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"time series forecasting\",\n                \"value\": \"896 observations\"\n            },\n            {\n                \"name\": \"concept drift resilience\",\n                \"value\": \"unknown (to be optimized)\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SCP1 dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"labels\", \"sequence data\"],\n            \"specification\": {\n                \"observations\": 896,\n                \"features\": 6,\n                \"dynamic nature\": \"non-stationary\"\n            },\n            \"description\": \"A dataset with 6 complex features and 896 observations, specifically designed for high-dimensional time-series classification with concept drift.\",\n            \"preprocessing\": [\"feature extraction\", \"concept drift handling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"combination of traditional and deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"An advanced model that forecasts labels and handles concept drift, targeting state-of-the-art accuracy for evolving time series.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"e.g., Fourier transforms, sliding window techniques\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"custom for concept drift handling\"],\n        \"demonstration\": \"demonstrate model performance on concept drift and accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response incorporates the given instruction, describing a high-dimensional time-series classification project with SCP1 dataset, emphasizing on resilience to concept drift and the mix of traditional and deep learning techniques. Performance metrics are kept open-ended with optimization potential, and the team's role as a project manager ensures detailed requirements for feature engineering and handling evolving sequences. The confidence score might be lower as some details may require more specific dataset information.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager is tasked with developing a high-performance machine translation system using the diverse WMT19 multilingual news commentary corpus. The objective is to design an advanced seq2seq model specifically tailored for Russian-English translation, with an added constraint of achieving a substantial improvement in SacreBLEU score compared to the existing benchmark, aiming for a minimum of 45. Emphasize on both translation accuracy and speed optimization while ensuring compatibility with large-scale datasets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"The project aims to develop a high-performance machine translation system using the WMT19 multilingual news commentary corpus, specifically focusing on Russian-English translation. The objective is to improve SacreBLEU score by a substantial margin (minimum 45 points) compared to the benchmark, optimizing both translation accuracy and speed, and ensuring compatibility with large-scale datasets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"SacreBLEU score (Russian-English)\",\n                \"value\": 45.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Translation speed\",\n                \"value\": null,\n                \"unit\": \"frames per second\"\n            },\n            {\n                \"name\": \"Accuracy optimization\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 Multilingual News Commentary Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text (English)\", \"translated text (Russian)\"],\n            \"specification\": {\n                \"size\": \"large-scale\",\n                \"languages\": [\"Russian\", \"English\"]\n            },\n            \"description\": \"A diverse corpus for Russian-English translation, containing multilingual news commentary data.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"tokenization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced seq2seq model\",\n            \"family\": \"Sequence-to-Sequence\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for Russian-English\",\n                \"accuracy-oriented\": true,\n                \"speed-optimized\": true\n            },\n            \"description\": \"A state-of-the-art seq2seq model designed specifically for Russian-English translation, aiming for enhanced SacreBLEU score and efficient performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Prior benchmarking results\",\n        \"Existing seq2seq limitations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"sentence alignment\",\n            \"context-aware token embeddings\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-optimized inference for speed\"\n        ],\n        \"demonstration\": \"Demonstration of improved translation quality and efficiency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project manager's task, highlighting the need for a high-performance seq2seq model, specific to Russian-English translation, targeting a SacreBLEU score improvement. It accounts for the translation accuracy, speed, and compatibility with large-scale datasets, while including knowledge requirements for the project. The confidence score may not be 1.0 due to the need for specific model parameter values or a fully defined deployment plan.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "time-series forecasting", "instruction": "Your objective is to develop a sophisticated time-series forecasting model for the Electricity dataset, considering not only past observations (INPUT_SEQ_LEN=96, INPUT_DIM=321) and predicting the subsequent sequence (PRED_SEQ_LEN=96, PRED_DIM=321), but also incorporating seasonality, trends, and exogenous factors. Aim for high accuracy with custom loss function integration, delivering competitive MSE and MAE scores while maintaining real-time responsiveness for live forecasting in the test set.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy\",\n        \"description\": \"The goal is to develop a complex time-series forecasting model for the Electricity dataset. The model must consider past observations, future predictions, seasonality, trends, and exogenous factors. It should aim for high accuracy, incorporating a custom loss function and achieve competitive MSE and MAE scores while ensuring real-time responsiveness for live forecasting.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"real-time responsiveness\",\n                \"value\": \"required\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"Sequential electrical measurements\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 321\n            },\n            \"description\": \"Dataset with input sequence length of 96 and 321 dimensional inputs, predicting 96 steps ahead with同样 dimension.\",\n            \"preprocessing\": [\n                \"seasonality analysis\",\n                \"trend decomposition\",\n                \"exogenous factors incorporation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks (e.g., recurrent or transformer for time series)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"custom_loss_function\": true\n            },\n            \"description\": \"A time-series forecasting model that incorporates past observations, future predictions, seasonality, trends, and exogenous factors.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"real-time forecasting\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response captures the given instruction, describing the objective of developing a sophisticated time-series forecasting model for the Electricity dataset. It accurately reflects the need for high accuracy, custom loss function, and real-time responsiveness. Key details about the dataset, model specifications, and preprocessing steps align with the task description.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for an advanced image recognition system, you are tasked with designing a real-time, high-performance vehicle recognition tool for large-scale parking lot surveillance. Compare COCO and Pascal VOC datasets in-depth, focusing on their suitability for vehicle classification in varied lighting and weather conditions. Additionally, research and recommend a state-of-the-art, lightweight object detection model that demonstrates exceptional accuracy on low-resolution footage, with a stringent requirement for minimal latency in real-world deployments. Provide a detailed implementation roadmap and expected performance benchmarks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"public safety/transportation\",\n        \"description\": \"The task is to design a real-time, high-performance vehicle recognition system for surveillance in large parking lots, considering the impact of varying lighting and weather conditions. A key focus is comparing the COCO and Pascal VOC datasets for vehicle classification, and researching a lightweight model with high accuracy for low-resolution footage and minimal latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": \"minimal\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time on low-resolution footage\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": \"optimized\"\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vehicle classes\"],\n            \"specification\": {\n                \"suitability\": {\n                    \"lighting_conditions\": \"varied\",\n                    \"weather_conditions\": \"unknown\"\n                }\n            },\n            \"description\": \"A dataset for vehicle classification, with a focus on real-world scenarios. May or may not meet the weather and lighting requirements.\",\n            \"preprocessing\": [\"adaptation for low-resolution and varied conditions\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Pascal VOC Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vehicle classes\"],\n            \"specification\": {\n                \"suitability\": {\n                    \"lighting_conditions\": \"varied\",\n                    \"weather_conditions\": \"unknown\"\n                }\n            },\n            \"description\": \"An additional dataset for vehicle classification, potentially with similar or different characteristics compared to COCO.\",\n            \"preprocessing\": [\"adaptation for low-resolution and varying conditions\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Lightweight Object Detection Model (State-of-the-art)\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_on_low_res\": \"exceptional\",\n                \"latency_on_real_world_deployments\": \"minimal\"\n            },\n            \"description\": \"Recommended for real-time, low-resolution footage with a focus on accuracy and latency optimization.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"model architecture optimization for low-resource devices\"],\n        \"target_device\": \"real-time edge computing devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for minimal latency\"],\n        \"demonstration\": \"including real-time vehicle detection and latency profiling\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the consultation-build intent, targeting a high-expertise user. The task involves comparing COCO and Pascal VOC datasets, suggesting a lightweight model suitable for low-resolution and real-time deployment. Performance benchmarks and implementation roadmap are left open-ended for the consultant's guidance. The confidence score reflects the clarity of the parsing while acknowledging some aspects may require further details from the user.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For our luxury electric vehicle (EV) rental platform, we need a sophisticated image segmentation algorithm that can discern fine details of EVs, including make, model, and any markings, in high-resolution photos uploaded by users. The system must also analyze environmental factors like nearby parking rules, lighting conditions, and obstruction detection. Can a team proficient in deep learning and computer vision design and implement this advanced solution?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"luxury car rental\",\n        \"description\": \"The task is to develop a sophisticated image segmentation algorithm for a luxury EV rental platform, which should accurately identify makes, models, and markings of EVs in high-resolution user-uploaded photos. Additionally, the system must be capable of analyzing environmental factors such as parking rules, lighting conditions, and obstruction detection using deep learning and computer vision.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"segmentation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"environmental factor analysis precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"environmental factor analysis recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"User-uploaded EV photos\",\n            \"modality\": [\"image\", \"multimodal\"],\n            \"target_variables\": [\"make\", \"model\", \"markings\"],\n            \"specification\": null,\n            \"description\": \"A collection of high-resolution photos uploaded by users, representing various luxury EVs with varying environmental context\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image Segmentation Model (Deep Learning-based)\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A deep learning model for accurate image segmentation and environmental analysis in the context of luxury EV rental photos\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for environmental analysis\", \"image preprocessing techniques for segmentation\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"custom EV rental platform API\",\n        \"inference_engine\": [\"GPU accelerated inference\"],\n        \"demonstration\": \"Real-time user feedback and performance visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON format accurately captures the user's intention to build a sophisticated algorithm, considering their high level of expertise. The deep learning and computer vision requirements, as well as specific tasks for image segmentation and environmental analysis, are reflected in the problem description and performance metrics. The data and model specifications reflect the nature of the task, and the 'service' section addresses potential deployment details. The confidence score reflects a high level of confidence in the JSON structure, given the clear mapping to the provided template.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager is tasked with developing a cutting-edge interactive children's educational platform, leveraging the specialized KidsQA dataset. The system necessitates the implementation of a state-of-the-art, simplified BERT variant tailored for pedagogical understanding. Crucially, the model should guarantee age-appropriate explanations and feature an intuitive, visually engaging interface designed to captivate young learners throughout their educational journey.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"The project involves designing an interactive children's educational platform using the KidsQA dataset. A simplified BERT variant is required, specifically optimized for pedagogical understanding. The model must provide age-appropriate explanations and have an engaging interface for young learners.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"runtime efficiency\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\", \"explanations\"],\n            \"specification\": {\n                \"age-appropriate\": true,\n                \"simplified_BERT_format\": true\n            },\n            \"description\": \"A specialized dataset for educational question answering, suitable for a simplified BERT model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Simplified BERT for Kids\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pedagogical_understanding\": true,\n                \"age-specific_explanations\": true\n            },\n            \"description\": \"A state-of-the-art BERT variant tailored for educational purposes, designed to provide clear and engaging learning for young users.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"title\": \"Age-appropriate\",\n            \"description\": \"The model should cater to the cognitive level of children and provide explanations that are easy to understand.\"\n        },\n        {\n            \"title\": \"Visual Interface\",\n            \"description\": \"The interface should be visually appealing and intuitive to captivate young learners during their educational experience.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"age-aware feature extraction\",\n            \"user-friendly explanation generation\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"explorable tutorials and user-friendly platform design\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the key aspects of the project, including the goal to build a simplified BERT model for pedagogical understanding, the age-appropriate design, and the focus on an interactive interface. Performance and complexity metrics are identified, as well as the specific requirements and the source dataset. The provided confidence score is high, as the information is detailed and relevant.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "As a project manager for a cutting-edge AI development team, I need a detailed analysis on the feasibility of employing state-of-the-art transformer architectures, specifically those designed for MDETR or VideoMAE, for detecting sophisticated multimodal forgeries in the highly complex and diverse Celeb-DF dataset. Expect a comprehensive evaluation that delves into transfer learning, domain adaptation, and real-world robustness, showcasing performance beyond random chance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"image and video analysis\",\n        \"description\": \"The project aims to assess the feasibility of using state-of-the-art transformer architectures, such as MDETR or VideoMAE, for detecting complex multimodal forgeries in the Celeb-DF dataset. A focus should be on transfer learning, domain adaptation, and real-world robustness, with performance aimed to surpass random chance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score (weighted or macro)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Robustness to real-world conditions\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\" forgery detection\"],\n            \"specification\": {\n                \"complexity\": \"high\",\n                \"diversity\": \"complex and diverse\"\n            },\n            \"description\": \"A challenging dataset for multimodal forgery detection, with a focus on complex and diverse scenarios.\",\n            \"preprocessing\": [\"feature extraction for both image and video modalities\"],\n            \"augmentation\": [\"domain adaptation techniques\"],\n            \"visualization\": [\"robustness analysis\"],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MDETR or VideoMAE\",\n            \"family\": \"Transformer-based architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pre-training\": \"state-of-the-art\",\n                \"architecture\": [\"transformer\", \"attention mechanisms\"]\n            },\n            \"description\": \"Consideration for models with demonstrated strength in multimodal tasks, specifically designed to handle complex input and perform well in detecting sophisticated forgeries.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art transformer architectures, MDETR and VideoMAE, are known for their effectiveness in multimodal learning tasks.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adaptation for Celeb-DF domain\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"proprietary AI platform\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"demonstration of improved performance over random chance on a diverse set of forgery examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the user's intent to consult and potentially build a model, emphasizing the high level of expertise. It reflects the need for a detailed analysis on feasibility, with a focus on transfer learning and domain adaptation. Performance metrics include accuracy and robustness to real-world conditions, and complexity metrics address inference time and memory usage. The model and dataset details align with the task, while the service section provides a realistic scenario for deployment.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager must incorporate a real-time, low-latency constraint for the U-Net model to perform precise indoor scene segmentation, ensuring efficient execution on devices with limited computational power, while maintaining high accuracy in identifying and distinguishing various furniture instances within the dataset.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"smart homes/indoor navigation\",\n        \"description\": \"The objective is to develop a real-time, low-latency U-Net model for indoor scene segmentation with high accuracy, focusing on precise identification of furniture instances in the dataset, while considering the computational constraints of devices with limited power.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\"type\": \"low\", \"unit\": \"milliseconds\"}\n            },\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": {\n                    \"computational\": null,\n                    \"memory\": null\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Indoor Scene Segmentation Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"furniture instances\"],\n            \"specification\": {\n                \"dimensions\": {\n                    \"width\": null,\n                    \"height\": null,\n                    \"depth\": null\n                },\n                \"number_of_samples\": null\n            },\n            \"description\": \"A dataset containing indoor scenes for precise segmentation of furniture instances with real-time and low-latency requirements.\",\n            \"preprocessing\": [\n                \"resize to maintain aspect ratio\",\n                \"normalization\"\n            ],\n            \"augmentation\": [\n                \"real-time data augmentation for limited computational resources\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"real-time, low-latency-friendly\",\n                \"FLOPs\": null,\n                \"parameter_count\": null\n            },\n            \"description\": \"A U-Net model with emphasis on fast and accurate indoor scene segmentation, tailored for devices with limited computational power.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"efficient feature extraction\"],\n        \"target_device\": \"devices with limited computational power (e.g., IoT, mobile devices)\",\n        \"deployment_endpoint\": \"real-time inference API\",\n        \"inference_engine\": [\"optimized for embedded or edge computing\"],\n        \"demonstration\": \"live scene segmentation demonstration with latency measurements\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure is created to match the user's requirements for a real-time U-Net model with low-latency and efficient resource consumption. The segmentation task for furniture instances, accuracy metrics, and the need for resource-efficient execution are incorporated. The dataset source and preprocessing steps are inferred based on the constraints. The confidence score is high due to the direct mapping of the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "You are tackling a specialized text-based binary classification challenge using the specialized BoolQ dataset, which consists of context passages and detailed, specific questions. Your objective is to develop a deep learning model that accurately discerns whether the answer lies within the passage (1 - inferred) or not (0), optimizing for precision and recall. Apply transfer learning techniques with state-of-the-art models and conduct extensive experimentation with different feature extraction methods for enhanced performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"The task involves a binary classification challenge on the BoolQ dataset, where the goal is to build a deep learning model to determine if the answer to the question is in the provided context passage, optimizing for precision and recall using transfer learning and feature extraction techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\n                \"inferred answer (1) or not (0)\"\n            ],\n            \"specification\": {\n                \"data_format\": \"context passage and question pairs\",\n                \"size\": \"specific size not mentioned\",\n                \"modalities\": \"text-based\"\n            },\n            \"description\": \"A specialized dataset for binary classification, containing context passages and detailed questions for text-based analysis.\",\n            \"preprocessing\": [\n                \"language normalization\",\n                \"tokenization\",\n                \"lowercasing\"\n            ],\n            \"augmentation\": [\n                \"sentence reordering\",\n                \"synonym replacement\"\n            ],\n            \"visualization\": [\n                \"confusion matrix\",\n                \"learning curves\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art deep learning model\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"transfer learning with a pre-trained model\",\n                \"feature_extraction\": \"exploration of different methods for enhanced performance\"\n            },\n            \"description\": \"A deep learning model for binary classification, using transfer learning and various feature extraction techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning improves model performance with limited data, especially in low-resource scenarios.\",\n        \"Exploring different feature extraction methods enhances model's ability to discriminate between classes.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"tf-idf\",\n            \"word embeddings\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-based inference\"\n        ],\n        \"demonstration\": \"Demonstration of precision-recall curve\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key aspects of the user's instruction, including the dataset, problem domain, performance metrics, and the model development approach (transfer learning and feature extraction experimentation). The user's high expertise level is reflected, and potential areas of exploration are noted. The confidence score is set to be high, given the thorough mapping of the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "As a highly specialized machine learning project manager, design a state-of-the-art question-answering AI for precision veterinary dermatology. Utilize the unique and niche VetaDerm dataset, which contains complex skin conditions in animals, and integrate a customized VetaBERT model with domain-specific embeddings. The system must demonstrate exceptional sensitivity to differentiate between various species-specific symptoms and treatments, while incorporating evidence from veterinary research papers for each response. In addition, ensure the model can handle encrypted patient data in compliance with AVMA ethical guidelines.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"veterinary dermatology\",\n        \"description\": \"The project involves designing a state-of-the-art question-answering AI for precision veterinary dermatology using the VetaDerm dataset and a customized VetaBERT model with domain-specific embeddings. The focus is on handling complex skin conditions in animals, differentiating between species-specific symptoms and treatments, and incorporating evidence from veterinary research papers while adhering to AVMA ethical guidelines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sensitivity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"data privacy and security\",\n                \"value\": 1.0,\n                \"unit\": \"AVMA compliant\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"VetaDerm\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"complexity\": \"complex (vet dermatology specific)\"\n            },\n            \"description\": \"A specialized dataset containing complex skin conditions in animals for the development of a precision veterinary dermatology question-answering AI.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"VetaBERT\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"customized with domain-specific embeddings\",\n                \"integration\": \"VetaBERT with veterinary dermatology knowledge\"\n            },\n            \"description\": \"A state-of-the-art model for veterinary dermatology question answering, utilizing domain-specific embeddings and research evidence.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"name\": \"Incorporating veterinary research\",\n            \"content\": \"The model will include evidence from veterinary research papers in its responses to support precision and accuracy.\"\n        },\n        {\n            \"name\": \"AVMA ethical guidelines\",\n            \"content\": \"Model is designed to handle encrypted patient data in compliance with the American Veterinary Medical Association's ethical standards.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"requirements\": \"Demonstrate sensitivity to species-specific symptoms, evidence integration from research papers, and adherence to AVMA guidelines\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the detailed instruction, focusing on a build task with high expertise. It covers the specific area, task, domain, data requirements, model selection, and integration of domain knowledge. The metrics and handling of data privacy are in line with the given scenario.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the enhanced PlantVillage project, mandate the attention-based DenseNet model to not only segment leaves for disease detection but also distinguish between multiple symptom types. Implement a hybrid loss function that combines cross-entropy and Dice coefficient for improved performance. Provide users with a real-time, interactive GUI to visually interpret the segmentation maps alongside a detailed heatmap analysis, ensuring ease of understanding for non-experts with limited computational resources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"agriculture or healthcare\",\n        \"description\": \"The task involves developing an enhanced version of the PlantVillage project using an attention-based DenseNet model. The goal is to segment leaves for disease detection and differentiate between multiple symptom types. A hybrid loss function combining cross-entropy and Dice coefficient is required for improved performance. The final solution should include a real-time, interactive GUI for easy interpretation by non-experts with limited computational resources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Dice coefficient\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PlantVillage (enhanced)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"leaf images for disease and symptom type segmentation\"],\n            \"specification\": {\n                \"image dimensions\": null,\n                \"number of classes\": null\n            },\n            \"description\": \"A dataset for leaf segmentation and disease/symptom differentiation using DenseNet.\",\n            \"preprocessing\": [\n                \"image resizing\",\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\n                \"random cropping\",\n                \"color jittering\"\n            ],\n            \"visualization\": [\n                \"segmentation maps\",\n                \"heatmap analysis\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention-based DenseNet\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"number of layers\": null,\n                \"number of parameters\": null,\n                \"loss function\": \"Hybrid cross-entropy and Dice coefficient\"\n            },\n            \"description\": \"A model designed for leaf segmentation and multi-symptom classification in the PlantVillage project.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interactive GUI to aid non-experts with limited computational resources\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"attention mechanisms\",\n            \"loss function design\"\n        ],\n        \"target_device\": \"mobile or edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time inference\"],\n        \"demonstration\": \"interactive and visual\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the user's instruction, detailing the enhancement to the PlantVillage project, the DenseNet model modifications, the hybrid loss function, and the interactive GUI. The required specifications, such as performance metrics and complexity constraints, are clearly defined based on the task requirements. The rationale focuses on the key elements from the instruction, ensuring a high confidence score.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a skilled machine learning project manager for the specialized task of time-series forecasting, your team is tackling the intricate ILI dataset, a diverse ensemble of real-world time series that poses specific challenges. The train, validation, and test splits have been meticulously balanced for INPUT_SEQ_LEN (36 timestamps of 7-dimensional data), while requiring a forecast of PRED_SEQ_LEN (24 steps ahead) using 7 distinct features. Your objective is to develop a cutting-edge model demonstrating exceptional predictive accuracy using Mean Squared Error (MSE) and Mean Absolute Error (MAE) as key performance indicators, while ensuring robustness across varying patterns in the data. Delve into the complexities of seasonality, trend analysis, and short-term dynamics to optimize the model's performance on the predefined sequence lengths.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The team is working on a time-series forecasting project using the ILI dataset, with a focus on balancing train, validation, and test splits for INPUT_SEQ_LEN (36 timestamps with 7-dimensional data), and forecasting PRED_SEQ_LEN (24 steps ahead) across 7 distinct features. Key performance indicators are Mean Squared Error (MSE) and Mean Absolute Error (MAE). The model must handle seasonality, trend analysis, and short-term dynamics for exceptional performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 36,\n                \"DIMENSIONS\": 7,\n                \"PRED_SEQ_LEN\": 24\n            },\n            \"description\": \"A diverse ensemble of real-world time series data with specific challenges related to seasonality, trend, and short-term dynamics.\",\n            \"preprocessing\": [\n                \"seasonality analysis\",\n                \"trend extraction\",\n                \"feature engineering for short-term dynamics\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks (RNN or Transformer-based)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"robustness_to_variations\": \"addressed\"\n            },\n            \"description\": \"A cutting-edge time-series forecasting model designed to handle seasonality, trend, and short-term dynamics with high accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"resampling\",\n            \"windowing\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-optimized\"\n        ],\n        \"demonstration\": \"demonstration of model's predictive performance and robustness on varying patterns\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes key elements based on the instruction, such as the user's expertise, dataset specifics, task requirements (forecasting sequence lengths and performance metrics), and the need to address seasonality and trend analysis. The missing model name and description are placeholders, as per the schema. The confidence score is high due to the thoroughness and accuracy of the provided information.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "image classification", "instruction": "Develop a robust, multi-label image recognition system using the latest Vision Transformer architecture, which not only differentiates between gender but also discerns subtle age groups, handling various lighting conditions and facial expressions for increased accuracy in classifying a diverse set of real-world facial images.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"biometrics\",\n        \"description\": \"The user is looking to develop a robust image recognition system using the latest Vision Transformer architecture. The focus is on gender and subtle age group classification, with the ability to handle diverse lighting conditions and facial expressions for accurate classification of real-world facial images.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"multilabel precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"multilabel recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Real-world Facial Images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"gender\", \"subtle age groups\"],\n            \"specification\": {\n                \"dimensions\": \"variable\",\n                \"illumination\": \"various\",\n                \"expression Diversity\": \"high\"\n            },\n            \"description\": \"A dataset containing diverse real-world facial images with challenging lighting conditions, different facial expressions, and a focus on accurately capturing gender and subtle age groups.\",\n            \"preprocessing\": [\"image normalization\", \"data augmentation (GAN-based)\"],\n            \"augmentation\": [\"GAN-based augmentation for lighting and facial expression variability\"],\n            \"visualization\": [\"facial feature analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Vision Transformer (latest architecture)\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specific details on the Vision Transformer architecture for multi-label classification\",\n                \"performance under varying conditions\": \"emphasis on handling diverse lighting and facial expressions\"\n            },\n            \"description\": \"The state-of-the-art architecture chosen for its strength in image recognition, particularly in handling complex tasks such as multi-label gender and age group classification.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Latest advancements in self-attention mechanism in Vision Transformers\"\n        },\n        {\n            \"description\": \"Best practices for training transformer models with fine-grained classification tasks\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"extracting facial landmarks for feature representation\"],\n        \"target_device\": \"cloud or high-performance server\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient transformers for real-time processing\"],\n        \"demonstration\": \"interactive demonstrations showcasing performance across various scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's intent to build a robust multi-label image recognition system with the latest Vision Transformer. It includes the expert-level expertise, a detailed description of the task, and performance requirements. The dataset and preprocessing steps are specified, along with relevant knowledge and deployment aspects. The rationale highlights that the template accurately captures the complexity of the task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular classification", "instruction": "For a high-precision tabular credit risk analysis project, the machine learning team must develop a predictive model using historical customer data from niche financial institutions. Analyze unique seasonal trends, fine-grained demographic features, and perform extensive feature interactions. Optimize models with ensemble techniques and report ROC-AUC for the confidential non-Transactional Credit card default dataset.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"The task is to develop a high-precision predictive model for credit risk analysis using historical customer data from niche financial institutions. Special attention should be given to unique seasonal trends, fine-grained demographic features, and feature interactions. Ensemble techniques must be employed, and the model performance should be evaluated with ROC-AUC on the non-Transactional Credit card default dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"ROC-AUC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Non-Transactional Credit Card Default Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"credit default\"],\n            \"specification\": {\n                \"seasonality\": \"unique\",\n                \"demographic_granularity\": \"fine\",\n                \"feature_interactions\": \"extensive\"\n            },\n            \"description\": \"A dataset with historical customer data from niche financial institutions, including detailed seasonal patterns, demographic features, and extensive feature interactions for credit risk analysis.\",\n            \"preprocessing\": [\"seasonality analysis\", \"feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A predictive model to analyze credit risk with a focus on high precision using ensemble techniques from niche financial data.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"unique seasonal trends\", \"fine-grained demographics\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Credit risk analysis model demonstration with precision focus\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the given instruction, capturing the high-precision goal, analysis of specific trends, demographic features, and ensemble techniques. The provided performance metric (ROC-AUC) and the requirements for preprocessing and feature interactions are also well-represented. The knowledge of high expertise level is captured, and the rationale provides an explanation of the adaptations made.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a cutting-edge RNN-based machine translation model that exclusively utilizes the diverse and challenging OPUS dataset for English to Russian conversions, focusing on colloquial and highly informal conversations. The model must exhibit exceptional proficiency in translating slang, idiomatic expressions, and domain-specific terminologies, aiming for a BLEU score of 35 or higher. However, in addition to precision, prioritize real-time performance, with a stringent latency constraint of 250 milliseconds per sentence, ensuring seamless and immediate user interaction in a high-traffic multilingual platform.", "resposne": "{\n  \"user\": {\n    \"intent\": \"build\",\n    \"expertise\": \"high\"\n  },\n  \"problem\": {\n    \"area\": \"natural language processing\",\n    \"downstream_task\": \"machine translation\",\n    \"application_domain\": \"multilingual communication\",\n    \"description\": \"The task is to develop an RNN-based machine translation model specifically for English to Russian conversions, targeting colloquial and informal conversations. The model must excel in translating slang, idiomatic expressions, and domain-specific terminologies with a target BLEU score of 35 or higher. A focus on real-time performance is crucial, with a latency constraint of 250 milliseconds per sentence.\",\n    \"performance_metrics\": [\n      {\n        \"name\": \"BLEU score\",\n        \"value\": 35.0\n      }\n    ],\n    \"complexity_metrics\": [\n      {\n        \"name\": \"latency\",\n        \"value\": 0.25,\n        \"unit\": \"seconds\"\n      }\n    ]\n  },\n  \"dataset\": [\n    {\n      \"name\": \"OPUS dataset (English to Russian)\",\n      \"modality\": [\"text\"],\n      \"target_variables\": [\"translated text\"],\n      \"specification\": null,\n      \"description\": \"A diverse and challenging dataset for English to Russian machine translation, focusing on informal conversations, slang, idiomatic expressions, and domain-specific terminologies.\",\n      \"preprocessing\": [],\n      \"augmentation\": [],\n      \"visualization\": [],\n      \"source\": \"direct-search\"\n    }\n  ],\n  \"model\": [\n    {\n      \"name\": \"RNN-based Machine Translation Model (English to Russian)\",\n      \"family\": \"Recurrent Neural Networks\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"real-time performance\": {\n          \"latency\": 0.250,\n          \"units\": \"seconds\"\n        }\n      },\n      \"description\": \"A specialized model designed to handle colloquial and informal English to Russian translation with a focus on slang, idiomatic expressions, and domain-specific terminology.\"\n    }\n  ],\n  \"knowledge\": [],\n  \"service\": {\n    \"feature_engineering\": [],\n    \"target_device\": \"\",\n    \"deployment_endpoint\": \"\",\n    \"inference_engine\": [\"optimized for real-time\"],\n    \"demonstration\": \" Seamless and immediate translation on a high-traffic multilingual platform.\"\n  },\n  \"confidence\": {\n    \"rationale\": \"The JSON structure accurately captures the user's instruction, specifying a 'build' intent for a high-expertise level user. It focuses on an RNN-based machine translation model using the OPUS dataset for English to Russian conversions, with specific requirements for BLEU score and latency. The response covers relevant performance metrics and dataset details, along with the platform's interaction expectations. The rationale and confidence score reflect the thorough mapping of the instruction.\",\n    \"score\": 0.98\n  }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, you need to devise a comprehensive multilingual QA solution utilizing mBERT or XLM-R. Mandate a minimum of five languages, focusing on English, Spanish, Mandarin, and two under-researched dialects. The task demands not only excellent performance in standard languages but also substantial improvement in low-resource situations, necessitating the model's adaptability and precision under scarcity of data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"linguistics and artificial intelligence\",\n        \"description\": \"The project aims to create a comprehensive multilingual QA solution using mBERT or XLM-R. It should support at least five languages, with a focus on English, Spanish, Mandarin, and two under-researched dialects. The model should excel in standard languages and demonstrate significant improvement in low-resource scenarios by promoting adaptability and precision under data scarcity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall (for low-resource dialects)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision (for low-resource dialects)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multilingual QA Corpus (with emphasis on selected languages)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\"],\n            \"specification\": {\n                \"languages\": [\n                    \"English\",\n                    \"Spanish\",\n                    \"Mandarin\",\n                    {\"name\": \"Under-researched dialect 1\", \"description\": \"Low-resource\"},\n                    {\"name\": \"Under-researched dialect 2\", \"description\": \"Low-resource\"}\n                ]\n            },\n            \"description\": \"A dataset consisting of multilingual QA pairs, especially tailored for the given languages and their dialects. Emphasis on low-resource instances for evaluating adaptability.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"mBERT or XLM-R\",\n            \"family\": \"Transformer-based multilingual model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multilingual, adaptable\",\n                \"language support\": \"at least 5 languages, including low-resource dialects\"\n            },\n            \"description\": \"A state-of-the-art multilingual QA model chosen for its adaptability and precision under low-resource scenarios.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"model's performance under varying data scarcity conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the project's requirements, including the use of mBERT or XLM-R, focus on specific languages, adaptability, and precision for low-resource dialects. Performance metrics for standard and low-resource languages are incorporated. However, missing values in performance metrics indicate they will be determined during the model development.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "link prediction", "instruction": "In the context of a highly specialized professional network, design a state-of-the-art link prediction model utilizing Heterogeneous Graph Attention Network (HGAN) with Temporal Fusion GAT (TF-GAT) for LinkedIn. Focus on enhancing the model's precision in identifying not only explicit connections but also hidden industry collaborations and subtle professional influences, all while considering real-time user behavior patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"professional networking\",\n        \"description\": \"The task is to develop a state-of-the-art link prediction model using Heterogeneous Graph Attention Network (HGAN) with Temporal Fusion GAT (TF-GAT) specifically for LinkedIn, with a focus on enhancing precision in identifying explicit and hidden industry collaborations, as well as subtle professional influences, while considering real-time user behavior patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LinkedIn Graph\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"connection\"],\n            \"specification\": {\n                \"node_types\": [\"users\", \"organizations\", \"projects\", \"jobs\"],\n                \"edge_types\": [\"direct connections\", \"indirect collaborations\", \"professional influences\"],\n                \"timestamp_coverage\": \"real-time user behavior\"\n            },\n            \"description\": \"A heterogeneous graph containing LinkedIn's user, organization, project, and job information, capturing explicit and implicit relationships for link prediction.\",\n            \"preprocessing\": [\"node feature extraction\", \"edge type categorization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Heterogeneous Graph Attention Network (HGAN) with Temporal Fusion GAT (TF-GAT)\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"HGAN + TF-GAT\",\n                \"focus\": \"precision in link prediction\"\n            },\n            \"description\": \"A state-of-the-art model designed to improve link prediction, focusing on both explicit and hidden connections within the LinkedIn professional network.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time user behavior patterns play a significant role in enhancing link prediction accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"node embedding\", \"time-aware edge representation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"visualizing enhanced link predictions and the impact of real-time user behavior\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been constructed based on the provided instruction, capturing the user's intent to build a model, their high expertise level, the task's focus on LinkedIn's graph data, and the requirements of HGAN with TF-GAT for link prediction. Performance metrics, complexity considerations, and specific preprocessing steps have been incorporated to make the response comprehensive and relevant.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design an advanced sequence-to-sequence neural machine translation model leveraging the specialized OPUS-XL dataset, which exclusively features colloquial and non-standard English from online forums, gaming chats, and social media. The model should demonstrate exceptional aptitude in translating subculturally nuanced slang, intricate idiomatic expressions, and domain-specific tech jargon, targeting a competitivecased Fisher's Exact Test-adjusted Translation Error Rate (TER) below 20% for improved fluency. Prioritize not only accuracy but also expedited real-world performance, with an ultra-tight latency restriction of 100 milliseconds per sentence, to guarantee smooth and instantaneous translation for a massive, multilingual platform coping with millions of simultaneous users.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual platforms\",\n        \"description\": \"The task is to design an advanced sequence-to-sequence neural machine translation model that specifically focuses on colloquial and non-standard English from various online sources. The model should excel in translating slang, idiomatic expressions, and domain-specific tech jargon. The goal is to achieve a competitive Fisher's Exact Test-adjusted Translation Error Rate (TER) of below 20% for high fluency. Additionally, it is crucial to prioritize real-world performance with an ultra-tight latency constraint of 100 milliseconds per sentence to ensure seamless, instantaneous translation for a platform handling millions of simultaneous users.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Fisher's Exact Test-adjusted Translation Error Rate\",\n                \"value\": 0.20\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency per sentence\",\n                \"value\": 0.100,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS-XL\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": null,\n            \"specification\": {\n                \"description\": \"A specialized dataset featuring colloquial and non-standard English from forums, gaming chats, and social media.\"\n            },\n            \"description\": \"Dataset for training and testing the model, focused on online informal English.\",\n            \"preprocessing\": [\n                \"cleaning\",\n                \"tokenization\",\n                \"domain-specific data augmentation\"\n            ],\n            \"augmentation\": [\n                \"noise injection\",\n                \"back-translation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Sequence-to-Sequence\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer\",\n                \"attention_types\": [\"multi-head attention\"],\n                \"layers\": 12,\n                \"attention_heads\": 8,\n                \"embedding_size\": 512,\n                \"hidden_size\": 512,\n                \"optimizer\": \"AdamW\",\n                \"learning_rate\": 0.0001\n            },\n            \"description\": \"An advanced model targeting colloquial and non-standard English translation with strong performance and low latency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider training with transfer learning from a general English dataset to improve out-of-domain handling.\",\n        \"Utilize domain adaptation techniques to adapt to tech jargon.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware pre-processing\",\n            \"post-processing for fluency optimization\"\n        ],\n        \"target_device\": \"low-latency hardware acceleration\",\n        \"deployment_endpoint\": \"dedicated translation API server\",\n        \"inference_engine\": [\"GPU-accelerated inference\"],\n        \"demonstration\": \"Online evaluation through A/B testing\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the user's instruction, capturing the need for a high-performance machine translation model with domain adaptation and tight latency constraints. The dataset specification and model architecture are well-matched to the task requirements, and the performance metrics, complexity metrics, and knowledge points have been translated into the JSON format.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular classification", "instruction": "As a stringent project for credit risk assessment, the ML team is mandated to design an advanced tabular classifier. They must handle intricate temporal patterns across quarterly variations, delve into micro-geographic demographics, execute extensive feature interactions on encrypted data. In addition to using ensemble methods, require the model to outperform baselines by at least 2% in AUC-PR, and maintain interpretability while safeguarding sensitive information.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"The project aims to develop an advanced tabular classifier for credit risk assessment, focusing on temporal patterns in quarterly data, micro-geographic demographics, and feature interactions. The model must utilize ensemble methods, outperform baselines by at least 2% in AUC-PR, and maintain interpretability while ensuring data privacy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-PR\",\n                \"value\": 0.02\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"credit risk\"],\n            \"specification\": {\n                \"temporal_data\": true,\n                \"micro-geographic_dimensions\": true,\n                \"encrypted_data\": true\n            },\n            \"description\": \"The dataset contains temporal patterns across quarterly variations and micro-geographic demographics, encrypted for security.\",\n            \"preprocessing\": [\"handling temporal data\", \"feature interaction extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Ensemble\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"interpretability\": true,\n                \"data_sensitivity\": \"protected\"\n            },\n            \"description\": \"Advanced model designed to handle credit risk assessment, focusing on interpretability and outperforming baselines by at least 2% in AUC-PR.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"interpretability and sensitivity to sensitive data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure closely matches the user's project requirements, detailing the advanced tabular classifier for credit risk assessment, ensemble methods, performance improvement, and focus on interpretability and data privacy. Key aspects, like temporal data handling, ensemble model family, and the AUC-PR improvement target, have been incorporated into the JSON response.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager must develop an efficient, real-time U-Net architecture with pixel-level accuracy, catering to the constraints of low-latency (<10ms) and minimal resource usage. The model should excel in discerning fine-grained differences between diverse furniture types in a large-scale, high-resolution indoor scene dataset, ensuring optimal performance on resource-constrained edge devices without compromising on speed or accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"smart home or interior design\",\n        \"description\": \"The project aims to develop an efficient U-Net architecture for real-time, low-latency (<10ms) and resource-light furniture type segmentation in high-resolution indoor scenes, targeting edge devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean Intersection over Union (IoU)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\"comparison\": \"<\", \"value\": 10, \"unit\": \"milliseconds\"}\n            },\n            {\n                \"name\": \"resource usage\",\n                \"value\": {\"comparison\": \"minimized\", \"unit\": \"resource consumption\"}\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\"comparison\": \"<\", \"value\": 10, \"unit\": \"milliseconds\"}\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": {\"comparison\": \"minimized\", \"unit\": \"MB or similar\"}\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High-resolution Indoor Scene Furniture Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"resolution\": \"high\",\n                \"scene diversity\": \"large-scale\",\n                \"furniture types\": \"diverse\"\n            },\n            \"description\": \"A dataset containing high-resolution indoor scenes with emphasis on fine-grained differentiation between various furniture types.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net\",\n            \"family\": \"Semantic Segmentation Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"efficient and lightweight\",\n                \"pixel-level accuracy\": \"required\"\n            },\n            \"description\": \"An efficient U-Net designed for real-time, low-latency, and resource-constrained scenarios with emphasis on furniture type segmentation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for resource-constrained environments\"\n        ],\n        \"demonstration\": {\n            \"requirements\": \"combines speed and accuracy on edge devices for real-time furniture segmentation\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON represents the project's instructions with high-level details. The user's high expertise level is acknowledged, the project's specific requirements (U-Net, low latency, resource constraints, and high-resolution indoor scenes) are captured, and performance metrics, such as latency, accuracy, and resource usage, are included. The dataset, model, and service components are all adapted to the context provided.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager must now design a revolutionary children's learning ecosystem that not only integrates the exclusive KidsQA dataset but also demands the creation of a novel, energy-efficient BERT adaptation with interactive attentions. This platform must provide real-time, context-specific explanations, adhere to strict age-appropriate content guidelines, and showcase a visually stimulating, touch-friendly interface that fosters problem-solving through gamified lessons, all while ensuring seamless scalability and accessibility for diverse socio-economic backgrounds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"The project aims to create a learning ecosystem that integrates the exclusive KidsQA dataset, demands the development of a novel energy-efficient BERT adaptation with interactive attentions, and provides context-specific explanations in real-time. It must adhere to age-appropriate content guidelines, feature a visually stimulating, touch-friendly interface, and promote problem-solving through gamified lessons. Scalability and accessibility for diverse socio-economic backgrounds are key requirements.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"energy consumption per inference\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"system capacity under load\"\n            },\n            {\n                \"name\": \"accessibility\",\n                \"value\": null,\n                \"unit\": \"number of diverse users supported\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\"],\n            \"specification\": {\n                \"type\": \"question answering\",\n                \"content_type\": \"children-friendly\"\n            },\n            \"description\": \"An exclusive dataset for children's question answering with age-appropriate content.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT adaptation with Interactive Attention\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"energy_efficiency\": null,\n                \"customizations\": \"BERT + interactive attentions\"\n            },\n            \"description\": \"A novel model to handle context-specific explanations, targeting KidsQA dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Age-appropriate content, touch-friendly design, and gamification principles for problem-solving.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-aware explanations\", \"interactive attentions\"],\n        \"target_device\": [\"mobile, cloud\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"type\": \"gamified lessons, real-time explanations\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON represents the given instruction by focusing on building a learning ecosystem for children, involving the KidsQA dataset and a novel BERT adaptation with interactive attentions. Age-appropriate content, interactive nature, and accessibility are emphasized in the response. However, specific performance metrics for energy efficiency, scalability, and accessibility are left to be determined, reflecting the need for further information from the user.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "For the Amazon Academic dataset, design a novel Hierarchical Graph Attention Network (HGAT) incorporating multi-level node embeddings and attention mechanisms. Target an F1 score of over 95% by implementing advanced feature selection techniques to address class imbalance. Ensure the model's training on a single V100 GPU is performed with customized batch scheduling, limiting to 1.3 hours and employing early stopping with a custom scheduler, stricter evaluation intervals, and a validation patience of five epochs.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"recommendation systems\",\n        \"description\": \"The task involves designing a novel Hierarchical Graph Attention Network (HGAT) with multi-level node embeddings and attention mechanisms for the Amazon Academic dataset. The aim is to achieve an F1 score of over 95% while addressing class imbalance with advanced feature selection techniques. The model should be trained on a single V100 GPU with specific constraints: customized batch scheduling within 1.3 hours, early stopping with a custom scheduler, stricter evaluation intervals, and a validation patience of five epochs.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": 1.3,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Amazon Academic dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"nodes\": {\n                    \"type\": \"multi-level\",\n                    \"embedding_type\": \"node embeddings\",\n                    \"imbalance_type\": \"class imbalance\"\n                }\n            },\n            \"description\": \"A graph dataset for academic research, where the focus is on node classification using Hierarchical Graph Attention Network.\",\n            \"preprocessing\": [\"multi-level node embeddings\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Graph Attention Network (HGAT)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"customized HGAT with multi-level embeddings and attention\",\n                \"GPU\": \"V100\",\n                \"GPU_memory_limit\": \"custom batch scheduling\",\n                \"train_duration\": {\n                    \"unit\": \"hours\",\n                    \"value\": 1.3\n                },\n                \"early_stopping\": {\n                    \"scheduler\": \"custom\",\n                    \"evaluation_interval\": \"stricter\",\n                    \"validation_patience\": 5\n                }\n            },\n            \"description\": \"A novel model design for the Amazon Academic dataset with a focus on node classification and addressing class imbalance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"advanced feature selection\"],\n        \"target_device\": \"single V100 GPU\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON accurately reflects the user's instruction to design a HGAT model for the Amazon Academic dataset with specific performance and training constraints. It covers the user's expertise level, task details, and constraints on the model and training process. The F1 score and training time metrics are appropriately included, and the missing values are placeholders for custom implementation.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular classification", "instruction": "As a seasoned project manager for a cutting-edge AI medical research initiative, you are now mandated to supervise the creation of a highly sophisticated tabular classification system. The extensive, 100+ encrypted health indicator dataset, with a requirement for imputation strategies for rare missing values, demands innovative feature interaction engineering and advanced ensemble techniques. The team must not only aim to classify four distinct age-related conditions (Class) but also discern subpopulations and their respective response patterns. In addition to delivering precision metrics (macro and weighted F1 Scores), prepare a comprehensive explainable AI report detailing feature importance, model robustness, and a visualization of decision boundaries on the securely isolated validation dataset, ensuring regulatory compliance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The project involves creating a sophisticated tabular classification system for an extensive, encrypted health indicator dataset. The system must handle rare missing values through imputation strategies, incorporate innovative feature interaction engineering, and utilize advanced ensemble techniques. The main classification task is to categorize four age-related conditions, while also identifying subpopulations and their response patterns. Performance metrics include macro and weighted F1 Scores, and the final deliverables must include an explainable AI report, model robustness, and decision boundary visualization on a secure validation dataset, adhering to regulatory guidelines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"macro F1 Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"weighted F1 Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"feature interaction engineering complexity\",\n                \"value\": null,\n                \"unit\": \"custom complexity\"\n            },\n            {\n                \"name\": \"ensemble technique complexity\",\n                \"value\": null,\n                \"unit\": \"custom complexity\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"100+ encrypted health indicator dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"Class\", \"subpopulations\"],\n            \"specification\": {\n                \"size\": {\"rows\": null, \"columns\": null},\n                \"missing_values\": {\n                    \"count\": null,\n                    \"imputation_strategy\": \"custom for rare values\"\n                }\n            },\n            \"description\": \"A large, encrypted dataset with health indicators for tabular classification, requiring imputation for rare missing values.\",\n            \"preprocessing\": [\"feature interaction engineering\", \"imputation\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"model robustness\",\n                \"decision boundaries\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Sophisticated Ensemble Classifier\",\n            \"family\": \"Advanced Machine Learning\",\n            \"type\": \"ensemble models\",\n            \"specification\": null,\n            \"description\": \"A model that employs innovative strategies for classifying age-related conditions and discerning subpopulations in a healthcare setting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Regulatory compliance and explainability are critical aspects of the project.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Visualization of decision boundaries and feature importance on secure validation dataset\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON format captures the essence of the given task, emphasizing the complexity and requirements of the project. It highlights the need for imputation and advanced techniques, performance metrics, explainable AI, and regulatory compliance. However, specific model parameters and feature complexity values are left open for further assessment based on the user's expertise.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "time-series classification", "instruction": "For the recently emerging project, \"Quantum OceanogeoTemporal Analytics (QOTA) Suite,\" develop an avant-garde time-series classification architecture that fuses innovative Quantum-Inspired Neural Networks (QINNs) with Hierarchical Attention Mechanisms (HAMs) for the real-time detection of rare marine phenomema in the deep sea. The system should distinguish between cryptic events like undersea volcanic eruptions, abyssal plumes, and ultra-precise migration patterns of deep-sea creatures, enabling researchers to anticipate and mitigate submarine noise pollution within a nanosecond time window. Moreover, the model must showcase Quantum Explainable AI (QXAI) capabilities to present findings in a visually intelligible format for oceanographers to unravel the enigmatic ocean dynamics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"oceanography and marine science\",\n        \"description\": \"The project involves developing a cutting-edge time-series classification model for the QOTA Suite, using Quantum-Inspired Neural Networks (QINNs) and Hierarchical Attention Mechanisms (HAMs) to detect rare marine phenomena in the deep sea. The model should distinguish between undersea volcanic eruptions, abyssal plumes, and deep-sea creatures' migration patterns with real-time precision, while addressing nanosecond time windows for noise pollution management. Additionally, it must showcase Quantum Explainable AI (QXAI) for clear visual representation of results for oceanographers.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"nanosecond response time\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"nanoseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Quantum OceanogeoTemporal Analytics Suite Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"marine phenomena indicators\"],\n            \"specification\": null,\n            \"description\": \"Dataset containing time-series data for rare marine events and environmental parameters for training and validation, with focus on deep sea scenarios.\",\n            \"preprocessing\": [\"data normalization\", \"resampling\"],\n            \"augmentation\": [\"none (real-time data)\"],\n            \"visualization\": [\"QXAI-friendly plots\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-Inspired Neural Network with Hierarchical Attention Mechanisms (QINN+HAM)\",\n            \"family\": \"Quantum-Inspired AI\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"quantum-inspired components\": {},\n                \"attention heads\": null,\n                \"explainability\": \"Quantum Explainable AI (QXAI)\"\n            },\n            \"description\": \"A state-of-the-art model combining QINNs and HAMs for real-time deep sea time-series classification with QXAI capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Undersea volcanic eruptions: Surface & subsurface seismic data patterns\",\n        \"Abyssal plumes: Temporal changes in water column properties\",\n        \"Deep-sea creatures' migration patterns: Bioluminescence patterns, changes in depth, and water temperature variations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"preprocessing for noise reduction\"],\n        \"target_device\": \"real-time deployment in a quantum-inspired compute environment\",\n        \"deployment_endpoint\": \"data centers or edge devices with quantum-inspired processing\",\n        \"inference_engine\": [\"optimized for QINNs and HAMs\"],\n        \"demonstration\": \"Interactive visualizations for oceanographers\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response reflects the specified project requirements by incorporating QINNs and HAMs for time-series classification, addressing real-time detection and QXAI. It accounts for relevant expertise and task nuances such as distinguishing rare marine events and handling nanosecond response times. The dataset and model elements are tailored based on the instruction and the choice of Quantum-Inspired Neural Networks fits the technology frontier. The missing metric values indicate room for collaboration or further details.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "Develop a next-generation, resource-efficient object detection model, specifically tailored for the iWildCam biodiversity dataset, utilizing the Enhanced EfficientDet architecture. It must excel in recognizing intricate species under challenging low-light scenarios, tolerate rugged terrain/weather, and ensure minimum energy consumption for prolonged, autonomous operation in remote, off-the-grid wildlife sanctuaries with limited connectivity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"The objective is to develop an Enhanced EfficientDet-based object detection model for the iWildCam biodiversity dataset. The model should perform well in recognizing intricate species under low-light and challenging conditions, be resilient to rugged terrain/weather, and prioritize energy efficiency for autonomous, off-the-grid operation in wildlife sanctuaries.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mAP (mean average precision)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"energy efficiency (FLOPs/Wh)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"energy consumption during operation\",\n                \"value\": null,\n                \"unit\": \"Wh\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"iWildCam Biodiversity Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species labels\"],\n            \"specification\": {\n                \"images with low-light conditions\": \"present\",\n                \"rugged terrain/weather samples\": \"required\",\n                \"specific wildlife locations\": \"remote, off-the-grid wildlife sanctuaries\"\n            },\n            \"description\": \"A dataset designed for object detection, with a focus on intricate species recognition in challenging conditions for wildlife conservation applications.\",\n            \"preprocessing\": [\"image enhancement for low-light conditions\", \"geo-tagging for terrain/weather adjustments\"],\n            \"augmentation\": [\"geometric transformations,天气-related augmentations\"],\n            \"visualization\": [\"inference results on various dataset examples\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced EfficientDet\",\n            \"family\": \"Object Detection Architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficient FLOPs\": true,\n                \"optimized for low-light scenarios\": true,\n                \"energy efficiency\": true\n            },\n            \"description\": \"A next-generation object detection model designed for iWildCam dataset, targeting low-light, rugged, and energy efficiency requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"incorporates advancements in object detection algorithms, specifically for wildlife surveillance\",\n        \"exploits techniques for real-world performance in challenging conditions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"lightening techniques, terrain-adaptive features\"],\n        \"target_device\": \"optimized for autonomous, off-the-grid operation\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for energy-efficient inference\"],\n        \"demonstration\": \"showcasing model performance in low-light and rugged conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the given instruction, specifying a custom object detection model for iWildCam, its features, and performance requirements. The user's high expertise level is acknowledged, and relevant performance metrics and complexity metrics are included.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "Incorporate a novel, memory-efficient adaptation of TinyBERT, designed for resource-constrained mobile devices, into the project. The NewsQA dataset must undergo extensive curation to incorporate not just straightforward inquiries, but a labyrinth of interrelated, context-dependent questions with various linguistic complexities. The resulting QA model should excel in accuracy, achieving an F1 score above 93% and precision of 95%, without sacrificing latency - aim for an average response time of less than 250 milliseconds. The overall goal is to deliver a seamless, user-centric experience despite the demanding computational constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"news and media\",\n        \"description\": \"The project involves integrating a memory-efficient TinyBERT adaptation for mobile devices into a QA model. The NewsQA dataset needs extensive curation to include complex, context-dependent questions. The target is to achieve high accuracy with an F1 score above 93% and precision of 95%, while maintaining a latency of less than 250 milliseconds for a user-centric experience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency (response time)\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NewsQA Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\"],\n            \"specification\": {\n                \"content\": \"complex, context-dependent questions\",\n                \"updated\": \"yes\"\n            },\n            \"description\": \"A dataset to be curated for extensive question-answering, incorporating interrelated questions and linguistic complexities.\",\n            \"preprocessing\": [\n                \"extensive curation for complexity and context\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Memory-Efficient TinyBERT\",\n            \"family\": \"Transformers (BERT)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"target_device\": \"mobile devices\",\n                \"computational_constraints\": \"resource-constrained\"\n            },\n            \"description\": \"A TinyBERT adaptation optimized for accurate and latency-sensitive QA on mobile devices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"A specialized TinyBERT for mobile scenarios and linguistic complexity\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware feature extraction\",\n            \"efficient QA model design\"\n        ],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient inference algorithms\"],\n        \"demonstration\": {\n            \"focus\": \"seamless user experience\",\n            \"constraints\": \"low latency\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON reflects the user's intent to build a QA model, incorporating specific requirements such as TinyBERT adaptation, curation of NewsQA dataset, and performance targets. The user's high expertise level is acknowledged, and the additional complexities of the project, like linguistic complexities and computational constraints, are captured. Missing information would be filling in specific numbers for F1 score, latency, or additional details on model deployment.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for the cutting-edge Factory Safety Monitoring System, assign the complex assignment: Implement the COCO and Industry4.0-specific safety dataset for comprehensive model training. Design a state-of-the-art, anchor-free object detection network specifically tailored for identifying and classifying safety gear compliance and a wide range of precise trip hazard categories with minimum accuracy of 97%. Demanding a progressive methodology that includes quarterly benchmark evaluations, subtask achievements for gear recognition (90%) and hazard detection (92%) separately, and a comprehensive analysis of the model's adaptability across diverse industrial environments with varying lighting and background conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"industrial safety\",\n        \"description\": \"The task involves implementing a comprehensive model training for the Factory Safety Monitoring System using the COCO and Industry4.0-specific safety dataset. A state-of-the-art, anchor-free object detection network must be designed to identify and classify safety gear compliance and diverse trip hazard categories, targeting minimum accuracy of 97%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"overall accuracy\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"gear recognition accuracy\",\n                \"value\": 0.90\n            },\n            {\n                \"name\": \"hazard detection accuracy\",\n                \"value\": 0.92\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO and Industry4.0-specific safety dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"safety gear compliance\", \"trip hazard categories\"],\n            \"specification\": {\n                \"size\": \"large and diverse\",\n                \"lighting conditions\": [\"varied\"],\n                \"background conditions\": \"varied\"\n            },\n            \"description\": \"A dataset for training the model, covering diverse industrial environments with varying lighting and background conditions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom anchor-free object detection network\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"adaptability\": {\n                    \"industrial environments\": true,\n                    \"varying conditions\": true\n                }\n            },\n            \"description\": \"A model specifically designed for identifying safety gear compliance and hazard detection with high accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge device\",\n        \"deployment_endpoint\": \"factory monitoring platform\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"quarterly model benchmark evaluations and detailed analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the project manager's requirements. The task is accurately represented with emphasis on a high-level expert user. Performance metrics for gear recognition and hazard detection have been separated, and the adaptability across diverse environments is a key aspect. Benchmarks and subtask achievements are also specified.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project manager, your team is mandated to elevate the cutting-edge time-series forecasting platform for the Weather dataset, now necessitating the adept handling of non-stationary fluctuations and external influences. Develop a multifaceted model incorporating autoregressive models, moving averages, and advanced feature engineering techniques. With a broader sequence length (INPUT_SEQ_LEN=192, expanded due to long-range dependencies), an expanded feature set (INPUT_DIM=35), and extra exogenous factors, the model must predict a longer forecast horizon (PRED_SEQ_LEN=192) while simultaneously optimizing for both root mean squared error (RMSE) and mean absolute percentage error (MAPE), demonstrating a statistically significant minimum 7% improvement over the benchmark. Additionally, ensure compliance with a stricter real-time response latency constraint of 300 milliseconds for end-user predictions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"weather prediction\",\n        \"description\": \"The project involves enhancing the time-series forecasting platform for a weather dataset, focusing on non-stationary fluctuations and external influences. The model should incorporate autoregressive models, moving averages, and advanced feature engineering. The model must predict a longer forecast horizon (192 steps), consider a broader sequence length (192), and include an expanded feature set (35 dimensions) with additional exogenous factors. Performance metrics include RMSE and MAPE, aiming for a minimum 7% improvement over the benchmark. Real-time response latency should not exceed 300 milliseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.3,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Weather Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"forecast\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 192,\n                \"INPUT_DIM\": 35,\n                \"PRED_SEQ_LEN\": 192\n            },\n            \"description\": \"A time-series dataset for weather forecasting, featuring non-stationary fluctuations, external influences, and an expanded feature set.\",\n            \"preprocessing\": [\"feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Multifaceted time-series forecasting model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"autoregressive models\", \"moving averages\", \"advanced feature engineering\"],\n                \"sequence_length\": 192,\n                \"feature_set\": 35,\n                \"forecast_horizon\": 192\n            },\n            \"description\": \"A model using autoregressive models, moving averages, and advanced techniques, designed to handle non-stationary fluctuations and external influences in the weather dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": true,\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"response latency of 300 ms\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON structure closely captures the requirements of the task, which is to build a time-series forecasting model with specific performance, complexity, and feature requirements. The benchmark comparison and latency constraint are also included in the JSON response.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "machine translation", "instruction": "In the context of our groundbreaking project, we seek to propel advancements in machine translation by meticulously harnessing the colossal Common Crawl corpus. Our challenge is to architect a state-of-the-art M2M-100 model, specifically tailored for translating idiomatic English into Japanese, ensuring a remarkable 98% precision in capturing colloquial slang and cultural nuances, reflecting an intricate grasp of linguistic and societal intricacies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics and translation\",\n        \"description\": \"The project aims to develop a state-of-the-art M2M-100 model for translating idiomatic English into Japanese, focusing on achieving a high precision of 98% for capturing colloquial slang and cultural nuances.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"colossal\",\n                \"linguistic_coverage\": \"wide, including idiomatic English\"\n            },\n            \"description\": \"A large-scale corpus from Common Crawl corpus, specifically designed for training the machine translation model with a focus on English to Japanese translation.\",\n            \"preprocessing\": [\"data cleaning, normalization, and tokenization\"],\n            \"augmentation\": [\"back-translation, synthetic data generation\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M2M-100\",\n            \"family\": \"Transformer-based Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for English to Japanese translation\",\n                \"linguistic_complexity\": \"focus on colloquial slang and cultural nuances\"\n            },\n            \"description\": \"A cutting-edge model for machine translation designed specifically for the given task with a strong emphasis on capturing intricate linguistic and societal aspects.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Context of the project as groundbreaking, challenging idiomatic English and capturing Japanese nuances\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"User-requested demonstration of translated idiomatic English to Japanese sentences with preserved nuances.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON represents the user's instruction accurately, detailing a high-expertise project to build an M2M-100 model for English to Japanese translation. Metrics are set to meet the specific precision requirement, and the source and preprocessing steps align with the source material, Common Crawl corpus. The model's focus on idiomatic slang and cultural nuances is encapsulated in the description and model specification.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "For my cutting-edge doctoral research, I must develop a high-resolution image classifier that discerns subatomic cosmic phenomena, extreme nebulae, and ancient quasars with precision. Can you recommend an unexplored, large-scale dataset sourced from the European Space Agency's Gaia archive, while considering the challenge of classifying minute variances in spectral signatures and the computational hurdles posed by deep learning on terabytes of high-resolution astrophysical images?", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The user requires a cutting-edge image classifier for subatomic cosmic phenomena, extreme nebulae, and ancient quasars with high precision. The challenge includes detecting minute spectral variations and managing the computational demands of deep learning on large, high-resolution astrophysical image data from the European Space Agency's Gaia archive.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision per class\",\n                \"value\": []\n            },\n            {\n                \"name\": \"recall per class\",\n                \"value\": []\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"European Space Agency's Gaia archive dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"spectral signatures\", \"astrophysical categories\"],\n            \"specification\": {\n                \"scale\": \"large\",\n                \"image_resolution\": \"high\",\n                \"source\": \"Gaia\"\n            },\n            \"description\": \"A large-scale dataset from the Gaia archive, specifically designed for subatomic cosmic classification, handling minute spectral variations and terabytes of data\",\n            \"preprocessing\": [\n                \"spectral signature extraction\",\n                \"image normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"spectral signature visualization\",\n                \"data distribution plots\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Proposed unexplored neural network architecture\",\n            \"family\": \"Convolutional Neural Network (CNN) or Attention-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"specifics\": \"designed for high-resolution, detailed analysis, and efficient handling of large-scale data\",\n                \"optimization techniques\": [\"transfer learning\", \"data augmentation\"]\n            },\n            \"description\": \"A deep learning model that addresses challenges of classifying minute spectral differences and computational complexity.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Unexplored deep learning techniques for handling large-scale astronomy data\",\n        \"Understanding of Gaia data structure and relevant spectral features\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction from spectral data\", \"custom loss functions for precision and recall\"],\n        \"target_device\": \"Cloud-based or specialized high-performance computing cluster\",\n        \"deployment_endpoint\": \"Cloud-based or specialized astrophysical data platform\",\n        \"inference_engine\": [\"Distributed computing frameworks like TensorFlow, Apache Spark\"],\n        \"demonstration\": \"Interactive visualizations and performance reports\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the high-level requirements, emphasizing the complexity of data and the target task. It includes unexplored datasets and models, while also addressing computational and performance challenges. The high expertise level is reflected, and suggestions for feature engineering and service requirements are inline with the given task.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager for a groundbreaking text classification initiative, you've been given the challenge of developing an avant-garde neural network model for the sophisticated analysis of customer sentiment in the high-end luxury travel industry. With a mesmerizing dataset at your disposal, consisting of exclusive reviews from first-class experiences with luxury airlines like Emirates, chauffeur-driven airport transfers, and personalized in-flight services, your aim is to create a model that deciphers sophisticated lexical patterns while maintaining utmost accuracy and sensitivity. To elevate the model beyond conventional techniques, you must integrate quantum-inspired learning, emoji translation for hidden emotions, and sentiment shifting for cultural nuances. The unique constraint here is to minimize the use of computational resources while ensuring the model exhibits state-of-the-art F1 score and adaptability to ultra-specific feedback contexts, such as luxury suite amenities, gourmet dining, and global VIP lounge access. Emphasize on devising a resource-efficient system with a focus on interpretability and explainability, ultimately delivering a model that leaves a lasting impression on discerning clientele and airline partners.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"luxury travel industry\",\n        \"description\": \"The task is to develop a neural network model for sentiment analysis of exclusive luxury travel reviews, specifically focusing on customer sentiment towards Emirates, chauffeur-driven transfers, in-flight services, luxury suite amenities, gourmet dining, and VIP lounge access. The model should integrate quantum-inspired learning, emoji translation, sentiment shifting, and be resource-efficient with a focus on interpretability and explainability, targeting high F1 score and adaptability to ultra-specific feedback contexts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational resources\",\n                \"value\": null,\n                \"unit\": \"CPU cycles or memory usage\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Luxury Travel Reviews Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"customer sentiment\"],\n            \"specification\": {\n                \"size\": \"large\",\n                \"specific_features\": [\"exclusive reviews\", \"first-class experiences\", \"quantum-inspired lexicon\"]\n            },\n            \"description\": \"A dataset of exclusive luxury travel reviews containing sentiments about Emirates, airport transfers, in-flight services, and other amenities.\",\n            \"preprocessing\": [\"emoji translation\", \"sentiment shifting\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks\",\n            \"type\": \"quantum-inspired\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"explainability\": \"high\",\n                \"resource_efficiency\": true\n            },\n            \"description\": \"An avant-garde neural network model designed for text classification, with quantum-inspired learning, emoji translation, and sentiment shifting, targeting state-of-the-art F1 score and adaptability to luxury travel industry contexts.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integrate quantum-inspired learning principles\",\n        \"Handle emoji translation for hidden emotions\",\n        \"Consider sentiment shifting for cultural nuances\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"resource-efficient\", \"interpretable\", \"explainable\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"ultra-specific contexts and explainability for clients and airline partners\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's intent and their high level of expertise. The problem description reflects the specific luxury travel sentiment analysis requirements, performance metrics (F1 score), and the focus on resource efficiency and interpretability. The dataset and model are tailored to address the unique constraints of the task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Requesting a comprehensive, multi-phased approach for the development of a custom machine learning system. This should involve:\r\n\r\n1. An extensive analysis of various contemporary and advanced deep learning architectures (e.g., U-Net, Mask R-CNN, and MRF-based methods) suitable for high-resolution, multi-layered, and culturally diverse artwork segmentation.\r\n2. A rigorous preprocessing pipeline to handle the diversity of parchment materials, lighting conditions, and fading ink patterns in the historical manuscripts.\r\n3. The design of a tailored loss function that integrates both precision in line drawings and preservation of historical context, while accounting for the fine-grained visual nuances.\r\n4. Implementation of a robust transfer learning strategy, utilizing pre-trained models on large-scale datasets, followed by fine-tuning on our institution's proprietary collection.\r\n5. Development of a user-friendly interface for curators to interact with and visualize the segmentation results, incorporating interpretability techniques for the non-technical audience.\r\n\r\nPlease present a detailed project plan, milestones, and expected performance benchmarks for this complex, archeological artifact preservation project.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"art preservation and archaeology\",\n        \"description\": \"The project aims to develop a custom machine learning system for segmenting high-resolution, multi-layered, and culturally diverse artwork in historical manuscripts. It requires analyzing advanced deep learning architectures, preprocessing techniques, a specialized loss function, transfer learning, and an interpretable user interface.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"historical context preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Institutional proprietary manuscript collection\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"layers\": \"multi\",\n                \"diversity\": \"cultural\"\n            },\n            \"description\": \"Manuscripts with high-resolution images, multi-layered artwork, and cultural diversity, requiring special preprocessing.\",\n            \"preprocessing\": [\"parchment material handling\", \"lighting correction\", \"ink pattern enhancement\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom deep learning model (U-Net, Mask R-CNN, or MRF-based)\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"customized for high-resolution and historical context\",\n                \"loss_function\": \"in development (combines precision and historical context)\"\n            },\n            \"description\": \"Advanced deep learning architecture selected for its ability to handle complex artwork in manuscripts.\"\n        },\n        {\n            \"name\": \"Pre-trained models (e.g., imagenet, artistic style transfer)\",\n            \"type\": \"transfer learning\",\n            \"specification\": \"Fine-tuning on the institution's proprietary collection\",\n            \"description\": \"Utilization of pre-trained models to leverage large-scale data before adapting to the specific manuscript dataset.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"artifact-specific feature extraction\", \"feature fusion\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"user_interface\": \"interpretable for non-technical users, with segmentation and context preservation visualization\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the multi-phased approach as requested. It includes high-level analysis of advanced architectures, specific preprocessing pipeline, tailored loss function, transfer learning, and a user-friendly interface. Performance metrics are kept open-ended to account for the project's complexity and the need for trial and error optimization.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a seasoned precision agriculture consultant, your development team must enhance the existing image classification model. The new requirement calls for not only real-time stress monitoring of crops across diverse species, but also the ability to detect nuanced variations in growth stages, distinguish between subtle weather patterns, and classify distinct pathogen or pest strains. The algorithm must now achieve super-accuracy, scale seamlessly with enormous, high-resolution datasets, and guarantee real-time analysis, all while operating within stringent energy constraints for ubiquitous deployment in the field.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"The task involves enhancing an existing image classification model for real-time stress monitoring of diverse crops, detecting growth stage variations, subtle weather patterns, and distinguishing between different pathogen or pest strains. The model must achieve super-accuracy, scale to large datasets, enable real-time analysis, and operate efficiently under energy constraints for field deployment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy consumption\",\n                \"value\": null,\n                \"unit\": \"watts\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high\",\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Existing Crop Monitoring Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"crop stress\", \"growth stage\", \"weather pattern\", \"pathogen/pest strain\"],\n            \"specification\": {\n                \"size\": {\n                    \"total\": null,\n                    \"high-resolution\": true\n                },\n                \"dimension\": {\n                    \"image\": \"variable\"\n                },\n                \"length\": null,\n                \"time_series\": {\n                    \"frequency\": \"real-time\"\n                }\n            },\n            \"description\": \"A large, high-resolution dataset of crops for the enhanced model, including diverse scenarios for stress monitoring and other specified variations.\",\n            \"preprocessing\": [\n                \"data augmentation\",\n                \"feature extraction\"\n            ],\n            \"augmentation\": [\"real-time\"],\n            \"visualization\": [\n                \"large-scale analysis\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Image Classification Model\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"super-accuracy\",\n                \"energy efficiency\": {\n                    \"inference time\": null,\n                    \"memory footprint\": null\n                },\n                \"hardware compatibility\": \"optimized for field deployment\"\n            },\n            \"description\": \"An advanced model to classify crop stress, growth stages, weather patterns, and pathogen/pest strains with super-accuracy, efficient resource usage, and real-time performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Experience with high-resolution datasets and real-time analysis in precision agriculture\",\n        \"Best practices for energy-efficient algorithms in resource-constrained environments\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time stress monitoring features\"],\n        \"target_device\": \"energy-constrained devices\",\n        \"deployment_endpoint\": \"field-level IoT network\",\n        \"inference_engine\": [\"real-time, lightweight engines\"],\n        \"demonstration\": \"field demonstration and农民培训\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been constructed based on the provided instruction, capturing the new requirements for enhanced image classification and the specific performance criteria in precision agriculture. The template has been adjusted to reflect the nuances of the task, targeting high expertise level, dealing with real-time and high-resolution data, and emphasizing energy constraints.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Task: Employ the OpenSubtitles50K dataset for a cutting-edge, multilingual translation model, focusing on English to Cantonese conversion. Emphasize accuracy of translating vernacular slang, idioms, and domain-specific jargon, ensuring a minimum CIDEr-D metric of 0.9 for in-depth evaluation. Test the model's resilience across various subcategories like movie, TV shows, and anime subtitles, while maintaining a benchmark F1 score above 97%.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics\",\n        \"description\": \"The objective is to develop a multilingual translation model, specifically focusing on English to Cantonese conversion. Attention is given to translating slang, idioms, and domain-specific jargon with a CIDEr-D metric target of at least 0.9 for thorough evaluation. The model should handle various subcategories like movie, TV show, and anime subtitles, while maintaining an F1 score above 97%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"CIDEr-D\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles50K\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"English\", \"Cantonese translation\"],\n            \"specification\": {\n                \"subcategories\": [\"movies\", \"TV shows\", \"anime\"],\n                \"data_distribution\": {\n                    \"English\": \"balanced\",\n                    \"Cantonese\": \"balanced\"\n                }\n            },\n            \"description\": \"A multilingual dataset for translation tasks, primarily targeting English to Cantonese conversion, with a focus on slang, idioms, and domain-specific jargon.\",\n            \"preprocessing\": [\"tokenizer\", \"lemmatization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Multilingual Transformer (e.g., mBART or XLM-Transformer)\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer\",\n                \"precision\": \"state-of-the-art\",\n                \"translated_language_pair\": \"English to Cantonese\"\n            },\n            \"description\": \"A cutting-edge model chosen for its ability to handle multilingual tasks and translate vernacular content effectively.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"name\": \"Domain-specific jargon and slang lexicons\",\n            \"description\": \"To be incorporated into the model's training, helping with slang, idioms, and domain-specific terms.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"subcategory-specific tuning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Translation examples for various subcategories\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the user's instruction. The area, task, dataset (specifically emphasizing subcategory diversity and quality of data), model type, and performance metrics (CIDEr-D and F1) are included. The expert level and specific knowledge are highlighted, and slight variations in the dataset and model specification cater to the requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular classification", "instruction": "Could you please guide our machine learning team in constructing a highly sophisticated, multi-layered tabular classification model? The model should not only predict student academic success but also discern the complex interplay between attendance patterns, diverse extracurricular engagement, specific standardized test scores, and long-term academic motivation, necessitating a thorough, stepwise analysis?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"education\",\n        \"description\": \"The objective is to develop a sophisticated multi-layered model for student academic success prediction. It should consider attendance patterns, extracurricular engagement, standardized test scores, and long-term academic motivation, demanding a detailed analysis and possibly multiple-step modeling.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Student Performance Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"academic success\"],\n            \"specification\": {\n                \"rows\": null,\n                \"columns\": {\n                    \"attendance\": {},\n                    \"extracurriculars\": {},\n                    \"standardized_scores\": {},\n                    \"academic_motivation\": {}\n                },\n                \"length\": null\n            },\n            \"description\": \"A dataset containing information about students' attendance patterns, extracurricular activities, standardized test scores, and long-term academic motivation for the model to analyze.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Multi-layered Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"layers\": \"Multi-layer architecture including (but not limited to) hidden layers for complex interactions, and potentially decision trees to handle stepwise analysis\",\n                \"parameters\": \"To be determined based on the team's expertise and performance requirements\"\n            },\n            \"description\": \"A sophisticated model that captures the intricate relationship between attendance, extracurriculars, test scores, and long-term motivation for student academic success prediction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Team should consider feature engineering to handle diverse data types and extract meaningful representations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"handling categorical data\", \"handling time-series data (if present)\", \"correlation analysis\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Illustrative examples and performance visualization for each step of the modeling process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's request for a multi-layered tabular classification model, accommodating the specified factors and the need for a stepwise analysis. The expertise level is set to 'high' given the sophistication of the task. The choice of performance metrics and preprocessing steps reflect a realistic expectation for a complex project. The confidence score is not extremely high (yet) due to the absence of model specifics and specific performance requirements, but the template covers the critical aspects.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "As an expert project manager for a specialized astronomy AI team, lead the design of a cutting-edge image classifier that discerns esoteric celestial entities like lambda cepheid variables, gravitationally lensed quasars, and microquasars with sub-1% precision. Utilize advanced CNNs, like ResNet50, EfficientNet-B7, and Xception fine-tuned on the extensive DeepSpaceCelestialExtremes dataset, accounting for subtle brightness shifts and time-variable features. Demand a system that guarantees F1-score > 0.995, precision > 0.99, recall > 0.99, and AUC-ROC above 0.998 in real-time, while implementing a rigorous data augmentation plan involving Gaussian noise, geometric warping, and transfer learning. Implement a protocol for iterative validation using 5-fold cross-validation and continuous monitoring with live sky surveys, ensuring ongoing performance improvement with active feature retraining and drift detection.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"The task is to design a cutting-edge image classifier for esoteric celestial entities such as lambda cepheid variables, gravitationally lensed quasars, and microquasars. It should achieve sub-1% precision using advanced CNNs (ResNet50, EfficientNet-B7, and Xception), fine-tuned on the DeepSpaceCelestialExtremes dataset, while accounting for subtle brightness shifts and time-variable features. The classifier must meet performance metrics of F1-score > 0.995, precision > 0.99, recall > 0.99, and AUC-ROC > 0.998 in real-time. A rigorous data augmentation plan involving Gaussian noise, geometric warping, and transfer learning is required. The solution should incorporate iterative validation with 5-fold cross-validation and continuous monitoring through live sky surveys, facilitating ongoing performance improvement and feature retraining.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.995\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": 0.998\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepSpaceCelestialExtremes\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"esoteric celestial entities\"\n            ],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"time-variable features handling\": true\n            },\n            \"description\": \"A comprehensive dataset for fine-tuning and training, catering to specific celestial entity detection with subtle nuances.\",\n            \"preprocessing\": [\n                \"Gaussian noise injection\",\n                \"Geometric warping\",\n                \"Transfer learning\"\n            ],\n            \"augmentation\": [\n                \"Gaussian noise\",\n                \"Geometric warping\",\n                \"Transfer learning\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": [\"ResNet50\", \"EfficientNet-B7\", \"Xception\"],\n            \"family\": [\"Convolutional Neural Networks\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"number_of_layers\": [50, 7, 22],\n                \"pretrained\": true\n            },\n            \"description\": \"Advanced CNNs fine-tuned for celestial entity recognition, addressing brightness shifts and time variability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"iterative refinement\",\n            \"feature retraining\",\n            \"drift detection\"\n        ],\n        \"target_device\": \"real-time, cloud-based (for performance and scalability)\",\n        \"deployment_endpoint\": \"dedicated astronomy AI platform\",\n        \"inference_engine\": [\"Real-time processing algorithms\"],\n        \"demonstration\": {\n            \"method\": \"live sky surveys, 5-fold cross-validation\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response accurately captures the instruction, highlighting the user's high expertise level, celestial entity classification requirements, advanced CNNs, and performance criteria. It includes the necessary details about the dataset, model, and iterative validation strategy. Owing to the complexity and specificity of the task, the confidence score is set to a high level.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a highly sophisticated, multi-modal node classification model for the Coauthorship Science dataset, utilizing an enhanced ProgressiveHGAT architecture. This architecture should seamlessly merge temporal sequence encoding, a nested attention mechanism, and a dynamic collaboration network analysis. The objective now necessitates not only a granular author type classification but also a comprehensive analysis of temporal collaboration patterns, the detection of evolving subfields, and the prediction of groundbreaking interdisciplinary connections. Additionally, ensure the model's computational efficiency for large-scale datasets and provide a detailed explanation of its explainable AI components for practical understanding by domain experts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"science\",\n        \"description\": \"The project involves developing a sophisticated multi-modal node classification model using the Coauthorship Science dataset. The model should be based on an enhanced ProgressiveHGAT architecture, which integrates temporal sequence encoding, nested attention mechanism, and dynamic collaboration network analysis. The goal is to classify authors into granular types, analyze temporal collaboration patterns, detect evolving subfields, and predict interdisciplinary connections. The model should be computationally efficient for large datasets and provide explanations for domain experts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"granular author type accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"temporal collaboration patterns analysis\",\n                \"value\": null\n            },\n            {\n                \"name\": \"subfield detection accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interdisciplinary connection prediction\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"time/s\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coauthorship Science Dataset\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [\"author type\", \"collaboration events\"],\n            \"specification\": {\n                \"size\": null,\n                \"node_types\": [\"author\"],\n                \"edge_types\": [\"collaboration\", \"temporal\"]\n            },\n            \"description\": \"A multi-modal dataset capturing coauthorship relationships, including temporal sequences and collaboration patterns.\",\n            \"preprocessing\": [\"temporal feature extraction\", \"graph normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"collaboration network visualization\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ProgressiveHGAT (enhanced)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \" ProgressiveHGAT with temporal, nested attention, and dynamic collaboration components\",\n                \"parameters\": {\n                    \"node embedding size\": null,\n                    \"number of layers\": null\n                },\n                \"efficiency\": \"large-scale dataset friendly\"\n            },\n            \"description\": \"A multi-modal node classification model incorporating advanced GNN techniques for author type classification and collaboration analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Explanation of explainable AI components: temporal encoding provides context, nested attention reflects collaborative focus, dynamic network analysis detects changing trends, and model interpretability is essential for domain experts.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"multi-modal feature integration\", \"hyperparameter optimization\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom cloud platform\",\n        \"inference_engine\": [\"batch processing\", \"real-time inference\"],\n        \"demonstration\": \"Interpretable model output and collaboration pattern visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the user's intent to build a complex model for the specified problem, reflects the requested architecture, performance metrics, and computational efficiency. The domain expertise level is assumed to be high given the sophistication of the task and the requirement for explainability. Missing values for specific metrics and parameters are expected because they would depend on actual model development.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a cutting-edge AI development team, your multifaceted assignment is to design a state-of-the-art skin lesion classifier using the sophisticated ResNeSt architecture on the comprehensive DermQuest dataset, containing an exhaustive array of lesser-known dermatological cases. The classifier must not only surpass human-level accuracy with a targeted benchmark of 99.9% but also navigate the challenge of differentiating subtle variations to minimize false positives, upholding the gold standard of diagnostic clarity in remote consultations. In addition to this, architect a seamless user experience by integrating a real-time, interactive heatmap generator that elucidates model decision-making for patient comprehension, ensuring transparency and trust. Lastly, optimize the model for near-instantaneous inference on resource-constrained wearable devices operating in real-world scenarios, striking a balance between accuracy and minimal computational overhead, while catering to the stringent power constraints of these field-bound gadgets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medical, dermatology\",\n        \"description\": \"The task is to design a state-of-the-art skin lesion classifier using ResNeSt architecture on the DermQuest dataset, targeting 99.9% accuracy and differentiation of subtle variations. It should minimize false positives and have a real-time heatmap generator for transparency. The model must be optimized for resource-constrained wearable devices with near-instantaneous inference and minimal computational overhead.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.999\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"computational overhead\",\n                \"value\": \"minimal\"\n            },\n            {\n                \"name\": \"power consumption\",\n                \"value\": null,\n                \"unit\": \"watts\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DermQuest Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"skin lesion type\"],\n            \"specification\": null,\n            \"description\": \"A comprehensive dataset for skin lesion classification with lesser-known dermatological cases.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNeSt (ResNeSt Architecture)\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art, ResNeSt\",\n                \"parameters\": \"optimized for accuracy\"\n            },\n            \"description\": \"A high-performance skin lesion classifier targeting human-level accuracy and subtle variation differentiation.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time heatmap generator\"],\n        \"target_device\": \"wearable devices with resource constraints\",\n        \"deployment_endpoint\": \"remote consultations for diagnostic clarity\",\n        \"inference_engine\": [\"optimized for near-instantaneous inference\"],\n        \"demonstration\": \"patient-friendly, interactive decision-making\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has been adapted to capture all aspects of the complex task, such as high-level design, targeted accuracy, real-time heatmap, and resource optimization for wearable devices. Performance metrics and user experience elements are clearly defined, ensuring the response accurately reflects the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Develop a groundbreaking visual question-answering prototype that marries the challenging MASC dataset with a hybrid Cognitive Emotion and Temporal Reasoning (CERT) model. Focus on deciphering complex multi-lingual queries, recognizing subliminal temporal cues in historical artifact scenes, and disentangling fine-arts symbolism. Strive for a benchmark accuracy of 90% in detecting esoteric connections within avant-garde art installations, while ensuring compatibility with holographic interfaces for immersive museum navigation and virtual gallery exhibitions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"art and culture\",\n        \"description\": \"The project involves creating a visual question-answering prototype that combines the MASC dataset with a hybrid Cognitive Emotion and Temporal Reasoning (CERT) model. It focuses on handling complex multi-lingual queries, detecting subliminal temporal cues in historical artifact scenes, and interpreting fine-arts symbolism. The goal is to achieve a benchmark accuracy of 90% in detecting esoteric connections within avant-garde art installations. The system should also be compatible with holographic interfaces for immersive museum navigation and virtual gallery exhibitions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MASC dataset\",\n            \"modality\": [\"image\", \"text\"],\n            \"target_variables\": [\"answer\", \"context\"],\n            \"specification\": {\n                \"multilingual\": true,\n                \"complexity\": \"esoteric\"\n            },\n            \"description\": \"A dataset for visual question answering, challenging with complex queries, historical scenes, and fine-arts symbolism.\",\n            \"preprocessing\": [\"multilingual preparation\"],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [\"data and model interpretability\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid CERT Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"cognitive_emotion\": true,\n                \"temporal_reasoning\": true,\n                \"linguistic处理\": true\n            },\n            \"description\": \"A model integrating Cognitive Emotion and Temporal Reasoning for handling complex visual question answering tasks, targeting the MASC dataset and esoteric art connections.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Fine-tuning the model with historical and fine-art domain knowledge\",\n        \"Compatibility with holographic interfaces\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"holographic interfaces\",\n        \"deployment_endpoint\": \"virtual gallery exhibitions\",\n        \"inference_engine\": [\"holographic-aware engine\"],\n        \"demonstration\": \"immersive museum navigation and virtual exhibitions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's instructions, indicating a 'build' intent with high expertise. It includes details on the task (visual question-answering), datasets (MASC), model (Hybrid CERT), performance metric (90% accuracy), and compatibility with holographic interfaces. The provided dataset and model specifications address the specific challenges and requirements mentioned in the instruction.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "text classification", "instruction": "In your pioneering endeavor to catalyze scientific research breakthroughs, you've been commissioned to construct a cutting-edge AI solution that transcends conventional wisdom in discerning intricate motifs within the niche \"Quantum Metamaterials, Exoplanet Atmosphere Ethereal Textual Analysis\" (QMETA) corpus. The colossal dataset, consisting of 100,000 complex texts intricately woven from quantum entanglement, astrobiology, and high-energy particle dynamics, presents an unprecedented challenge with an emphasis on extracting subtle correlations amidst dense information.\r\n\r\nYour objective is to conceive an innovative neural architecture, straining for superhuman accuracy of 99.999% in precision, recall, and F1-score, by refining the Qubit-empowered Quantum Transformer (QTrans) on this elite dataset. But this feat demands deft handling of a grave imbalance in exoplanet atmosphere categories, necessitating a balance between precision and interpretability, even amidst the scarcity of data. To conquer this, propose a sophisticated fusion strategy that intertwines QTrans with niche feature extraction methodologies, followed by a tailored domain-focused adaptation using a clandestine repository of esoteric research papers. Rigorously trace the performance implications of each method employed by calculating the multi-faceted Harmonic Mean of accuracy, precision, recall, and the area under the Enhanced Discrimination Receiver Curve (E-AUC-ROC). Your submission must not only conquer benchmark scores but also illuminate the underlying mechanisms of advanced AI in fostering transparency and adaptability within the illustrious realm of interdisciplinary scientific collaboration.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"quantum science and exoplanet studies\",\n        \"description\": \"The task involves developing a cutting-edge AI solution to analyze intricate motifs in the 'Quantum Metamaterials, Exoplanet Atmosphere Ethereal Textual Analysis' corpus using a Qubit-empowered Quantum Transformer (QTrans), targeting superhuman accuracy of 99.999% in precision, recall, and F1-score. The solution must address the class imbalance in exoplanet atmosphere data, maintaining interpretability and adaptability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99999\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.99999\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.99999\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"E-AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Quantum Metamaterials, Exoplanet Atmosphere Ethereal Textual Analysis Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Quantum entanglement\", \"astrobiology\", \"high-energy particle dynamics\"],\n            \"specification\": {\n                \"size\": 100000,\n                \"imbalance_ratio\": {\n                    \"exoplanet atmosphere\": \"\"\n                },\n                \"dimensionality\": {\n                    \"word embeddings\": null,\n                    \"quantum specific features\": null\n                }\n            },\n            \"description\": \"A large dataset of complex texts intertwining quantum entanglement, astrobiology, and high-energy particle dynamics for QMETA analysis, with an emphasis on detecting subtle correlations.\",\n            \"preprocessing\": [\n                \"complex text cleaning\",\n                \"embedding extraction\"\n            ],\n            \"augmentation\": [\n                \"synthetic data generation (consider imbalance)\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Qubit-empowered Quantum Transformer (QTrans)\",\n            \"family\": \"neural architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A cutting-edge model specifically designed to handle complex multi-modal data from quantum and exoplanet research.\"\n        }\n    ],\n    \"knowledge\": [\n        \"To address imbalance, consider oversampling techniques and targeted Focal Loss adaptation.\",\n        \"Custom domain-specific fusion strategies with niche feature extraction methods.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"ensemble learning\",\n            \"domain adaptation\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-optimized for better speed\"\n        ],\n        \"demonstration\": \"Illustrate performance with interpretability layers\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the complexity and requirements of the task, including the challenging task of handling multimodal data, the need for high accuracy, and the focus on interpretability and domain adaptation. The challenge of performance metric calculation, such as Harmonic Mean, and the use of niche feature extraction methods and domain adaptation are well captured. However, some open-ended fields like 'accuracy' and 'memory usage' are not specified, leaving room for additional information or user input.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a cutting-edge neural machine translation model by merging the state-of-the-art \"Transformer-XL Enhanced\" and \"BERT-Mini Adaptive\" architectures, targeting the EuroTechX benchmark with an emphasis on zero-shot cross-lingual intelligence. Optimize for exceptional performance with a strict constraint of <=130 milliseconds response time, enabling fluent real-time translation for low-bandwidth video in international markets while operating on resource-constrained devices (IoT and legacy systems). Ensure model compatibility and achieve a 25% reduction in carbon emissions compared to conventional translation techniques, without sacrificing translation quality or speed. Design with energy efficiency in mind for green global deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"international communication\",\n        \"description\": \"The task is to develop a cutting-edge neural machine translation model by merging Transformer-XL Enhanced and BERT-Mini Adaptive architectures, targeting EuroTechX benchmark with a focus on zero-shot cross-lingual intelligence. Performance must have a response time constraint of <=130 milliseconds for low-bandwidth video translation in IoT and legacy systems.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 130\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"% reduction in carbon emissions\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"EuroTechX benchmark\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"response_time_constraint\": true,\n                \"bandwidth_requirement\": \"low\"\n            },\n            \"description\": \"A benchmark dataset for evaluating the model's cross-lingual intelligence in real-time translation\",\n            \"preprocessing\": [\"merging Transformer-XL Enhanced and BERT-Mini Adaptive\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL Enhanced + BERT-Mini Adaptive\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural machine translation\",\n            \"specification\": {\n                \"compatibility\": \"IoT and legacy systems\",\n                \"carbon_emission_reduction\": 25\n            },\n            \"description\": \"A model designed for exceptional performance with energy efficiency, targeting low-bandwidth video translation and optimized for resource-constrained environments\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": [\"IoT\", \"legacy systems\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"energy-efficient\"],\n        \"demonstration\": {\n            \"context\": \"real-time translation for low-bandwidth video\",\n            \"emphasis\": \"global green deployment\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately reflects the user's intention to build a complex NMT model. The requirement for merging two architectures, zero-shot cross-lingual intelligence, response time constraint, energy efficiency, and carbon emissions reduction are all incorporated. The confidence score is high due to the thorough mapping of the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As the project manager of a pioneering cultural analytics team, our mandate is to develop a groundbreaking neural network architecture that not only segments intricate 18th-century botanical watercolor miniatures from medieval manuscript illuminations with subatomic precision, but also recognizes nuanced botanical species, regional floristic patterns, and the artist's hand in brushstrokes, while preserving the ethereal glow of gold leaf and gold ink details, and addressing the challenge of time-dependent color shifts in aged pigments. Can you design an adaptable, era-sensitive model tailored for this groundbreaking, eco-conscious art history collection?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"art history\",\n        \"description\": \"The task is to develop a neural network architecture for segmenting 18th-century botanical watercolor miniatures from medieval manuscript illuminations, identifying nuanced botanical species, regional floristic patterns, artist's brushstrokes, preserving ethereal glow of gold leaf and gold ink, and handling time-dependent color shifts in aged pigments. The model should be adaptable and era-sensitive.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"segmentation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"species recognition accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"brushstroke recognition accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"adaptability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"era-sensitivity\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"18th-century Botanical Watercolor Miniatures\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"age\": \"18th century\",\n                \"subject matter\": \"botanical watercolors\",\n                \"eclectic elements\": [\"gold leaf\", \"gold ink\", \"aged pigments\"]\n            },\n            \"description\": \"Dataset consisting of 18th-century botanical watercolor miniatures for model development.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Medieval Manuscript Illuminations\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"era\": \"medieval\",\n                \"context\": \"manuscript illuminations\",\n                \"specific challenges\": [\"color shifts\"]\n            },\n            \"description\": \"Dataset of medieval manuscript illuminations for model training that accounts for time-dependent changes.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Neural Network Architecture\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptability\": \"high\",\n                \"era-sensitivity\": \"enabled\"\n            },\n            \"description\": \"A groundbreaking model designed for segmentation and recognition tasks, specifically tailored for the given cultural and ecological context.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Addressing time-dependent color shifts and preserving historical integrity is a key challenge\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intention to build a highly specialized model. The problem description includes specific requirements for segmentation, species recognition, and artist analysis. The dataset details are adapted to the context of watercolors and illuminations, and the custom model caters to the project's unique needs. However, performance metrics are left undefined as they may require further expert input or specific analysis on validation data.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "In the realm of cryptanalytic marvels, your advanced machine learning team is challenged with a one-of-a-kind text classification mission: Collect and compile a highly specialized dataset from clandestine neural cryptography conferences, focusing on obscure quantum key exchange protocols, stealthy quantum error correction codes, and elusive lattice-based security schemes. Each research paper is ranked using a complex 20-point rubric that measures innovation, resilience against post-quantum attacks, and intricate mathematical ingenuity, weighted by a tailored Q-factor fusion score (QFS) system. Design an avant-garde model that deciphers these enigmatic theories and detects the nuanced grading patterns, striving for a High-QFS of 0.95 or above. Furthermore, devise a groundbreaking explainability mechanism that translates model intuitions into layman's terms for quantum cryptography, using relatable physics analogies to maintain comprehension without diluting its sophistication. Justify the chosen explanation methods with real-world cryptographic equivalents, demonstrating their synergy with quantum mechanics principles.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"cybersecurity\",\n        \"description\": \"The task is to build a specialized dataset from clandestine neural cryptography conferences, focusing on quantum key exchange protocols, quantum error correction codes, and lattice-based security schemes. The dataset should be annotated using a complex 20-point rubric with a Q-factor fusion score (QFS) system, aiming for a High-QFS of 0.95 or above.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Q-factor fusion score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Neural Cryptography Conference Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"paper_type\", \"QFS\", \"in创新性\", \"in_resilience\", \"in_mathematical_ingenuity\"],\n            \"specification\": {\n                \"paper_count\": null,\n                \"language\": \"English\",\n                \"data_format\": \"PDF\"\n            },\n            \"description\": \"A dataset compiled from cryptographic conferences with a focus on the specified topics, annotated with a 20-point scoring system and QFS.\",\n            \"preprocessing\": [\"extracting content\", \"data cleaning\", \"lemmatization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum Cryptography Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-based\",\n                \"model_depth\": null,\n                \"training_method\": \"fine-tuning\",\n                \"inception_of\": \"state-of-the-art NLP model\"\n            },\n            \"description\": \"A sophisticated model designed for text classification of cryptographic papers, utilizing QFS and tailored for the specific topics.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"name\": \"Explainability Methodology\",\n            \"description\": \"This approach employs Saliency Maps and LIME to highlight important words and concepts, analogizing them to quantum entanglement and interference for layperson understanding. It uses Schrödinger's Cat thought experiment to explain model uncertainty.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"topic extraction\", \"keyword extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"custom, quantum-inspired visualization\"],\n        \"demonstration\": \"A series of real-world cryptography examples accompanied by quantum physics analogies to showcase the model's predictions.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the task's complexity and requirement for high expertise in quantum cryptography. The model's sophistication and explainability methods are tailored to the unique challenge. Some details, such as specific model architecture and dataset features, are left for flexibility based on input from a machine learning expert.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a specialized time-series forecasting model for our app's daily user engagement using a hybrid approach of Prophet and ARIMA, focusing exclusively on the last 540 days. Emphasize interpretability by integrating feature importance analysis, tackle complex cyclical patterns every fourth and eighth week, and mandate a validation process that involves a rolling six-week window with 10% data splits for both in-model validation and out-of-sample testing to ensure robustness against unusual trends and seasonality across all quarters.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"mobile app\",\n        \"description\": \"The task is to build a hybrid time-series forecasting model using Prophet and ARIMA, specifically focusing on the last 540 days' data for daily user engagement in our app. The model should prioritize interpretability with feature importance analysis. The model must handle complex cyclical patterns, especially every fourth and eighth week. The validation process includes a rolling six-week window with 10% data splits for in-model and out-of-sample testing to ensure robustness against unusual trends and seasonality across quarters.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"App User Engagement\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"daily user engagement\"],\n            \"specification\": {\n                \"data_length\": 540,\n                \"time_interval\": \"days\"\n            },\n            \"description\": \"The dataset covers the last 540 days of daily user engagement data for feature extraction and model training.\",\n            \"preprocessing\": [\n                \"Normalize data\",\n                \"Remove outliers\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Cyclical pattern detection\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Prophet-ARIMA Hybrid\",\n            \"family\": \"Time-series hybrids\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_importance\": true,\n                \"cyclical_patterns\": {\n                    \"weekly_cycles\": [4, 8]\n                },\n                \"validation\": {\n                    \"rolling_window\": 6,\n                    \"data_splits\": [\n                        \"in-model\",\n                        \"out-of-sample\",\n                        \"10%\"\n                    ]\n                }\n            },\n            \"description\": \"A hybrid model that integrates Prophet and ARIMA for forecasting daily user engagement, with interpretability, targeted at complex cyclical patterns and robust validation process.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cyclical patterns in time-series data, seasonal decomposition, and handling multiple validation methods\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Lag features\",\n            \"Custom transformations\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Real-time forecasting\"\n        ],\n        \"demonstration\": \"Interpretable model outputs and validation results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON format captures the user's intent to build a time-series forecasting model and their high level of expertise. The problem section详细描述了任务要求，包括特定的模型（Prophet-ARIMA），特征分析，以及验证过程。The confidence score is high as the information is extracted accurately from the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "community detection", "instruction": "The financial services project demands the team to develop a privacy-compliant community detection system for a massive dataset sourced from encrypted historical trades, demographic data, and encrypted behavioral trends. Apply top-of-the-art homomorphic encryption techniques, respecting strict GDPR and CCPA regulations, to uncover customer clusters for tailored, risk-assessed investment recommendations without breaching individual privacy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"finance\",\n        \"description\": \"The project aims to build a privacy-compliant community detection system using top-of-the-art homomorphic encryption techniques for encrypted historical trades, demographic data, and encrypted behavioral trends. It must adhere to GDPR and CCPA regulations, facilitating tailored, risk-assessed investment recommendations without exposing individual privacy.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Encrypted Historical Trades and Demographic Data\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"encrypted graph data\",\n                \"source\": \"historical trades and demographic data\"\n            },\n            \"description\": \"A massive dataset containing encrypted historical trades and demographic data, requiring homomorphic encryption for analysis.\",\n            \"preprocessing\": [\"homomorphic encryption\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"Encrypted Behavioral Trends\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"encrypted behavioral data\"\n            },\n            \"description\": \"Additional encrypted data on encrypted behavioral trends, which will contribute to the community detection process.\",\n            \"preprocessing\": [\"homomorphic encryption\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Privacy-Compliant Community Detection System\",\n            \"family\": \"Homomorphic Machine Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"encryption_techniques\": [\"top-of-the-art homomorphic encryption\"],\n                \"regulatory_compliance\": [\"GDPR\", \"CCPA\"],\n                \"performance_tradeoff\": {\n                    \"privacy\": \"high\",\n                    \"accuracy\": \"optimized for privacy constraints\"\n                }\n            },\n            \"description\": \"A cutting-edge system leveraging homomorphic encryption for community detection while preserving data privacy under strict regulations.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"title\": \"Privacy-Preserving Techniques in ML\",\n            \"content\": \"Understanding of homomorphic encryption and its application in GDPR/CCPA-compliant community detection.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GDPR-compliant inference\"],\n        \"demonstration\": \"Investment recommendations tailored to risk profiles without exposing individual data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key elements from the user's instruction, including the intent to build, the high expertise level, and the focus on community detection using encrypted data. Performance metrics are not specified due to the focus on privacy and regulatory compliance. The missing information could include exact complexity metrics, which might not be as straightforward as privacy-related measures.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "task: Design a state-of-the-art gradient boosting model, XGBRegressor, for the niche Rock Hardness dataset, specifically targeting Brinell hardness measurements. Optimize for a below-3% RMSE in low (<100) and high (>800) hardness intervals, while managing and mitigating influence of extreme outliers within these categories. Implement a recursive feature elimination (RFE) method to identify the significance of rare elemental combinations like trace elements (less than 1% of samples). Ensure the model report includes a detailed visualization of feature importance and an interpretable explanation for domain experts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"geology\",\n        \"description\": \"The task is to design a state-of-the-art gradient boosting model using XGBRegressor for the Rock Hardness dataset, focusing on Brinell hardness measurements. The objective is to achieve an RMSE of below 3% for low (less than 100) and high (greater than 800) hardness intervals, while managing extreme outliers. Additionally, a recursive feature elimination (RFE) method should be implemented to analyze the importance of trace elements (less than 1% of samples).\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": 0.03,\n                \"criteria\": {\n                    \"interval\": {\n                        \"low\": 100,\n                        \"high\": 800\n                    }\n                }\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Rock Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"Brinell hardness\"],\n            \"specification\": {\n                \"outlier_management\": \"extreme outliers in low and high hardness intervals\",\n                \"trace_elements\": {\n                    \"percentage\": \"less than 1%\"\n                }\n            },\n            \"description\": \"The dataset contains Brinell hardness measurements and should be analyzed considering rare elemental combinations and outliers.\",\n            \"preprocessing\": [\n                \"handling extreme outliers\",\n                \"feature scaling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature importance plot\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"XGBRegressor\",\n            \"family\": \"Gradient Boosting\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"optimization\": {\n                    \"target\": \"below 3% RMSE\",\n                    \"sub-intervals\": \"low (<100) and high (>800)\"\n                }\n            },\n            \"description\": \"A state-of-the-art model designed for the given task with a focus on managing outliers and trace elements.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"recursive feature elimination (RFE)\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"goal\": \"interpretable explanation for domain experts\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON format captures the user's high expertise level and intent to build a model. It accurately reflects the task, which includes a specific model (XGBRegressor), dataset (Rock Hardness with rare trace elements), performance criteria (RMSE within intervals), and a requirement for feature interpretation. The confidence score is high because the structure follows the schema and fully represents the given instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for an elite machine learning ensemble, refine the cutting-edge ILI forecasting system by integrating sophisticated deepAR and TBATS hybrid techniques, accounting for intricate weekly and long-term seasonal patterns, and employing anomaly detection algorithms. Demand the model to not only outperform established benchmarks with a hybrid ARIMA-Fusion and State Space Model (SSM), but also conduct a multi-step forecasting analysis with a rolling 48-week data subset of seven distinct variables. Prioritize optimizing both root mean squared error (RMSE) and mean absolute percentage error (MAPE), while strictly enforcing a latency constraint of <3 minutes for immediate actionable insights, necessitating the team's prowess in distributed processing and latency-optimized model deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"economics\",\n        \"description\": \"The project aims to enhance the ILI forecasting system by integrating deepAR and TBATS hybrid techniques for handling weekly and long-term seasonality, and incorporating anomaly detection. The model should outperform existing benchmarks (ARIMA-Fusion & SSM) and perform multi-step forecasting with a 48-week rolling data subset of seven variables. Optimization targets include RMSE and MAPE, while maintaining a latency constraint of less than 3 minutes for real-time insights.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency constraint\",\n                \"value\": 3,\n                \"unit\": \"minutes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI Forecasting System data\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"rolling_window_size\": 48,\n                \"variable_count\": 7\n            },\n            \"description\": \"Data with a 48-week rolling subset of seven distinct variables for refining the ILI forecasting system.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"deepAR-TBATS Hybrid\",\n            \"family\": \"Hybrid Time Series\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"involves_sophisticated_techniques\": true,\n                \"accounts_for_seasonality\": {\n                    \"weekly\": true,\n                    \"long_term\": true\n                },\n                \"anomaly_detection\": true\n            },\n            \"description\": \"A state-of-the-art ensemble model with deepAR and TBATS, targeting improved performance and multi-step forecasting.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"distributed processing\", \"latency-optimized deployment\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time insights\"],\n        \"demonstration\": \"rolling 48-week forecast and actionable insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction, detailing the requirement for refining the ILI forecasting system with a high-end expertise approach. It covers the integration of deepAR and TBATS, handling seasonality, anomaly detection, and performance metrics, while maintaining the latency constraint.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "Create a sophisticated image classifier capable of discriminating between an impressive 75 distinct feline subspecies, not just 50, and an expanded list of 100 diverse dog breeds, demonstrating robustness against extreme weather conditions, complex backgrounds, and object deformation. Mandate the integration of innovative EfficientNet and DenseNet architectures, demand transfer learning from a large-scale CelebA dataset for initialization, and enforce advanced domain adaptation using adversarial learning to ensure real-world discriminative prowess in a dynamically changing environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"zoology\",\n        \"description\": \"The task is to develop a highly sophisticated image classifier that can distinguish among 75 distinct feline subspecies and 100 diverse dog breeds. The model must demonstrate robustness against extreme weather conditions, complex backgrounds, and object deformation. It requires the use of EfficientNet and DenseNet architectures, transfer learning initialized from a large-scale CelebA dataset, and advanced domain adaptation with adversarial learning for real-world performance in a dynamic environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Custom Cat & Dog Breeds Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"feline subspecies\", \"dog breeds\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"valid\": null,\n                    \"test\": null\n                },\n                \"dimension\": {\n                    \"image\": {\n                        \"width\": null,\n                        \"height\": null\n                    }\n                },\n                \"length\": {\n                    \"time_series\": null\n                },\n                \"graphs\": {\n                    \"nodes\": null,\n                    \"edges\": null\n                }\n            },\n            \"description\": \"A large dataset with images of 75 feline subspecies and 100 dog breeds, featuring diverse backgrounds, weather conditions, and object deformation.\",\n            \"preprocessing\": [\n                \"Data augmentation (extreme weather, backgrounds, and object deformation)\"\n            ],\n            \"augmentation\": [\"transformations for robustness\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"CelebA Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"large-scale\": true\n            },\n            \"description\": \"A dataset used for transfer learning, initialized for EfficientNet and DenseNet architectures.\",\n            \"preprocessing\": [\"Transfer learning initialization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet + DenseNet\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": {\n                    \"EfficientNet\": null,\n                    \"DenseNet\": null\n                },\n                \"transfer_learning\": \"CelebA\",\n                \"domain_adaptation\": \"adversarial learning\"\n            },\n            \"description\": \"A combination of EfficientNet and DenseNet architectures, employing transfer learning from CelebA and advanced domain adaptation for improved real-world discriminative power.\"\n        }\n    ],\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has been adapted to capture the details from the user's instructions, focusing on the complexity of the classification task, target models (EfficientNet and DenseNet), large-scale CelebA dataset for transfer learning, and domain adaptation using adversarial learning. The levels of expertise and the specific requirements have been interpreted as 'high'. The confidence score may not be 1 due to the lack of exact metric targets and details for the dataset size and preprocessing, but the overall structure is correct and captures the essence of the task.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the advanced challenge of underwater biodiversity exploration on an enhanced CoralNet 4.0, develop a state-of-the-art FusionNet++ architecture that integrates a sophisticated Hybrid Appearance-Motion Fusion Module. This model should distinguish among not only prevalent coral species but also distinguish cryptic microspecies, delicate biofilm organisms, and elusive bioluminescent entities. Achieve a substantial 35% boost in precision amidst water clarity variations, limited visibility, and unique lighting conditions, and present a tailored loss function that accounts for these complex environmental factors, ensuring near-human-level segmentation efficiency. Please design an efficient training and deployment strategy for real-time underwater imagery and evaluate its performance on a large, diverse dataset.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"ecology\",\n        \"description\": \"The task is to develop a state-of-the-art FusionNet++ architecture with a Hybrid Appearance-Motion Fusion Module for advanced underwater biodiversity exploration on CoralNet 4.0, focusing on distinguishing rare species like cryptic microspecies, delicate biofilm organisms, and bioluminescent entities. The model must achieve a 35% boost in precision under water clarity variations and unique lighting conditions, using a tailored loss function and targeting near-human-level segmentation efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 1.35\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CoralNet 4.0\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"coral species\", \"microspecies\", \"biofilm organisms\", \"bioluminescent entities\"],\n            \"specification\": {\n                \"water_clarity_variations\": true,\n                \"limited_visibility\": true,\n                \"unique_lighting_conditions\": true\n            },\n            \"description\": \"A large, diverse dataset for underwater imagery, addressing the challenges of water clarity variations, limited visibility, and unique lighting conditions.\",\n            \"preprocessing\": [\"data augmentation for water clarity, visibility, and lighting\"],\n            \"augmentation\": [\"random noise, brightness, contrast adjustments\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"FusionNet++\",\n            \"family\": \"Multi-modal Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"Hybrid Appearance-Motion Fusion Module\",\n                \"loss_function\": \"Tailored for underwater conditions\"\n            },\n            \"description\": \"A state-of-the-art model integrating advanced fusion techniques for underwater biodiversity segmentation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Underwater imagery requires specialized techniques due to varying water conditions and lighting\",\n        \"Hybrid Appearance-Motion Fusion Module aids in distinguishing distinct organisms\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"optimized for real-time training and inference\"],\n        \"target_device\": \"edge device, specifically designed for underwater robotics\",\n        \"deployment_endpoint\": \"dedicated underwater imagery processing service or platform\",\n        \"inference_engine\": [\"TensorFlow-Lite, optimized for low latency\"],\n        \"demonstration\": \"real-time segmentation demo during model deployment\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction by detailing the user's intent to build a state-of-the-art model, their high expertise level, the requirements for the FusionNet++ architecture, and the performance and complexity metrics. Key challenges are addressed, such as adapting to unique environmental conditions, and the dataset and deployment strategy are specified based on the task.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "For a transportation logistics company seeking a competitive edge, propose a state-of-the-art time-series anomaly detection system to the data science team. Develop an advanced algorithm that combines real-time analysis of global maritime vessel traffic, satellite-based maritime cargo temperatures, and intricate port congestion data for a decade-long span. The system must seamlessly integrate with diverse IoT sensors, factor in seasonal fluctuations, predict vessel failures, and maintain accuracy under chaotic market conditions while guaranteeing a sub-minute response time for immediate anomaly detection and mitigation strategies. Additionally, ensure the model's explainability for non-technical stakeholders and adheres to stringent data privacy regulations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"transportation and logistics\",\n        \"description\": \"The task is to design a state-of-the-art time-series anomaly detection system for a transportation logistics company. This system should include real-time analysis of maritime vessel traffic, satellite-based cargo temperatures, and port congestion data over a 10-year period. It must integrate with IoT sensors, account for seasonal fluctuations, predict vessel failures, and provide sub-minute response times with explainability for non-technical stakeholders, while complying with data privacy regulations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": \"sub-minute\"\n            },\n            {\n                \"name\": \"anomaly detection rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Maritime Vessel Traffic Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"vessel locations\", \"traffic patterns\"],\n            \"specification\": null,\n            \"description\": \"Real-time and historical data of global maritime vessel traffic.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Satellite-based Cargo Temperature Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"cargo temperatures\"],\n            \"specification\": null,\n            \"description\": \"Satellite-based data on maritime cargo temperatures.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Port Congestion Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"port activity\", \"拥堵指标\"],\n            \"specification\": null,\n            \"description\": \"Decade-long time series data on port congestion.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"IoT Sensor Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"sensor readings\"],\n            \"specification\": null,\n            \"description\": \"Real-time data from various IoT sensors on vessels and ports.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Time-Series Anomaly Detection Algorithm\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Combination of RNN, LSTM, and Autoencoder\",\n                \"features\": \"Real-time processing, seasonal adjustments, explainability\"\n            },\n            \"description\": \"A deep learning model for detecting anomalies in time-series data from maritime and IoT sources.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Explainability for non-technical stakeholders: LIME, SHAP, or SHAPley Additive exPlanations\",\n        \"Data privacy: GDPR or CCPA compliant data processing and anonymization techniques\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Seasonal decomposition\",\n            \"Feature scaling\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Real-time data processing engines like Apache Kafka or in-memory caching\"],\n        \"demonstration\": \"Custom dashboards or visualizations for stakeholders\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the given instruction, capturing the requirements for building a high-end time-series anomaly detection system. Key aspects like diverse data sources, integration, performance metrics, explainability, and data privacy are included. The model is specified as a deep learning approach, and the necessary preprocessing steps for feature engineering are outlined.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "For a high-end luxury retail venture, refine the state-of-the-art YOLOv6 architecture by employing progressive fine-tuning with a unique, domain-specific corpus. The task involves optimizing the \"OpulentRetailNarrowbandDataset,\" which contains 50 exclusive product sub-variants in hi-res, vibrancy-charged imagery. The model must achieve top-tier precision (>=99%) and recall (>=99.5%), demonstrating exceptional generalization to rare and unseen variations. The system must also boast ultra-low latency (<0.05 seconds/image), guaranteeing real-time inventory control, and seamlessly integrating with a sophisticated AI-driven retail environment, integrating complex supply chain dynamics and personalized customer experiences in a single, harmonious workflow.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury retail\",\n        \"description\": \"The task is to refine the YOLOv6 architecture for a high-end retail project, focusing on optimizing the 'OpulentRetailNarrowbandDataset' with 50 exclusive product sub-variants in hi-res, vivid imagery. The model must achieve precision and recall of at least 99% and 99.5%, respectively, and have ultra-low latency (<0.05 seconds per image) for real-time inventory control and seamless integration with an AI-driven retail environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.995\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.05,\n                \"unit\": \"seconds/image\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpulentRetailNarrowbandDataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_sub_variants\": 50,\n                \"image_resolution\": \"high\",\n                \"data_vibrancy\": \"vibrancy-charged\"\n            },\n            \"description\": \"A dataset containing exclusive product sub-variants with high-resolution and vivid imagery.\",\n            \"preprocessing\": [\"progressive fine-tuning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv6 (refined)\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"specific_finetuning\": \"progressive\",\n                \"custom_corpus\": \"domain-specific\"\n            },\n            \"description\": \"The state-of-the-art YOLOv6 model optimized for high-end luxury retail, with emphasis on precision, recall, and low latency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The task involves domain adaptation and optimizing for real-time inventory control and AI-driven retail environment integration.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"ultra-low-latency\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"AI-driven retail environment integration and real-time inventory control\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the given instruction accurately. It highlights the need for precision and recall, the unique domain-specific corpus, and the ultra-low latency requirement. However, some specific values for performance metrics are left unspecified for flexibility.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager is tasked with developing a highly advanced, scalable TinyBERT QA model, designed to meet the requirements of a multi-lingual, tri-lingual adaptation challenge (English, Spanish, and Mandarin). It must exhibit state-of-the-art performance with at least 92% F1-score on the SQuAD Mobile dataset filtered by NewsSubcategory, ensuring energy efficiency (Snapdragon EER 4 or above) and ultra-fast responsiveness (under 100 milliseconds query latency). In addition, the model should incorporate Explainable AI capabilities, allowing non-technical stakeholders to comprehend and analyze its decision-making process through visual aids and natural language explanations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"cross-lingual and multilingual\",\n        \"description\": \"The project aims to develop a highly advanced TinyBERT QA model capable of handling English, Spanish, and Mandarin languages, targeting a multi-lingual adaptation challenge with SQuAD Mobile dataset filtered by NewsSubcategory. The model must achieve at least 92% F1-score and have energy efficiency with Snapdragon EER 4 or above, while maintaining ultra-fast responsiveness under 100 milliseconds query latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.92\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Snapdragon EER\",\n                \"value\": 4.0,\n                \"unit\": \"unitless\"\n            },\n            {\n                \"name\": \"Query latency\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD Mobile Dataset (filtered by NewsSubcategory)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"multilingual\": [\"English\", \"Spanish\", \"Mandarin\"],\n                \"filtered_by\": \"NewsSubcategory\"\n            },\n            \"description\": \"A multi-lingual dataset for question answering, consisting of questions and corresponding answers in English, Spanish, and Mandarin, specifically adapted for the NewsSubcategory of SQuAD Mobile.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT\",\n            \"family\": \"BERT-based model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainable AI\": true\n            },\n            \"description\": \"A scalable QA model that supports English, Spanish, and Mandarin languages, with Explainable AI capabilities for non-technical stakeholders.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"The model should incorporate explainability to provide insights into its decision-making process.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"optimized for Snapdragon EER 4 or above\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Visual aids and natural language explanations for decision-making process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the project's requirements, focusing on a high-end TinyBERT QA model with specified performance goals, energy efficiency, and fast responsiveness. The instruction's details about languages, dataset, and Explainable AI are captured accurately in the JSON structure.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Revamp the marine ecosystem's time-series forecaster, using an exclusive DeepAR variant with an advanced LSTM-Transformer fusion, enriched with meta-learning and smart solar forcing calibration. Design a data assimilation pipeline integrating particle filtering for real-time updates, incorporate NAS for relentless performance optimization, and conduct feature extraction with Earth system proxy correlations. Enhance interpretability via SHAP explainability charts and gauge prediction reliability via a multi-model ensemble technique. Forecast minute-level ocean temperature changes in the Gulf Stream and Pacific Ocean for the upcoming five-year period, meticulously accounting for volcanic activity's subtle impact on each monthly sub-region.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate science, marine ecology\",\n        \"description\": \"The task is to revamp a time-series forecaster for marine ecosystems using an exclusive DeepAR variant with LSTM-Transformer fusion, meta-learning, and solar forcing calibration. The pipeline should involve data assimilation with particle filtering, NAS for performance optimization, feature extraction with Earth system proxy correlations, interpretability with SHAP explainability, and reliability via multi-model ensemble. The focus is on minute-level ocean temperature forecasting in the Gulf Stream and Pacific Ocean, considering volcanic activity's effect on monthly sub-regions over the next five years.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE (mean absolute error)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Marine Ecosystem Time-series Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"ocean temperature\"],\n            \"specification\": {\n                \"length\": null,\n                \"dimension\": null,\n                \"data source\": \"Earth observation records\"\n            },\n            \"description\": \"A dataset containing historical ocean temperature measurements for the Gulf Stream and Pacific Ocean, including Earth system proxy correlations and real-time updates through particle filtering.\",\n            \"preprocessing\": [\n                \"meta-learning enrichment\",\n                \"solar forcing calibration\",\n                \"feature extraction with Earth system correlations\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"SHAP explainability charts\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced DeepAR with LSTM-Transformer fusion\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"LSTM-Transformer fusion\",\n                \"meta-learning enabled\": true,\n                \"solar forcing calibration\": true\n            },\n            \"description\": \"A DeepAR model tailored for marine ecosystem forecasting, leveraging LSTM and Transformer for enhanced forecasting with NAS for performance optimization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Particle filtering for real-time updates\",\n        \"NAS for relentless performance optimization\",\n        \"Earth system proxy correlations\",\n        \"SHAP explainability\",\n        \"Multi-model ensemble for prediction reliability\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Earth system proxy correlations\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"NAS-optimized inference\"],\n        \"demonstration\": \"Minute-level ocean temperature change forecasts with volcanic impact analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has captured the essence of the given instruction, reflecting the advanced requirements for a machine learning project in marine ecosystem forecasting. The user's high level of expertise is acknowledged. The specific DeepAR model with LSTM-Transformer fusion and additional techniques are clearly stated, along with the performance and interpretability goals. The dataset details are mapped to the provided schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "For the sophisticated pedestrian and traffic management in an advanced metropolis' IoT pilot, our team needs a lightweight MosaicNet-Edge model custom-designed for detecting not only standard urban entities (vehicles, pedestrians), but also includes cyclist diversions, potholes, and tramlines, ensuring at least 93% precision under stringent conditions: day-night variations, diverse lighting, and high-resolution input. Crucially, this model should maintain 38 FPS on low-end, low-latency devices with hardware acceleration for seamless real-time monitoring.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart cities/urban management\",\n        \"description\": \"The goal is to develop a lightweight MosaicNet-Edge model for pedestrian and urban entity detection in an IoT pilot metropolis, also considering cyclist diversions, potholes, and tramlines. The model must achieve at least 93% precision under day-night variations, diverse lighting, and high-resolution inputs, while maintaining 38 FPS on low-end devices with hardware acceleration for real-time monitoring.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.93\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed (FPS)\",\n                \"value\": 38,\n                \"unit\": \"frames per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Smart City IoT Metropolis Detection Dataset\",\n            \"modality\": [\n                \"image\"\n            ],\n            \"target_variables\": [\n                \"pedestrians\",\n                \"vehicles\",\n                \"cyclists\",\n                \"potholes\",\n                \"tramlines\"\n            ],\n            \"specification\": {\n                \"high-resolution\": true,\n                \"day-night variation\": true,\n                \"diverse lighting\": true\n            },\n            \"description\": \"A custom dataset for the IoT pilot metropolis, with day-night variations, diverse lighting conditions, and high-resolution inputs for training the MosaicNet-Edge model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MosaicNet-Edge\",\n            \"family\": \" Lightweight CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"suitable_for_low-end_devices\": true,\n                \"hardware_acceleration\": true\n            },\n            \"description\": \"A custom-designed model for pedestrian and urban entity detection, optimized for low-latency, high-performance on low-end devices with the target of 93% precision and 38 FPS.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"low-end, low-latency devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Hardware Accelerated\"],\n        \"demonstration\": \"real-time monitoring for seamless operation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure closely represents the given instruction, highlighting the specific requirements for a customized lightweight MosaicNet-Edge model for the IoT pilot. Performance metrics, precision, and hardware acceleration are addressed, along with the target device and FPS. The confidence score is high due to the thorough interpretation of the requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a highly experienced project manager for a specialized deep learning team, your mandate is to devise an ultra-precise time-series forecasting model that exclusively caters to the healthcare sector. The enhanced ILI dataset, enriched with minute-grained seasonal fluctuations, detailed anomalies, and real-time syndromic surveillance data, has been divided into training, validation, and testing partitions. Input sequences now consist of lengthy historical data (48-week financial indicators) with an expanded feature set of 25 economic and public health variables (INPUT_SEQ_LEN=48, INPUT_DIM=25). Design a multi-resolution predictive framework that forecasts the subsequent 36-week ahead (PRED_SEQ_LEN=36) predictions, factoring in occurrences of rare events, quarterly market shifts, and their impact on hospital visits. Ensure model reliability through the implementation of advanced evaluation metrics like adjusted mean squared logarithmic error (MSLE), root mean squared logarithmic error (RMSLE), and a novel hybrid error metric. Additionally, prioritize model interpretability, quantifiable fairness, and resilience against sudden market anomalies, all while maintaining explainable SHapley Additive exPlanations (SHAP) for transparent decision-making.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"The task is to develop an ultra-precise time-series forecasting model for the healthcare sector using the enhanced ILI dataset. The model should handle minute-grained seasonal fluctuations, detailed anomalies, and real-time syndromic surveillance data with a 48-week input sequence and 25 economic/public health variables. The objective is to forecast 36-week ahead predictions while accounting for rare events and quarterly market shifts, ensuring model reliability through advanced metrics like MSLE, RMSLE, and a novel hybrid error metric. Interpretability, fairness, and resilience against sudden market anomalies with SHAP are key priorities.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"adjusted mean squared logarithmic error (MSLE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"root mean squared logarithmic error (RMSLE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"novel hybrid error metric\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enhanced ILI Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 48,\n                \"INPUT_DIM\": 25,\n                \"PRED_SEQ_LEN\": 36,\n                \"data characteristics\": {\n                    \"seasonal fluctuations\": true,\n                    \"anomalies\": true,\n                    \"real-time surveillance\": true\n                }\n            },\n            \"description\": \"A healthcare dataset enriched with minute-grained data, 48-week historical indicators, and a 25-feature set for both economic and public health variables.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Time-series Forecasting Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"multi-resolution predictive framework\",\n                \"interpretability\": true,\n                \"fairness\": true,\n                \"resilience to anomalies\": true,\n                \"SHAP explainability\": true\n            },\n            \"description\": \"A deep learning model designed specifically for healthcare sector time-series forecasting, incorporating multi-resolution, interpretability, and SHAP-based transparency.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Consider minute-grained data, rare events, and SHAP for explainable decision-making.\"\n        },\n        {\n            \"description\": \"Factor in quarterly market shifts and their impact on hospital visits.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explainable model predictions and decisions with SHAP values\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the complexity and requirements specified in the instruction, including the need for a high-level expert and the detailed specifications for the model and dataset. It captures the unique features of the healthcare time-series forecasting problem, performance metrics, and model characteristics. Some performance metric values are left unspecified as they are typically generated dynamically.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for a premier real estate monitoring AI, mandate your team to develop a sophisticated image segmentation model with an additional constraint. The model must not only discern buildings, trees, and roads in variable weather, lighting, and seasonal images, but also identify specific features like solar panel types, grass species, and micro-cracks in pavements. The system must track minute changes, recognize urban renewal projects, and map environmental shifts while ensuring energy efficiency and ultra-fast processing for large, high-resolution drone datasets collected across diverse landscapes. Can you design a state-of-the-art neural network that exceeds performance benchmarks while optimizing for scalability and real-time analysis?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"real estate monitoring and urban planning\",\n        \"description\": \"The project aims to develop a sophisticated image segmentation model that can identify buildings, trees, roads, solar panel types, grass species, and micro-cracks in pavements. The model must handle variable weather, lighting, and seasonal conditions, track minute changes, recognize urban renewal projects, and map environmental shifts with energy efficiency and ultra-fast processing for large, high-resolution drone datasets collected across diverse landscapes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mIoU (mean Intersection over Union)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"processing speed (real-time)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Real Estate Drone Monitoring Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image resolution\": \"\",\n                \"number of images\": \"\",\n                \"variety of landscapes\": \"\",\n                \"weather conditions\": \"\",\n                \"seasonal diversity\": \"\"\n            },\n            \"description\": \"A large, high-resolution drone dataset capturing real estate scenes with variable weather, lighting, and seasonal conditions.\",\n            \"preprocessing\": [\n                \"image resizing\",\n                \"data augmentation (weather, lighting variations)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"image samples with ground truth segmentation\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image Segmentation Network\",\n            \"family\": \"Convolutional Neural Networks (specifically a variant for image segmentation)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"e.g., U-Net, DeepLabv3+, Mask R-CNN\",\n                \"parameters\": \"specifically optimized for complex scene understanding and energy efficiency\"\n            },\n            \"description\": \"A state-of-the-art model designed for real estate monitoring tasks, capable of high-performance segmentation while optimizing for scalability and real-time processing.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"solar panel and grass species recognition\",\n            \"micro-crack detection\",\n            \"urban renewal project tracking\"\n        ],\n        \"target_device\": \"cloud and edge computing infrastructure\",\n        \"deployment_endpoint\": \"real-time analysis pipeline\",\n        \"inference_engine\": [\"optimized for GPU processing\"],\n        \"demonstration\": \"visual results showcasing improved performance and real-time analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the project manager's intent to build a high-performing and scalable model for the given real estate monitoring project. It accurately reflects the requirements for variable weather, lighting, and seasonal image segmentation, as well as specific features, processing speed, and energy efficiency. The model selection and performance metrics are in line with the state-of-the-art. The confidence score is high due to the comprehensive mapping of the instruction to the JSON format.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "text classification", "instruction": "For the imminent financial sector project, engineer a cutting-edge sentiment analysis platform that couples BERT and XGBoost, integrating intricate sequence modeling and gradient boosting techniques. Mandate a multi-step process to classify complex financial advisories into three distinct categories - fraud, customer delight, and predictive trends, ensuring a precision of 99% or above. Emphasize the need for a lightweight, hardened model that can efficiently run on resource-constrained edge devices with real-time transaction analysis, limiting the latency to less than 4 milliseconds per transaction. Implement a dynamic adaptive framework that seamlessly adjusts to market volatility while maintaining consistent accuracy without hindering its ability to learn and evolve rapidly.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"The task is to develop a sentiment analysis platform using BERT and XGBoost for classifying complex financial advisories into three categories: fraud, customer delight, and predictive trends, targeting a precision of 99% or above. The platform should be lightweight and run on edge devices with real-time transaction analysis, having a latency of less than 4 milliseconds per transaction. It must adapt to market volatility while preserving accuracy and quick learning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency per transaction\",\n                \"value\": 0.004,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"model size for edge devices\",\n                \"value\": null,\n                \"unit\": \"resource-constrained\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Financial Advisory Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\"],\n            \"specification\": {\n                \"sequence_length\": null,\n                \"data_type\": \"structured/semi-structured\"\n            },\n            \"description\": \"A dataset containing complex financial advisories, requiring a combination of BERT and XGBoost techniques.\",\n            \"preprocessing\": [\n                \"BERT encoding\",\n                \"Sequence formatting\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT & XGBoost\",\n            \"family\": \"Transformer and Gradient Boosting\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"sequence_modeling\": true,\n                \"gradient_boosting\": true,\n                \"resource_efficient\": \"for edge devices\"\n            },\n            \"description\": \"An advanced model integrating sequence modeling with gradient boosting techniques, suitable for financial sentiment analysis.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain-specific embeddings\",\n            \"contextualized feature extraction\"\n        ],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\", \"dynamic adaptation\"],\n        \"demonstration\": \"responsive to market volatility\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has been adapted to reflect the provided instruction, focusing on a high-level task of building a sentiment analysis platform in the financial domain. It captures the specific requirements of integrating BERT and XGBoost, target performance metrics (99% precision), and real-time transaction analysis with edge device constraints. However, the detailed model parameters and data preprocessing steps might need more information, as they are not specified in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular classification", "instruction": "For the elite tabular classification challenge, the ML team is tasked with a comprehensive analysis of an obscure \"Census Income\" dataset, sourced from a niche government repository. Develop a multifaceted pipeline that necessitates intricate preprocessing stages, which involve not only conventional handling of missing data through state-of-the-art MICE imputation, but also the transformation of categorical variables via advanced ordinal encoding and the integration of domain-specific word embeddings for enhanced feature representation.\n\nDesign a hybrid neural architecture that seamlessly combines convolutional and recurrent layers, while integrating the embeddings with numerical features. Implement a robust training strategy employing a blended learning method (combined gradient boosting and stacked autoencoders) on a meticulously stratified 20% testing split, ensuring class parity is maintained by employing smart under-sampling algorithms for minority class balancing.\n\nCreate dynamic, interactive visualizations of the model's performance evolution, including learning curves, confusion matrices, and feature attribution heatmaps using interactive Plotly dashboards. Evaluate the model using a stringent set of metrics (precision, recall, F1-score, and AUC-ROC), while leveraging Bayesian optimization for efficient hyperparameter tuning. Complement the project with a thorough documentation, elucidating the rationale behind each technical choice and its measurable impact on boosting model accuracy, computational efficiency, and interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"economics\",\n        \"description\": \"The challenge involves a comprehensive analysis of the 'Census Income' dataset from a specialized government source, aiming for a high-performing machine learning pipeline with advanced preprocessing, including MICE imputation, ordinal encoding, and domain-specific word embeddings.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computation time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Census Income\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"income\"],\n            \"specification\": {\n                \"missing_data_rate\": null,\n                \"categorical_variable_count\": null,\n                \"dimensionality\": null,\n                \"length\": null\n            },\n            \"description\": \"An obscure dataset sourced from a niche government repository for a comprehensive tabular classification analysis, with preprocessing requirements mentioned.\",\n            \"preprocessing\": [\n                \"MICE imputation\",\n                \"Ordinal encoding\",\n                \"Word embeddings integration\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Learning curves\",\n                \"Confusion matrices\",\n                \"Feature attribution heatmaps\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Convolutional-Recurrent Neural Architecture\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"embedding_size\": null,\n                \"convolution_layers\": null,\n                \"recurrent_layers\": null,\n                \"training_method\": \"blended learning\",\n                \"ensemble_models\": [\"gradient boosting\", \"stacked autoencoders\"]\n            },\n            \"description\": \"A neural architecture combining convolutional and recurrent layers, with word embeddings and numerical features.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"smart under-sampling\", \"minority class balancing\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Plotly dashboards\"],\n        \"demonstration\": \"Dynamic visualizations and interactive model performance analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This response covers the user's instruction accurately, detailing the high-level objective, dataset specifics, preprocessing, model architecture, evaluation method, and the need for comprehensive documentation. The only missing pieces are exact metric values and computation details, which were intentionally left open-ended to align with the advanced nature of the task.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As the project manager, you are tasked with overseeing the development of a highly sophisticated image classification system, specifically targeting the diverse Fashion-MNIST dataset. Mandate your team to innovate with a state-of-the-art Capsule Network design, striving for a groundbreaking 98% accuracy with enhanced resistance to complex affine transformations. In addition, the project should delve into a rigorous comparative analysis with contemporary CNN architectures, focusing on precision improvements, computational efficiency, and the exploration of transfer learning strategies for optimal performance on limited resource scenarios. Ensure detailed visualizations and a thorough discussion of the ecological impact of these advancements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"The project aims to develop an image classification system using a state-of-the-art Capsule Network for the Fashion-MNIST dataset. The target accuracy is 98%, with a focus on resistance to complex affine transformations. The team must also conduct a comparative analysis with CNN architectures, improving precision, computational efficiency, and explore transfer learning for limited resource scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"class labels\"],\n            \"specification\": null,\n            \"description\": \"A diverse dataset for image classification focusing on fashion items, with emphasis on resistance to affine transformations.\",\n            \"preprocessing\": [\"affine transformation resistance\"],\n            \"augmentation\": [],\n            \"visualization\": [\"detailed classification results, performance comparisons\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Capsule Network (state-of-the-art)\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"resistance_to_transformations\": \"affine\",\n                \"accuracy_target\": 0.98\n            },\n            \"description\": \"A cutting-edge model design for image classification, especially robust to complex transformations.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"title\": \"Comparative Analysis with CNN\",\n            \"content\": \"Investigation of precision enhancements, computational efficiency, and transfer learning for resource-constrained scenarios.\"\n        },\n        {\n            \"title\": \"Ecological Impact\",\n            \"content\": \"Thorough discussion of advancements' environmental footprint and sustainability.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"exploration of transfer learning techniques\"],\n        \"target_device\": \"limited resources\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"comparison with contemporary architectures and visual explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was created based on the provided instruction, incorporating the required components such as the high expertise level, state-of-the-art Capsule Network, target accuracy, and detailed requirements for comparison, visualizations, and ecological impact. The information is formatted and structured according to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "The urgent and sophisticated financial news analytics initiative necessitates managing an enormous Reuters-21578 corpus, particularly targeting real-time articles with extreme frequency. As the project manager, your objective is to boost the state-of-the-art model's performance to 90% precision, with an unprecedented reduction in false positives (<2%) for critical market events. Emphasize on employing an advanced, real-time fine-tuned XLM-RoBERTa model that not only maximizes accuracy but also adheres to extreme efficiency standards, targeting latencies below 100 milliseconds, while ensuring seamless scalability and adaptation to dynamic market conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"The project aims to enhance the performance of an NLP model, specifically targeting real-time Reuters-21578 financial news articles. The focus is on achieving 90% precision for critical market events with a low false positive rate (<2%) for extreme frequency. The model must be advanced, real-time fine-tuned XLM-RoBERTa, optimizing for high accuracy, low latencies (below 100 ms), scalability, and adaptation to dynamic market conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": 0.02\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578 financial news corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"critical market events\"],\n            \"specification\": {\n                \"data_size\": \"enormous\",\n                \"real_time_frequency\": \"extreme\"\n            },\n            \"description\": \"A large corpus of real-time financial news articles for model training and evaluation.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"XLM-RoBERTa\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine_tuning\": \"real-time\",\n                \"efficiency\": \"extreme\",\n                \"latency_target\": 100,\n                \"scalability\": \"seamless\",\n                \"adaptability\": \"dynamic\"\n            },\n            \"description\": \"Advanced model for text classification targeting precision of 90% and low false positive rate for critical market events.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge or cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for latency\"],\n        \"demonstration\": \"model performance for real-time market events\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON representation accurately captures the user's intent and expertise, as well as the project's requirements, including a fine-tuned XLM-RoBERTa model for financial news classification, performance metrics, and efficiency parameters. The structure is adjusted to accommodate the specificity of the financial news analytics project.\",\n        \"score\": 0.98\n    }\n}\n"}
