# The following code is for "tabular classification" task.
import os, random, time, json

# define GPU location
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import torch

import numpy as np
import pandas as pd
import gradio as gr

# TODO: import other required library here, including libraries for datasets and (pre-trained) models like HuggingFace and Kaggle APIs. If the required module is not found, you can directly install it by running `pip install your_module`.
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score, f1_score


SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)
np.random.seed(SEED)

# Define device for model operations
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

DATASET_PATH = "_experiments/datasets"  # path for saving and loading dataset(s) (or the user's uploaded dataset) for preprocessing, training, hyperparamter tuning, deployment, and evaluation

# Data preprocessing and feature engineering
def preprocess_data():
    # TODO: this function is for data preprocessing and feature engineering

    # Run data preprocessing

    # Should return the preprocessed data
    return processed_data


def train_model(model, train_loader):
    # TODO: this function is for model training loop and optimization on 'train' and 'valid' datasets
    # TODO: this function is for fine-tuning a given pretrained model (if applicable)

    # Should return the well-trained or finetuned model.
    return model


def evaluate_model(model, test_loader):    
    # In this task, we use Accuracy and F1 metrics to evaluate the tabular classification performance.
    # The `performance_scores` should be in dictionary format having metric names as the dictionary keys
    # TODO: the first part of this function is for evaluating a trained or fine-tuned model on the 'test' dataset with respect to the relevant downstream task's performance metrics
    # Define the `y_true` for ground truth and `y_pred` for the predicted classes here.
    
    performance_scores = {
        'ACC': accuracy_score(y_true, y_pred),
        'F1': f1_score(y_true, y_pred)
    }

    # TODO: the second part of this function is for measuring a trained model complexity on a samples with respect to the relevant complexity metrics, such as inference time and model size
    # The `complexity_scores` should be in dictionary format having metric names as the dictionary keys

    # Should return model's performance scores
    return performance_scores, complexity_scores


def prepare_model_for_deployment():
    # TODO: this function is for preparing an evaluated model using model compression and conversion to deploy the model on a particular platform

    # Should return the deployment-ready model
    return deployable_model


def deploy_model():
    # TODO: this function is for deploying an evaluated model with the Gradio Python library

    # Should return the url endpoint generated by the Gradio library
    return url_endpoint


# The main function to orchestrate the data loading, data preprocessing, feature engineering, model training, model preparation, model deployment, and model evaluation
def main():
    """
    Main function to execute the tabular classification pipeline.
    """

    # TODO: Step 1. Retrieve or load a dataset from hub (if available) or user's local storage (if given)
    dataset = None

    # TODO: Step 2. Create a train-valid-test split of the data by splitting the `dataset` into train_loader, valid_loader, and test_loader.
    # Here, the train_loader contains 70% of the `dataset`, the valid_loader contains 20% of the `dataset`, and the test_loader contains 10% of the `dataset`.
    train_loader, valid_loader, test_loader = (
        None,
        None,
        None,
    )  # corresponding to 70%, 20%, 10% of `dataset`

    # TODO: Step 3. With the split dataset, run data preprocessing and feature engineering (if applicable) using the "preprocess_data" function you defined
    processed_data = preprocess_data()

    # TODO: Step 4. Define required model. You may retrieve model from available hub or library along with pretrained weights (if any).
    # If pretrained or predefined model is not available, please create the model according to the given user's requirements below using PyTorch and relevant libraries.
    model = None

    # TODO: Step 5. train the retrieved/loaded model using the defined "train_model" function
    # TODO: on top of the model training, please run hyperparamter optimization based on the suggested hyperparamters and their values before proceeding to the evaluation step to ensure model's optimality

    model = train_model()

    # TODO: evaluate the trained model using the defined "evaluate_model" function
    model_performance, model_complexity = evaluate_model()

    # TODO: compress and convert the trained model according to a given deployment platform using the defined "prepare_model_for_deployment" function
    deployable_model = prepare_model_for_deployment()

    # TODO: deploy the model using the defined "deploy_model" function
    url_endpoint = deploy_model()

    return (
        processed_data,
        model,
        deployable_model,
        url_endpoint,
        model_performance,
        model_complexity,
    )

if __name__ == "__main__":
    processed_data, model, deployable_model, url_endpoint, model_performance, model_complexity = main()
    print("Model Performance on Test Set:", model_performance)
    print("Model Complexity:", model_complexity)