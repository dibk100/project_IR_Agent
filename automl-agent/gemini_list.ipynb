{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e0412f",
   "metadata": {},
   "source": [
    "# gemini 확인\n",
    "\n",
    "Step 1 (재현 로직 디버깅):\n",
    "gemini-2.5-flash-lite (빠르고 저렴 → 코드 안정화에 집중)\n",
    "\n",
    "Step 2 (논문 구조 제대로 테스트):\n",
    "gemini-2.5-flash (성능 + 속도 균형)\n",
    "\n",
    "Step 3 (최종 보고·비교):\n",
    "gemini-2.5-pro (최고 성능, 결과 품질 강조할 때)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26f75c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001 Embedding Gecko ['embedText', 'countTextTokens']\n",
      "models/gemini-2.5-pro-preview-03-25 Gemini 2.5 Pro Preview 03-25 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-05-20 Gemini 2.5 Flash Preview 05-20 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash Gemini 2.5 Flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite-preview-06-17 Gemini 2.5 Flash-Lite Preview 06-17 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro-preview-05-06 Gemini 2.5 Pro Preview 05-06 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro-preview-06-05 Gemini 2.5 Pro Preview ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro Gemini 2.5 Pro ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp Gemini 2.0 Flash Experimental ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash Gemini 2.0 Flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-001 Gemini 2.0 Flash 001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp-image-generation Gemini 2.0 Flash (Image Generation) Experimental ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-lite-001 Gemini 2.0 Flash-Lite 001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite Gemini 2.0 Flash-Lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-preview-image-generation Gemini 2.0 Flash Preview Image Generation ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview-02-05 Gemini 2.0 Flash-Lite Preview 02-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview Gemini 2.0 Flash-Lite Preview ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-pro-exp Gemini 2.0 Pro Experimental ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-pro-exp-02-05 Gemini 2.0 Pro Experimental 02-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-exp-1206 Gemini Experimental 1206 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp-01-21 Gemini 2.5 Flash Preview 05-20 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp Gemini 2.5 Flash Preview 05-20 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp-1219 Gemini 2.5 Flash Preview 05-20 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-tts Gemini 2.5 Flash Preview TTS ['countTokens', 'generateContent']\n",
      "models/gemini-2.5-pro-preview-tts Gemini 2.5 Pro Preview TTS ['countTokens', 'generateContent']\n",
      "models/learnlm-2.0-flash-experimental LearnLM 2.0 Flash Experimental ['generateContent', 'countTokens']\n",
      "models/gemma-3-1b-it Gemma 3 1B ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it Gemma 3 4B ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it Gemma 3 12B ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it Gemma 3 27B ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e4b-it Gemma 3n E4B ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e2b-it Gemma 3n E2B ['generateContent', 'countTokens']\n",
      "models/gemini-flash-latest Gemini Flash Latest ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-flash-lite-latest Gemini Flash-Lite Latest ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-pro-latest Gemini Pro Latest ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite Gemini 2.5 Flash-Lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-image-preview Nano Banana ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-preview-09-2025 Gemini 2.5 Flash Preview Sep 2025 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite-preview-09-2025 Gemini 2.5 Flash-Lite Preview Sep 2025 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-robotics-er-1.5-preview Gemini Robotics-ER 1.5 Preview ['generateContent', 'countTokens']\n",
      "models/embedding-001 Embedding 001 ['embedContent']\n",
      "models/text-embedding-004 Text Embedding 004 ['embedContent']\n",
      "models/gemini-embedding-exp-03-07 Gemini Embedding Experimental 03-07 ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-exp Gemini Embedding Experimental ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-001 Gemini Embedding 001 ['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent']\n",
      "models/aqa Model that performs Attributed Question Answering. ['generateAnswer']\n",
      "models/imagen-3.0-generate-002 Imagen 3.0 ['predict']\n",
      "models/imagen-4.0-generate-preview-06-06 Imagen 4 (Preview) ['predict']\n",
      "models/imagen-4.0-ultra-generate-preview-06-06 Imagen 4 Ultra (Preview) ['predict']\n",
      "models/imagen-4.0-generate-001 Imagen 4 ['predict']\n",
      "models/imagen-4.0-ultra-generate-001 Imagen 4 Ultra ['predict']\n",
      "models/imagen-4.0-fast-generate-001 Imagen 4 Fast ['predict']\n",
      "models/veo-2.0-generate-001 Veo 2 ['predictLongRunning']\n",
      "models/veo-3.0-generate-preview Veo 3 ['predictLongRunning']\n",
      "models/veo-3.0-fast-generate-preview Veo 3 fast ['predictLongRunning']\n",
      "models/veo-3.0-generate-001 Veo 3 ['predictLongRunning']\n",
      "models/veo-3.0-fast-generate-001 Veo 3 fast ['predictLongRunning']\n",
      "models/gemini-2.5-flash-preview-native-audio-dialog Gemini 2.5 Flash Preview Native Audio Dialog ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.5-flash-exp-native-audio-thinking-dialog Gemini 2.5 Flash Exp Native Audio Thinking Dialog ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-live-001 Gemini 2.0 Flash 001 ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-live-2.5-flash-preview Gemini Live 2.5 Flash Preview ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-live-preview Gemini 2.5 Flash Live Preview ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-native-audio-latest Gemini 2.5 Flash Native Audio Latest ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025 Gemini 2.5 Flash Native Audio Preview 09-2025 ['countTokens', 'bidiGenerateContent']\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "g_token = os.getenv(\"GEMINI_API_KEY\")\n",
    "# API 키 설정\n",
    "genai.configure(api_key=g_token)\n",
    "\n",
    "# 모델 목록 나열\n",
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name, m.display_name, m.supported_generation_methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e5b42",
   "metadata": {},
   "source": [
    "# 허깅페이스 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f131de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 접근 성공! 모델 이름: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Hugging Face API 객체 생성\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "# 모델 접근 확인\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "try:\n",
    "    model_info = api.model_info(model_id)\n",
    "    print(f\"모델 접근 성공! 모델 이름: {model_info.modelId}\")\n",
    "except Exception as e:\n",
    "    print(f\"모델 접근 실패: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50736b3",
   "metadata": {},
   "source": [
    "# 모델 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6e4e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7bb14a7d704bf79b81d3981781b051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 22:49:30.367236: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-30 22:49:30.373909: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-30 22:49:30.381013: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-30 22:49:30.383563: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-30 22:49:30.389941: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-30 22:49:30.846348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2452404cead34eeeabb1af8743870b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0046f8618eca4b3abcd7d38391f6137c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b224782320e74453b2d5ce5ed1dbb79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dc100c7a414c71a7b658c77436d12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a34fd32556c4a93a8a31a2c808432d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d268e5d8898941508fa37a3dad177e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45824561de14f33a8450f8c28fe96cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c666c61f32a04071872aecd6a5d7d345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c48d87ea75405eb45f23822fc27e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2bd15e253747828de2a735bbef79fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# HF 캐시 이미 설정\n",
    "os.environ[\"HF_HOME\"] = \"/home/dibaeck/hf_cache\"\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model = AutoModel.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA 사용 가능:\", torch.cuda.is_available())\n",
    "print(\"GPU 개수:\", torch.cuda.device_count())\n",
    "print(\"현재 GPU 이름:\", torch.cuda.get_device_name(0))\n",
    "print(\"GPU 메모리 할당:\", torch.cuda.memory_allocated(0))\n",
    "print(\"GPU 메모리 예약:\", torch.cuda.memory_reserved(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018f8f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "from transformers import cached_path\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# 실제 모델 파일 경로 확인\n",
    "path = cached_path(f\"{model_name}/pytorch_model-00001-of-00003.safetensors\")\n",
    "print(path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
