{"task": "time-series classification", "instruction": "As a project manager, your team's objective is to develop a state-of-the-art time series classification model for the Heartbeat dataset, which contains intricate, real-world electrocardiogram patterns. With a train, validation, and test split (90:5:5), design a model using advanced deep learning techniques (Transformer-based architecture) that can handle variable-length sequences (INPUT_SEQ_LEN ranging from 400 to 410) and high-dimensional inputs (INPUT_DIM = 61). Ensure the model's interpretability is maintained while maximizing accuracy up to 98%, and conduct feature importance analysis for the top 10% contributing features during inference.", "resposne": "{\n  \"user\": {\n    \"intent\": \"build\",\n    \"expertise\": \"high\"\n  },\n  \"problem\": {\n    \"area\": \"time-series analysis\",\n    \"downstream_task\": \"time-series classification\",\n    \"application_domain\": \"healthcare\",\n    \"description\": \"The goal is to develop a state-of-the-art time series classification model using a Transformer-based architecture for the Heartbeat dataset with variable-length sequences (400 to 410) and high-dimensional inputs (61). The model should achieve accuracy up to 98% while maintaining interpretability and performing feature importance analysis for the top 10% contributing features.\",\n    \"performance_metrics\": [\n      {\n        \"name\": \"Accuracy\",\n        \"value\": 0.98\n      }\n    ],\n    \"complexity_metrics\": []\n  },\n  \"dataset\": [\n    {\n      \"name\": \"Heartbeat Dataset\",\n      \"modality\": [\"time series\"],\n      \"target_variables\": [\"Heartbeat patterns\"],\n      \"specification\": {\n        \"INPUT_SEQ_LEN\": {\n          \"min\": 400,\n          \"max\": 410\n        },\n        \"INPUT_DIM\": 61\n      },\n      \"description\": \"A real-world dataset with intricate electrocardiogram patterns for time series classification.\",\n      \"preprocessing\": [],\n      \"augmentation\": [],\n      \"visualization\": [],\n      \"source\": \"direct-search\"\n    }\n  ],\n  \"model\": [\n    {\n      \"name\": \"Transformer-based Time Series Classification Model\",\n      \"family\": \"Advanced Deep Learning\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"interpretability\": true,\n        \"feature_importance_analysis\": {\n          \"top_percentage\": 10\n        }\n      },\n      \"description\": \"A model using Transformer architecture for time series classification with variable-length sequences and high-dimensional inputs on the Heartbeat dataset.\"\n    }\n  ],\n  \"knowledge\": [],\n  \"service\": {\n    \"feature_engineering\": [],\n    \"target_device\": \"\",\n    \"deployment_endpoint\": \"\",\n    \"inference_engine\": [],\n    \"demonstration\": \"Include interpretation of top 10% feature importance in model explanation\"\n  },\n  \"confidence\": {\n    \"rationale\": \"The JSON response covers the critical aspects of the user's instruction, such as model type, dataset specifics, performance metric, and interpretability. However, since the detailed specifications on inference time or memory requirements were not provided in the instruction, they are left empty. The model's feature importance analysis is mentioned based on the request.\",\n    \"score\": 0.97\n  }\n}\n"}
{"task": "image classification", "instruction": "As a cutting-edge AI project manager, you've been tasked with developing a state-of-the-art machine learning solution for an underwater biodiversity identification challenge using the Ocean Biodiversity Imaging Dataset. The team must employ the EfficientNet-B7 architecture, combined with novel self-supervised pre-training techniques, to accurately classify and identify not only species but also the health status and behavioral patterns of marine creatures in highly dynamic environments like coral reefs, kelp forests, and deep trenches. The model should demonstrate exceptional performance in low-light scenarios and cope with variable water clarity. Key performance indicators should include species-wise F1-score, inter-observer agreement, and a system-level precision-recall curve. The goal is to achieve an overall precision of 98% while showcasing interpretability of the model's decision-making process.", "resposne": "{\n  \"user\": {\n    \"intent\": \"build\",\n    \"expertise\": \"high\"\n  },\n  \"problem\": {\n    \"area\": \"computer vision\",\n    \"downstream_task\": \"image classification\",\n    \"application_domain\": \"marine biology\",\n    \"description\": \"Developing a state-of-the-art machine learning solution for underwater biodiversity identification using the Ocean Biodiversity Imaging Dataset. The focus is on EfficientNet-B7 architecture with self-supervised pre-training, classifying species, health status, and behavioral patterns in low-light and variable water clarity. Key performance indicators include species-wise F1-score, inter-observer agreement, and a system-level precision-recall curve. A goal of 98% precision is set, and interpretability of model decision-making is essential.\",\n    \"performance_metrics\": [\n      {\n        \"name\": \"Species-wise F1-score\",\n        \"value\": null\n      },\n      {\n        \"name\": \"Inter-observer agreement\",\n        \"value\": null\n      },\n      {\n        \"name\": \"Precision-Recall curve\",\n        \"value\": null\n      },\n      {\n        \"name\": \"Overall precision\",\n        \"value\": 0.98\n      }\n    ],\n    \"complexity_metrics\": []\n  },\n  \"dataset\": [\n    {\n      \"name\": \"Ocean Biodiversity Imaging Dataset\",\n      \"modality\": [\"image\"],\n      \"target_variables\": [\"species\", \"health status\", \"behavioral patterns\"],\n      \"specification\": null,\n      \"description\": \"A dataset for underwater biodiversity identification, focusing on dynamic environments like coral reefs, kelp forests, and deep trenches.\",\n      \"preprocessing\": [],\n      \"augmentation\": [],\n      \"visualization\": [],\n      \"source\": \"direct-search\"\n    }\n  ],\n  \"model\": [\n    {\n      \"name\": \"EfficientNet-B7\",\n      \"family\": \"EfficientNet\",\n      \"type\": \"neural networks\",\n      \"specification\": null,\n      \"description\": \"A state-of-the-art model for image classification, combining with self-supervised pre-training for underwater biodiversity identification.\"\n    }\n  ],\n  \"knowledge\": [],\n  \"service\": {\n    \"feature_engineering\": [],\n    \"target_device\": \"\",\n    \"deployment_endpoint\": \"\",\n    \"inference_engine\": [],\n    \"demonstration\": \"model interpretability\"\n  },\n  \"confidence\": {\n    \"rationale\": \"The JSON response reflects the requirements of the user's instruction for the AI project, including the expert level, problem context, performance metrics, and the specific model to be used. Some fields, such as the exact values for performance metrics, are left as null, assuming those will be filled based on the model's training and evaluation. Interpretability is a key requirement, which is included in the 'demonstration' field of the service section.\",\n    \"score\": 0.97\n  }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for the advanced machine learning team, your mandate is to develop a high-performance time series classification system. Utilizing the complex, unbalanced Heartbeat dataset with hierarchical feature extraction (INPUT_SEQ_LEN=405, INPUT_DIM=61), design an ensemble model that incorporates multiple temporal and frequency domain techniques. Implement cross-validation strategies, fine-tune hyperparameters, and optimize for AUC-ROC score in addition to accuracy. The goal is to achieve state-of-the-art performance on both train, validation, and a undisclosed test set, ensuring robustness and generalization across diverse classes (labels {0, 1}).", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Develop a high-performance time series classification system using the complex, unbalanced Heartbeat dataset with INPUT_SEQ_LEN=405 and INPUT_DIM=61. The model should incorporate multiple temporal and frequency domain techniques, including ensemble methods. Implement cross-validation strategies, fine-tune hyperparameters, and optimize for AUC-ROC and accuracy. The target is to achieve state-of-the-art performance across train, validation, and an undisclosed test set, ensuring robustness and generalization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Heartbeat\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"class\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 405,\n                \"INPUT_DIM\": 61,\n                \"imbalance\": \"unbalanced\",\n                \"classes\": [0, 1]\n            },\n            \"description\": \"A complex and unbalanced time series dataset for classification, with hierarchical feature extraction.\",\n            \"preprocessing\": [\"hierarchical feature extraction\"],\n            \"augmentation\": [\"temporal and frequency domain techniques\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"temporal_methods\": [\"dynamic\", \"temporal convolutional networks\"],\n                \"frequency_domain_methods\": [\"spectral analysis\", \"wavelet transforms\"],\n                \"ensemble_components\": \"multi-level\"\n            },\n            \"description\": \"An ensemble model integrating temporal and frequency domain techniques for time series classification on the Heartbeat dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The Heartbeat dataset requires advanced techniques due to its complexity and unbalanced distribution.\",\n        \"Achieving state-of-the-art performance necessitates fine-tuning and optimization.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"cross-validation strategies\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for AUC-ROC and accuracy\"],\n        \"demonstration\": \"visualizations of model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the project manager's responsibilities to develop a state-of-the-art time-series classification system. The user's intent and expertise level are captured, and the dataset, area, and downstream task are clearly defined. The performance metrics (AUC-ROC and accuracy) and ensemble model specifications are based on the given instruction. However, since the specific values for hyperparameters, AUC-ROC, and accuracy scores are not provided, they are set to null. The confidence score could be higher if these values were more explicitly defined.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "link prediction", "instruction": "Develop a novel Triplet Attention-based Graph Convolutional Recurrent Neural Network (TAGCRN) for the fine-grained prediction of protein-protein interactions in the Highly Dynamic Yeast Cell Cycle Network. The model should not only enhance link prediction accuracy but also incorporate temporal dynamics and context-specific interactions, while maintaining interpretability and computational efficiency under limited computational resources. Additionally, design a user-friendly visualization tool to present the predicted interactions and network fluctuations in the form of heatmaps and dynamic graphs for biologist's intuitive analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Develop a novel Triplet Attention-based Graph Convolutional Recurrent Neural Network (TAGCRN) for fine-grained prediction of protein-protein interactions in the Highly Dynamic Yeast Cell Cycle Network. The model should enhance accuracy, consider temporal dynamics and context-specific interactions, prioritize interpretability and computational efficiency under limited resources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"link prediction accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Highly Dynamic Yeast Cell Cycle Network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"protein-protein interactions\"],\n            \"specification\": null,\n            \"description\": \"A dynamic protein-protein interaction network in yeast's cell cycle, capturing context-specific interactions.\",\n            \"preprocessing\": [\"noise reduction\", \"temporal feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"heatmaps\",\n                \"dynamic graphs\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TAGCRN\",\n            \"family\": \"Graph Convolutional Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretable\": true,\n                \"computational_efficiency\": \"limited\",\n                \"specific_features\": \"Triplet Attention\"\n            },\n            \"description\": \"A deep learning model that incorporates Triplet Attention for protein-protein interaction prediction in the yeast cell cycle network.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Temporal dynamics and context-specific interactions are key in yeast's cell cycle.\",\n        \"Interpretability is important for biologists to understand the model's predictions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"noise reduction\", \"temporal feature engineering\"],\n        \"target_device\": \"PC or cloud\",\n        \"deployment_endpoint\": \"user-friendly interface\",\n        \"inference_engine\": [\"optimized for limited resources\"],\n        \"demonstration\": \"Interactive heatmaps and dynamic graphs for biologist's analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given user instruction, including the need for a custom model (TAGCRN) and visualization tools for biologists. User expertise, the graph machine learning area, and protein-protein interaction prediction task are clearly captured. The provided dataset source, modality, and the importance of interpretability and computational efficiency are also included. However, specific performance targets (accuracy, computational efficiency) are left to be filled based on model results, and the deployment endpoint is suggested to be a user-friendly interface for bioinformatics analysis.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a project manager for a high-traffic video platform, your team is tasked with developing a state-of-the-art recommendation system using the 'Bilibili Danmu' dataset. The project must handle diverse, real-time user engagement data, incorporate deep text analysis, optimize for speed to handle continuous viewing sequences, and demonstrate an effective personalized filtering strategy with minimal latency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"video streaming\",\n        \"description\": \"As a project manager for a high-traffic video platform, your team is required to develop a state-of-the-art recommendation system using the Bilibili Danmu dataset. The system must handle diverse real-time user engagement data, incorporate deep text analysis, optimize for speed to cater to continuous viewing sequences, and demonstrate effective personalized filtering with minimal latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-PR\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Bilibili Danmu\",\n            \"modality\": [\"text\", \"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_size\": null,\n                \"time_series_frequency\": null,\n                \"sequence_length\": null\n            },\n            \"description\": \"Real-time user engagement data from Bilibili's video platform, including text comments and viewing sequences.\",\n            \"preprocessing\": [\"deep text analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning-based recommendation system\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep neural network with recurrent or sequential components\",\n                \"input_embedding_size\": null,\n                \"hidden_layers\": null,\n                \"output_layers\": null\n            },\n            \"description\": \"A state-of-the-art recommendation system designed for real-time video engagement on a high-traffic platform, utilizing text analysis and optimized for low latency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Effective use of deep text analysis for understanding user engagement in the context of video streaming\",\n        \"The importance of optimizing for speed and minimizing latency for a high-traffic platform\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"user profiling\", \"sequence-based encoding\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"production recommendation engine\",\n        \"inference_engine\": [\"TensorFlow serving\", \"Apache MXNet\"],\n        \"demonstration\": \"live A/B testing and user feedback on performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is crafted based on the given project management task. The user's intention to build a recommendation system is clear. The system's requirements, including deep text analysis and speed optimization, are captured, as well as the importance of handling real-time data. Performance metrics like latency, accuracy, and AUC-PR are included without specific values as they are subjective and would typically be fine-tuned in the development process. The dataset, model, and service options are appropriately chosen based on the requirements, but some specifics are left to the actual development team's decision.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for a high-precision media campaign cost prediction, you've received a complex data science challenge. The diverse Media Campaign Cost Dataset, now enhanced with intricate temporal and categorical features, has been divided into non-overlapping train, validation, and test sets. Your objective is to develop a state-of-the-art regression model that accounts for feature interactions and time dependencies. The key metric for success is the minimized root mean squared log error (RMSLE), and you must strive for exceptional performance across all sets. Additionally, provide a detailed report outlining your feature engineering strategy and model interpretation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"Develop a state-of-the-art regression model for media campaign cost prediction in a high-precision marketing context, incorporating temporal and categorical features from the enhanced Media Campaign Cost Dataset. The model must account for feature interactions and time dependencies, with a focus on minimizing RMSLE. A detailed feature engineering strategy and model interpretation report are required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared log error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Media Campaign Cost Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"campaign_cost\"],\n            \"specification\": {\n                \"enhancements\": {\n                    \"temporal_features\": true,\n                    \"categorical_features\": true\n                },\n                \"split\": {\n                    \"train\": \"non-overlapping\",\n                    \"validation\": \"non-overlapping\",\n                    \"test\": \"non-overlapping\"\n                }\n            },\n            \"description\": \"A complex dataset with temporal and categorical features for media campaign cost prediction, divided into train, validation, and test sets.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art regression model (e.g., deep learning, ensemble methods, or time-series-specific model)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_interactions\": true,\n                \"time_dependence\": true\n            },\n            \"description\": \"A regression model designed to handle the complex features, temporal dependencies, and optimized for RMSLE.\"\n        }\n    ],\n    \"knowledge\": [\n        \"In this project, account for feature interactions and temporal dependencies are crucial for accurate cost predictions.\",\n        \"RMSLE minimization highlights the focus on precision in the model performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A detailed feature engineering and model interpretation report will be provided\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the project manager's perspective and the requirements for a complex data science project. The user's expertise level is high, and the project area is in tabular data analysis (marketing) with a regression task. The dataset properties and the need for state-of-the-art model and feature engineering are accurately captured. The performance metric (RMSLE) is specified without a target value, indicating that it might be a goal to achieve or a metric to be calculated during development. The confidence score is high due to the clear task description and mapping to the JSON schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a seasoned project manager, I need you to develop an AI-driven plant species classifier using the 'Plant Seedlings Dataset' for a high-precision, real-time botany application. In addition to recommending a cutting-edge deep learning architecture, detail the expected accuracy above 95% in different lighting conditions and propose optimization techniques to ensure at least 200ms response time for mobile deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"botany\",\n        \"description\": \"Develop an AI-driven plant species classifier for a high-precision, real-time botany application using the 'Plant Seedlings Dataset'. Expected accuracy should be above 95% in various lighting conditions, and optimization techniques must ensure at least 200ms response time for mobile deployment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95,\n                \"condition\": \"various lighting conditions\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.2,\n                \"unit\": \"seconds\",\n                \"target_device\": \"mobile\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Seedlings Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\"],\n            \"specification\": null,\n            \"description\": \"A dataset for plant seedlings classification, suitable for a real-time botany application.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning (specifically recommended by you, based on context)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art for plant species classification\",\n                \"mobile_optimized\": true\n            },\n            \"description\": \"A deep learning model designed for high-precision plant species classification in real-time, adaptable to varying lighting conditions.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimization techniques for real-time and mobile deployment\"\n        ],\n        \"demonstration\": \"A mobile-friendly UI design with live inference examples showcasing accuracy and response times\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a classifier with a high accuracy requirement. Expertise level is assumed to be high due to the 'seasoned project manager' context. The problem area, downstream task, and application domain are derived from the specific plant classification task. The dataset is specified as image-based, and source is a direct search. The model family, architecture, and response time optimization requirements are directly addressed in the instruction. However, if there are specific cutting-edge architectures you have in mind, this would need to be included.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for an advanced land use analysis team, we're seeking a computationally efficient satellite image analysis expert who specializes in utilizing cutting-edge segmentation models (e.g., U-Net, Mask R-CNN, or DeepLabV3+) for precise classification of complex land features, including waterbodies, dense forests, and sprawling urban landscapes. The challenge lies in optimizing model performance within a reasonable computational window of a few hours, while maintaining accuracy and generalizability across the diverse Sentinel-2 dataset. Provide a detailed proposal outlining your preferred model, its training process, and expected runtime analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning and land use\",\n        \"description\": \"Seeking a computationally efficient satellite image analysis model, specifically focusing on advanced segmentation techniques like U-Net, Mask R-CNN, or DeepLabV3+, for precise classification of waterbodies, dense forests, and urban landscapes. The objective is to optimize performance within a few hours while maintaining accuracy and generalizability across the diverse Sentinel-2 dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"speed (CPU time)\",\n                \"value\": \"a few hours\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"waterbodies\", \"dense forests\", \"urban landscapes\"],\n            \"specification\": {\n                \"size\": {\n                    \"width\": null,\n                    \"height\": null,\n                    \"bands\": null\n                },\n                \"temporal_resolution\": null,\n                \"spectral_resolution\": null\n            },\n            \"description\": \"A diverse satellite dataset with high-resolution imagery for land use analysis.\",\n            \"preprocessing\": [\"data normalization, resizing, and augmentation\"],\n            \"augmentation\": [\"geometric transformations, random noise\"],\n            \"visualization\": [\"pixel-level, class maps, and saliency maps\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net, Mask R-CNN, or DeepLabV3+\",\n            \"family\": \"deep learning segmentation models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"U-Net, Mask R-CNN, or DeepLabV3+\",\n                \"hyperparameters\": {\n                    \"batch size\": null,\n                    \"learning rate\": null,\n                    \"optimization algorithm\": null\n                },\n                \"optimized for\": \"computational efficiency\",\n                \"inference speed\": null\n            },\n            \"description\": \"A state-of-the-art deep learning model for multi-class satellite image segmentation.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction from raw Sentinel-2 data\"],\n        \"target_device\": \"optimized for on-premise or cloud-based high-performance GPU computing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"TensorFlow, PyTorch, or open-source libraries\"],\n        \"demonstration\": \"Performance analysis including runtime and accuracy comparison with baselines\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a model, high level of expertise, and the problem's focus on satellite image analysis and segmentation. The area of computer vision, specific downstream task, and application domain are specified. The metrics (accuracy and CPU time) are included based on the optimization goal, but the exact values are left open for the model selection. The dataset, models, and services sections are tailored to the context, while leaving room for implementation-specific details.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "time-series anomaly detection", "instruction": "As a machine learning expert, you've been tasked with developing a cutting-edge, explainable-time-series model that fuses traditional statistical methods with advanced deep learning techniques for real-time detection of not only anomalous server traffic patterns but also subtle hardware failures in a multi-tier, high-availability data center environment. The model must incorporate autoencoder architectures for efficient feature extraction and employ a novel, hierarchical anomaly scoring system to prioritize critical issues. Additionally, the project should include a custom visualization dashboard to present alerts and anomaly profiles in an actionable and user-friendly manner, while adhering to stringent latency and resource constraints. The deployment should involve integrating the model into the existing distributed infrastructure and formulating a robust monitoring system with automated recovery mechanisms. Provide a detailed project roadmap, expected performance metrics, and a methodology for continuous model reevaluation and fine-tuning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"infrastructure management\",\n        \"description\": \"Develop an explainable-time-series model combining traditional statistical methods and deep learning for real-time detection of anomalous server traffic and hardware failures in a high-availability data center. The model should use autoencoders for feature extraction, implement a hierarchical anomaly scoring system, and include a custom visualization dashboard for actionable alerts. Project must adhere to latency constraints, resource efficiency, and involve model integration, monitoring, and automated recovery.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Model accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Latency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Resource efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Anomaly detection precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Anomaly detection recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multi-tier data center logs\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_format\": \"structured\",\n                \"sample_rate\": null,\n                \"feature_dimensions\": null\n            },\n            \"description\": \"Logs from various server tiers, including hardware performance and network traffic data.\",\n            \"preprocessing\": [\n                \"Data cleaning and normalization\",\n                \"Feature extraction for autoencoder training\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Autoencoder with hierarchical anomaly scoring\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Autoencoder-based\",\n                \"methodologies\": \"Statistical fusion with deep learning\",\n                \"feature_extraction_technique\": \"Efficient autoencoder\"\n            },\n            \"description\": \"A state-of-the-art time-series model that combines traditional and deep learning for anomaly detection with autoencoder-based feature extraction and hierarchical scoring system.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Application of explainability for model understanding and trust\",\n        \"Custom visualization for effective communication of anomalies to stakeholders\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Autoencoder model training and fine-tuning\",\n            \"Monitoring infrastructure integration\"\n        ],\n        \"target_device\": \"Real-time, distributed infrastructure\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Distributed compute clusters\"],\n        \"demonstration\": \"Dashboard with actionable anomaly profiles and recovery guidance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response reflects the provided instruction, capturing the user's intent to build a cutting-edge model, their high expertise level, and the specific requirements for the problem domain (real-time anomaly detection with hierarchical scoring, etc.). The requirements for datasets, preprocessing, performance metrics, model details, and service aspects are extracted from the instruction. However, actual metric values, such as accuracy and latency, are left unspecified since they depend on actual model performance and engineering efforts. Confidence score is high due to the clear mapping to the JSON schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "The project for the legal team requires a sophisticated, privacy-compliant question-answering system, which should employ an enhanced DeBERTa variant. This model must demonstrate multi-step reasoning to decipher legal documents from the LegalQA dataset, ensuring compliance with GDPR while maintaining high accuracy and providing detailed explanations of its answers in legal terminology. Additionally, outline a detailed deployment strategy and include metrics for continuous model monitoring and improvement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"legal\",\n        \"description\": \"Design a privacy-compliant question-answering system using an enhanced DeBERTa variant. The model must handle multi-step reasoning on legal documents from the LegalQA dataset and adhere to GDPR. It should provide detailed explanations in legal terminology and have a deployment strategy along with continuous monitoring and improvement metrics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explanation_comprehensiveness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"GDPR_compliance\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LegalQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"document_type\": \"legal\",\n                \"size\": null,\n                \"language\": null\n            },\n            \"description\": \"A dataset for legal question answering, containing real-world documents with multi-step reasoning challenges.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced DeBERTa\",\n            \"family\": \"Transformer-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": null,\n                \"multi_step_reasoning\": true\n            },\n            \"description\": \"A privacy-enhanced DeBERTa variant specifically designed for legal question answering with multi-step reasoning capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Ensuring GDPR compliance while maintaining high accuracy demands a thorough understanding of privacy regulations in the legal context.\",\n        \"Detailed explanations in legal terminology require a model capable of generating structured responses with context.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Legal terminology adaptation\",\n            \"GDPR compliance techniques\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": {\n            \"service\": \"cloud-based platform\",\n            \"environment\": \"secure\"\n        },\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"Interactive interface with detailed explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a machine learning model (expertise level: high). The problem area, downstream task, and dataset are clearly defined based on the legal question answering scenario. Performance metrics (accuracy, explanation comprehensiveness) and complexity metric (GDPR compliance) are identified. The model specification, deployment strategy, and metrics for continuous improvement are included according to the user's requirements. However, exact metric values and some dataset specifications (like size and language) are not provided, as they are not explicitly mentioned in the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the Marine Biodiversity AI initiative, task the machine learning team with a multifaceted challenge: Develop a state-of-the-art PSPNet++ model for underwater image segmentation on CoralNet, focusing on enhanced coral and marine life recognition in complex underwater environments. Require the model to exhibit adaptability to water turbidity gradients, natural lighting fluctuations, and temperature variations, and target a substantial and statistically significant increase in Dice Score compared to previous top-performing models, with detailed evaluation of model robustness through cross-validation techniques.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"environmental science\",\n        \"description\": \"Develop a state-of-the-art PSPNet++ model for underwater image segmentation on CoralNet, focusing on coral and marine life recognition in challenging underwater conditions like water turbidity gradients, natural lighting fluctuations, and temperature variations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Dice Score\",\n                \"value\": { \"increase\": \"statistically significant\", \"comparison\": \"previous top-performing models\" }\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CoralNet\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"underwater images\"],\n            \"specification\": {\n                \"adaptability\": {\n                    \"water_turbidity\": \"gradient\",\n                    \"lighting_fluctuations\": \"natural\",\n                    \"temperature_variations\": \"considered\"\n                }\n            },\n            \"description\": \"A dataset for underwater image segmentation, particularly focusing on coral and marine life in complex underwater environments.\",\n            \"preprocessing\": [\"image normalization for varying conditions\"],\n            \"augmentation\": [\n                \"data augmentation techniques for turbidity, lighting, and temperature variations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"PSPNet++\",\n            \"family\": \"Semantic Segmentation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptivity\": true,\n                \"robustness\": \"cross-validation techniques\"\n            },\n            \"description\": \"A state-of-the-art deep learning model for underwater image segmentation with improved performance on coral and marine life recognition.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Challenges include adapting to complex underwater environments and evaluating robustness against varying conditions.\",\n        \"Improvement on Dice Score indicates a focus on enhancing model accuracy in segmentation tasks.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"detailed evaluation and comparison with previous models\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction by indicating a 'build' intent for a machine learning project. The problem area, task, target metrics, adaptability requirements, and dataset details are clearly stated. Performance metrics, like Dice Score increase, are specified, along with methods for model robustness evaluation. The expert level of the user and the importance of cross-validation are accurately captured. However, some specific aspects like exact values for metrics or additional preprocessing steps are not provided as they are not in the given instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for the multilingual AI team, we need to develop a cutting-edge machine translation system capable of handling underrepresented tongues, specifically focusing on Swahili to English. Utilize the extensive Masakhane dataset, ingeniously integrating Transformer and RNN architectures for optimal performance. Your objective is to push the boundaries by enhancing BLEU scores by a substantial 15% and to conduct exhaustive comparative analysis with prior state-of-the-art models, ensuring minimal errors and cultural sensitivity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"international communication\",\n        \"description\": \"Develop a cutting-edge machine translation system focusing on Swahili to English translation. Utilize the Masakhane dataset and integrate Transformer and RNN architectures for optimal performance, targeting a 15% improvement in BLEU scores and ensuring minimal errors and cultural sensitivity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score improvement\",\n                \"value\": 0.15\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Masakhane\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": null,\n            \"description\": \"A multilingual dataset specifically focusing on underrepresented languages like Swahili, designed for machine translation tasks.\",\n            \"preprocessing\": [\"data cleaning, tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Swahili-English MT with Transformer and RNN\",\n            \"family\": [\"Transformer\", \"Recurrent Neural Networks\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"Integrating Transformer and RNN for Swahili to English translation\",\n                \"target_improvement\": \"BLEU score\"\n            },\n            \"description\": \"A state-of-the-art machine translation system designed to handle Swahili and English, utilizing the Masakhane dataset's unique challenges.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Swahili is an underrepresented language in machine translation literature.\",\n        \"Transformer and RNN integration can significantly enhance performance.\",\n        \"Cultural sensitivity is critical for accurate translations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain adaptation, noise reduction\"],\n        \"target_device\": \"high-performance computing environment\",\n        \"deployment_endpoint\": \"custom translation service\",\n        \"inference_engine\": [\"GPU inference\"],\n        \"demonstration\": \"error analysis and user-centric evaluation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been parsed based on the provided user instruction. The intent to build a machine translation system is clearly defined, along with the requirements for the expert-level user. The Masakhane dataset, architectures, and performance metrics (specifically the BLEU score improvement) have been incorporated. However, additional details might be needed for preprocessing, augmentation, and model specifics (e.g., model size, training time).\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "For my advanced astrophotography project, I require a deep learning model to classify diverse celestial objects in nebulae, pulsars, and exoplanets. The dataset must be large-scale, unlabeled, and withstand noisy images. Evaluate the most suitable architecture (CNN or transformers) for this challenge and discuss the required preprocessing techniques to ensure state-of-the-art accuracy despite the complexity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"For an advanced astrophotography project, a deep learning model is needed to classify celestial objects such as nebulae, pulsars, and exoplanets. The dataset is large-scale, unlabeled, and should handle noisy images.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"state-of-the-art accuracy\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"train\": \"large-scale\",\n                    \"validation\": \"large-scale\",\n                    \"test\": \"large-scale\"\n                },\n                \"label_type\": \"unlabeled\",\n                \"noise_level\": \"noisy\"\n            },\n            \"description\": \"A large, unlabeled dataset suitable for astrophysical image classification, designed to handle noisy images.\",\n            \"preprocessing\": [\n                \"image cleaning or denoising techniques\",\n                \"image augmentation for handling unlabeled data\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": [\n                {\"architecture\": \"CNN\", \"reasoning\": \"Suitable for image data\"},\n                {\"architecture\": \"transformers\", \"reasoning\": \"Potentially effective for handling complex tasks if proven beneficial by experimentation\"}\n            ],\n            \"specification\": {\n                \"training Process\": {\n                    \"accuracy_target\": \"state-of-the-art\"\n                }\n            },\n            \"description\": \"Deep learning model that explores both CNNs and transformers for celestial object classification, with a focus on achieving state-of-the-art accuracy even in noisy images.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Considering the complexity of astrophotography, experimenting with state-of-the-art architectures like transformers could be advantageous.\",\n        \"Appropriate preprocessing techniques and data augmentation methods are crucial for handling unlabeled and noisy images.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"deep feature extraction\", \"transfer learning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the high-level requirements of the project: a machine learning model to classify celestial objects, handling a large-scale, unlabeled, and noisy dataset. It highlights the exploration of both CNN and transformers, as well as the need for preprocessing and state-of-the-art accuracy. However, specific models and preprocessing techniques are left open for selection based on further research or user preferences.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "question answering", "instruction": "The project's primary objective is to develop a highly specialized, sentiment-aware QA system for real-time e-commerce customer support, leveraging the AmazonQA dataset. Utilize a state-of-the-art DistilBERT model fine-tuned for product-related intent detection and emotion understanding. Mandate an exceptional response time of 0.9 seconds to guarantee seamless user experiences and enhance customer loyalty through prompt interactions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Develop a sentiment-aware QA system for real-time e-commerce customer support, specifically focusing on product-related intent detection and emotion understanding, using the AmazonQA dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.9,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AmazonQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"intent\", \"emotion\"],\n            \"specification\": {\n                \"domain\": \"e-commerce\",\n                \"task\": \"QA\",\n                \"product-related data\": \"required\"\n            },\n            \"description\": \"A dataset designed for fine-tuning DistilBERT for product intent detection and emotion understanding in e-commerce customer support.\",\n            \"preprocessing\": [\"sentiment analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"DistilBERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"task\": [\"intent detection\", \"emotion understanding\"],\n                \"fine-tuned\": \"state-of-the-art\"\n            },\n            \"description\": \"A DistilBERT model optimized for sentiment-aware question answering in real-time e-commerce customer support.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art DistilBERT for product-related intent and emotion understanding in a QA system is critical.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"sentiment enrichment\"],\n        \"target_device\": \"high-performance hardware\",\n        \"deployment_endpoint\": \"real-time customer support server\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"response time demonstration and user feedback loop\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the given instruction. The intent to build is clear, expertise level is high, and the specific area (NLP), downstream task (QA), and dataset (AmazonQA) are identified. Performance metrics like response time and inference time are mentioned, with the response time set to 0.9 seconds to meet the requirement. The source and preprocessing of the dataset are explained, and the model fine-tuning details are specified. Confidence score is 0.97 due to the direct mapping to the schema and assumed high level of understanding.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "object detection", "instruction": "As a specialized project manager, your mandate is to oversee the design of an advanced hazard detection system for precision manufacturing. The team must utilize the 'COCO-Extended Industrial Dataset,' combining 500,000 images capturing diverse industrial scenarios. Apply a specialized Anchor-Free CNN architecture, Enhanced Industrial SafetyNet, aiming for a minimum precision of 97% in detecting safety equipment conformity and minimizing trip hazards. Additionally, require the model to demonstrate robustness in low-light and motion-blurred conditions, with weekly performance monitoring and a clear implementation plan for deployment in real-time monitoring systems.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"precision manufacturing\",\n        \"description\": \"Design an advanced hazard detection system using the COCO-Extended Industrial Dataset. The system requires a specialized Anchor-Free CNN architecture, Enhanced Industrial SafetyNet, with a minimum precision of 97% for safety equipment conformity and trip hazard detection. The model must demonstrate robustness in low-light and motion-blurred conditions, and include weekly performance monitoring and an implementation plan for real-time monitoring.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97,\n                \"target\": true\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"robustness (low light)\",\n                \"value\": \"optimized\",\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"robustness (motion-blurred)\",\n                \"value\": \"optimized\",\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO-Extended Industrial Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": 500000,\n                \"data_distribution\": \"diverse industrial scenarios\"\n            },\n            \"description\": \"A dataset containing 500,000 images for industrial scenarios, including hazard detection and safety equipment conformity\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                {\n                    \"technique\": \"low-light simulation\"\n                },\n                {\n                    \"technique\": \"motion blur\"\n                }\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Industrial SafetyNet\",\n            \"family\": \"Anchor-Free CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Advanced Anchor-Free\",\n                \"performance_conditions\": \"low light and motion-blurred robustness\"\n            },\n            \"description\": \"A specialized CNN for industrial hazard detection, focusing on Enhanced Industrial SafetyNet with a precision requirement of at least 97% for safety equipment conformity and trip hazard detection.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time monitoring systems\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"real-time\"\n        ],\n        \"demonstration\": \"weekly performance monitoring and clear deployment plan\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's task, capturing the project's focus, the architecture details, performance requirements, and additional constraints. The high level of expertise is assumed based on the advanced task and requirements. The dataset details and model specifications match the instruction. However, without further specifics on the exact performance metrics for low-light and motion-blur robustness, they are set to 'optimized'. The confidence score is high but not 1.0 due to the lack of explicit target values for complexity metrics and the absence of deployment endpoint.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "For a cross-lingual challenge, design a real-time, energy-efficient Transformer-XL model that translates TED Talks from English to Portuguese. The tool must cater to low-resource devices, achieving near-native accuracy while minimizing battery drain and Ultra-Lightweight design for optimal user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual\",\n        \"description\": \"Design a real-time Transformer-XL model for translating TED Talks from English to Portuguese, targeting low-resource devices with a focus on energy efficiency and Ultra-Lightweight design for optimal user experience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"near-native accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"Battery drain\"\n            },\n            {\n                \"name\": \"Ultra-Lightweight design\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TED Talks (English to Portuguese)\",\n            \"modality\": [\"text\", \"audio\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": null,\n            \"description\": \"A cross-lingual dataset containing TED Talks for training the real-time translation model.\",\n            \"preprocessing\": [\n                \"audio-to-text transcription\"\n            ],\n            \"augmentation\": [\n                \"back-translation\"\n            ],\n            \"visualization\": [\n                \"neural network architecture visualization\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"low-resource optimization\": true,\n                \"real-time performance\": true,\n                \"energy efficiency\": {\n                    \"optimize_for\": \"low-resource devices\"\n                },\n                \"ultra-lightweight\": {\n                    \"optimization level\": \"optimimal\"\n                }\n            },\n            \"description\": \"A real-time, energy-efficient Transformer-XL model tailored for low-resource devices, translating between English and Portuguese with efficiency in mind.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Low-resource language translation requires transfer learning and adaptation strategies.\",\n        \"Real-time performance and energy efficiency are crucial for portable devices.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"dynamic vocabulary expansion\"\n        ],\n        \"target_device\": \"low-resource devices\",\n        \"deployment_endpoint\": \"API for real-time translation\",\n        \"inference_engine\": [\n            \"lightweight inference methods\"\n        ],\n        \"demonstration\": \"User-friendly mobile app or web interface for on-the-go translation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the user's instructions accurately, with a high expertise level for building a machine learning project. The problem description covers the task of cross-lingual translation, low-resource devices, and performance metrics. The dataset includes English TED Talks and targets text-to-text conversion. The model is specified as a Transformer-XL with real-time and energy-efficient characteristics. The needed services, such as API deployment and user experience design, are also included. However, specific performance targets and complexity metrics values are not provided, which would be required for a full confidence score.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "node classification", "instruction": "Utilize the extensive Mouse Brain Structural Connectivity dataset, which encompasses millions of synapse connections and fine-grained spatial details, to design a highly efficient Graph Wavelet Neural Network (GWNN) for node classification. Emphasize on optimizing precision and recall to precisely distinguish among rare and abundant neuron types, while ensuring the model efficiently accounts for intricate multi-scale spatial dependencies within the brain's intricate wiring.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Design a Graph Wavelet Neural Network (GWNN) using the Mouse Brain Structural Connectivity dataset with millions of synapse connections and fine-grained spatial details. The objective is to optimize precision and recall for rare and abundant neuron types, while accounting for multi-scale spatial dependencies efficiently.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Structural Connectivity Dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"synapses\": \"millions\",\n                    \"spatial_details\": \"fine-grained\"\n                },\n                \"dimensions\": {\n                    \"connectivity_data\": \"multi-scale spatial dependencies\"\n                }\n            },\n            \"description\": \"A large-scale dataset for graph-based analysis, detailing synapse connections and the brain's intricate wiring with fine-grained resolution.\",\n            \"preprocessing\": [\"data cleaning, normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"structural connectivity maps\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Wavelet Neural Network (GWNN)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization\": [\"efficiency in terms of FLOPs\", \"handling large graph data\"]\n            },\n            \"description\": \"A Graph Wavelet Neural Network designed for node classification, targeting precision and recall optimization for distinct neuron types in the Mouse Brain Structural Connectivity dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The dataset's complexity and scale require a highly efficient model like GWNN.\",\n        \"Precision and recall are crucial for distinguishing rare and abundant neuron types.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction from multi-scale data\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for edge or cloud\"],\n        \"demonstration\": \"Model performance demonstration on diverse neuron types\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the detailed requirements in the user's instruction, including the dataset, model selection, and performance metrics. It accounts for the user's high expertise level, the complexity of the brain connectivity data, and the focus on precision and recall. However, specific numerical targets for precision and recall are not provided, so they are set to null. The service section mentions a need for optimized inference engine and feature extraction from multi-scale data, but no exact implementation details are given.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "For a cutting-edge machine translation project, design a state-of-the-art bidirectional LSTM model utilizing the extended Europarl v11 dataset, considering low-resource languages and domain-specific nuances. In addition to accuracy, optimize the model for minimal latency (<200 milliseconds), taking into account hardware constraints and real-time requirements for an international multilingual chat application supporting high-frequency, simultaneous conversations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"Design a state-of-the-art bidirectional LSTM model for a machine translation project targeting low-resource languages, with an emphasis on domain-specific nuances using the extended Europarl v11 dataset. The model should optimize for minimal latency (<200 milliseconds), considering hardware constraints and real-time requirements for a high-frequency, simultaneous international multilingual chat application.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 0.2\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extended Europarl v11\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"low_resource_languages\": true,\n                \"domain_nuances\": true,\n                \"specific_text_domain\": \"international multilingual chat\"\n            },\n            \"description\": \"A large-scale dataset for low-resource machine translation, with domain-specific nuances in the extended Europarl v11.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Bidirectional LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real_time_performance\": true,\n                \"latency_target\": 200\n            },\n            \"description\": \"A state-of-the-art model designed for low-resource machine translation with a focus on domain-specific nuances, using bidirectional LSTM architecture.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Low-resource language translation challenges and the importance of domain adaptation for improved performance.\",\n        \"Need for optimizing latency for real-time, high-frequency conversations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"optimized for hardware constraints\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time\"\n        ],\n        \"demonstration\": \"Support for high-frequency, simultaneous conversations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers all aspects of the given instruction: user's intent, problem details (area, task, hardware constraints, and application domain), dataset specifics, and model requirements (state-of-the-art, latency, and constraints). However, the accuracy metric is left undefined as it's typically a result of model training and evaluation. Similarly, the model's latency is provided as a hard constraint. The user's level of expertise is assumed to be high given the advanced modeling requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As the project manager, your objective is to enhance the smart city parking app by integrating not only real-time space detection but also forecasting future parking trends based on historical data. Mandate the M2Det model to handle complex urban scenarios with varying architectures, ensuring a 95% detection rate under challenging conditions like occlusions and dynamic lighting. The system must also include a self-adaptive algorithm to optimize resource allocation, aiming for sub-150ms response times while maintaining energy efficiency for edge deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city\",\n        \"description\": \"Enhance the smart city parking app by integrating real-time space detection with M2Det model for complex urban scenarios, maintaining a detection rate of 95% under occlusions and dynamic lighting. Additionally, require a forecasting system for future parking trends based on historical data, and implement a self-adaptive algorithm for resource allocation with sub-150ms response times and energy efficiency for edge deployment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"detection rate\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 150,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": \"optimized\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"smart city parking data\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"variations\": {\n                    \"complex urban scenarios\": {\n                        \"occlusions\": true,\n                        \"dynamic lighting\": true\n                    }\n                }\n            },\n            \"description\": \"Historical and real-time data for training and evaluating M2Det model for parking space detection\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M2Det\",\n            \"family\": \"Object Detection Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"sophistication\": \"for complex urban scenarios\",\n                \"detection_rate\": \"under 95% occlusions and dynamic lighting\"\n            },\n            \"description\": \"Advanced model for real-time object detection specifically for smart city parking, designed to handle varying architectures with complex urban scenarios.\"\n        },\n        {\n            \"name\": \"forecasting system\",\n            \"family\": \"time-series forecasting\",\n            \"type\": \"statistical models\",\n            \"specification\": {\n                \"functionality\": \"parking trend forecasting\"\n            },\n            \"description\": \"A data-driven approach to predict future parking trends based on historical data\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"self-adaptive resource allocation algorithm\"],\n        \"target_device\": \"edge\",\n        \"deployment_endpoint\": \"smart city parking app\",\n        \"inference_engine\": [\"optimized for energy efficiency\"],\n        \"demonstration\": \"response times below 150ms and energy-efficient for edge deployment\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a model and enhance the parking app. The area and downstream task are identified as computer vision and object detection, respectively. The performance metrics and constraints on response times and energy efficiency are clearly captured. The details of the M2Det model and its requirements are specified. The historical data source is assumed to be a user-provided link. The knowledge about self-adaptive algorithm and edge deployment is incorporated. Missing information might include more details on preprocessing steps and specific forecasting methods.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a seasoned project manager for a high-performing machine learning team, your objective is to tackle a multifaceted data science challenge. Using the Crab Age Dataset with intricate mix of categorical and numerical variables, you must create an ensemble of models, optimize feature selection, and apply time-series techniques. The task is to not only develop a regression model but also implement a cross-validation strategy and tune hyperparameters for enhanced performance. The ultimate goal is to minimize the mean absolute error (MAE) on unseen train, validation, and test sets, demonstrating superior predictive accuracy in a step-by-step, methodical process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"multifaceted data science challenge\",\n        \"description\": \"Create an ensemble of models, optimize feature selection, and apply time-series techniques to the Crab Age Dataset with mixed variables. Develop a regression model with a focus on cross-validation, hyperparameter tuning, and minimizing mean absolute error (MAE) on unseen sets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Crab Age Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"age\"],\n            \"specification\": {\n                \"categorical_features\": {\n                    \"type\": \"boolean_array\",\n                    \"description\": \"Whether each variable is categorical or numerical\"\n                },\n                \"time_series_variables\": {\n                    \"type\": \"boolean_array\",\n                    \"description\": \"Whether any variables represent time series\"\n                }\n            },\n            \"description\": \"A dataset containing a mix of categorical and numerical variables, used for a multifaceted data science challenge.\",\n            \"preprocessing\": [\n                \"One-hot encoding for categorical variables\",\n                \"Time-series feature extraction for time-series variables\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Ensemble Regression\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_variants\": [\n                    {\"model1\": {\"description\": \"First model variant\"}},\n                    {\"model2\": {\"description\": \"Second model variant\"}}\n                ],\n                \"cross_validation\": {\n                    \"type\": \"k-fold\",\n                    \"folds\": 5,\n                    \"description\": \"Use k-fold cross-validation strategy\"\n                },\n                \"hyperparameter_tuning\": {\n                    \"description\": \"Hyperparameter tuning using grid search or Bayesian optimization\"\n                }\n            },\n            \"description\": \"An ensemble of models with optimized feature selection and time-series techniques, for tabular regression on the Crab Age Dataset.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Feature importance analysis\", \"Model interpretation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Custom time-series libraries\", \"Ensemble model inference\"],\n        \"demonstration\": \"Step-by-step demonstration of model development process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the given instruction, including the user's intent (build), high expertise level, the specific area and task (tabular data analysis with time-series techniques), the challenge of feature selection and model ensemble, the requested cross-validation and hyperparameter tuning for enhanced performance, and the final goal of minimizing MAE. However, the actual model variants, pre-processing steps, and inference engines were not detailed due to their complexity, and the mean absolute error target value is not provided, leaving it null.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "Develop a sophisticated, real-time multimodal forensics system using the extensive DocTamper dataset. Design a fusion architecture integrating text analysis via deep convolutional neural networks (CNNs) and recurrent neural networks (RNNs) for textual irregularities. Utilize image processing techniques like convolutional autoencoders and feature-level fusion. Strive for a minimum 95% accuracy with incremental improvements in false positive reduction, and ensure the creation of an intuitive, interactive interface catering to non-experts with minimalistic visual cues and clear explanations of detected anomalies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"cybersecurity\",\n        \"description\": \"Develop a sophisticated, real-time multimodal forensics system using the DocTamper dataset. The system should combine text analysis with deep CNNs and RNNs for text irregularities, and image processing through convolutional autoencoders with feature-level fusion. Target accuracy is at least 95% with a focus on reducing false positives. The system should have an intuitive, interactive interface for non-experts with minimalistic visual cues and clear explanations of anomalies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": 1.0\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DocTamper\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"description\": \"Extensive dataset for multimodal forgery detection\"\n            },\n            \"description\": \"A multimodal dataset for detecting text and image irregularities in documents\",\n            \"preprocessing\": [\n                \"Text preprocessing for deep learning\",\n                \"Image resizing and normalization\"\n            ],\n            \"augmentation\": [\n                \"Adversarial data augmentation for both text and images\"\n            ],\n            \"visualization\": [\n                \"Anomaly detection heatmap for images\",\n                \"Confidence score highlighting in textual content\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Multimodal Fusion Architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"text_analysis\": {\n                    \"CNN\": {},\n                    \"RNN\": {}\n                },\n                \"image_processing\": {\n                    \"Convolutional Autoencoder\": {}\n                },\n                \"fusion\": \"Feature-level\"\n            },\n            \"description\": \"A fusion model combining text and image analysis for real-time multimodal forgery detection\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incremental improvements require careful hyperparameter tuning and model selection.\",\n        \"Fusion architecture design needs to balance detection accuracy and false positive reduction.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Text embeddings (e.g., Word2Vec, BERT) for better analysis\",\n            \"Image-to-text alignment for more accurate comparison\"\n        ],\n        \"target_device\": \"real-time capable device\",\n        \"deployment_endpoint\": \"Web-based or cloud platform\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"Interactive demo with clear demonstrations of anomalies and explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is tailored to the provided instruction, covering the required aspects of a machine learning project. User's intent to build, high expertise, problem area, target performance, and details about the dataset, models, and requirements for interface and service are included. However, some implementation-specific details like specific CNN and RNN architectures or exact feature engineering methods were left unspecified to allow for flexibility.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "For an avant-garde literature translation venture, you've been commissioned to develop a quantum neural machine translation system that not only translates rare Korean literary works into English with Shakespearean fluency but also captures the original author's poetic nuances. You possess a private collection of 100,000 hand-picked, lesser-known Korean epics, and need to ensure near-real-time processing while maintaining the cultural subtleties in dialogue. Please outline the technical stack, data preprocessing methods, and any cutting-edge model adaptations required for this challenging task.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"literature translation\",\n        \"description\": \"Develop a quantum neural machine translation system to translate rare Korean literary works into English with Shakespearean fluency, preserving the author's poetic nuances. The system must handle a private collection of 100,000 lesser-known Korean epics and require near-real-time processing while maintaining cultural subtleties.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"translation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Shakespearean fluency metric\",\n                \"value\": null\n            },\n            {\n                \"name\": \"poetic nuances preservation metric\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"near-real-time\",\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Private Korean Epics Collection\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": 100000,\n                \"languages\": [\"Korean\", \"English\"],\n                \"domain\": \"literature\",\n                \"rarity\": \"lesser-known\"\n            },\n            \"description\": \"A private collection of 100,000 hand-picked rare Korean epics for training and evaluation.\",\n            \"preprocessing\": [\"data cleaning\", \"tokenization\", \"morphological analysis\"],\n            \"augmentation\": [\"quantum data augmentation\"],\n            \"visualization\": [\"data flow analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Quantum Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"quantum-enhanced neural network\",\n                \"model Adapations\": [\n                    \"Shakespearean style transfer\",\n                    \"Poetic nuance preservation algorithms\"\n                ]\n            },\n            \"description\": \"A cutting-edge machine translation model utilizing quantum neural networks to capture rare Korean literary fluency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Quantum-enhanced processing is needed for near-real-time translation\",\n        \"Adapting existing models for Shakespearean style is crucial\",\n        \"Preserving poetic nuances in translation is a challenge\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance quantum computing infrastructure\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Quantum-friendly\"],\n        \"demonstration\": \"A live demo showcasing the translated literary works\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the instruction, reflecting the need to build a machine translation system, the technical requirements, and the expertise level of the user. Performance metrics are specified as unknown as the specific values would require expert knowledge, and complexity metrics focus on near-real-time processing and quantum adaptations. The data preprocessing, model adaptations, and source of the dataset are all derived from the instruction. However, some aspects such as the exact feature engineering techniques and deployment details are left unspecified.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Develop a sophisticated urban green space segmentation model using the Vaihingen dataset, focusing on a novel adaptation of SegNet. The model must accurately classify not just vegetation types but also distinguish between healthy, degraded, and invasive plant species. It should demonstrate robust performance across diverse lighting, seasonal shifts, and weather patterns, with specific requirements for real-time adaptation and long-term predictive analytics for comprehensive urban planning strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"Develop a sophisticated urban green space segmentation model using the Vaihingen dataset, based on a novel adaptation of SegNet. The model should classify vegetation types and discriminate between healthy, degraded, and invasive plant species, with robust performance under varying lighting, seasonality, and weather conditions. It must support real-time adaptation and long-term predictive analytics for comprehensive urban planning strategies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"healthy/degred/invasive species classification\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time adaptation\",\n                \"value\": null,\n                \"unit\": \"time (e.g., ms per inference)\"\n            },\n            {\n                \"name\": \"long-term predictive analytics\",\n                \"value\": null,\n                \"unit\": \"prediction window (e.g., days)\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vegetation types\", \"health status\", \"species\"],\n            \"specification\": {\n                \"resolution\": null,\n                \"image_size\": null,\n                \"bands\": null\n            },\n            \"description\": \"A dataset for urban green space segmentation, containing diverse lighting, seasonal, and weather conditions.\",\n            \"preprocessing\": [\"novel SegNet adaptation\"],\n            \"augmentation\": [\n                \"lighting variation\",\n                \"seasonal shifts\",\n                \"weather patterns simulation\"\n            ],\n            \"visualization\": [\"performance across different scenarios\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Adapted SegNet\",\n            \"family\": \"SegNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"novel adaptation\",\n                \"number_of_layers\": null,\n                \"parameters\": null,\n                \"real_time_inference\": null,\n                \"long_term_prediction\": null\n            },\n            \"description\": \"A SegNet-based model designed for urban green space segmentation, capable of real-time adaptation and long-term predictive analytics.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptation for diverse environmental conditions is crucial.\",\n        \"Robustness against varying lighting, seasonality, and weather needs careful model design.\",\n        \"Real-time adaptation and long-term predictive analytics enhance model实用性\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge device or cloud\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"GPU-optimized\", \"Mobile-friendly\"],\n        \"demonstration\": \"demonstration of real-time adaptation and predictive analytics in urban planning scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the given instruction and follows the JSON schema. The user's intent to build a model, the high-level of expertise, and the specific task (image segmentation with SegNet adaptation) are clearly captured. The performance metrics and complexity requirements are mentioned but not specified, indicating that they need to be estimated or fine-tuned. The dataset and model details are mapped to the provided problem description, and the service requirements reflect the need for real-world applicability.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager, you are overseeing a sophisticated deep learning project for time-series forecasting in the energy sector. The Electricity dataset, now expanded with domain-specific context and exogenous variables, has been meticulously divided into train (70%), validation (15%), and test sets (15%) to challenge the AI's ability to handle seasonality and non-linear patterns. The input sequences consist of historical data points (INPUT_SEQ_LEN=96, INPUT_DIM=321) representing various meteorological conditions and plant operations. The objective is to develop a state-of-the-art model that not only predicts the next 96-step future output (PRED_SEQ_LEN=96, PRED_DIM=321) but also exhibits excellence in both mean squared error (MSE) and mean absolute error (MAE) metrics, while demonstrating robustness to outliers and real-time data anomalies. Emphasize on interpretability and model explainability in this complex task.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy sector\",\n        \"description\": \"A sophisticated deep learning project for time-series forecasting in the energy sector using the Electricity dataset with expanded context and exogenous variables. The model should handle seasonality and non-linear patterns, predict the next 96 steps, and aim for low MSE and MAE. Robustness to outliers and real-time anomalies is essential, as well as interpretability and model explainability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"train_percentage\": 70,\n                \"validation_percentage\": 15,\n                \"test_percentage\": 15,\n                \"context_exogenous\": true\n            },\n            \"description\": \"An Electricity dataset expanded with domain-specific context and exogenous variables, divided into train, validation, and test for time-series forecasting tasks.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning (e.g., LSTM, Transformer)\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A state-of-the-art deep learning model, possibly incorporating techniques like LSTMs or Transformers, for forecasting time-series data in the energy sector with a focus on interpretability and explainability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The project deals with complex seasonality and non-linear patterns in time-series data.\",\n        \"Robustness to real-time anomalies and outliers is crucial.\",\n        \"Emphasis on interpretability and model explainability is specified.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"explanatory_interpretability\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the details provided in the instruction, such as the deep learning project context for time-series forecasting in the energy sector, dataset specifics, and emphasis on performance metrics and model explainability. However, the expected values for MSE and MAE metrics are not provided and are set to null, as is the case in the provided examples.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for a machine learning-driven surveillance system, you seek to develop an advanced vehicle recognition algorithm that reliably identifies distinct car models in low-resolution parking lot footage. Compare the COCO and Pascal VOC datasets based on their suitability for automotive object detection, considering factors like image diversity, annotation quality, and performance in real-world scenarios. Additionally, research and recommend a state-of-the-art, efficient model with at least 90% accuracy under resource constraints, ensuring real-time processing without compromising on detection precision.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"surveillance system\",\n        \"description\": \"As a project manager, seek to develop an advanced vehicle recognition algorithm for low-resolution parking lot footage, comparing COCO and Pascal VOC datasets for suitability in automotive object detection based on image diversity, annotation quality, and real-world performance. Need a state-of-the-art model with at least 90% accuracy under resource constraints for real-time processing and precision.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": true,\n                \"unit\": \"yes\"\n            },\n            {\n                \"name\": \"resource constraints\",\n                \"value\": \"yes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_diversity\": \"high\",\n                \"annotation_quality\": \"varied\"\n            },\n            \"description\": \"Comparative analysis for automotive object detection\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Pascal VOC\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_diversity\": \"moderate\",\n                \"annotation_quality\": \"known\"\n            },\n            \"description\": \"Evaluating suitability for automotive object detection in real-world scenarios\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art efficient model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"detail needed\",\n                \"speed\": \"real-time\",\n                \"precision\": \"90%\"\n            },\n            \"description\": \"Recommendation for automotive object detection with real-world performance and resource efficiency\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time processing demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON includes a clear user intent to consult and build a model, focusing on the task of dataset comparison and a state-of-the-art model recommendation. The problem area is identified as computer vision, specifically automotive object detection. The user's expertise is high. Performance metrics are set to the required accuracy of 90% and emphasizes real-time processing. Complexities such as resource constraints are mentioned. However, specific model names and detailed specifications are left out for further consultation with the user.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As a project manager for the cutting-edge machine learning team, you're tasked with developing a state-of-the-art time series forecasting system. Utilizing the intricate Traffic dataset, which showcases diverse real-world patterns, we've meticulously divided it into three non-overlapping subsets: train, validation, and test sets. The input data consists of historical observations with a standardized sequence length of 96 time steps and 862 distinct features (INPUT_SEQ_LEN=96, INPUT_DIM=862). Your objective is to design and implement a deep learning architecture that not only forecasts the subsequent 96-step sequence (PRED_SEQ_LEN=96, PRED_DIM=862) with exceptional accuracy but also exhibits robustness across multiple seasonality and trend variations. In addition to optimizing for mean squared error (MSE) and mean absolute error (MAE), strive to incorporate advanced techniques such as anomaly detection and rolling forecasts to demonstrate model sophistication. Provide a comprehensive report detailing the model selection, performance benchmarking, and any innovative approach employed for enhanced forecasting prowess.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"transportation, logistics\",\n        \"description\": \"Develop a state-of-the-art time series forecasting system using the Traffic dataset, with a focus on accurate 96-step predictions (PRED_SEQ_LEN=96, PRED_DIM=862), robustness to seasonality and trend variations, and incorporating anomaly detection and rolling forecasts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Traffic Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 862\n            },\n            \"description\": \"A real-world dataset showcasing diverse patterns, divided into train, validation, and test subsets for time series forecasting tasks.\",\n            \"preprocessing\": [\"standardization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning (e.g., LSTM, Transformer, or a custom architecture)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"sequence length\": 96,\n                    \"feature dimension\": 862\n                },\n                \"advanced_techniques\": [\n                    \"anomaly detection\",\n                    \"rolling forecasts\"\n                ]\n            },\n            \"description\": \"A deep learning model for time series forecasting, designed to handle long-term sequences and exhibit robustness against varying trends and seasonality.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating advanced techniques like anomaly detection and rolling forecasts adds value to the model's sophistication.\",\n        \"Using the Traffic dataset with diverse patterns helps demonstrate the model's adaptability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"standardization\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Comprehensive model performance report and implementation details.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent, task (building a time-series forecasting system), dataset specifics, and requirements for model robustness and advanced features. The performance metrics (MSE and MAE) are left unspecified for optimization. The expert level of the user, as well as the data pre-processing step and innovative approach, are clearly detailed. The confidence score is high due to the clear instruction and the relevance of the response to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a machine learning project manager for a cutting-edge sentiment analysis firm, you're tasked with fine-tuning a specialized BERT model using the BoolQ dataset. This dataset consists of domain-specific technical articles and user-generated questions, where the objective is to discern if the answer can be logically deduced from the article. Ensure a precision-oriented evaluation, targeting an accuracy of at least 95% on a held-out test set with balanced classes and customized evaluation scripts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"sentiment analysis\",\n        \"description\": \"Fine-tune a specialized BERT model for sentiment analysis on the BoolQ dataset, which contains domain-specific technical articles and user-generated questions for determining if the answer can be logically deduced from the article. Focus on precision with an accuracy goal of at least 95% on a balanced held-out test set and customized evaluation scripts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"answerability\"],\n            \"specification\": {\n                \"balance\": \"balanced\",\n                \"size\": {\n                    \"train\": \"not specified\",\n                    \"validation\": \"not specified\",\n                    \"test\": \"not specified\"\n                },\n                \"text_length\": \"not specified\"\n            },\n            \"description\": \"A domain-specific dataset for sentiment analysis containing technical articles and questions, with a focus on logical deduction from the content.\",\n            \"preprocessing\": [\"BERT tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Class distribution analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Specialized BERT\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT\",\n                \"pre-trained\": \"specific\",\n                \"fine-tuning_details\": \"not specified\"\n            },\n            \"description\": \"A high-performance BERT model tailored for sentiment analysis on the BoolQ dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow serving\", \"Hugging Face inference API\"],\n        \"demonstration\": \"Performance on balanced held-out test set with customized evaluation scripts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent (building a model), expertise level, problem domain (sentiment analysis), and specific task (BERT fine-tuning on BoolQ). The dataset details, including its modality, target variable, and balanced test set requirement, are incorporated. Performance metrics such as accuracy and precision are set based on the instruction. The model family and type, as well as the fine-tuning process, are described without specifics since they vary by the chosen BERT model. The target device and inference engine options are suggested based on typical deployment scenarios for sentiment analysis. The missing details, such as the exact size of the dataset splits, can be filled in by contacting the user.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for the machine learning development team, you are assigned to tackle a multistep, complex time-series forecasting challenge. The ILI dataset, now expanded with domain-specific preprocessing, consists of train, validation, and test partitions. Each input sequence of length 36 (INPUT_SEQ_LEN=36, INPUT_DIM=7) requires intricate feature extraction and historical context analysis. The objective is to develop a deep learning architecture that not only forecasts the subsequent 24-step sequence (PRED_SEQ_LEN=24, PRED_DIM=7) but also demonstrates interpretability and resilience across various seasonal patterns. Performance should be assessed using both mean squared error (MSE) and mean absolute error (MAE), with a focus on minimizing prediction variance and improving generalization over unseen data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Tackle a complex time-series forecasting challenge using the ILI dataset, which has undergone domain-specific preprocessing. The model should handle input sequences of length 36, extract intricate features, analyze historical context, and forecast the next 24-step sequence. Emphasis is on interpretability, robustness to seasonal patterns, and minimizing prediction variance while improving generalization on unseen data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"prediction variance\",\n                \"value\": null,\n                \"unit\": \"variance\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 36,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 24,\n                \"PRED_DIM\": 7\n            },\n            \"description\": \"The dataset for time-series forecasting with domain-specific preprocessing, split into train, validation, and test partitions.\",\n            \"preprocessing\": [\"domain-specific\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"required\",\n                \"resilience_to_seasonal_patterns\": \"required\",\n                \"model_structure\": {\n                    \"architecture\": \"recurrent or transformer-based\"\n                }\n            },\n            \"description\": \"A deep learning architecture designed for intricate time-series forecasting, including interpretability and resilience to varying seasonal patterns.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"intrinsic feature extraction\", \"historical context analysis\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"include visualizations of model interpretations and prediction explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been parsed based on the given instruction, reflecting the project management role and the specific time-series forecasting requirements. The high-level intent to build a model and the use of domain knowledge ('high' expertise) are clearly stated. The problem area, task, and performance metrics are derived from the instruction. The complex nature of the dataset and the preprocessing steps are noted. The model requirements are specified as deep learning, with a focus on interpretability and handling seasonal patterns. The missing values in the metrics are due to the lack of specific target values in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a specialized project manager for a cutting-edge AI research team, your objective is to develop a state-of-the-art deep learning model using the unique \"Biomass Emission Dynamics\" dataset. This contains intricate, high-resolution time series data collected from densely instrumented field sites, partitioned into training (60%.), validation (20%), and unseen test sets (20%). Each input sequence consists of precise, 1-year-long sensor readings (INPUT_SEQ_LEN=8,760 hourly measurements, INPUT_DIM=15, with additional seasonal and geographical factors). The challenge is to design a model that accurately predicts the complex categorization of emissions (labels: {A, B, C, D}) with sub-1% error on unseen seasonal patterns. The evaluation will focus on F1-score, necessitating a comprehensive understanding of seasonality and long-term dependencies in the data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"environmental research\",\n        \"description\": \"Develop a state-of-the-art deep learning model using the 'Biomass Emission Dynamics' dataset, which contains intricate, high-resolution time series data. The model should accurately predict emissions categorization (A, B, C, D) with a sub-1% error on unseen seasonal patterns, focusing on F1-score evaluation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Biomass Emission Dynamics\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"A\", \"B\", \"C\", \"D\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 8760,\n                \"INPUT_DIM\": 15,\n                \"split_distribution\": [\"60%\", \"20%\", \"20%\"]\n            },\n            \"description\": \"Intricate time series data with 1-year-long sensor readings, seasonal and geographical factors, divided into training, validation, and unseen test sets.\",\n            \"preprocessing\": [\"feature engineering for seasonality and geographical factors\"],\n            \"augmentation\": [\"time-series data augmentation\"],\n            \"visualization\": [\"temporal pattern analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art for time-series classification\"\n            },\n            \"description\": \"A deep learning model designed for high-precision time-series classification, considering seasonality and long-term dependencies in the Biomass Emission Dynamics dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Understanding seasonality and long-term dependencies are critical for achieving sub-1% error on unseen seasonal patterns.\",\n        \"Model should be equipped to handle complex time series data with multiple categorical labels.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Results focused on visualizing performance on unseen seasonal patterns\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the provided schema and captures the essence of the project manager's objective. Expertise level is assumed to be high based on the role. Performance metric is specified (F1-score) with a target value. Dataset details, including the partition and input specifications, are extracted from the description. Model type is deep learning, and the requirement for handling seasonality and long-term dependencies is captured. However, exact model architecture is not provided since it would depend on the specific state-of-the-art technology. The confidence is high, assuming clear instructions.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "link prediction", "instruction": "Develop a highly specialized link prediction model for Tencent Weibo, utilizing a state-of-the-art Attention-based Graph Neural Network with Hierarchical Attention Mechanisms (HAGNN). Focus on enhancing the model's discriminative capacity for identifying subtle emotional bonds and influence patterns within the platform's massive user network.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"social networking\",\n        \"description\": \"Develop a highly specialized link prediction model for Tencent Weibo using a state-of-the-art Attention-based Graph Neural Network with Hierarchical Attention Mechanisms (HAGNN). Emphasis should be placed on enhancing discriminative capacity for identifying subtle emotional bonds and influence patterns in the massive user network.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean average precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tencent Weibo\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"nodes\": {\n                    \"number\": null,\n                    \"embedding_dimensions\": null\n                },\n                \"edges\": {\n                    \"number\": null,\n                    \"types\": [\"emotional_bonds\", \"influence_patterns\"]\n                }\n            },\n            \"description\": \"A large-scale social network dataset from Tencent Weibo with emotional bonds and influence patterns data.\",\n            \"preprocessing\": [\n                \"node feature extraction\",\n                \"edge type identification\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"network structure analysis\",\n                \"emotional bond and influence pattern patterns\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention-based GNN with Hierarchical Attention Mechanisms (HAGNN)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"hidden_layer_dimensions\": null,\n                \"attention_heads\": null,\n                \"hierarchical_depth\": null\n            },\n            \"description\": \"An advanced model for link prediction, particularly designed to capture emotional bonds and influence patterns in Tencent Weibo\"\n        }\n    ],\n    \"knowledge\": [\n        \"HAGNN architecture with attention helps to identify subtle patterns\",\n        \"Emphasis on discriminative capacity distinguishes this model from others\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"sentiment analysis for node embeddings\",\n            \"influence factor extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-optimized\"\n        ],\n        \"demonstration\": \"showcase of improved link prediction results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes relevant information based on the instruction, such as user intent, problem domain, and specific model type. Performance metrics are not defined in detail due to the absence of target values in the instruction, so they are set to null. The dataset and model descriptions are derived from the given details on Tencent Weibo and the HAGNN model. However, further specification such as node/edge numbers, attention heads, and hierarchical depths are not provided since they are not specified in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a specialized AI research team, you are tasked with overseeing the development of an advanced time-series forecasting model in the renewable energy sub-sector, leveraging the GreenEnergy dataset. This dataset now integrates domain knowledge and real-time external factors, necessitating a granular 85% train, 10% validation, and 5% test split to test the AI's prowess in managing inter-annual variations, complex non-linearities, and extreme weather events. The input sequences consist of highly specific historical data points (INPUT_SEQ_LEN=144, including solar radiation, wind speed, and 450 operational indicators), each reflecting minute-to-minute shifts. The model should not only accurately forecast the next 120-step output (PRED_SEQ_LEN=120) for power generation, but also showcase remarkable performance in log-cumulative absolute error (LCAE), mean absolute percentage error (MAPE), and resilience to sudden data anomalies encountered in smart grid environments. Emphasis must be placed on deploying Explainable Artificial Intelligence (XAI) techniques for transparency and deep understanding of the model's decision-making process in this high-stakes, real-time energy management scenario.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"renewable energy\",\n        \"description\": \"Develop an advanced time-series forecasting model for power generation using the GreenEnergy dataset with real-time external factors and domain knowledge. The model should handle inter-annual variations, complex non-linearities, and extreme weather events, with a granular split (85% train, 10% validation, 5% test). The input sequences consist of specific historical data (144 data points including solar radiation, wind speed, and 450 operational indicators) with a focus on forecasting the next 120 steps (120-step output). Performance should be measured by LCAE, MAPE, and resilience to sudden data anomalies in a smart grid context. XAI techniques are essential for model transparency and explainability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"log-cumulative absolute error (LCAE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute percentage error (MAPE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"resilience to sudden data anomalies\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"GreenEnergy\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"power generation\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 144,\n                \"PRED_SEQ_LEN\": 120,\n                \"data features\": [\"solar radiation\", \"wind speed\", \"operational indicators\"]\n            },\n            \"description\": \"Dataset with historical renewable energy data and real-time external factors for time-series forecasting.\",\n            \"preprocessing\": [\"data cleaning, normalization, feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Time-Series Forecasting Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"sequence length\": 144,\n                \"prediction horizon\": 120,\n                \"complexity\": \"handles inter-annual variations, non-linearities, and anomalies\"\n            },\n            \"description\": \"A deep learning model designed for renewable energy time-series forecasting, including support for XAI techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Domain knowledge integrated in the dataset\",\n        \"Real-time external factors influence\",\n        \"Smart grid environment considerations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"handling domain-specific features\", \"anomaly detection\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"streaming processing\"],\n        \"demonstration\": \"Explainable predictions in high-stakes energy management scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's instruction, detailing the project requirements for the forecasting model, dataset, and final steps. Performance metrics (LCAE and MAPE) and XAI techniques are included, reflecting the specific task and challenges. The instruction provides enough detail to construct a confident JSON response, though some metric values are left null as they are not specified. The confidence score is high due to the clear mapping to the schema.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "image segmentation", "instruction": "Forge an advanced urban vegetation monitoring system employing a fine-tuned ResNet-based SegNet architecture on the High-Resolution Vaihingen dataset. The model must discern not only native flora but also distinguish subtle variations between health states (novel stress indicators), invasive species, and respond promptly to photometric variations, seasonal changes, and intermittent weather conditions. Achieve peak performance with real-time learning and long-term forecasting for data-driven sustainable city planning scenarios, ensuring resilience and biodiversity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"sustainable urban planning\",\n        \"description\": \"Forge an advanced urban vegetation monitoring system using a fine-tuned ResNet-based SegNet architecture on the High-Resolution Vaihingen dataset. The model should detect native flora, distinguish health states (stress indicators), identify invasive species, and respond to photometric variations, seasonal changes, and weather conditions in real-time, enabling long-term forecasting for data-driven city planning, with a focus on resilience and biodiversity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean intersection over union (mIoU)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time learning\",\n                \"value\": 1\n            },\n            {\n                \"name\": \"long-term forecasting\",\n                \"value\": 1\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High-Resolution Vaihingen dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"resolution\": {\n                    \"width\": \"high\",\n                    \"height\": \"high\"\n                },\n                \"data_type\": \"high-resolution aerial images\"\n            },\n            \"description\": \"Fine-grained urban imagery for vegetation monitoring tasks\",\n            \"preprocessing\": [\n                \"data augmentation (photometric variations)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Fine-tuned ResNet-based SegNet\",\n            \"family\": \"Deep Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ResNet-based SegNet\",\n                \"initial_model\": \"pre-trained on ImageNet\",\n                \"customization\": \"Fine-tuned for vegetation monitoring and health state differentiation\"\n            },\n            \"description\": \"A state-of-the-art image segmentation model adapted for urban vegetation monitoring, invasive species detection, and resilience to seasonal changes.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Subtle health state differentiation is crucial for early detection and mitigation.\",\n        \"Real-time learning and long-term forecasting are key for dynamic city planning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance computing or edge devices\",\n        \"deployment_endpoint\": \"specific cloud platform or edge compute infrastructure\",\n        \"inference_engine\": [\n            \"optimized for real-time performance\"\n        ],\n        \"demonstration\": \"Interactive visual dashboard with real-time predictions and analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is built based on the given instruction, considering the user's intent to build a model, expertise level, and specific requirements for an advanced urban vegetation monitoring system. Performance metrics like accuracy, mIoU, real-time learning, and long-term forecasting have been included. The dataset and model details are derived from the instruction, focusing on the High-Resolution Vaihingen dataset and the ResNet-based SegNet architecture. However, specific metric values are left unspecified to accommodate the model tuning process.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "Deploy a specialized Graph Wavelet Neural Network (GWNN) for distinguishing micron-level neuronal subtypes in the exceptionally intricate Mouse Brain Atlas, leveraging the MindSpore SDK. Prioritize optimizing F1-score for rare and abundant classes while considering the model's ability to capture hierarchical interactions in densely interconnected regions, preserving minimal computational overhead. Assure scalability for large-scale data processing on parallel high-performance hardware.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"biomedical research\",\n        \"description\": \"Deploy a specialized Graph Wavelet Neural Network (GWNN) for identifying micron-level neuronal subtypes in the Mouse Brain Atlas using MindSpore SDK. The model should optimize F1-score for both rare and abundant classes, account for hierarchical interactions in densely connected regions, and maintain minimal computational overhead for scalability on large-scale, parallel high-performance hardware.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational overhead\",\n                \"value\": 0,\n                \"unit\": \"Normalized\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Atlas\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"neuronal subtype\"],\n            \"specification\": {\n                \"density\": \"densely interconnected\",\n                \"size\": \"large-scale\"\n            },\n            \"description\": \"A graph dataset representing the Mouse Brain Atlas with micron-level neuronal subtypes as targets.\",\n            \"preprocessing\": [\"MindSpore data preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Wavelet Neural Network (GWNN)\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specialized for hierarchical interactions\",\n                \"optimization\": \"for F1-score, specifically\"\n            },\n            \"description\": \"A Graph Wavelet Neural Network designed to classify neuronal subtypes in the Mouse Brain Atlas using MindSpore SDK, with emphasis on scalability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"MindSpore SDK's compatibility for the graph neural network task\",\n        \"Hierarchical interactions and optimized F1-score for rare and abundant classes\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"leveraging MindSpore for efficient feature extraction\"],\n        \"target_device\": \"parallel high-performance hardware\",\n        \"deployment_endpoint\": \"optimized for scalability\",\n        \"inference_engine\": [\"MindSpore\"],\n        \"demonstration\": \"Emphasis on scalability and performance demonstration with large-scale data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the key elements of the user's instruction, including the deployment of a specialized GWNN, the target dataset, and the use of MindSpore SDK. The expert level of expertise is assumed based on the complexity of the problem and the mention of parallel high-performance hardware. However, specific F1-score targets are not provided, and the performance metrics and computational overhead are initially estimated to be minimal without a specified value. Other details like exact data preprocessing steps are assumed based on common practices for graph datasets. Overall, the missing details can be filled based on the specific task requirements.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "The project manager for a specialized deep learning team is to enhance the ImageClassifier-SLD task on the Stanford Cars Dataset v2.0. Emphasis is on developing a state-of-the-art Inception-v4+ architecture, resilient to occlusions, subtle color variations, and dynamic weather patterns. The model must surpass a 97% mean Average Precision (mAP), with per-class precision and recall at least 90% each, demonstrating robustness in real-world scenarios. A presentation of visually augmented testing results and a comparative analysis with prior top-performing models must be provided.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"automotive\",\n        \"description\": \"The objective is to enhance the ImageClassifier-SLD task on the Stanford Cars Dataset v2.0 using a state-of-the-art Inception-v4+ architecture. The model must be resilient to occlusions, subtle color variations, and dynamic weather patterns, targeting a mAP of at least 97% and per-class precision and recall of 90%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean Average Precision (mAP)\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"per-class precision\",\n                \"value\": 0.90\n            },\n            {\n                \"name\": \"per-class recall\",\n                \"value\": 0.90\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Dataset v2.0\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"train\": {\n                        \"total_samples\": \"unknown\"\n                    },\n                    \"validation\": {\n                        \"total_samples\": \"unknown\"\n                    },\n                    \"test\": {\n                        \"total_samples\": \"unknown\"\n                    }\n                },\n                \"dimensionality\": \"image\"\n            },\n            \"description\": \"A specialized dataset for image classification, focusing on the Stanford Cars Dataset v2.0 with added challenges of occlusions, subtle color variations, and dynamic weather conditions.\",\n            \"preprocessing\": [\n                \"data augmentation for occlusions, color variations, and weather patterns\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ImageClassifier-SLD (Inception-v4+)\",\n            \"family\": \"Inception\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Inception-v4+\",\n                \"resilience\": [\n                    \"to occlusions\",\n                    \"color variations\",\n                    \"dynamic weather patterns\"\n                ]\n            },\n            \"description\": \"A deep learning model employing the Inception-v4+ architecture designed for image classification on the Stanford Cars Dataset v2.0 with high robustness requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Model resilience to occlusions, color variations, and dynamic weather is a key focus.\",\n        \"Visual augmentation results and a comparative analysis with previous top-performers will be presented.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A presentation showcasing visually augmented testing results and model comparison\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given project requirement by specifying the user intent, the high expertise level of the deep learning team, the task in computer vision, the target dataset and its challenges, the performance metrics, the Inception-v4+ model specifications, and the presentation requirements. The lack of exact dataset sizes and model performance parameters is noted, as these details were not specified in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "link prediction", "instruction": "Develop a deep learning model, Enhanced PIN (EPI-Net), using an attention mechanism for the Yeast Protein Interaction Network. The model must not only predict links but also incorporate temporal dynamics and cross-protein domain information. Ensure interpretability and validate with nested cross-validation, comparing its performance with existing methods like Graph Convolutional Networks and DeepWalk.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Develop a deep learning model, Enhanced PIN (EPI-Net), incorporating an attention mechanism for the Yeast Protein Interaction Network. The model should consider temporal dynamics and cross-protein domain information and prioritize interpretability. Use nested cross-validation for model evaluation and compare performance with existing methods like Graph Convolutional Networks and DeepWalk.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean average precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Yeast Protein Interaction Network\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"nodes\": {\n                    \"size\": null,\n                    \"features\": [\n                        \"temporal dynamics\",\n                        \"cross-protein domain information\"\n                    ]\n                },\n                \"edges\": {\n                    \"quantity\": null,\n                    \"features\": [\"link prediction signals\"]\n                }\n            },\n            \"description\": \"A graph dataset with temporal and cross-protein domain information for protein interaction predictions.\",\n            \"preprocessing\": [\n                \"Temporal feature extraction\",\n                \"Domain feature engineering\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EPI-Net\",\n            \"family\": \"Attention Mechanisms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true\n            },\n            \"description\": \"A deep learning model that integrates attention, temporal dynamics, and cross-protein domain information for link prediction on the Yeast Protein Interaction Network.\"\n        },\n        {\n            \"name\": \"Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {}\n        },\n        {\n            \"name\": \"DeepWalk\",\n            \"type\": \"classical machine learning\",\n            \"specification\": {}\n        }\n    ],\n    \"knowledge\": [\n        \"Interpretablility is a key requirement for the model.\",\n        \"Comparisons with GCNs and DeepWalk will help assess the superiority of EPI-Net\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Temporal and domain-specific feature engineering\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"validation\": \"Nested cross-validation results\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been parsed accurately into JSON format, following the schema. The user's intent is to build a model, EPI-Net, with high-level expertise. The problem area (graph machine learning), task (link prediction), and domain (bioinformatics) are clear. Performance metrics are not provided explicitly, so they are set to null. Complexity metrics require specific values, which are not provided in the instruction, so they are also set to null. The EPI-Net model, GCNs, and DeepWalk are accounted for.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a project manager for a leading media streaming platform, your team must develop a state-of-the-art video recommendation system using the extensive 'Bilibili Danmu' dataset. Emphasize on scalability, advanced natural language processing, and real-time speed for seamless sequential viewing habits, while optimizing for user satisfaction in a highly competitive market.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"media streaming\",\n        \"description\": \"Develop a state-of-the-art video recommendation system using the Bilibili Danmu dataset, focusing on scalability, advanced NLP, real-time speed, and user satisfaction for competitive streaming experience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": \"optimized\"\n            },\n            {\n                \"name\": \"real-time speed\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"user satisfaction\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": \"efficient\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Bilibili Danmu\",\n            \"modality\": [\"video\", \"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_size\": \"large\",\n                \"time_series\": true\n            },\n            \"description\": \"A large dataset containing video and text data from Bilibili's live streaming platform, for analyzing sequential viewing habits.\",\n            \"preprocessing\": [\"text normalization\", \"sequence labeling\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"user behavior patterns\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"sequential recommendation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"deep learning\", \"RNN/GRU/LSTM\"],\n                \"model complexity\": \"complex\"\n            },\n            \"description\": \"A scalable, deep learning-based recommendation system with advanced NLP components for real-time, sequential video recommendations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Bilibili Danmu dataset's diversity and size demands a scalable model design.\",\n        \"NLP processing and real-time speed are crucial for a smooth user experience in video streaming.\",\n        \"Optimizing for user satisfaction requires continuous model improvement and personalized recommendations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"collaborative filtering\",\n            \"user profiling\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"CDN for real-time delivery\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Apache Arrow\"],\n        \"demonstration\": \"Interactive dashboard showcasing personalized recommendations and user feedback analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is parsed to reflect the project manager's requirements for a recommendation system on the Bilibili Danmu dataset. The user's intent to build, high expertise, and the focus areas are captured. Performance metrics and complexity are defined based on the context. The model is tailored to a neural network-based sequential recommendation system, while the data source, preprocessing, and additional factors like collaboration and deployment are detailed. The high confidence score is due to the clear mapping of the instruction to the JSON structure and the provided rationale.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "tabular regression", "instruction": "For a cutting-edge data science competition, you must analyze the Wild Blueberry Yield Dataset, which boasts a plethora of intricate, interrelated numerical features. After meticulously dividing it into non-overlapping training, validation, and test partitions, your challenge is to engineer a state-of-the-art predictive model that forecasts the yield with sub-quantitative precision. The performance must surpass existing benchmarks, and your metric of interest is the demanding mean absolute percentage error (MAPE), adding an extra layer of complexity to the regression task.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Analyze the Wild Blueberry Yield Dataset with intricate interrelated numerical features, divide it into non-overlapping train, validation, and test partitions, and build a state-of-the-art predictive model for yield forecasting with sub-quantitative precision. Performance must surpass existing benchmarks and the metric of interest is the mean absolute percentage error (MAPE).\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wild Blueberry Yield Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"yield\"],\n            \"specification\": null,\n            \"description\": \"A comprehensive dataset containing intricate numerical features for a blueberry yield prediction competition.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art regression model\",\n            \"type\": \"neural networks\", \n            \"specification\": {\n                \"description\": \"Advanced model architecture designed for regression tasks, including in-depth feature processing and optimized for low MAPE\"\n            },\n            \"description\": \"\"\n        }\n    ],\n    \"knowledge\": [\n        \"The competition requires a high-performance model that can handle complex interrelated features and strict precision criteria.\",\n        \"The target is to surpass existing benchmarks using the MAPE as the primary metric.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Competition-specific performance demonstration protocol to showcase MAPE performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction, reflecting the high-level details like the dataset, task, and performance metric. The user's expertise is inferred to be high due to the mention of a 'cutting-edge' data science competition. The missing values (like specific model, performance target for MAPE, or feature engineering steps) reflect the ambiguity in the instruction, while the model type is set to 'neural networks' as they are often associated with high-performance models for tabular data analysis. The confidence score is set to 0.9, acknowledging that the details might require more clarification.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for the AI team, I need you to develop an advanced image recognition system that not only classifies plant seedlings from the 'Plant Seedlings Dataset' but also differentiates between 50 distinct species with sub-species variations. The model should integrate transfer learning using a real-time, resource-constrained device, ensuring at least 90% accuracy within 1 second per image while maintaining mobile deployment efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"agriculture or plant science\",\n        \"description\": \"Develop an advanced image recognition system for classifying plant seedlings from the 'Plant Seedlings Dataset' and differentiating between 50 distinct species with sub-species variations. The model must integrate transfer learning, be suitable for real-time, resource-constrained device use, and maintain accuracy of at least 90% with an inference time of 1 second per image, considering mobile deployment efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 1,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Seedlings Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"seedling species and sub-species\"],\n            \"specification\": {\n                \"size\": \"large enough to cover 50 species with sub-species\",\n                \"image resolution\": \"to match real-time and resource constraints\"\n            },\n            \"description\": \"A dataset specifically for plant seedling classification, containing images of 50 distinct species with sub-species variations.\",\n            \"preprocessing\": [\"transfer learning-based data preprocessing\"],\n            \"augmentation\": [\"real-time, resource-constrained data augmentation\"],\n            \"visualization\": [\"deployment-friendly performance visualization\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (suitably lightweight for resource-constrained devices)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"transfer learning-based\",\n                \"parameters\": \"optimized for mobile deployment\",\n                \"FLOPs\": \"minimized for efficiency\"\n            },\n            \"description\": \"A transfer learning-based image classification model designed for resource-constrained devices, aiming for at least 90% accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning on lightweight models is crucial for real-time and resource-constrained devices.\",\n        \"Mobile deployment efficiency and high accuracy are key priorities\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"model pruning for mobile hardware\"],\n        \"target_device\": \"real-time, resource-constrained mobile device\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for mobile platforms\"],\n        \"demonstration\": \"mobile deployment and real-time performance demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been constructed based on the given instruction. It covers the user's intent to build a high-end image recognition system, the specific task (image classification), and the requirements like accuracy, inference time, and mobile deployment. The dataset and model information is derived from the task and the constraints, and the expert-level expertise of the user. However, there may be missing details on exact model architecture, preprocessing, and augmentation techniques, as these would typically involve a more detailed analysis of the dataset and target hardware.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager overseeing a cutting-edge Earth observation project, you're tasked with harnessing the potential of advanced Sentinel-2 satellite imagery for land type classification. Your team must evaluate not just one, but a range of state-of-the-art segmentation models (CNNs, U-Nets, or DeepLabV3) for their suitability, considering factors like computational efficiency, multi-resolution analysis, and transfer learning. It's imperative to achieve high accuracy in differentiating water bodies, dense forests, and urban landscapes, while maintaining real-time processing capabilities and minimizing training time. Provide a detailed comparison of options, detailing speed and accuracy trade-offs.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\", \n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"remote sensing, Earth observation\",\n        \"description\": \"Evaluate multiple state-of-the-art segmentation models (CNNs, U-Nets, DeepLabV3) for land type classification using Sentinel-2 satellite imagery. Focus on computational efficiency, multi-resolution analysis, transfer learning, achieving high accuracy in water bodies, dense forests, and urban landscapes, with real-time processing and minimal training time.\",\n        \"performance_metrics\": [\n            {\"name\": \"accuracy\", \"value\": null},\n            {\"name\": \"computational efficiency\", \"value\": null},\n            {\"name\": \"multi-resolution analysis\", \"value\": null},\n            {\"name\": \"transfer learning\", \"value\": null},\n            {\"name\": \"real-time processing\", \"value\": null},\n            {\"name\": \"training time\", \"value\": null}\n        ],\n        \"complexity_metrics\": [\n            {\"name\": \"inference time\", \"value\": null, \"unit\": \"seconds\"},\n            {\"name\": \"memory size\", \"value\": null, \"unit\": \"MB\"}\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"land type\"],\n            \"specification\": {\n                \"bands\": {\n                    \"visible\": {},\n                    \"near infrared\": {},\n                    \"multispectral\": {}\n                },\n                \"size\": { \"width\": null, \"height\": null },\n                \"spatial_res\": null\n            },\n            \"description\": \"High-resolution satellite imagery from Sentinel-2 for land type classification.\",\n            \"preprocessing\": [\"georeferencing\", \"image normalization\"],\n            \"augmentation\": [\"data augmentation for satellite imagery\"],\n            \"visualization\": [\"class activation maps\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": [\"CNN\", \"U-Net\", \"DeepLabV3\"],\n            \"family\": [\"Convolutional Neural Networks\", \"U-Net Architecture\", \"DeepLabV3\"],\n            \"type\": [\"neural networks\"],\n            \"specification\": {\n                \"computational_efficiency\": null,\n                \"multi_resolution\": null,\n                \"transfer_learning Capabilities\": null,\n                \"accuracy_metrics\": {\n                    \"water_bodies\": null,\n                    \"dense_forests\": null,\n                    \"urban_landscapes\": null\n                }\n            },\n            \"description\": \"Candidate state-of-the-art models for land type classification.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction from Sentinel-2\"],\n        \"target_device\": \"cloud-based or high-performance computing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"GPU-accelerated frameworks\"],\n        \"demonstration\": \"model performance demonstration on real-time Sentinel-2 data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the user's instruction, describing the task to consult and build models for land type classification. High expertise is assumed, given the project's complexity. Metrics related to speed (accuracy, computational efficiency), multi-resolution, transfer learning, and real-time processing are identified. However, since specific numerical values for accuracy and other metrics are not provided, these are marked as null. Similarly, the dataset details and model specifications are based on the context, but further clarification might be needed for more precise information.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "node classification", "instruction": "Design and develop a novel Graph Attention Network (GAT) architecture for target entity disambiguation in the ScholarNet dataset, incorporating temporal and semantic edge attributes. The model should achieve state-of-the-art performance with an F1-score above 90%, while demonstrating efficient handling of dynamic citation patterns. Furthermore, conduct an extensive ablation study to analyze the impact of edge weighting schemes and attention heads on the model's effectiveness, and provide a comprehensive interpretability report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic\",\n        \"description\": \"Design a novel Graph Attention Network (GAT) architecture for target entity disambiguation in the ScholarNet dataset. Incorporate temporal and semantic edge attributes. Aim for state-of-the-art performance with an F1-score above 90% and demonstrate efficient handling of dynamic citation patterns. Conduct an extensive ablation study on edge weighting schemes and attention heads.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ScholarNet\",\n            \"modality\": [\"graph\", \"time series\", \"text\"],\n            \"target_variables\": [\"target entity\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"dimension\": {\n                    \"node\": null,\n                    \"edge\": null\n                },\n                \"length\": null\n            },\n            \"description\": \"A graph dataset containing temporal and semantic attributes for target entity disambiguation in a scholarly context.\",\n            \"preprocessing\": [\"temporal feature extraction\", \"semantic attribute processing\"],\n            \"augmentation\": [],\n            \"visualization\": [\"performance metric analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Novel GAT\",\n            \"family\": \"Graph Attention Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"num_attention_heads\": null,\n                \"edge_weighting schemes\": null\n            },\n            \"description\": \"A custom GAT architecture designed for target entity disambiguation on ScholarNet with support for temporal and semantic edge attributes.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Temporal and semantic edge attributes are crucial for accurate disambiguation in ScholarNet.\",\n        \"Efficient handling of dynamic citation patterns is a design focus.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"edge weighting adaptation\",\n            \"attention head variations\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"ScholarNet API\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"Interactive performance visualization dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent (to build a model), their expertise, and the specific requirements of the project. The area and downstream task are determined based on the Graph Attention Network (GAT) and the ScholarNet dataset. The performance metric (F1-score above 90%) and a complexity metric (inference time) are specified. Preprocessing, data modalities, and source are also derived from the instruction. However, specific model details like the number of attention heads or edge weighting schemes need more context, thus are left to be further discussed with the user.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "time-series forecasting", "instruction": "Develop a scalable, ensemble-based time series forecasting system using a combination of advanced deep learning architectures (e.g., LSTM, Prophet, and ARIMA) for our mobile app's DAU data. Implement a feature engineering process to extract seasonal patterns and anomalies, and apply regularization techniques to ensure robustness against overfitting. Additionally, design a custom evaluation metric that considers both short-term and long-term predictions, and present a detailed report comparing the model's performance with previous methods.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"mobile apps\",\n        \"description\": \"Develop a scalable time series forecasting system for DAU data in a mobile app using an ensemble of advanced deep learning models (LSTM, Prophet, and ARIMA), incorporating feature engineering for seasonal patterns and anomalies, and applying regularization to prevent overfitting. Create a custom evaluation metric that considers both short-term and long-term predictions, and provide a performance comparison with previous methods.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"custom evaluation metric\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": 1.0,\n                \"unit\": \"high\"\n            },\n            {\n                \"name\": \"regularization robustness\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mobile App DAU Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"DAU\"],\n            \"specification\": {\n                \"time_frequency\": \"\",\n                \"data_length\": \"\"\n            },\n            \"description\": \"DAU data for the mobile app with a focus on scalability and feature extraction.\",\n            \"preprocessing\": [\n                \"seasonal pattern extraction\",\n                \"anomaly detection\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble of LSTM, Prophet, and ARIMA\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"models\": [\"LSTM\", \"Prophet\", \"ARIMA\"],\n                \"ensemble_method\": \"stacking\"\n            },\n            \"description\": \"A deep learning ensemble model designed for scalable time series forecasting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Extracting seasonal patterns and detecting anomalies is crucial for accurate forecasting\",\n        \"Custom evaluation metric accounts for both short-term and long-term predictions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"seasonality decomposition\", \"time series feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\", \"batch\"],\n        \"demonstration\": \"performance and comparison with previous methods\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the given schema, capturing the user's intent, the detailed problem requirements, and the expected dataset specifications. The performance metric (custom evaluation) is left unspecified as it's dependent on the exact metric defined by the user. Regularization robustness is not explicitly quantified, as it's a subjective aspect. The medium/high level of expertise and the area of time-series analysis are clear, and the model details and feature engineering steps align with the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a celestial-themed project manager, your team must develop a comprehensive image classification system for diverse astronomical objects. Locate the 'GalaxyZoo2' dataset from multiple trusted sources, then evaluate and compare performances of at least three state-of-the-art convolutional neural networks (CNNs) - VGG16, ResNet50, and InceptionV3 - under varying image preprocessing techniques. Justify the choice of a high-precision model while addressing challenges like class imbalance and noise reduction.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"As a celestial-themed project manager, develop a comprehensive image classification system for diverse astronomical objects using the GalaxyZoo2 dataset. Evaluate and compare performances of VGG16, ResNet50, and InceptionV3 CNNs under different image preprocessing techniques. Address class imbalance and noise reduction challenges.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"GalaxyZoo2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"image_dimensions\": {\n                    \"width\": null,\n                    \"height\": null,\n                    \"channels\": 3\n                },\n                \"source_distribution\": \"trusted sources\"\n            },\n            \"description\": \"A diverse astronomical image dataset for the study of galaxy classification.\",\n            \"preprocessing\": [\n                \"various\"\n            ],\n            \"augmentation\": [\n                \"to address class imbalance and noise reduction\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"VGG16\",\n            \"family\": \"CNNs\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"ResNet50\",\n            \"family\": \"CNNs\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"InceptionV3\",\n            \"family\": \"CNNs\",\n            \"type\": \"neural networks\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art models like VGG16, ResNet50, and InceptionV3 are chosen for their advanced image recognition capabilities.\",\n        \"Class imbalance is handled through data augmentation or balancing techniques to ensure all object classes are represented equally.\",\n        \"Noise reduction is crucial for accurate classifications, perhaps through techniques such as denoising autoencoders or image filtering.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"handling class imbalance\",\n            \"noise reduction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"explanatory visualizations\": [\n                \"comparison of model performances\",\n                \"effect of preprocessing techniques\"\n            ]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent, high expertise level, and the problem context in the astronomical image classification. Performance metrics are left unspecified for now, and complexity metrics are placeholders for optimization considerations. The dataset details are derived from the instruction, while possible preprocessing, augmentation, and challenges (class imbalance, noise reduction) are taken into account. The choice of models and justifications align with state-of-the-art practice.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "node classification", "instruction": "As a project manager for the PubMed GraphSAGE node classification task, request a highly specialized and scalable model with fine-tuned transfer learning capabilities. Target not only accuracy but also computational efficiency, aiming for at least 95% precision and sub-50ms latency on unseen biomedical datasets with diverse graph structures, while considering hardware constraints for mobile deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"biomedical research\",\n        \"description\": \"As a project manager for the PubMed GraphSAGE node classification task, request a highly specialized and scalable model with fine-tuned transfer learning capabilities. Target accuracy, precision over 95%, and sub-50ms latency on unseen biomedical datasets with diverse graph structures. Consider hardware constraints for mobile deployment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"resource efficiency (mobile)\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PubMed GraphSAGE\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"graph_structure_diversity\": \"diverse\",\n                \"dataset_size\": \"unspecified\",\n                \"graph_density\": \"unspecified\"\n            },\n            \"description\": \"Unseen biomedical datasets with diverse graph structures for node classification task in PubMed GraphSAGE.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"GraphSAGE\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": \"fine-tuned\",\n                \"architecture\": \"highly specialized\",\n                \"mobile_compatibility\": true\n            },\n            \"description\": \"A specialized, scalable GraphSAGE model with fine-tuned transfer learning for node classification in PubMed GraphSAGE datasets with mobile deployment constraints.\"\n        }\n    ],\n    \"knowledge\": [\n        \"PubMed GraphSAGE's diverse graph structures demand a model that can handle complexity.\",\n        \"Focus on computational efficiency and hardware constraints for mobile deployment.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been mapped to a JSON response that captures the user's high-level expertise, the node classification task, and the specific requirements such as fine-tuned transfer learning, high accuracy, and mobile deployment. Performance metrics (accuracy, precision, latency) and consideration of diverse graph structures and hardware constraints are captured. However, some specific metrics (such as inference time and resource utilization) are not provided as the instruction doesn't give exact numbers. This leaves room for further clarification if necessary.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "community detection", "instruction": "Utilize the CiteSeer citation network to perform an in-depth analysis using a Label Propagation technique for discovering intricate research communities, considering implicit subgroups and interdependencies. Implement a scalable and dynamically adaptable algorithm that can seamlessly integrate new nodes without compromising performance or requiring repetitive retraining.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Perform an in-depth analysis of the CiteSeer citation network using Label Propagation technique to discover intricate research communities, accounting for implicit subgroups and interdependencies. Implement a scalable and adaptable algorithm that can handle new nodes without retraining.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CiteSeer\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"node_type\": \"citation network\",\n                \"edges\": \"citation links\"\n            },\n            \"description\": \"A citation network dataset for graph-based research community detection.\",\n            \"preprocessing\": [\"handling implicit subgroups, interdependencies\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Label Propagation\",\n            \"type\": \"graph machine learning\",\n            \"specification\": {\n                \"scalability\": \"dynamic adaptation\",\n                \"efficiency\": \"with new node integration\"\n            },\n            \"description\": \"ALabel Propagation-based algorithm designed for discovering research communities on the CiteSeer citation network, considering scalability and integration of new nodes.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Label Propagation exploits node relationships to infer community memberships\",\n        \"Scalability and dynamic adaption are essential due to citation network's evolving nature\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"showing community detection results and updates upon new node addition\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is built based on the instruction, mapping user's intent, problem details, and requirements to a specific machine learning project. User's intent for building a model with a high expertise level, the focus on community detection using a Label Propagation technique on CiteSeer, and the need for scalability and node addition are captured. Performance metrics and complexity constraints are included, but their specific values are not provided. The algorithm's adaptability and demonstration of new node integration are also detailed.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a seasoned project manager for the AI development team, craft a comprehensive plan to tackle this intricate text classification challenge using the DAIGT dataset. Request a multi-step approach for feature extraction, comparison of generation styles, and evaluation with cross-validation, aiming for state-of-the-art accuracy and interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"language and information retrieval\",\n        \"description\": \"Craft a comprehensive plan for addressing an intricate text classification challenge using the DAIGT dataset, involving multi-step feature extraction, comparison of generation styles, and evaluation with cross-validation targeting state-of-the-art accuracy and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"feature_types\": [\"bag-of-words\", \"TF-IDF\", \"embeddings\"],\n                \"label_distribution\": null\n            },\n            \"description\": \"A dataset for text classification task, expected to contain intricate challenges.\",\n            \"preprocessing\": [\n                \"Cleaning and normalization\",\n                \"Tokenization\",\n                \"Removing stopwords\",\n                \"Leveraging domain-specific preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Data exploration and distribution\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Text Classification Model (e.g., Transformer, BERT-based, or LSTM)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Hierarchical or ensemble of models\",\n                \"feature_combination\": [\"concatenation\", \"attention mechanism\"]\n            },\n            \"description\": \"A deep learning model tailored for text classification, incorporating multi-step feature extraction and interpretability techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art performance may involve fine-tuning pre-trained models,\",\n        \"Interpretability can be achieved through attention mechanisms or explainable AI methods.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature extraction using advanced techniques like contextual embeddings\",\n            \"Feature selection for dimensionality reduction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Compressing model for faster inference\"\n        ],\n        \"demonstration\": \"Model explanation and interpretability techniques for stakeholders\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the user's intent to build a model (expertise level is high), the NLP aspect of text classification, the need for state-of-the-art accuracy, and a multi-step approach. The DAIGT dataset is assumed to require external link since no direct source is specified, and domain-specific preprocessing is included. The model type is specified as neural networks, with a possible Transformer or BERT-based architecture. The performance metrics like accuracy are left unspecified for the model to potentially achieve the desired state. Confidence score is set at 0.97 due to the comprehensiveness of the provided plan, yet the model architecture and performance values will require more details.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "text classification", "instruction": "As a seasoned project manager for a high-profile AI classification initiative, you need to develop a scalable solution using the 20 Newsgroups dataset. Select not just one, but a hybrid ensemble of machine learning models, optimizing for accuracy and efficiency. Also, estimate the training time on diverse hardware configurations, factoring in parallelization and incremental learning, while adhering to strict deployment deadlines.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Develop a scalable solution for a high-profile AI classification project using the 20 Newsgroups dataset. The solution should involve a hybrid ensemble of machine learning models with an emphasis on accuracy and efficiency. Include estimations for training time on diverse hardware configurations, considering parallelization and incremental learning, and meeting strict deployment deadlines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"efficiency\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"category\"],\n            \"specification\": {\n                \"number_of_classes\": null,\n                \"size\": null,\n                \"features\": null,\n                \"text_length\": null\n            },\n            \"description\": \"A large multi-class text classification dataset for newsgroups.\",\n            \"preprocessing\": [\n                \"text normalization\",\n                \"vectorization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"hybrid ensemble\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"models\": [],\n                \"ensemble_method\": null,\n                \"parallelization\": true\n            },\n            \"description\": \"A scalable solution combining multiple machine learning models for enhanced classification accuracy and efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hybrid ensembles can improve performance by combining the strengths of various models.\",\n        \"Parallelization and incremental learning can enhance efficiency on diverse hardware.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"parallel training on different hardware configurations\"\n        ],\n        \"target_device\": \"diverse (e.g., cloud, edge, mobile)\",\n        \"deployment_endpoint\": \"must adhere to project deadlines\",\n        \"inference_engine\": [\"Distributed computing libraries, e.g., TensorFlow, PyTorch\"],\n        \"demonstration\": \"Include performance benchmarking on different hardware scenarios.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's high-level expertise and intent to build a scalable solution. The project context for natural language processing and the 20 Newsgroups dataset are appropriately identified. Performance metric requirements (accuracy) and hardware-related complexity considerations (estimating training time and efficiency) are addressed. However, specific model details and performance targets are not given as the instruction is not specific enough, so they are marked as null. The ensemble method and model combination will depend on data exploration.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for the High-Volume Scientific Exploration Task (HVSET) project, you're assigning the team to develop a domain-specific BERT-based model, leveraging RoBERTa architecture, fine-tuned on the arXiv-SQuAD dataset. The model must exhibit exceptional performance, targeting a minimum F1 score of 95% in answering complex, multi-disciplinary questions within a tolerable latency of 1.5 seconds, ensuring efficient research query processing in the field of astrophysics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"astronomy/astrophysics\",\n        \"description\": \"Develop a domain-specific BERT-based model with RoBERTa architecture, fine-tuned on the arXiv-SQuAD dataset. The model should achieve an F1 score of at least 95% and have a latency of 1.5 seconds for answering complex multi-disciplinary questions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 1.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv-SQuAD\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_source\": \"SQuAD dataset\",\n                \"domain\": \"astrophysics\"\n            },\n            \"description\": \"A dataset for fine-tuning a BERT-based model on complex, multi-disciplinary scientific questions, specifically from the astrophysics domain.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-based (RoBERTa)\",\n            \"family\": \"BERT family\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"RoBERTa\",\n                \"fine-tuning_dataset\": \"arXiv-SQuAD\",\n                \"latency_constraint\": true\n            },\n            \"description\": \"A domain-specific BERT model utilizing RoBERTa, designed for efficient research query processing in astrophysics.\"\n        }\n    ],\n    \"knowledge\": [\n        \"BERT's pre-training on large text corpus and RoBERTa's improvements in transfer learning are important for this project.\",\n        \"Efficient query processing is crucial given the latency requirement\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstration of question answering on complex astrophysics questions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a model, targeting a complex astrophysics problem. The problem area, NLP and downstream task (question answering), and expertise level are included. Performance metrics are defined, with a focus on the F1 score, and a latency constraint is set. The arXiv-SQuAD dataset is specified, along with the BERT-RoBERTa model details. The missing information points are assumed to be filled based on the context, and their significance isn't mentioned.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a state-of-the-art multi-modal fusion node classification model using a novel Hierarchical Attention Graph Wavelet Convolutional Neural Network (HAGW-CNN) on the merged Mouse Brain Connectivity and Electrophysiology dataset. The model should not only distinguish between various neuron subtypes with exceptional accuracy, but also detect functional connectivity patterns. Additionally, the algorithm must incorporate explainability techniques to provide insights into the decision-making process and be scalable for handling large-scale datasets with millions of nodes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Develop a state-of-the-art multi-modal fusion node classification model using a novel Hierarchical Attention Graph Wavelet Convolutional Neural Network (HAGW-CNN) on the merged Mouse Brain Connectivity and Electrophysiology dataset. The model should have high accuracy in distinguishing neuron subtypes, detect functional connectivity patterns, and incorporate explainability techniques for insights and scalability to handle large-scale datasets with millions of nodes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"functional connectivity detection\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Connectivity and Electrophysiology dataset\",\n            \"modality\": [\n                \"graph\",\n                \"multimodal\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"nodes\": {\n                    \"total\": null,\n                    \"range\": null\n                },\n                \"edges\": {\n                    \"total\": null,\n                    \"range\": null\n                },\n                \"modalities\": {\n                    \"connectivity\": null,\n                    \"electrophysiology\": null\n                }\n            },\n            \"description\": \"A large-scale dataset combining mouse brain connectivity and electrophysiology data, merged for model training.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HAGW-CNN\",\n            \"family\": \"Hierarchical Attention Graph Wavelet Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"novel architecture\",\n                \"explainability\": true,\n                \"scalability\": {\n                    \"nodes\": \"millions\"\n                }\n            },\n            \"description\": \"A cutting-edge model for node classification in multi-modal brain data, focusing on neuron subtypes and functional connectivity patterns.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Novel HAGW-CNN architecture expected to provide advanced performance in multi-modal data fusion\",\n        \"Incorporating explainability techniques for interpretability\",\n        \"Algorithm must scale for large-scale data with millions of nodes\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Insights into decision-making process and large-scale handling\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the provided instruction, capturing the user's intent, problem context, and requirements. The model name, family, and type are specified according to the novel HAGW-CNN. Accuracy, functional connectivity, and explainability are mentioned as performance targets. The dataset is described as large and multimodal, with 'infer-search' indicating that specific details were inferred from the problem description. The confidence score is set to 0.95 due to the assumed clarity of the instruction and mapping to the template.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "For the upcoming project, the machine learning development team is tasked with implementing an advanced, multi-step unsupervised time-series anomaly detection system. The objective is to surpass an average accuracy of 70% across all 250 Hexagon ML/UCR datasets, specifically focusing on lightweight transformer models that exhibit exceptional efficiency without compromising performance. In addition, please provide a detailed evaluation strategy and a comparative analysis with competing model architectures.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"business or industrial\",\n        \"description\": \"Implement an advanced, multi-step unsupervised time-series anomaly detection system that aims to surpass an average accuracy of 70% across all 250 Hexagon ML/UCR datasets, focusing on lightweight transformer models with high efficiency and performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.7\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model size\",\n                \"value\": \"lightweight\",\n                \"unit\": \"parameters\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": \"optimized for efficiency\",\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hexagon ML/UCR datasets\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_datasets\": 250,\n                \"data_format\": \"raw\"\n            },\n            \"description\": \"A comprehensive set of datasets for evaluating time-series anomaly detection models, especially focused on performance across diverse datasets.\",\n            \"preprocessing\": [\"feature extraction\", \"data normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"visualize performance across datasets\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"lightweight transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer\",\n                \"efficiency measures\": {\n                    \"FLOPs\": \"low\",\n                    \"parameters\": \"minimal\"\n                }\n            },\n            \"description\": \"A state-of-the-art, lightweight transformer model designed for efficient time-series anomaly detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hexagon ML/UCR datasets' diversity requires a model that can perform well across a wide range of temporal data.\",\n        \"Efficiency is crucial for real-world deployments where computational resources are limited.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature engineering for anomaly detection\"],\n        \"target_device\": \"cloud or IoT edge devices\",\n        \"deployment_endpoint\": \"potentially a dedicated anomaly detection platform\",\n        \"inference_engine\": [\"optimized for performance and memory usage\"],\n        \"demonstration\": \"include a visualization of the model's performance on a sample dataset and a performance comparison against competing models\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is tailored to the provided instruction, capturing the user's intent, high expertise level, and the specific requirements (accuracy target, lightweight transformer, dataset complexity, and evaluation strategy). The confidence score is high because the structure is clear and the requested details are included.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the machine learning development team, your objective is to enhance the text classification model for a nuanced Textual Entailment task on a diverse dataset. Implement a robust model that not only achieves high accuracy but also exhibits robustness to domain variations and context dependency. Use transfer learning and propose an ensemble method, ensuring a minimum F1-score of 0.9 for each class, while adhering to strict interpretability requirements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"textual entailment\",\n        \"description\": \"Enhance a text classification model for the Textual Entailment task on a diverse dataset, focusing on robustness, domain variations, context dependency, transfer learning, and achieving a minimum F1-score of 0.9 for each class. The model should prioritize interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Diverse Textual Entailment Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"entailment\"],\n            \"specification\": {\n                \"size\": \"not specified\",\n                \"domain_diversity\": \"diverse\",\n                \"context_dependency\": \"high\"\n            },\n            \"description\": \"A dataset designed for Textual Entailment with emphasis on domain variations and context dependency.\",\n            \"preprocessing\": [\"transfer learning-based data augmentation\"],\n            \"augmentation\": [\"data augmentation techniques for domain adaptation\"],\n            \"visualization\": [\"interpretability analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble method\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability_requirements\": \"high\"\n            },\n            \"description\": \"A robust text classification model, employing transfer learning and an ensemble method, for Textual Entailment task on a diverse dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning from pre-trained models on a wide range of texts enhances robustness to domain variations.\",\n        \"Interpretability is crucial for this task to ensure transparency and decision explanations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction from raw text data\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"models compatible with strict interpretability\"],\n        \"demonstration\": \"demonstration of model performance and interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was created based on the given instruction, highlighting the build intent, high expertise, the Textual Entailment area, and the requirement for transfer learning and interpretability. The F1-score of 0.9 for each class and domain variations were specified. A diverse dataset with emphasis on context dependency was assumed, with the source indicated as a link for external data fetching. The model type was inferred as a neural network with an ensemble method, and the specific interpretation of the F1-score ensures strict adherence to the requirement. The confidence score is high due to the direct mapping of the instruction to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "For the ambitious High School Adapting Social Dynamics Project, design a state-of-the-art Temporal Graph Attention Network (TGAT) with self-organizing memory, integrating real-time data streams and multiple temporal scales. The model must not only classify influential nodes in evolving networks but also track shifts in power dynamics and influential clusters, ensuring interpretability and scalability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"social dynamics\",\n        \"description\": \"Design a state-of-the-art Temporal Graph Attention Network (TGAT) for the High School Adapting Social Dynamics Project. The model should integrate real-time data streams, address multiple temporal scales, and classify influential nodes while tracking shifts in power dynamics and influential clusters. It must ensure interpretability and scalability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High School Adapting Social Dynamics\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"real-time_data_streams\": true,\n                \"temporal_scales\": true\n            },\n            \"description\": \"A real-time evolving dataset with multiple temporal scales to evaluate the TGAT model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Temporal Graph Attention Network (TGAT)\",\n            \"family\": \"Temporal Graph Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"self-organizing_memory\": true\n            },\n            \"description\": \"A state-of-the-art model with self-organizing memory, designed for real-time data streams and detecting influential nodes in evolving networks.\"\n        }\n    ],\n    \"knowledge\": [\n        \"TGAT benefits from temporal attention mechanism and can handle dynamic graph structures.\",\n        \"Incorporating self-organizing memory enhances adaptability and interpretability.\",\n        \"The model must be scalable to accommodate large networks and changing patterns\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Model must be accompanied by explanations of influential nodes and power dynamics changes\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the provided instruction, reflecting the user's intent to build a TGAT model for the High School Adapting Social Dynamics Project. The user's expertise level, problem domain, and specific requirements for the model and dataset are clearly captured. The performance metrics (accuracy, interpretability, and scalability) are included as guidelines for evaluation but left unspecified since values are typically provided by the experimenter. The missing information is marked with null, and the reasoning is given in the confidence section.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For a specialized urban greenspace analysis, develop a tailored SegNet++ architecture, leveraging the Vaihingen dataset with high-resolution orthophotos. Emphasize the differentiation between native, exotic, and invasive plant species, including subtle health indicators, while ensuring robust performance under dawn-dusk radiometry and seasonal shifts. Optimize for precision in diverse microclimates for city sustainable planning recommendations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"Develop a specialized SegNet++ architecture for urban greenspace analysis, leveraging the Vaihingen dataset with high-resolution orthophotos. The model should differentiate between native, exotic, and invasive plant species, including subtle health indicators, while maintaining robust performance under dawn-dusk radiometry and seasonal changes. The goal is to optimize for precision in diverse microclimates for city sustainable planning recommendations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"plant species classification\"],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"data_sources\": \"high-resolution orthophotos\"\n            },\n            \"description\": \"A high-resolution dataset for urban greenspace analysis, containing native, exotic, and invasive plant species identification along with health indicators.\",\n            \"preprocessing\": [\"radiometry normalization\", \"seasonal adjustments\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SegNet++\",\n            \"family\": \"Semantic Segmentation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"SegNet++\",\n                \"focus_on\": [\"native\", \"exotic\", \"invasive species differentiation\"]\n            },\n            \"description\": \"A customized SegNet++ model for urban greenspace analysis, specifically tailored for plant species differentiation and city sustainable planning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model must handle variations in radiometry and seasonal shifts.\",\n        \"Emphasis on subtle health indicators in plant species is crucial.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"extracting microclimate features\"\n        ],\n        \"target_device\": \"cloud-based GPU\",\n        \"deployment_endpoint\": \"city sustainable planning platform\",\n        \"inference_engine\": [\"TensorFlow-serving\"],\n        \"demonstration\": \"visual results and performance analysis across microclimates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intention to build a model, their expertise, the problem's area, and the specific task (image segmentation). The Vaihingen dataset with high-resolution orthophotos is referenced, along with the emphasis on performance aspects. The model name, family, and type are provided, as well as a description. Metrics such as precision are suggested, though the exact value isn't specified since it depends on the model performance. The dataset preprocessing, augmentation, and service-related details are included based on the user's requirements.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "question answering", "instruction": "For a large-scale enterprise, develop a real-time, high-throughput question-answering system utilizing the TriviaQA dataset. Mandate a transformer-based architecture, like GPT-4 (with improved training on Long Form Knowledge), capable of deciphering factoids and intricate queries. The system must deliver responses within 100 milliseconds while maintaining efficiency amid substantial user concurrency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"enterprise\",\n        \"description\": \"Develop a real-time, high-throughput question-answering system for a large enterprise using the TriviaQA dataset. The system must employ a transformer-based architecture, specifically GPT-4 with improved long-form knowledge training, and be able to handle factoid and intricate queries. It should have a response time of 100 milliseconds while maintaining efficiency under high concurrency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 100\n            },\n            {\n                \"name\": \"throughput\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"user concurrency\",\n                \"value\": null,\n                \"unit\": \"users\"\n            },\n            {\n                \"name\": \"average response time under concurrency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TriviaQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"avg_query_length\": null,\n                \"data_format\": \"structured\"\n            },\n            \"description\": \"A large-scale question-answering dataset for developing a real-time system with GPT-4 architecture.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GPT-4 (Long Form Knowledge)\",\n            \"family\": \"transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"GPT-4\",\n                \"training\": {\n                    \"long-form knowledge\": true\n                },\n                \"parameters\": {\n                    \"number_of_layers\": null,\n                    \"attention_heads\": null,\n                    \"hidden_size\": null,\n                    \"number_of_parameters\": null\n                },\n                \"inference_speed\": null,\n                \"memory_consumption\": null\n            },\n            \"description\": \"A transformer-based architecture, designed for real-time question answering with improved long-form knowledge and scalability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Using a transformer-based architecture like GPT-4 trained with Long Form Knowledge is crucial for complex Q&A tasks.\",\n        \"Real-time response with high throughput requires efficient handling of concurrency and low latency.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time, high-throughput system\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU accelerated\", \"optimized for latency\"],\n        \"demonstration\": \"Illustrate the system's performance through benchmarking under various concurrency levels\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the instruction, reflecting the task of building a real-time QA system, specifying GPT-4 as the architecture, and emphasizing on performance metrics such as response time and efficiency under concurrency. However, certain details like the exact preprocessing steps, TriviaQA dataset size, and GPT-4 architecture specifications are not provided as they would typically involve more specific implementation details or optimizations. The confidence score is set to a high value, assuming the instruction provides a clear guidance for the project.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for a cutting-edge machine learning development team, you've been tasked with a unique challenge: to design and implement a state-of-the-art deep learning model for a niche time-series classification problem. Your team has been assigned the \"Satellite Glint Anomaly Detection\" dataset, which contains highly detailed, large-scale geospatial data (INPUT_SEQ_LEN = 4096, with an expanded feature set of 18 dimensions, including spectrographic and topographic information). The objective is to detect anomalies in satellite reflections caused by unexpected events, such as urban growth or extreme weather, with a label space of {0, 1, 2, 3} representing varying levels of anomaly severity. The model should not only achieve high accuracy but also demonstrate exceptional interpretability and scalability, as clients require explainable predictions for critical decision-making. In addition, the project must adhere to strict latency requirements, with models needing to process and classify sequences in real-time. Your team will be evaluated not only on overall accuracy but also on the efficiency of their solution in terms of computational resources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"remote sensing\",\n        \"description\": \"Design and implement a state-of-the-art deep learning model for Satellite Glint Anomaly Detection. The dataset contains large-scale geospatial data (INPUT_SEQ_LEN=4096) with a 18-dimensional feature set (spectrographic and topographic). The objective is to detect anomalies in satellite reflections with a severity label space of {0, 1, 2, 3}. The model should be accurate, interpretable, scalable, and meet real-time latency requirements for critical decision-making.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": {\n                    \"type\": \"explainable AI\",\n                    \"specifics\": \"Model should have clear interpretability features for client-side understanding\"\n                }\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"Efficient handling of large sequence data\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"type\": \"real-time\",\n                    \"requirement\": \"Models must process and classify sequences in real-time\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational_resources\",\n                \"value\": \"Efficiency in terms of CPU, memory, and FLOPs\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"End-to-end inference time for real-time requirements\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Satellite Glint Anomaly Detection\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"anomaly severity\"],\n            \"specification\": {\n                \"input_length\": 4096,\n                \"feature_dimensions\": 18,\n                \"data_type\": \"geospatial with spectrographic and topographic info\"\n            },\n            \"description\": \"Contains large-scale geospatial data with 18-dimensional expanded feature set, including for anomaly detection in satellite reflections.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\" \n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"State-of-the-art deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"Deep Learning architecture for time-series data\",\n                \"explainability\": \"Integrated explainability techniques\"\n            },\n            \"description\": \"Advanced deep learning model specifically designed for satellite glint anomaly detection, emphasizing interpretability and real-time performance.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"Edge/Real-time environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Optimized for real-time inference\"\n        ],\n        \"demonstration\": \"Demonstration of explainable predictions and real-time classification\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the project manager's task for a time-series classification problem with real-world constraints. The user's intent is to build a model, their expertise is assumed to be high, and the problem description, performance metrics, and complexity requirements are derived from the given instruction. Since the user doesn't specify a target accuracy value, the metric is set to null. The dataset details are inferred from the 'Satellite Glint Anomaly Detection' source. The deep learning model reflects the specified objectives of accuracy, interpretability, and real-time performance.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a cutting-edge machine learning project manager, your team has been tasked with exploring the under-explored domain of low-resource language translation using the MuST-C dataset. Focus on developing a dual-encoder multilingual model that can seamlessly translate not only between Russian and English but also bridge five less-common languages from the African continent. The model should achieve a semantic similarity score measured by the Mean Average Precision (MAP) above 80% and demonstrate exceptional contextual understanding through human evaluation. Additionally, incorporate explainable AI techniques to ensure transparency in translation decisions. Document the entire pipeline, adhering to Explainable AI practices, and provide a comprehensive comparative analysis with state-of-the-art models.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"low-resource language translation\",\n        \"description\": \"Explore the under-explored domain of low-resource language translation using the MuST-C dataset. Develop a dual-encoder multilingual model capable of translating between Russian and English, as well as five African languages. Aim for a Mean Average Precision (MAP) score above 80% and demonstrate contextual understanding through human evaluation. Incorporate explainable AI techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Average Precision (MAP)\",\n                \"value\": 0.8\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MuST-C\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translations\"],\n            \"specification\": {\n                \"languages\": [\"Russian\", \"English\", \"African languages\"],\n                \"specificity\": \"low-resource\"\n            },\n            \"description\": \"A dataset for low-resource language translation focusing on Russian, English, and five African languages.\",\n            \"preprocessing\": [\n                \"Clean and preprocess text data\",\n                \"Low-resource data augmentation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dual-Encoder Multilingual Model\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"encoder-decoder\",\n                \"language coverage\": \"Russian-English-African\",\n                \"explainability\": \"incorporated\"\n            },\n            \"description\": \"A state-of-the-art dual-encoder model designed for low-resource language translation and incorporating explainable AI techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Focus on low-resource scenarios\",\n        \"Incorporate explainable AI for transparency\",\n        \"Human evaluation to assess context understanding\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Extract context features\",\n            \"Contextualized word embeddings\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Deploy with explainable inference methods\"],\n        \"demonstration\": \"Demonstrate translation examples with model explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the requirements of the low-resource language translation project, including the use of the MuST-C dataset, the development of a dual-encoder model, and the incorporation of explainable AI. The user's intent and expertise level are clearly captured. The performance metric (MAP) and aim for context understanding are specified. The model type, data preprocessing, and human evaluation are based on the given task. However, the target device and deployment details are not specified since they weren't given in the instruction. The confidence score is set to 0.97, as the provided information is deemed accurate but relies on the completeness of the input.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, you're tasked with developing a highly engaging and interactive question-answering platform for elementary school children. Utilize the tailored KidsQA dataset and a simplified BERT variant specifically designed for educational content. Mandate a user experience that prioritizes visual appeal and age-appropriate language, ensuring the model generates responses with utmost clarity and ease, while promoting cognitive development.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Develop a highly interactive question-answering platform for elementary school children, incorporating the KidsQA dataset and a simplified BERT variant suitable for educational content. Focus on visual appeal, age-appropriate language, clarity, and cognitive development.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": \"fast\",\n                \"unit\": \"seconds per query\"\n            },\n            {\n                \"name\": \"user engagement\",\n                \"value\": \"high\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"custom\",\n                \"domain\": \"educational\",\n                \"language_level\": \"elementary\"\n            },\n            \"description\": \"A dataset specifically designed for educational question-answering with young audiences.\",\n            \"preprocessing\": [\n                \"age-appropriate tokenization\",\n                \"domain-specific filtering\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"interactivity and clarity analytics\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Simplified BERT for Educational Content\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_depth\": \"reduced for simplicity\",\n                \"BERT variant\": \"simplified for educational use\",\n                \"language complexity\": \"elementary\"\n            },\n            \"description\": \"A modified BERT model optimized for educational question answering with young users.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Simplicity in design is key for a 5th-grade audience\",\n        \"Visual appeal and clarity in responses boost engagement\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual enrichment\",\n            \"question-type classification\"\n        ],\n        \"target_device\": \"mobile-friendly\",\n        \"deployment_endpoint\": \"educational platform\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"promotes cognitive development through interactive responses\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents a clear task of building an interactive QA platform for elementary school children. The problem area, task, and application domain are accurately mapped from the instruction. Performance metrics have not been specified since the focus is on user experience and cognitive development. The model and dataset details are derived from the instruction, and the service requirements reflect the desired user experience. However, specific metric values are not provided, leaving room for interpretation.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "text classification", "instruction": "As a project manager for the advanced machine learning team, your objective is to develop a highly accurate text classification model using the BoolQ dataset. Along with traditional binary inference (0: no entailment, 1: entailment), introduce a novel constraint that measures model robustness against linguistic variations and evaluate its F1 score alongside accuracy, ensuring multi-faceted performance assessment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Develop a highly accurate text classification model using the BoolQ dataset. The model should handle binary inference tasks (no entailment, entailment) and introduce a novel constraint to measure robustness against linguistic variations. Perform evaluation using both F1 score and accuracy for a comprehensive performance assessment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"category\"],\n            \"specification\": {\n                \"task\": \"binary text classification\",\n                \"linguistic_variations\": \"yes\"\n            },\n            \"description\": \"A dataset for text classification with binary inference tasks and a focus on linguistic variations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novel_constraint\": \"robustness against linguistic variations\"\n            },\n            \"description\": \"A state-of-the-art neural network model, specifically designed for text classification and incorporating a novel constraint for improved robustness.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction, indicating that the user is seeking to build a text classification model for the advanced machine learning team. The problem area, task, and dataset are clearly defined. The metrics required for evaluation (accuracy and F1 score) are included without specific values because they will need to be optimized during model development. The model type is specified as a neural network and the novel constraint is mentioned. The high expertise level of the user is also captured. However, certain details like preprocessing, augmentation, or specific model architecture are not specified due to the lack of details in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a specialized project manager for a cutting-edge deep learning team, your objective is to develop a highly accurate and robust model for classifying intricate real-life industrial sensor data. The \"Ethanol Concentration v2.0\" dataset, consisting of 1751 unique sensor readings per sample (INPUT_SEQ_LEN) across 3 distinct zones (training, validation, and testing), with 3 distinct chemical signatures (INPUT_DIM) will be employed. Your team must design an optimized neural architecture that employs temporal context and feature extraction to predict labels within {0, 1, 2, 3} while meeting a stringent minimum accuracy of 95% on unseen sequences. Additionally, document the impact of hyperparameter tuning and the chosen loss function on overall model performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"industrial\",\n        \"description\": \"Develop a highly accurate and robust deep learning model for classifying industrial sensor data from the 'Ethanol Concentration v2.0' dataset, which contains 1751 unique sensor readings per sample across 3 zones with 3 chemical signatures. The model should utilize temporal context and feature extraction to predict labels {0, 1, 2, 3} and must achieve at least 95% accuracy on unseen sequences. Document the impact of hyperparameter tuning and the chosen loss function on performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ethanol Concentration v2.0\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"labels\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 1751,\n                \"INPUT_DIM\": 3,\n                \"zones\": [\"training\", \"validation\", \"testing\"]\n            },\n            \"description\": \"A real-life industrial dataset with sensor data across 3 zones and 3 chemical signatures.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"temporal convolutional neural network (TCN) or recurrent neural network (RNN) with attention mechanism\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"neural_architecture\": \"sequence processing with temporal context and feature extraction\",\n                \"loss_function\": \"yet_to_determine\",\n                \"optimizer\": \"yet_to_determine\"\n            },\n            \"description\": \"A deep learning model for time-series classification with temporal and feature extraction capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model requires advanced time-series analysis techniques due to the intricate sensor data.\",\n        \"Accuracy of at least 95% is crucial.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow\", \"PyTorch\", \"TensorFlow.js\"],\n        \"demonstration\": \"Illustrative results of hyperparameter tuning and loss function analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the provided user's intent and requirements. The area and downstream task are determined from the problem description. The dataset details match the provided information, and a generic specification of the model (family, type, and desired capabilities) are included. Performance metrics are set as per the required accuracy of 95%, and space for documenting impact of hyperparameter tuning and chosen loss function is provided. However, actual metrics for time and space complexity have not been included as the instruction doesn't provide specific values. The confidence score is 0.95 based on the provided details, assuming these are general requirements that can be filled in with more specific information in a real project.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the cutting-edge AI-driven aerial analytics team, your mandate is to develop a state-of-the-art YOLOv4++ architecture specifically tailored for real-time, high-resolution imagery segmentation on the diverse DOTA dataset. Emphasize multi-class object recognition, not just vehicles, but also intricate urban structures, while maintaining rigorous precision (99%) and recall (98%) metrics. Strive for near-instantaneous (<100ms) inference, ensuring scalability for massive concurrent data processing in the dynamic urban environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"aerial analytics\",\n        \"description\": \"Develop a state-of-the-art YOLOv4++ architecture for real-time, high-resolution imagery segmentation on the DOTA dataset. Focus on multi-class object recognition, including vehicles and intricate urban structures. Maintain precision (99%) and recall (98%) metrics while ensuring near-instantaneous (<100ms) inference for scalability in dynamic urban environments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.1, \n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DOTA\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"resolution\": {\n                    \"width\": \"high\",\n                    \"height\": \"high\"\n                },\n                \"diversity\": \"dynamic urban environment\"\n            },\n            \"description\": \"A diverse high-resolution dataset for real-time object segmentation, including multi-class urban structures and vehicles.\",\n            \"preprocessing\": [\n                \"high-resolution images\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv4++\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art for real-time image segmentation\",\n                \"focus\": \"multi-class object recognition\"\n            },\n            \"description\": \"An advanced YOLOv4 architecture designed specifically for real-time, high-resolution imagery segmentation on the DOTA dataset, prioritizing precision and recall.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advancing in multi-class object recognition is crucial for a comprehensive aerial analytics solution.\",\n        \"Scalability is essential for dynamic urban environments with massive concurrent data processing.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"multi-class label generation\"],\n        \"target_device\": \"embedded or edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"high-resolution, near-instantaneous, and accurate segmentation demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response effectively captures the details of the given project instruction, reflecting the requirements for a high-performance YOLOv4++ architecture, multi-class recognition, and the desired metrics. The user's high level of expertise and the project's focus on scalable, real-time aerial analytics are highlighted. Specifics like preprocessing and augmentation steps are inferred from the requirements. The confidence score is high as most information has been captured from the instruction.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "question answering", "instruction": "For an oil and gas conglomerate, design a cutting-edge QA system based on the domain-specific Salsbury Oils TriviaQA extension. Utilize an advanced BERT-Enhanced model trained intensively on obscure industry trivia and intricate chemical-plant questions. The system must prioritize response speed (<50 ms) with load balancing for 10,000 concurrent users, ensuring seamless performance and reliability in a hazardous environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"oil and gas industry\",\n        \"description\": \"Design a cutting-edge QA system for an oil and gas conglomerate, using the Salsbury Oils TriviaQA extension. The system should be based on an advanced BERT-Enhanced model trained on specialized industry trivia and complex chemical-plant questions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 50\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"average response time per 10k concurrent users\",\n                \"value\": 50\n            },\n            {\n                \"name\": \"load balancing\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Salsbury Oils TriviaQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain-specificity\": \"extensive\",\n                \"question_type_coverage\": \"industry trivia and intricate chemical-plant questions\"\n            },\n            \"description\": \"An advanced trivia dataset tailored for the oil and gas industry with complex questions.\",\n            \"preprocessing\": [\n                \"BERT model fine-tuning\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-Enhanced QA System\",\n            \"family\": \"Transformer-based QA models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT with industry-specific enhancements\",\n                \"training details\": \"intensive training on obscure industry trivia\"\n            },\n            \"description\": \"A state-of-the-art QA system utilizing BERT for the oil and gas domain.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"load balancing for 10,000 concurrent users\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"hazardous environment compatible\",\n        \"inference_engine\": [\"optimized for low latency, e.g., serving libraries like TensorFlow Serving or TensorFlow Lite\"],\n        \"demonstration\": \"highly performant and reliable with response times below 50 ms\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a QA system, high expertise, and the specific requirements such as the domain of the dataset, BERT-based model, response time constraints, and load balancing for concurrent users. The missing load balancing efficiency and other service details can be filled during the design phase, as they are not directly provided in the instruction.\",\n        \"score\": 0.92\n    }\n}"}
{"task": "text classification", "instruction": "As the project manager, you must supervise the team in developing a highly sophisticated Textual Entailment model that not only fine-tunes a pre-trained model on a comprehensive, domain-adapted dataset, but also demands exceptional adaptability to unseen contexts. Mandate an ensemble of at least three tailored models, each achieving a multi-class F1-score of 0.9 or above, with a focus on explainable AI and limiting computational overhead. Ensure strict adherence to model bias mitigation techniques.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"NLP and AI research\",\n        \"description\": \"Develop a highly sophisticated Textual Entailment model that fine-tunes a pre-trained model on a comprehensive, domain-adapted dataset. The model should demonstrate excellent adaptability to unseen contexts, utilize an ensemble of at least three tailored models, each achieving a multi-class F1-score of 0.9 or above, and emphasize explainable AI while minimizing computational overhead. Bias mitigation techniques must be strictly followed.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"ensemble F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Comprehensive Domain-Adapted Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"textual entailment\"],\n            \"specification\": {\n                \"size\": null,\n                \"diversity\": \"comprehensive and domain-adapted\"\n            },\n            \"description\": \"A large, diverse dataset suitable for fine-tuning the Textual Entailment model.\",\n            \"preprocessing\": [\"domain adaptation techniques\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Textual Entailment Ensemble\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_size\": 3,\n                \"explainability\": true,\n                \"computational_efficiency\": true\n            },\n            \"description\": \"An ensemble of at least three models, each achieving high F1-score, focusing on explainability and computational efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptability to unseen contexts is essential for a Textual Entailment model.\",\n        \"Mandating explainable AI ensures model transparency and trustworthiness.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain adaptation techniques\", \"explainable AI methods\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"Context-specific, interpretable predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's intent to build a model, with a high level of expertise. The problem area, task, and domain are specified accurately from the instruction. Performance metrics, including ensemble F1-score, are clearly defined, and model requirements like explainability and efficiency are met. Dataset details are derived from the text. Missing parts like specific model names, exact dataset size, or computational overhead requirements can be clarified by further consultation with the project manager. The confidence score is high, given the comprehensive structure of the response.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "The project manager is sought to oversee the development of a cutting-edge, iterative deep-learning architecture for time-series anomaly detection in real-world industrial environments, targeting precision above 85% across the highly specialized subset of 100 Hexagon ML/UCR datasets known for their complex, noisy patterns. Emphasize on integrating novel attention mechanisms within lightweight LSTNet variants and benchmark against state-of-the-art architectures like Fourier Neural Operators (FNOs) and Efficient Transformer Networks, ensuring runtime optimization for edge devices while maintaining stringent resource constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"industrial\",\n        \"description\": \"Oversee the development of a cutting-edge, iterative deep-learning architecture for time-series anomaly detection in real-world industrial environments. Aim for precision above 85% on the complex, noisy subset of 100 Hexagon ML/UCR datasets, focusing on lightweight LSTNet variants with novel attention mechanisms. Benchmarks should include state-of-the-art architectures like Fourier Neural Operators and Efficient Transformer Networks, while optimizing for runtime on edge devices and adhering to strict resource constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"runtime optimization for edge devices\",\n                \"value\": null,\n                \"unit\": \"optimized\"\n            },\n            {\n                \"name\": \"resource constraints\",\n                \"value\": \"stringent\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hexagon ML/UCR datasets\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"description\": \"Subset of datasets known for complex, noisy patterns\"\n            },\n            \"description\": \"Set of 100 real-world industrial datasets with challenging noise and pattern complexity.\",\n            \"preprocessing\": [\"novel attention mechanisms\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Iterative Deep-Learning Architecture\",\n            \"family\": \"Lightweight LSTNet Variants with Attention\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"mention\": [\"novel attention mechanisms\", \"LSTNet variants\", \"FNOs\", \"Efficient Transformer Networks\"],\n                \"resourceoptimized\": \"for edge devices\"\n            },\n            \"description\": \"A state-of-the-art, deep learning model designed for industrial time-series anomaly detection, focusing on lightweight architecture and precision.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Focus on novel attention mechanisms for handling complex, noisy patterns\",\n        \"Benchmark against FNOs and Efficient Transformer Networks\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Emphasizes runtime optimization and resource constraints in an industrial context.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers all elements in the schema, mapping the user's instruction to specific problem areas, performance goals, dataset selection, and model requirements. Attention is given to novel techniques and benchmarking against state-of-the-art. However, details like specific preprocessing steps and augmentation techniques are not provided, as these would typically be more granular and dependent on the engineering team's expertise.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As an expert project manager for an avant-garde AI labeling project, design a robust solution by fusing various advanced classification algorithms (e.g., deep learning, SVM, and random forest) on the 20 Newsgroups dataset. Emphasize interpretability, explainability, and ensemble fusion, while meticulously optimizing hyperparameters for peak performance. Conduct a rigorous performance benchmark across multiple hardware configurations (GPU, TPU, and CPU) considering parallel processing, dynamic resource allocation, and incremental model updates. Ensure timely deployment, considering real-time adjustments and hardware compatibility constraints, all while meeting stringent deadlines and adhering to ethical AI principles.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI and data management\",\n        \"description\": \"Design a robust AI labeling project using advanced classification algorithms (deep learning, SVM, and random forest) on the 20 Newsgroups dataset. Focus on interpretability, explainability, ensemble fusion, and optimize hyperparameters for high performance. Compare across GPU, TPU, and CPU hardware configurations, considering parallel processing, dynamic resource allocation, and incremental model updates. Aim for timely deployment with real-time adjustments and hardware compatibility, while maintaining ethical AI principles.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"parallel processing speed\",\n                \"value\": null,\n                \"unit\": \"fps\"\n            },\n            {\n                \"name\": \"dynamic resource allocation efficiency\",\n                \"value\": null,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"classification labels\"],\n            \"specification\": {\n                \"number_of_classes\": null,\n                \"size\": null\n            },\n            \"description\": \"A dataset for multi-class text classification, consisting of around 20,000 newsgroup documents.\",\n            \"preprocessing\": [\"tokenization\", \"vectorization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance plots\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble of deep learning, SVM, and random forest\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"model architecture\": [\"deep learning\", \"SVM\", \"random forest\"],\n                \"hyperparameters\": {\n                    \"deep learning\": {},\n                    \"SVM\": {},\n                    \"random forest\": {}\n                }\n            },\n            \"description\": \"A fused model combining multiple advanced classification algorithms for enhanced performance and interpretability.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"hyperparameter tuning\",\n            \"ensemble learning\"\n        ],\n        \"target_device\": [\n            \"GPU\",\n            \"TPU\",\n            \"CPU\"\n        ],\n        \"deployment_endpoint\": \"cloud-based\",\n        \"inference_engine\": [\"batch processing\", \"real-time updates\"],\n        \"demonstration\": \"user-friendly interface for model explanations and performance monitoring\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the given instruction and schema, focusing on the build intent for a machine learning project. The problem area is natural language processing due to the text classification task, and the user's high expertise level is reflected. Performance metrics like accuracy and interpretability are included, but the specific expected values are not provided. Complexity metrics such as parallel processing speed and resource allocation efficiency are also specified. The 20 Newsgroups dataset and the ensemble model with three algorithms are clearly described. The service section covers deployment, inference, and demonstration, all aligned with ethical principles.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "As a project manager for the specialized PubMed Arboviridae protein interaction network analysis, demand an innovative graph neural network model that integrates domain-specific adaptations from pre-trained SARS-CoV-2 immune response models. Strive for precision above 97% in classifying host-virus interactions, with a stringent requirement of below 30ms response time on a Raspberry Pi 4 for portable healthcare devices, while ensuring compatibility with low-power hardware and real-time data streaming.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"biomedical research\",\n        \"description\": \"As a project manager, seek an innovative graph neural network model for analyzing PubMed Arboviridae protein interaction networks, with domain-specific adaptations from pre-trained SARS-CoV-2 immune response models. Target precision is above 97% for host-virus interaction classification, with a strict response time requirement of less than 30ms on Raspberry Pi 4 for portable healthcare devices, ensuring compatibility with low-power hardware and real-time data streaming.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 30,\n                \"unit\": \"ms\",\n                \"optimization_target\": \"strict\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PubMed Arboviridae Protein Interaction Network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domainadaptation\": true,\n                \"source_model\": \"SARS-CoV-2 immune response model\"\n            },\n            \"description\": \"Dataset for protein interaction network analysis with domain-specific adaptations from pre-trained SARS-CoV-2 models.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Graph Neural Network with domain-specific adaptations\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"97%\",\n                \"deployment_platform\": [\"Raspberry Pi 4\", \"low-power hardware\"]\n            },\n            \"description\": \"An innovative model designed for high-precision host-virus interaction classification on graph data, adapted from SARS-CoV-2 immune response models.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Domain-specific adaptations are key for improved performance in Arboviridae protein interaction analysis.\",\n        \"Compatibility with low-power hardware for portable healthcare devices is essential.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"Raspberry Pi 4, low-power hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"compatible with real-time data streaming\"],\n        \"demonstration\": \"Mobile healthcare device testing and real-time interaction demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project manager's requirements, including the need for a GNN model, domain adaptation, performance targets, and hardware specifications. However, specific model architecture details and preprocessing steps are not provided due to the limited information in the instruction. The confidence score is high given the clear task and the availability of relevant information in the structured format.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "Incorporate the IAU's taxonomy of astronomical objects into your project. Retrieve the 'GalaxyZoo2' dataset from authoritative databases and multiple versions, ensuring data integrity. Perform an in-depth analysis by fine-tuning not only VGG16, ResNet50, and InceptionV3 but also incorporating recent SOTA models like EfficientNet and DenseNet. Implement and compare multiple image preprocessing pipelines, considering adaptive resizing, color normalization, and noise reduction strategies. Address the issue of class imbalance using oversampling, undersampling, or combined techniques, and assess their impact on model robustness. Lastly, justify the selection of the most efficient model not only based on accuracy but also computational efficiency and generalization performance in the presence of varying levels of noise and dataset complexities.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Incorporate IAU's taxonomy of astronomical objects into a project, analyze the GalaxyZoo2 dataset from multiple authoritative sources ensuring data integrity. Compare various image classification models including VGG16, ResNet50, InceptionV3, EfficientNet, and DenseNet, while considering preprocessing techniques and class imbalance. Focus on model efficiency, robustness, and generalization in noisy and complex datasets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"generalization performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"GalaxyZoo2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"astronomical object class\"],\n            \"specification\": {\n                \"data_versions\": [\"authoritative\"],\n                \"data_integrity\": \"ensured\"\n            },\n            \"description\": \"Galaxy classification dataset incorporating IAU's taxonomy, sourced from multiple authoritative databases.\",\n            \"preprocessing\": [\n                \"adaptive resizing\",\n                \"color normalization\",\n                \"noise reduction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"VGG16\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        },\n        {\n            \"name\": \"ResNet50\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        },\n        {\n            \"name\": \"InceptionV3\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        },\n        {\n            \"name\": \"EfficientNet\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        },\n        {\n            \"name\": \"DenseNet\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        }\n    ],\n    \"knowledge\": [\n        \"IAU's taxonomy provides a standardized categorization for astronomical objects.\",\n        \"Class imbalance can impact model performance, thus handling techniques like oversampling, undersampling, or a combination is essential.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"impact of varying noise and dataset complexities on model\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a project with high-level requirements. The problem area, task, and dataset requirements are extracted from the instruction. Performance and complexity metrics are included, but no specific values are provided as they would typically depend on the analysis. The model selection and preprocessing steps are aligned with the task and recent trends. However, detailed model specifications, preprocessing pipeline effectiveness, and class imbalance handling outcomes are not included due to the lack of specific metrics or values.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a forward-thinking project manager for an elite Earth monitoring initiative, your mandate is to optimize the utilization of sophisticated Sentinel-2 satellite data for land use classification into an extensive array of classes. Instruct your skilled ML team to critically analyze and benchmark multiple high-performance segmentation models, such as next-generation CNNs, U-Nets with enhanced skip connections, and DeepLabV3 variants, while taking into account fine-grained metrics like F1-score, Intersection over Union (IoU), and dynamic resizing adaptability. Emphasize the importance of near real-time processing, energy efficiency, low latency, and GPU-accelerated training, all while striving for state-of-the-art accuracy in distinguishing intricate land formations like estuaries, intricate forest ecosystems, and dense urban sprawl, within a limited hardware budget. Your report should elucidate complex trade-offs between model sophistication and computational efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"earth monitoring\",\n        \"description\": \"Optimize Sentinel-2 satellite data for land use classification into diverse classes, focusing on next-generation CNNs (e.g. CNNs, U-Nets with enhanced skip connections, and DeepLabV3 variants). Key performance metrics include F1-score, IoU, and dynamic resizing adaptability. Emphasize near real-time processing, energy efficiency, low latency, and GPU-accelerated training. Target accuracy should be state-of-the-art for identifying estuaries, intricate forest ecosystems, and dense urban sprawl within a limited hardware budget.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Intersection over Union (IoU)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Near-real-time processing\",\n                \"value\": null,\n                \"unit\": \"time\"\n            },\n            {\n                \"name\": \"Energy efficiency\",\n                \"value\": null,\n                \"unit\": \"energy per operation\"\n            },\n            {\n                \"name\": \"Latency\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"GPU-accelerated training\",\n                \"value\": true\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2 Satellite Data\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"High-resolution satellite data for land use classification with diverse classes like estuaries, forests, and urban areas.\",\n            \"preprocessing\": [\"dynamic resizing adaptability\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Next-generation CNNs\",\n            \"family\": \"CNNs\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"U-Nets with Enhanced Skip Connections\",\n            \"family\": \"U-Nets\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"DeepLabV3 Variants\",\n            \"family\": \"DeepLabV3\",\n            \"type\": \"neural networks\"\n        }\n    ],\n    \"knowledge\": [\n        \"Complexity trade-offs must be considered for state-of-the-art accuracy in resource-constrained environments.\",\n        \"Fine-grained evaluation is crucial for land use classification with nuanced classes.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"F1-score optimization\", \"IoU improvement\"],\n        \"target_device\": \"GPU-accelerated\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"GPU-based\"],\n        \"demonstration\": \"Include real-time processing and model performance visuals\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent (to build a model) and their high level of expertise. The problem details, including the area (computer vision), task (image segmentation), and specific models, are parsed accurately from the instruction. The metrics like F1-score and IoU are included, but without specific target values. The emphasis on near real-time, energy efficiency, and GPU-acceleration is noted, as well as the complexities and trade-offs. However, the exact GPU-accelerated training time, energy efficiency figures, and model-specific specifications are not provided due to lack of numerical values.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Task: Design a high-performance SegNet-based urban green space segmentation model using the Vaihingen dataset. The model must identify not only standard vegetation categories but also differentiate between precise sub-species: healthy, stressed, and invasive plants. It should exhibit exceptional resilience to various lighting conditions, seasonal changes, and unpredictable weather, while ensuring real-time adaptability and delivering long-term predictive analytics for actionable urban planning. Also, incorporate a novel transfer learning technique and optimize for minimal computational resources during inference.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"Design a SegNet-based urban green space segmentation model using the Vaihingen dataset. The model should identify not only standard vegetation categories but differentiate between healthy, stressed, and invasive plants. It must be resilient to lighting conditions, seasonal changes, and unpredictable weather, be real-time adaptable, and provide long-term predictive analytics for urban planning. Incorporate a novel transfer learning technique and optimize for minimal computational resources during inference.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specific sub-species segmentation accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time inference speed\",\n                \"value\": null,\n                \"unit\": \"frames per second\"\n            },\n            {\n                \"name\": \"computational resource efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": {\n                    \"total_images\": null,\n                    \"unique_subspecies_labels\": {\n                        \"healthy\": null,\n                        \"stressed\": null,\n                        \"invasive\": null\n                    }\n                },\n                \"image_dimensions\": {\n                    \"height\": null,\n                    \"width\": null\n                }\n            },\n            \"description\": \"An urban green space dataset for image segmentation, including multiple vegetation categories and sub-species differentiation.\",\n            \"preprocessing\": [\"light normalization\", \"seasonal adjustment\"],\n            \"augmentation\": [\"data augmentation with lighting and weather conditions\"],\n            \"visualization\": [\"segmentation mask analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SegNet-based Green Space Segmentation Model\",\n            \"family\": \"SegNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novel_transfer_learning\": true,\n                \"optimized_inference\": true,\n                \"model_size\": null,\n                \"flops\": null\n            },\n            \"description\": \"A SegNet model designed for urban green space segmentation, featuring sub-species differentiation, transfer learning, and computational resource optimization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating transfer learning for improved model performance on limited data\",\n        \"Adapting to real-time conditions and long-term predictive analytics for actionable urban planning\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"lighting feature extraction\", \"seasonal feature engineering\"],\n        \"target_device\": \"edge devices with minimal compute\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Mobile inference framework optimized for computational efficiency\"],\n        \"demonstration\": \"Real-time segmentation results and predictive analytics for decision-making\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the provided instruction, reflecting a high-expertise user's request for a segmentation model. The information about the area, downstream task, and dataset are detailed, but exact performance values are not specified. Novel transfer learning and computational resource optimization are clearly mentioned. The response is likely to be incomplete without the user providing detailed model specifications and the exact performance targets.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "For a high-precision, multilingual adaptation, develop an edge computing-enabled Transformer-XL variant specifically tailored for translating TED Talks in real-time between five underrepresented languages, including English to Portuguese. Emphasize on efficient hardware acceleration, low-bit quantization, and dynamic vocabulary pruning to optimize performance on IoT devices with limited computational power, targeting >98% translation accuracy, <5% runtime latency, and an average energy consumption reduction of 40% compared to conventional models. Ensure an intuitive, power-saving UI designed for long battery life on portable devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"education, multilingual\",\n        \"description\": \"Develop an edge computing-enabled Transformer-XL variant for real-time translation between five underrepresented languages, including English to Portuguese. Focus on efficient hardware acceleration, low-bit quantization, and dynamic vocabulary pruning for IoT devices with limited compute power.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"translation accuracy\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"runtime latency\",\n                \"value\": 0.05,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"energy consumption reduction\",\n                \"value\": 0.4,\n                \"unit\": \"40%\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"hardware acceleration\",\n                \"value\": \"optimized\"\n            },\n            {\n                \"name\": \"low-bit quantization\",\n                \"value\": true\n            },\n            {\n                \"name\": \"dynamic vocabulary pruning\",\n                \"value\": \"for IoT devices\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TED Talks dataset (specifically for the target languages)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source_size\": \"extensive (TED Talks corpus)\",\n                \"multilingual\": true\n            },\n            \"description\": \"Real-time translation data from TED Talks in underrepresented languages\",\n            \"preprocessing\": [\n                \"multilingual corpus preprocessing\",\n                \"translation dataset splitting\"\n            ],\n            \"augmentation\": [\n                \"real-time data variations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Edge-Transformer-XL\",\n            \"family\": \"Transformer-XL\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-XL variant\",\n                \"computational_efficiency\": \"edge computing enabled\"\n            },\n            \"description\": \"A Transformer-XL designed specifically for edge computing, targeting low-resource IoT environments\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"adaptation for edge devices\",\n            \"power-saving techniques\"\n        ],\n        \"target_device\": \"IoT devices with limited compute\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time processing on IoT\"],\n        \"demonstration\": \"UI showcasing power-saving and long battery life for portable devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the provided instruction, focusing on the task of building a machine translation model, specifying the expertise level, and the requirements for accuracy, latency, and energy efficiency. The task includes an edge computing-enabled Transformer-XL, multilingual support, and specific performance optimization techniques. The dataset and model details are derived from the problem description. The UI requirements are clearly defined.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project lead for the next-generation international AI collaboration, your mandate is to design a highly-specialized multilingual AI translation model that excels in translating niche languages like Swahili to English. Leveraging the expansive and diverse Masakhane corpus, commandeer a groundbreaking hybrid approach combining advanced Transformer and Recurrent Neural Network (RNN) architectures with domain-specific adaptation techniques. The challenge lies in not just boosting the BLEU score by an impressive 25%, but also optimizing for low-resource language proficiency and achieving a precision boost of 10% without sacrificing fluency or cultural nuances. Additionally, conduct a comprehensive, peer-reviewed benchmark analysis with the latest competitors, presenting a detailed error analysis report and addressing ethical implications in algorithm design.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"international collaboration, translation services\",\n        \"description\": \"Design a highly-specialized multilingual AI translation model, focusing on niche languages like Swahili to English. Utilize the Masakhane corpus, incorporate a hybrid approach with Transformer and RNN architectures, and employ domain-specific adaptation techniques. Aim for a BLEU score improvement of 25%, precision boost of 10%, and maintain fluency and cultural nuances. Conduct a benchmark analysis, detailed error analysis, and address ethical implications.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 25.0\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Masakhane corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translations\"],\n            \"specification\": {\n                \"language pairs\": \"Swahili to English\",\n                \"diversity\": \"expansive and diverse\"\n            },\n            \"description\": \"A multilingual corpus for training a specialized translation model on niche languages\",\n            \"preprocessing\": [\"domain adaptation techniques\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer + RNN hybrid\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"commandeer advanced models\",\n                \"domain_adaptation\": \"true\"\n            },\n            \"description\": \"A multilingual model combining Transformers and RNNs, tailored for niche language translation and domain-specific tasks\"\n        }\n    ],\n    \"knowledge\": [\n        \"Leveraging the Masakhane corpus for niche languages\",\n        \"Importance of domain adaptation and maintaining cultural nuances\",\n        \"Challenges in optimizing low-resource language proficiency\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain adaptation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Error analysis and benchmark report\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a specialized translation model, drawing upon their high level of expertise. The problem area, downstream task, and application domain are clearly described, along with performance metrics such as the BLEU score and precision. The Masakhane corpus and domain-specific preprocessing steps are mentioned. The model's hybrid architecture and domain adaptation are specified. Ethical implications are mentioned, but no specific metric is provided for them. The confidence score is slightly lower due to the lack of precision target value and some details in the error analysis.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "image classification", "instruction": "As a specialized project manager for precision agriculture, design an AI-powered smartphone app that utilizes the 'Cameroonian Wildflowers Micro-Data Set'. Specify a state-of-the-art convolutional neural network (CNN) architecture tailored for low-light scenarios, ensuring above 97% accuracy in recognizing 500 rare species. Concoct a strategy for transfer learning, fine-tuning, and ensemble techniques to achieve not only peak accuracy but also a stringent target of under 150ms latency for efficient mobile deployments across various connectivity levels.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Design an AI-powered smartphone app that utilizes the 'Cameroonian Wildflowers Micro-Data Set' for precision agriculture. The model should use a state-of-the-art convolutional neural network (CNN) architecture optimized for low-light scenarios with a target accuracy of over 97% for recognizing 500 rare species.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 150,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cameroonian Wildflowers Micro-Data Set\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_dim\": null, \n                \"number_of_samples\": 500, \n                \"low_light_sensitive\": true\n            },\n            \"description\": \"A dataset for low-light image classification of rare wildflower species in Cameroon.\",\n            \"preprocessing\": [\n                \"Data augmentation for low-light conditions\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Low-light CNN Architecture\",\n            \"family\": \"convolutional neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"customized CNN for low-light scenarios\",\n                \"latency_target\": \"150ms (mobile)\"\n            },\n            \"description\": \"A specialized CNN designed for image classification in low-light conditions with high accuracy requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning will be employed to leverage pre-trained models.\",\n        \"Fine-tuning to adapt the model to the Cameroonian wildflowers data.\",\n        \"Ensemble techniques for improved accuracy and latency trade-off.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"preprocessing for low-light conditions\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient inference for mobile deployments\"],\n        \"demonstration\": \"Efficient mobile app prototype showcasing the image classification performance and latency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a project (AI-powered smartphone app for precision agriculture), the high level of expertise, the specific area (computer vision), and the requirements for low-light CNN architecture and latency. Performance metrics, problem description, and dataset properties are derived from the instruction. However, since the specific dimensions of the image dataset and the exact details of the CNN architecture are not provided, they are left as potential areas for further clarification.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "For a cutting-edge project in medical time series analysis, the Electrocardiogram (ECG) Monitoring dataset has been meticulously partitioned into distinct train, validation, and test sets, each containing unique, 405-step patient waveforms (61-channel granularity). The goal is to develop a highly accurate deep learning model, leveraging advanced RNN architectures, to forecast binary diagnostic labels (0 and 1) for patients. The model must demonstrate state-of-the-art performance, ensuring interpretability and using precision-weighted F1 score as the primary evaluation metric.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"medical\",\n        \"description\": \"Develop a cutting-edge deep learning model using advanced RNN architectures to forecast binary diagnostic labels from ECG Monitoring dataset. The dataset is partitioned into distinct train, validation, and test sets with 405-step patient waveforms and a 61-channel granularity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision-weighted F1 score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electrocardiogram (ECG) Monitoring\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"binary diagnostic labels\"],\n            \"specification\": {\n                \"length\": 405,\n                \"channels\": 61\n            },\n            \"description\": \"Dataset partitioned into train, validation, and test sets with patient ECG waveforms of 405 steps and 61 channels.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced RNN architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true\n            },\n            \"description\": \"A highly accurate deep learning model designed for binary time-series classification in the medical domain, leveraging RNN architectures.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The need for state-of-the-art performance and interpretability indicate a focus on sophisticated model architectures and explainability techniques.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the information provided in the instruction, such as the area of time-series analysis, the goal of developing a highly accurate model, and the use of a precision-weighted F1 score as the primary metric. The dataset details, source, and model type are derived directly from the instruction. However, specific model architecture details or preprocessing steps are not included since they are not detailed in the instruction. The confidence score is set to 0.95 due to the clarity of the task and the adequate mapping to the JSON schema.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "visual question answering", "instruction": "For the impending middle school exhibition, you are proposed to develop a sophisticated Visual Question Answering (VQA) system utilizing the TextVQA dataset. Mandate the fusion of advanced transformers and multi-modal reasoning, ensuring high precision, interpretability, and a step-by-step explanation of image-text alignment algorithms to demonstrate deep comprehension.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"education or research\",\n        \"description\": \"Develop a sophisticated Visual Question Answering (VQA) system for a middle school exhibition using the TextVQA dataset. Requirements include advanced transformers, multi-modal reasoning, high precision, interpretability, and a step-by-step explanation of image-text alignment algorithms.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": \"high\",\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"image-text alignment explainability\",\n                \"value\": \"detailed\",\n                \"unit\": \"step-by-step\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TextVQA\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"large\",\n                \"modalities\": [\"transformer-compatible\", \"multi-modal\"]\n            },\n            \"description\": \"A multimodal dataset designed for Visual Question Answering, focusing on complex reasoning and understanding of text and images.\",\n            \"preprocessing\": [\n                \"fine-tuning transformers on multi-modal data\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"model interpretability visualizations\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Transformers (e.g., M6 or ViLT)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-modal transformer\",\n                \"intermediate_layers\": \"interpretable\",\n                \"multimodal_fusion\": \"explicit\"\n            },\n            \"description\": \"A highly advanced transformer-based VQA model incorporating multi-modal reasoning for precise answers and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced transformers enhance representation learning from both textual and visual inputs\",\n        \"Multi-modal reasoning is crucial for context-aware and complex VQA\",\n        \"Interpretability ensures transparency in the model's decision-making process\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"AI platform or server\",\n        \"inference_engine\": [\n            \"TensorRT\"\n        ],\n        \"demonstration\": \"Interactive VQA demonstration with detailed alignment explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent, high expertise level, and the specific requirements of a VQA system for a school exhibition. The problem area, downstream task, and dataset (TextVQA) are accurately identified, along with the desired performance metrics and complexity constraints. The model is described in a way that highlights advanced transformers, multi-modal reasoning, and interpretability. The dataset source indicates the need for an external link to fetch the dataset. The confidence score is high due to the clear mapping between the instruction and the JSON structure.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "tabular regression", "instruction": "You are now assigned to a multi-faceted data science project that involves regression analysis. Utilizing the Wild Blueberry Yield Dataset, which contains an extensive array of interrelated numerical features, you must meticulously preprocess and analyze the train, validation, and test partitions separately. The goal is to develop a state-of-the-art predictive model capable of forecasting yield with exceptional accuracy. To measure your model's prowess, adhere to the stringent Mean Absolute Error (MAE) evaluation metric, and ensure that it outperforms contemporary benchmarks in the field.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Work on a multi-faceted data science project that involves regression analysis using the Wild Blueberry Yield Dataset. The dataset consists of interrelated numerical features, and the goal is to build a state-of-the-art predictive model with high accuracy (targeting MAE benchmark). The model should outperform contemporary baselines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wild Blueberry Yield Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"yield\"],\n            \"specification\": null,\n            \"description\": \"An extensive dataset with interrelated numerical features, divided into train, validation, and test partitions for the regression analysis project.\",\n            \"preprocessing\": [\n                \"Data cleaning and normalization\",\n                \"Handling missing values\",\n                \"Outlier detection and treatment\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Feature correlation analysis\",\n                \"Data distribution plots\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced machine learning algorithms (e.g., ensemble, deep learning, or highly tuned models)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"complexity\": \"\",\n                \"training speed\": \"\",\n                \"inference_time\": \"\",\n                \"optimization\": \"\"\n            },\n            \"description\": \"A state-of-the-art predictive model for tabular data regression, focusing on optimizing for MAE with the Wild Blueberry Yield Dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Understanding the interdependency among numerical features is crucial.\",\n        \"Adapting advanced techniques to handle complex relationships in the dataset is essential.\",\n        \"Comparing with contemporary models to benchmark and improve performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature engineering for better model performance\",\n            \"Hyperparameter tuning for improved MAE\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstration of model performance with improved MAE compared to benchmarks\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON accurately reflects the user's intent to build a state-of-the-art model, while considering the high level of expertise. The problem area, task, and performance metric are well-defined. Specific preprocessing steps and visualizations are included based on the nature of the dataset. However, exact model specifications, especially benchmark performance, are not provided as they may require external research or a more detailed understanding of contemporary literature.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager, you need to design a state-of-the-art text classification system for IMDB reviews, incorporating both binary sentiment (positive/negative) and fine-grained emotions (e.g., love, hate, surprise). Aim for a multi-label classification model with a minimum F1-score of 90% and provide detailed performance metrics for each category. Additionally, implement an ensemble of models and analyze their combined effect on overall accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"sentiment analysis\",\n        \"description\": \"Design a state-of-the-art text classification system for IMDB reviews with binary sentiment (positive/negative) and fine-grained emotions. The model should have a minimum F1-score of 90% for each category, and an ensemble of models is required, with an analysis of their combined effect on overall accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"IMDB Reviews\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"binary_sentiment\", \"fine_grained_emotions\"],\n            \"specification\": {\n                \"examples\": null,\n                \"label_distribution\": {\n                    \"positive\": {},\n                    \"negative\": {},\n                    \"fine_grained\": {\"love\": {}, \"hate\": {}, \"surprise\": {}}\n                }\n            },\n            \"description\": \"Dataset containing IMDB reviews for binary sentiment and fine-grained emotion analysis.\",\n            \"preprocessing\": [\n                \"tokenization\",\n                \"lemmatization\",\n                \"stop words removal\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ensemble\",\n            \"family\": \"multi-label classification\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_1\": {\n                    \"type\": \"model_name\",\n                    \"parameters\": {}\n                },\n                \"model_2\": {\n                    \"type\": \"model_name\",\n                    \"parameters\": {}\n                },\n                \"ensemble_method\": \"voting\"\n            },\n            \"description\": \"An ensemble of multi-label classification models for IMDB reviews\"\n        }\n    ],\n    \"knowledge\": [\n        \"Using ensemble methods can improve performance by leveraging multiple models.\",\n        \"Fine-grained emotions require context-sensitive approaches to achieve high F1-scores.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"POS tagging\",\n            \"Named entity recognition\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU inference\"\n        ],\n        \"demonstration\": \"A visualization dashboard showcasing performance per emotion category\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the project manager's requirements for a text classification system. The area, task, and domain are clearly stated. Performance metrics include F1-score with a minimum of 90% for each emotion. Ensemble of models is mentioned, along with a plan for pre-processing and feature engineering. The model specifications are left open for interpretation of the state-of-the-art models suitable for the task. The confidence score is high due to the clear user intent and specific requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a machine learning project manager, your team must develop an efficient text classification system using the 20 Newsgroups dataset. Select a model that balances accuracy, interpretability, and computational efficiency. Additionally, propose a parallel training strategy to minimize training time without sacrificing performance, keeping hardware constraints in mind.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval or text analytics\",\n        \"description\": \"Develop an efficient text classification system for the 20 Newsgroups dataset, focusing on accuracy, interpretability, and computational efficiency. A parallel training strategy is required to minimize training time within hardware constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": {\"unit\": \"hours\", \"default\": null}\n            },\n            {\n                \"name\": \"hardware constraints\",\n                \"value\": \"considered in parallel training strategy\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"category\"],\n            \"specification\": null,\n            \"description\": \"A text-based dataset for multi-class text classification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks or ensemble models\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"computational efficiency\": \"optimized\"\n            },\n            \"description\": \"A machine learning model designed for text classification with a focus on balance between accuracy, interpretability, and efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Parallel computing techniques can be leveraged for faster training on resource-limited hardware.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"strategies for efficient model deployment and performance demonstration under hardware constraints\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent, expertise level, and the specific requirements of the project (i.e., efficient model selection and parallel training strategy). The area and downstream task are identified based on the natural language processing aspect of the task (text classification). The performance metrics (accuracy, interpretability, computational efficiency) and hardware constraints are specified, while their values are left unspecified for the time being. The user's request for a parallel training strategy indicates a potential need for explaining how this aspect will be handled.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "question answering", "instruction": "As the project manager for an advanced AI development team, we are tasked with refining a highly specialized BERT-based question-answering system using the SQuAD 2.0 dataset. Emphasis should be on achieving cutting-edge performance with a minimum F1 score of 95% in expert mode, while maintaining response times within a challenging 1.5-second latency window for users interacting with our state-of-the-art RoBERTa model. Conduct research on novel training techniques, fine-tuning strategies, and model compression methods to optimize both accuracy and efficiency while ensuring competitive edge over current benchmarks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"Refine a highly specialized BERT-based question-answering system using the SQuAD 2.0 dataset, targeting cutting-edge performance with a minimum F1 score of 95% in expert mode and maintaining a 1.5-second latency for users interacting with the RoBERTa model.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 1.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A dataset for training and evaluating a BERT-based question-answering system, specifically SQuAD 2.0.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"task_specific_features\": \"question answering\",\n                \"model_family\": \"RoBERTa\",\n                \"target_performance\": \"expert mode\"\n            },\n            \"description\": \"A highly specialized BERT-based question-answering system with emphasis on efficiency and competitive performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Focus on novel training techniques for optimizing accuracy and efficiency.\",\n        \"Investigate fine-tuning strategies to achieve high F1 score.\",\n        \"Explore model compression methods for improved latency and resource utilization.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the main requirements of the user's instruction, including refining a specialized BERT-based system, the target dataset, performance metrics, and efficiency constraints. Novel training techniques, fine-tuning, and model compression are addressed as areas of focus, ensuring the system's advanced performance and competitive edge. The user's intent and their high expertise level are also reflected. The only missing piece might be specifying the exact versions or specific configurations of BERT and RoBERTa models, but these can be inferred from common practice.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Request the machine learning team to design a sophisticated multilingual translation system using the extensive TED Talks dataset. Emphasize on implementing M-BART architecture for translation between English and Portuguese, while considering challenges like real-time performance, low-power efficiency for portable devices, and the meticulous optimization of model size to minimize battery drain and memory footprint for optimal handheld experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual communication\",\n        \"description\": \"Ask the machine learning team to design a sophisticated multilingual translation system, particularly focusing on M-BART architecture for English to Portuguese translation. Key challenges include real-time performance, low-power efficiency for portable devices, and optimizing model size to minimize battery drain and memory usage for a seamless handheld experience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time translation speed\",\n                \"value\": null\n            },\n            {\n                \"name\": \"low-power efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model size (memory footprint)\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TED Talks Multilingual Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"English\", \"Portuguese\"],\n                \"size\": null,\n                \"structured\": false\n            },\n            \"description\": \"An extensive dataset for multilingual translation, specifically focusing on English and Portuguese TED Talks.\",\n            \"preprocessing\": [\"text normalization\", \"tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M-BART\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"M-BART\",\n                \"language_pair\": \"English-Portuguese\",\n                \"training_dataset\": \"TED Talks\"\n            },\n            \"description\": \"A state-of-the-art multilingual translation model based on M-BART architecture, optimized for English to Portuguese translation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"M-BART is suitable for its capabilities in sequence-to-sequence learning, making it effective in multilingual translation.\",\n        \"The focus on low-power efficiency and real-time performance is crucial for handheld devices.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adaptation for portable devices\"],\n        \"target_device\": \"low-power portable devices\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"real-time, lightweight inference\"],\n        \"demonstration\": \"user-friendly, low-latency translation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's request, with high expertise, a focus on building, and a request for a multilingual translation system using M-BART for English-Portuguese. The challenge of real-time performance, low-power efficiency, and model optimization is highlighted in the complexity metrics. The TED Talks dataset is specified, and relevant preprocessing steps, model features, and deployment requirements are detailed. However, specific performance targets or exact model size details are not provided as the instruction does not specify them.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "link prediction", "instruction": "For a complex citation network analysis, using the Citeseer dataset, could you design an efficient algorithm that integrates feature extraction, node embedding, and a multi-layered predictive model to forecast relationships among papers, accounting for temporal dynamics and genre variations?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Design an efficient algorithm for complex citation network analysis on the Citeseer dataset. The algorithm should integrate feature extraction, node embedding, and a multi-layered predictive model, considering both temporal dynamics and genre variations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Citeseer\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_nodes\": null,\n                \"number_of_edges\": null,\n                \"temporal_timestamps\": null,\n                \"genre_variations\": null\n            },\n            \"description\": \"A citation network dataset for analyzing complex relationships among academic papers, incorporating temporal and genre variability.\",\n            \"preprocessing\": [\"feature extraction\", \"node normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"graph visualization of citation network\",\n                \"time-evolution visualization\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Efficient Citation Network Model\",\n            \"family\": \"graph neural networks with node embedding\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"layers\": [\"feature extraction layer\", \"node embedding layer\", \"multi-layer predictive model\"],\n                \"temporal_integration\": true,\n                \"genre_adaptation\": true\n            },\n            \"description\": \"A state-of-the-art model combining feature extraction, node embeddings, and a multi-layered architecture for citation network analysis accounting for temporal and genre dynamics.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Temporal-aware models like RNNs or LSTMs could be useful in capturing temporal dynamics.\",\n        \"Genre variations may require domain-specific feature extraction or adaptations in the embedding layer.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for temporal and genre\", \"genre-specific feature selection\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"graph-based prediction algorithms\", \"GPU-accelerated execution\"],\n        \"demonstration\": \"A demonstrable prototype or prediction results for different temporal and genre slices of the dataset\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the complex nature of the task, which involves a graph machine learning project on the Citeseer dataset. The user's intent to build a model, high level of expertise, and the problem's application domain are considered. Key elements like feature extraction, node embedding, multi-layered model, temporal dynamics, and genre variations are incorporated into the problem description and the model requirements. The model's specification includes a focus on these aspects, but specific metric values are not given, as the task description didn't provide them. The confidence score reflects the overall understanding but leaves room for additional information on the model's performance.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "time-series forecasting", "instruction": "For a cutting-edge AI project, develop a DeepAR time series forecasting model on the comprehensive dataset of monthly precipitation across diverse regions, integrating complex climate patterns like ENSO indices. Emphasize state-of-the-art accuracy, adaptive continuous learning, and real-time updating capabilities, ensuring the model's resilience to climate fluctuations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate science\",\n        \"description\": \"Develop a DeepAR time series forecasting model on a comprehensive monthly precipitation dataset across diverse regions, integrating complex climate patterns like ENSO indices. The model should have state-of-the-art accuracy, adaptive continuous learning, and real-time updating capabilities, while being resilient to climate fluctuations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory_size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"real_time_updating_capability\",\n                \"value\": true\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Monthly Precipitation Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"precipitation\"],\n            \"specification\": {\n                \"data_length\": null,\n                \"time_frequency\": \"monthly\",\n                \"diverse_regions\": true\n            },\n            \"description\": \"A comprehensive dataset of monthly precipitation with integrated ENSO indices and diverse regional climate data.\",\n            \"preprocessing\": [\n                \"normalization\",\n                \"integration of ENSO indices\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"DeepAR\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_complexity\": null,\n                \"statefulness\": true,\n                \"continual_learning\": true\n            },\n            \"description\": \"A DeepAR model for time series forecasting, designed for high accuracy with adaptive continuous learning and real-time updating capabilities, specifically for monthly precipitation data integrating ENSO indices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art accuracy requires a sophisticated modeling approach, considering complex climate patterns and climatic fluctuations.\",\n        \"Adaptive continuous learning ensures the model's ability to adapt to new data for improved predictions.\",\n        \"Real-time updating capabilities are crucial to handle climate fluctuations in real-world scenarios.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"ENSO index integration\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Serving\"],\n        \"demonstration\": \"Visualizations of forecasted precipitation and model performance over time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the given instruction, capturing the high-level build intent, user's expertise, and the specific requirements of a DeepAR model with advanced features. Performance metrics are left unspecified to allow for flexibility based on the user's expectations. Some complexities like inference time and memory size are likely to be optimized based on the chosen model and cloud deployment. However, without further details, these are set as null to prompt for additional input.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for a specialized machine learning team, devise a highly sophisticated DTW-based model that not only classifies intricate sign language gestures in the Sign Language Time Series dataset but also differentiates between micro-movements and temporal variations. Ensure the model meets stringent 99% precision and recall standards, with a user-centric focus on clear, real-time communication for a diverse deaf community, integrating advanced visualization tools for interpretive feedback.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"specialized technology for the deaf community\",\n        \"description\": \"Design a highly sophisticated DTW-based model for classifying intricate sign language gestures in the Sign Language Time Series dataset, including the ability to differentiate between micro-movements and temporal variations. The model must achieve a 99% precision and recall, focusing on clear, real-time communication for a diverse deaf community, and integrate advanced visualization tools for interpretive feedback.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sign Language Time Series\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"gesture classes\", \"micro_movements\", \"temporal_variations\"],\n            \"specification\": {\n                \"length\": \"variable\",\n                \"dimensionality\": \"multivariate\",\n                \"description\": \"Intricate time series data representing sign language gestures\"\n            },\n            \"preprocessing\": [\"noise reduction\", \"temporal feature extraction\"],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [\"real-time gesture tracking\", \"interpretation visualizations\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Dynamic Time Warping (DTW)-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"customized for sign language recognition with micro-movements and temporal variations\",\n                \"training_method\": \"supervised\",\n                \"input_format\": \"time series\"\n            },\n            \"description\": \"A state-of-the-art neural network designed to classify complex sign language gestures and differentiate micro-movements using the DTW algorithm.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model needs to consider the unique challenges in sign language time series data, such as non-uniformity and variability.\",\n        \"Integration of advanced visualization tools is essential to facilitate user interpretability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"user-specific gesture recognition adaptation\"],\n        \"target_device\": \"real-time communication devices for deaf community\",\n        \"deployment_endpoint\": \"accessible via API or dedicated software\",\n        \"inference_engine\": [\"efficient, optimized for real-time processing\"],\n        \"demonstration\": \"user-friendly demonstration with interpretive explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been tailored to the provided instruction, reflecting the user's intent to build a highly sophisticated model, their high expertise level, and the specific problem description. The area and downstream task have been identified based on sign language and time-series analysis. Performance metrics (precision and recall) are set at the stringent 99% level. The dataset requirements are mentioned, including the source, and preprocessing and augmentation steps are suggested. The model's customization, target device, and integration with advanced visualization tools are also accounted for. However, exact details like the specific DTW-based architecture and its implementation are not provided as they would typically vary based on the chosen methodology.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "object detection", "instruction": "The project manager is tasked with integrating a precision-enhanced Faster R-CNN model for the 'LuxuryRetail-110K' dataset, focusing on high-end luxury product detection and enumeration on exclusive store shelves. Performance must exceed 95% in precision and yield a detailed breakdown by sub-genres, ensuring minimal discrepancies in the upscale inventory management system.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury retail\",\n        \"description\": \"Integrate a precision-enhanced Faster R-CNN model for detecting and counting high-end luxury products on exclusive store shelves, targeting the 'LuxuryRetail-110K' dataset. The model's precision must be above 95%, and a breakdown by sub-genres is required with minimal discrepancies in the inventory management system.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LuxuryRetail-110K\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product_id\", \"sub-genre\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": 1024,\n                    \"height\": 1024\n                },\n                \"annotation_format\": \"xml\",\n                \"sub-genre_count\": {\n                    \"total\": null,\n                    \"expected_distribution\": null\n                }\n            },\n            \"description\": \"A dataset focused on luxury retail, containing high-resolution images for detecting high-end products on store shelves.\",\n            \"preprocessing\": [\"augmentation for upscale resolution\"],\n            \"augmentation\": [\n                \"random cropping\",\n                \"color jitter\"\n            ],\n            \"visualization\": [\"dataset distribution by sub-genre\"],\n            \"source\": \"user-link\",\n            \"direct_access\": {\n                \"url\": \"http://example.com/luxuryretail110k\"\n            }\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Precision-Enhanced Faster R-CNN\",\n            \"family\": \"Faster R-CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_improvements\": [\"detectronv2\", \"FPN\"],\n                \"enhancement_modules\": [\"precision-boosting\"]\n            },\n            \"description\": \"An advanced Faster R-CNN model designed for precision-focused luxury product detection with feature pyramid networks and precision boosting techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Fine-tuning for the luxury retail domain will enhance detection performance.\",\n        \"Sub-genre distribution analysis may require custom loss functions or specialized training techniques.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"transfer learning from pre-trained models\"],\n        \"target_device\": \"cloud-based GPU server\",\n        \"deployment_endpoint\": \"custom inventory management API\",\n        \"inference_engine\": [\"TensorRT\"],\n        \"demonstration\": \"Dashboard showing real-time product detection and sub-genre breakdown.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided user instruction has been accurately parsed, capturing the intent to build a model, the expert knowledge level, and the specific problem requirements. The Faster R-CNN model is specified, with a focus on precision, and the dataset details, such as size, sub-genre distribution, and augmentation techniques, are included. However, the performance of the model on sub-genres is uncertain, and might require further clarification or analysis. The confidence score is high, considering the clear instruction and mapping to the JSON schema, but might decrease if the sub-genre analysis details are not fully covered.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the enhanced financial news analytics system, mandate the team to develop a real-time Reuters-21578 news categorization model with fastText. Achieve at least 87% accuracy, focusing on precision, particularly in crucial market sectors, and incorporate a novel feature to filter out breaking news false alarms within a strict latency of 30 seconds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Develop a real-time Reuters-21578 news categorization model with fastText, targeting at least 87% accuracy, prioritizing precision in crucial market sectors. Include a feature to filter out breaking news false alarms within a latency of 30 seconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.87\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 30,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A news dataset for real-time text classification with a focus on financial news.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"fastText\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"real-time news categorization\",\n                \"specifics\": \"fastText with accuracy as the primary objective\"\n            },\n            \"description\": \"A fastText model designed for real-time Reuters-21578 news categorization, prioritizing precision in market-relevant sectors.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model should focus on precision for crucial market sectors to provide actionable insights.\",\n        \"Breaking news filter is required to handle latency constraints\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time processing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"CPU-optimized for speed\"],\n        \"demonstration\": {\n            \"method\": \"live news stream with filtered classification results\",\n            \"time_constraint\": \"30-second latency\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a model, their high level of expertise, and the specific requirements of the project - real-time news categorization with a fastText model, targeted performance, and filtering false alarms. However, for the precision value in the performance metrics, as it's particularly important, a more detailed discussion may be needed with the user for the desired value. The latency has been specified in terms of response time. The service includes an inference engine focused on speed and a plan for demonstrating the model's performance.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "For the extensive Enron Email dataset with intricate hierarchical relationships and encrypted communication channels, design a highly efficient and fault-tolerant RGCN model. Aim to classify not only employees but also their roles and sub-networks with multiple levels of engagement. Optimize for distributed processing on a large-scale cluster, ensuring low latency even while handling gigabytes of data and real-time updates.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"cybersecurity\",\n        \"description\": \"Design a highly efficient and fault-tolerant RGCN model for the Enron Email dataset with intricate hierarchical relationships and encrypted communication channels. The goal is to classify employees and their roles, as well as identify sub-networks with multi-level engagement. Optimize for distributed processing on a large-scale cluster with low latency for handling gigabytes of data and real-time updates.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"training\": null,\n                    \"prediction\": null\n                },\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email Dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"employee roles\", \"sub-networks\"],\n            \"specification\": {\n                \"size\": null,\n                \"average degree\": null,\n                \"hierarchical_depth\": null\n            },\n            \"description\": \"A large-scale dataset with hierarchical relationships and encrypted communication channels, intended for RGCN model development.\",\n            \"preprocessing\": [\"data cleaning\", \"node feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"network graphs\", \"performance monitoring\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RGCN Model\",\n            \"family\": \"Relational Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"layers\": null,\n                    \"dropout_rate\": null,\n                    \"layer_types\": []\n                },\n                \"optimizer\": null,\n                \"distributed_processing\": true\n            },\n            \"description\": \"A fault-tolerant RGCN model designed to handle the complexities of the Enron Email dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The encrypted communication channels require advanced node classification methods.\",\n        \"Hierarchical relationships demand a model with multi-level understanding and scalability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"node embedding\",\n            \"edge weight calculation\"\n        ],\n        \"target_device\": \"large-scale cluster\",\n        \"deployment_endpoint\": \"real-time update system\",\n        \"inference_engine\": [\"Apache Spark\", \"Distributed computing framework\"],\n        \"demonstration\": \"interactive dashboard showcasing real-time performance and results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's request, including the complex graph structure, encrypted communication, and the need for distributed processing. Performance metrics are set to null as they are typically optimized during model training and deployment, and specific numerical values are not provided. The dataset preprocessing, model architecture, and service-related details are derived from the instruction. The missing values for specific performance metrics can be filled in after model training is completed.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager is tasked with spearheading a cutting-edge machine translation project using the diverse WMT19 multilingual news commentary dataset, focusing on Russian to English translation. The goal is not just to develop a top-tier seq2seq model, but to outperform current benchmarks with a BLEU score of at least 45, ensuring seamless text transfer and adapting to domain-specific nuances. Comprehensive evaluation must consider aspects like fluency, adequacy, and cultural sensitivity, alongside rigorous cross-lingual comparisons with alternative model architectures.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics\",\n        \"description\": \"The project aims to develop a cutting-edge machine translation system, focusing on Russian to English translation using the WMT19 multilingual news commentary dataset. The objective is to outperform current benchmarks with a BLEU score of at least 45, ensuring seamless text transfer and cultural sensitivity. Evaluation will consider fluency, adequacy, and cultural nuances, and will involve cross-lingual comparisons with alternative model architectures.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 45.0\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 Multilingual News Commentary\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"diversity\": \"diverse\",\n                \"language_pair\": \"Russian to English\"\n            },\n            \"description\": \"A large-scale multilingual dataset for machine translation, specifically focusing on Russian to English translations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-based seq2seq model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"outperforming_benchmarks\": true,\n                \"domain_adaptation\": true,\n                \"target_bleu\": 45\n            },\n            \"description\": \"A state-of-the-art deep learning model for sequence-to-sequence translation, optimized for Russian to English translation and targeting a high BLEU score.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The WMT19 dataset's diversity and large-scale nature help in training a robust translation system.\",\n        \"Domain-specific nuances and cultural sensitivity are key factors in this project.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Comprehensive evaluation via fluent, adequate, and culturally sensitive translations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction, with a focus on a high-experience user, a machine translation project, and the WMT19 dataset. The project goal, including the performance metric (BLEU score), is well-defined, and the model family and specifications are based on current benchmark models. However, specific model details and preprocessing steps are left to the discretion of the machine learning engineer, and may require further investigation.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "community detection", "instruction": "As a project manager for a machine learning team tasked with analyzing Protein-Protein Interaction (PPI) networks, your objective is to design a scalable, explainable, and bioinformatics-friendly framework that utilizes unsupervised deep learning techniques to identify novel biological communities with interpretability in mind. The project should be structured in a way that allows a junior data scientist with limited experience in the field to contribute effectively, while also demonstrating the potential of deep learning in biological network analysis. Please outline a step-by-step plan, focusing on accessible tools, data preprocessing, model selection, and evaluation metrics, while ensuring interpretability through methods like attention mechanisms or explainable AI techniques.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Design a scalable, explainable framework using unsupervised deep learning for Protein-Protein Interaction (PPI) network analysis. The project should be beginner-friendly for junior data scientists and showcase deep learning's potential in biological network analysis with interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUPRC (Area Under the Precision-Recall Curve)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"NMI (Normalized Mutual Information)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Modularity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory Usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PPI Network Dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"nodes\": {\n                    \"total\": null,\n                    \"node_types\": [\n                        \"Proteins\"\n                    ]\n                },\n                \"edges\": {\n                    \"total\": null,\n                    \"types\": [\n                        \"Interaction\"\n                    ]\n                }\n            },\n            \"description\": \"A Protein-Protein Interaction network dataset for unsupervised community detection.\",\n            \"preprocessing\": [\n                \"Node feature extraction (e.g., protein sequence or functional annotations)\",\n                \"Edge feature extraction (e.g., interaction type)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Community structure visualizations\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Scalable Unsupervised Deep Learning Model\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Unsupervised, possibly with attention mechanisms\",\n                \"explainability_features\": \"Attention mechanisms or explainable AI techniques\"\n            },\n            \"description\": \"An advanced, scalable GNN tailored for unsupervised community detection in PPI networks, enabling interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Novel bioinformatics-friendly techniques for network analysis with deep learning\",\n        \"Attention mechanisms and explainable AI for interpretable biological communities\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature extraction and normalization\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Graph-specific GPU acceleration\"],\n        \"demonstration\": \"Tutorials or worked examples for junior data scientists\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction, with the user's intention to build a machine learning framework for a biologically oriented task. The project's requirements for scalability, explainability, and accessibility to junior data scientists are reflected in the problem area, structure, and model specification. Performance metrics like F1 Score and Modularity are expected for community detection, though their values are not specified. Complexity metrics are mentioned, but not the specific values due to the lack of detail in the instruction.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "image segmentation", "instruction": "The specific task for the machine learning team is to develop a high-resolution attention-based DenseNet model using the PlantVillage Leaf Disease dataset. Emphasize on distinguishable segmentation for multiple distinct diseases, ensuring model efficiency and providing interactive visualizations for non-technical end-users to comprehend complex disease patterns and guide precise disease management in horticulture.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Develop a high-resolution attention-based DenseNet model for plant leaf disease detection and segmentation in the PlantVillage Leaf Disease dataset. The model should be capable of distinguishing between multiple distinct diseases and prioritize efficiency. Provide interactive visualizations for non-technical users to understand complex patterns and guide disease management in horticulture.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"dice similarity coefficient\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PlantVillage Leaf Disease Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"leaf disease classification, segmentation\"],\n            \"specification\": {\n                \"image_shape\": {\n                    \"height\": null,\n                    \"width\": null,\n                    \"depth\": 3\n                },\n                \"num_classes\": null\n            },\n            \"description\": \"A high-resolution dataset for leaf disease detection and segmentation, emphasizing multiple distinct diseases in plant leaves.\",\n            \"preprocessing\": [\n                \"Resizing to a consistent resolution\",\n                \"Data augmentation for robustness\"\n            ],\n            \"augmentation\": [\n                \"Random rotation, translation, and flipping\"\n            ],\n            \"visualization\": [\n                \"Segmentation heatmaps, class-wise overlay visualizations\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention-based DenseNet\",\n            \"family\": \"DenseNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"DenseNet with attention modules for improved disease recognition\",\n                \"depth\": null,\n                \"growth_rate\": null,\n                \"block_config\": null\n            },\n            \"description\": \"A deep learning model designed for high-resolution leaf disease segmentation, leveraging DenseNet and attention mechanisms.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Attention mechanisms enhance disease distinguishability\",\n        \"Efficient architecture is crucial for horticultural application\",\n        \"Interactive visualizations require user-friendly design and explanations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Transfer learning from pre-trained models\",\n            \"Feature extraction for interpretability\"\n        ],\n        \"target_device\": \"Edge device for real-time monitoring in field conditions\",\n        \"deployment_endpoint\": \"A user-friendly web portal for visualization and disease management guidance\",\n        \"inference_engine\": [\"Lightweight libraries for efficient deployment\"],\n        \"demonstration\": \"Create a demo showcasing segmentation results and visual explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction, mapping the user's intent to build, their expertise level, the specific problem (computer vision task of image segmentation), and the relevant details like the PlantVillage dataset and DenseNet model. Performance metrics and complexity constraints are included based on the need for accuracy and efficiency. Preprocessing, augmentation, and visualization steps are incorporated from the task description. The confidence score is set to 0.95 due to the clarity of the provided information, but it remains open for potential missing details about the model's detailed architecture or exact performance metrics.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a forward-thinking project manager for a cross-lingual technology venture, you're tasked with developing an efficient, mobile-friendly translation app that focuses on the under-explored Quechua-Spanish pair. Utilize the limited JW300 corpus and design an advanced LSTM architecture integrated with cutting-edge attention mechanisms. Strive for superior adaptability to dialects and cultural nuances, while aiming for a state-of-the-art BLEU score of 35 or higher, reflecting precision in translation quality in resource-constrained environments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual technology\",\n        \"description\": \"As a forward-thinking project manager, develop an efficient, mobile-friendly translation app focusing on the Quechua-Spanish language pair using the JW300 corpus. Design an advanced LSTM architecture with attention mechanisms, targeting superior adaptability to dialects and cultural nuances. The goal is to achieve a state-of-the-art BLEU score of 35 or higher in resource-constrained environments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 35.0\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"JW300 corpus (Quechua-Spanish)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"limited\",\n                \"language_pair\": \"Quechua-Spanish\"\n            },\n            \"description\": \"A resource-limited dataset for translation, specifically focusing on the Quechua-Spanish language pair.\",\n            \"preprocessing\": [\"language tokenization\", \"cultural adaptation\"],\n            \"augmentation\": [\"backtranslation\"],\n            \"visualization\": [\"sentence similarity analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced LSTM with Attention Mechanisms\",\n            \"family\": \"neural networks\",\n            \"type\": \"sequence-to-sequence\",\n            \"specification\": {\n                \"architecture\": \"LSTM\",\n                \"attention_types\": [\"attention mechanism(s)\"]\n            },\n            \"description\": \"An advanced neural network model with LSTM cells and attention mechanisms for the Quechua-Spanish translation task.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Targeting cultural nuances and dialect adaptability requires understanding of local linguistic variations.\",\n        \"LSTM and attention mechanisms have shown promise in NMT, particularly in resource-constrained scenarios.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain adaptation techniques\", \"low-resource learning\"],\n        \"target_device\": \"mobile-friendly\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient inference algorithms for resource-constrained devices\"],\n        \"demonstration\": \"Mobile app demonstrations showcasing translation quality on Quechua sentences\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the project manager's intentions and requirements for the translation app. The problem area, task, and performance metric (BLEU score) are derived from the instruction. The dataset, model, and additional requirements such as efficiency, cultural adaptability, and mobile-friendliness are well-reflected. However, since the instruction does not specify the exact preprocessing steps for the JW300 corpus, a common generalization like 'language tokenization' and 'cultural adaptation' is used.\",\n        \"score\": 0.92\n    }\n}"}
{"task": "multimodal emotion recognition", "instruction": "The project manager needs a detailed plan for integrating MELD with a novel fusion technique, combining transformer and CNN architectures. Emphasis should be on cross-modal learning, achieving precision above 0.85 in each emotion category, and optimizing for real-time performance on diverse datasets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal emotion recognition\",\n        \"application_domain\": \"computer vision and natural language processing\",\n        \"description\": \"Integrate MELD dataset with a novel fusion technique combining transformer and CNN architectures for cross-modal learning. Aim for precision above 0.85 in each emotion category and optimize for real-time performance across diverse datasets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"real-time\",\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MELD\",\n            \"modality\": [\"multimodal\"],\n            \"target_variables\": [\"emotion categories\"],\n            \"specification\": null,\n            \"description\": \"A multimodal emotion recognition dataset suitable for transformer and CNN fusion.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-CNN Fusion\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"transformer+CNN hybrid\",\n                \"target_latency\": \"real-time\"\n            },\n            \"description\": \"A state-of-the-art model combining transformer and CNN architectures for cross-modal emotion recognition in MELD with a focus on high precision and real-time performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"diverse (mobile, cloud, etc.)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"a detailed deployment plan with guidelines for real-time performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the core elements from the instruction. The user's intent to build a model is clear, and their high level of expertise is noted. The problem area is determined to be multimodal learning with emphasis on emotion recognition. The performance metrics, including precision and real-time inference, have been included. MELD dataset is identified as the focus without providing a specific source since the requirement for a novel fusion is mentioned. The model description is generic enough to accommodate the Transformer-CNN fusion, while leaving room for detailed design specifications. No additional information is missing from the provided instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a project manager for the Bilibili Danmu enhancement initiative, mandate the machine learning team to design a highly-advanced, context-aware sequential recommendation system. Emphasize on integrating a state-of-the-art transformer model, derived from the BERT family, specifically BERT4Rec, to analyze intricate user comment patterns. Mandate real-time adaptability for comment sentiment analysis and ensure the system excels in predicting diverse user preferences while maintaining seamless performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"social media\",\n        \"description\": \"As a project manager for Bilibili's Danmu enhancement, mandate the machine learning team to design a context-aware, real-time sequential recommendation system. The system should utilize a state-of-the-art transformer model, BERT4Rec, for intricate user comment pattern analysis, and include sentiment analysis with the capability to adapt to real-time data. The main goal is to predict diverse user preferences with optimal performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean average precision at k (MAP@k)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"NDCG@k\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"real-time\",\n                \"unit\": \"seconds per inference\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\", \"time series\"],\n            \"target_variables\": [\"user comment sequences\", \"sentiment\"],\n            \"specification\": {\n                \"comment_length\": null,\n                \"sentiment_data_interval\": null\n            },\n            \"description\": \"A combination of comment sequences and real-time sentiment data for user interaction on Bilibili Danmu.\",\n            \"preprocessing\": [\n                \"tokenization\",\n                \"sentence segmentation\",\n                \"context window embedding\"\n            ],\n            \"augmentation\": [\n                \"synthetic comment generation\",\n                \"time-aware sampling\"\n            ],\n            \"visualization\": [\n                \"sequence pattern analysis\",\n                \"sentiment distribution\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT4Rec\",\n            \"family\": \"Transformer family, specifically BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_depth\": null,\n                \"transformer_layers\": null,\n                \"BERT layers\": null\n            },\n            \"description\": \"A state-of-the-art context-aware sequential recommendation model based on BERT4Rec for comment analysis and real-time sentiment integration.\"\n        }\n    ],\n    \"knowledge\": [\n        \"BERT4Rec's transformer architecture allows for advanced analysis of comment sequences.\",\n        \"Context-awareness is critical for understanding diverse user preferences in social media recommendation systems.\",\n        \"Real-time sentiment analysis is crucial for personalized user experience.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"sequence modeling\", \"contextual embeddings\"],\n        \"target_device\": \"cloud environment\",\n        \"deployment_endpoint\": \"Bilibili platform\",\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"Interactive recommendations based on user comment patterns and sentiment.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is derived from the provided instructions, focusing on a high-end user's request to build a context-aware sequential recommendation system. The area is specified as recommendation systems, downstream task as sequential recommendation, and application domain as social media. The user's intent, model sophistication, and performance metrics are adequately captured. The dataset includes text and time-series components, with emphasis on preprocessing and augmentation. The confidence score is high due to the clear mapping to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "As the project manager, I need a tailored implementation of ChebNet++ for large-scale node classification on Amazon's commodity co-purchase graph. Emphasize fine-grained, community-specific feature extraction, and demand a minimum of 95% F1-score. Additionally, the algorithm must incorporate real-time ETL capabilities and demonstrate resilience to graph structure fluctuations with minimal downtime.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Tailor a ChebNet++ implementation for large-scale node classification on Amazon's commodity co-purchase graph, focusing on fine-grained, community-specific feature extraction. Target a minimum F1-score of 95% and ensure real-time ETL capabilities while maintaining resilience to graph structure fluctuations with minimal downtime.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time ETL capabilities\",\n                \"value\": null,\n                \"unit\": \"boolean\"\n            },\n            {\n                \"name\": \"resilience to graph structure fluctuations\",\n                \"value\": null,\n                \"unit\": \"downtime reduction\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Amazon commodity co-purchase graph\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"scale\": \"large\",\n                \"graph_structure\": \"commodity co-purchase\",\n                \"dynamic\": {\n                    \"is_real_time\": true,\n                    \"structure_changes\": \"fluctuations\"\n                }\n            },\n            \"description\": \"Amazon's large-scale co-purchase graph for node classification task.\",\n            \"preprocessing\": [\"fine-grained feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ChebNet++\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_extraction_technique\": \"fine-grained and community-specific\",\n                \"adaptation\": \"real-time ETL\"\n            },\n            \"description\": \"Customized ChebNet++ implementation for large-scale node classification on Amazon's commodity co-purchase graph\"\n        }\n    ],\n    \"knowledge\": [\n        \"Fine-grained feature extraction is crucial for distinguishing within large, interconnected communities.\",\n        \"Real-time ETL is essential for handling the dynamism of Amazon's co-purchase graph.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"fine-tuning for resilience\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"require minimal downtime during graph structure changes\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the requirements stated, including the ChebNet++ model customization, large-scale graph, node classification, and performance expectations. The fine-grained feature extraction, real-time ETL, and F1-score target are captured. However, the resilience in the model specification is not solely attributed to the ETL, as it also mentions fine-tuning for resilience, which may involve adapting to graph structure changes. The user's lack of a specific expertise level was assumed as high based on the project manager role.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a specialized RNN-based neural machine translation model, leveraging the domain-specific OPUS-Xtra dataset for English to Russian conversational slang and idiomatic expressions. Target a BLEU score of 35 while maintaining a real-time performance of less than 250 milliseconds per sentence, ensuring seamless translations in informal chat scenarios without compromising user engagement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"conversational AI\",\n        \"description\": \"Develop a specialized RNN-based neural machine translation model for English to Russian translation, focusing on conversational slang and idiomatic expressions. Target BLEU score should be 35 and ensure real-time performance of less than 250 milliseconds per sentence.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 35\n            },\n            {\n                \"name\": \"Inference time per sentence\",\n                \"value\": 0.25,\n                \"unit\": \"seconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Real-time performance\",\n                \"value\": 250,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS-Xtra (English to Russian)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"conversational slang and idiomatic expressions\",\n                \"language_pair\": \"English to Russian\"\n            },\n            \"description\": \"A domain-specific dataset for English to Russian conversational translation, focusing on slang and idiomatic expressions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"RNN-based Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"Recurrent Neural Network (specifically designed for NMT)\",\n                \"target_language\": \"Russian\",\n                \"domain_focused\": \"conversational slang and idiomatic expressions\"\n            },\n            \"description\": \"A specialized RNN-based NMT model tailored for English to Russian translation of informal chat scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Leveraging domain-specific OPUS-Xtra dataset improves translation quality for slang and idiomatic expressions.\",\n        \"Important to optimize real-time performance to maintain user engagement.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"device with real-time translation capabilities\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Smooth and fast translation of informal chat content\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intention to build a model, high level of expertise, and specific requirements for NMT model, dataset, and performance metrics. Domain-specific OPUS-Xtra is incorporated, target BLEU score and real-time performance specified. However, the exact model architecture and specific preprocessing steps are not provided, as they may vary based on the chosen RNN architecture and additional preprocessing for domain adaptation. The confidence score reflects the clarity and specificity of the given instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "link prediction", "instruction": "As a project manager for Tencent Weibo, design an advanced AGNN-based graph learning model with temporal attention and node attribute enhancement. Mandate the model to not only predict social links but also differentiate subtle relationship dynamics and incorporate real-time user behavior data for enhanced accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"social network analysis\",\n        \"description\": \"Design an advanced AGNN-based graph learning model for Tencent Weibo, with temporal attention and node attribute enhancement. The model should predict social links, differentiate subtle relationship dynamics, and incorporate real-time user behavior data for enhanced accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tencent Weibo Graph\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"social links\"],\n            \"specification\": {\n                \"node_attribute_types\": [\"textual\", \"time-series\"],\n                \"edge_types\": [\"social\"],\n                \"time_steps\": null\n            },\n            \"description\": \"A real-world social network dataset for Tencent Weibo, containing node attributes, temporal behavior data, and social links.\",\n            \"preprocessing\": [\"node attribute enhancement\", \"temporal feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"link dynamics visualization\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced AGNN\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"AGNN with temporal attention mechanism and node attribute enhancement\",\n                \"input_features\": [\"node attributes\", \"time-series behavior\"],\n                \"output_features\": [\"predicted links\", \"relationship dynamics\"],\n                \"hyperparameters\": {\n                    \"attention mechanism\": null,\n                    \"node attribute fusion\": null\n                }\n            },\n            \"description\": \"A Graph Neural Network model specifically designed for Tencent Weibo, incorporating AGNN, temporal attention, and node attribute enhancement.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating real-time user behavior is crucial for higher accuracy.\",\n        \"Temporal attention enhances model's ability to capture relationship dynamics.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"attribute fusion\",\n            \"time-series feature extraction\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"Tencent Weibo API\",\n        \"inference_engine\": [\n            \"TensorFlow serving\",\n            \"Apache Spark\"\n        ],\n        \"demonstration\": \"live user behavior-based link prediction and relationship analytics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's request to build a high-level machine learning model. The details on graph learning, advanced AGNN, temporal attention, and node attribute enhancement align with the instruction. Performance metrics for prediction and complexity have been included. However, without more specific information on the model architecture and the exact performance targets, the values for performance and complexity metrics are set to null. The model will likely require collaboration and fine-tuning, given the challenging task of differentiating subtle relationship dynamics.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project demands the DOTA dataset's high-resolution aerial imagery segmentation with YOLOv4 enhanced for multi-class object and infrastructure detection. Guarantee at least 95% mIoU, maintain real-time performance (under 100ms) on diverse scenes, and optimize for energy efficiency on edge devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"Use the DOTA dataset for high-resolution aerial imagery segmentation with enhancements to YOLOv4 for multi-class object and infrastructure detection. The model must have at least 95% mean intersection over union (mIoU), maintain real-time performance under 100ms, and optimize for energy efficiency on edge devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mIoU\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"energy efficiency on edge devices\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DOTA\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"data_format\": \"aerial imagery\"\n            },\n            \"description\": \"A dataset focused on high-resolution aerial imagery for multi-class object and infrastructure detection tasks.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"YOLOv4\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"enhanced for multi-class object and infrastructure detection\",\n                \"real_time_performance\": true,\n                \"target_inference_speed\": 100\n            },\n            \"description\": \"A YOLOv4 variant optimized for high mIoU, multi-class objects, and infrastructure detection on high-resolution DOTA images with real-time performance constraint.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Energy efficiency in edge computing and model architecture optimizations for this scenario are crucial to meet the project requirements.\",\n        \"Balancing accuracy and inference speed is essential for achieving real-time performance on diverse scenes.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for edge deployment\"],\n        \"demonstration\": \"showcase diverse scene performance and energy consumption\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the main aspects of the user's instruction, including the dataset, model, and performance metrics. The expert level of the user and the specific problem area (computer vision) are clear. The mIoU target, real-time performance, and edge device compatibility are explicitly included. The requirement for energy efficiency is left as a null value since a specific target is not provided, but the focus on the optimization is noted. The confidence score is high as the response covers the key elements of the project requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a visionary project director for a multicultural tech startup, your objective is to design a next-generation, responsive translation platform specifically tailored for Quechua-Spanish dialects. Utilize the restricted JW300 dataset creatively, and mandate the implementation of a sophisticated bidirectional LSTM model that incorporates both multi-head self-attention and encoder-decoder attention mechanisms. Emphasize the app's capacity to adapt to intricate regional variations and cultural subtleties through extensive fine-tuning, while targeting a stringent BLEU score of 38 to guarantee peak translation accuracy in low-resource settings. Additionally, outline a detailed strategy for continuous evaluation and adaptation based on user feedback, ensuring the app remains agile in the face of evolving linguistic complexities.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual technology\",\n        \"description\": \"Design a responsive translation platform for Quechua-Spanish dialects. Utilize the JW300 dataset creatively and implement a sophisticated bidirectional LSTM model with multi-head self-attention and encoder-decoder attention mechanisms. Ensure adaptability to regional variations and cultural subtleties through fine-tuning, targeting a BLEU score of 38 for low-resource settings. Continuously evaluate and adapt based on user feedback.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 38\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"JW300\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_pair\": \"Quechua-Spanish\",\n                \"restricted_access\": true\n            },\n            \"description\": \"A dataset for machine translation between Quechua and Spanish dialects, creatively leveraged for the project.\",\n            \"preprocessing\": [\n                \"Language-specific cleaning\",\n                \"Tokenization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Bidirectional LSTM with Multi-Head Self-Attention and Encoder-Decoder Attention\",\n            \"family\": \"LSTM-based Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention_mechanisms\": [\"multi-head self-attention\", \"encoder-decoder attention\"],\n                \"tailored_for_low_resource\": true\n            },\n            \"description\": \"A model for machine translation specifically designed to handle Quechua-Spanish dialects and adapt to regional variations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"App focus on Quechua-Spanish dialects and cultural nuances\",\n        \"Fine-tuning to address low-resource challenges\",\n        \"Agile adaptation based on user feedback\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Fine-tuning based on user-generated data\"],\n        \"target_device\": \"Mobile and Cloud\",\n        \"deployment_endpoint\": \"Continuous monitoring and adaptation\",\n        \"inference_engine\": [\"Multi-threaded for efficient translations\"],\n        \"demonstration\": \"User-friendly interface showcasing translation accuracy and personalized settings\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's objective, area of expertise, the requirements for the model, the dataset, and the desired evaluation strategy. The instruction has been parsed in a structured format that corresponds to the schema. The model specification and performance metrics are in line with the requirements for a bidirectional LSTM model targeting low-resource settings. However, the specific preprocessing steps for the JW300 dataset are assumptions, as they are not provided in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "For the complex Enron Email dataset, develop a RGCN model with advanced anomaly detection capabilities. The model must handle multi-level encrypted networks, classify employees by job roles and subgroups with high accuracy, and maintain real-time performance on a distributed cluster. Ensure fault-resilience, low latency, and scalability up to terabyte levels for continuous, uninterrupted operation amidst rapid data growth.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Develop a RGCN model for the Enron Email dataset with advanced anomaly detection capabilities. The model should handle multi-level encrypted networks, classify employees by job roles and subgroups with high accuracy, and maintain real-time performance in a distributed cluster with fault-resilience, low latency, and scalability up to terabyte levels for continuous operation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"terabytes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email Dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\n                \"job role\",\n                \"subgroups\"\n            ],\n            \"specification\": {\n                \"encrypted\": true,\n                \"network_levels\": \"multi-level\",\n                \"data_growth_rate\": \"rapid\"\n            },\n            \"description\": \"A complex graph dataset with multi-level encrypted networks for the Enron Email corpus.\",\n            \"preprocessing\": [\n                \"network anonymization\",\n                \"data encryption handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RGCN with Anomaly Detection\",\n            \"family\": \"Recurrent Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"encryption_handling\": \"advanced\",\n                \"classification_accuracy\": \"high\",\n                \"distributed_computing\": \"true\"\n            },\n            \"description\": \"A RGCN designed for multi-level encrypted networks, targeting job role and subgroup classification with anomaly detection capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The Enron Email dataset demands advanced handling of multi-level encrypted networks.\",\n        \"Classifier must achieve high accuracy in a distributed, fault-resilient setup.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"node feature engineering\",\n            \"graph feature extraction\"\n        ],\n        \"target_device\": \"distributed cluster\",\n        \"deployment_endpoint\": \"custom cluster deployment\",\n        \"inference_engine\": [\n            \"real-time inference\"\n        ],\n        \"demonstration\": \"scalability demonstration with terabyte data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the user's intent to build a machine learning model, considering the complexity of the Enron Email dataset and the requirement for advanced functionalities. The problem area (graph machine learning), downstream task (node classification), and performance metrics are derived from the instruction. However, specific metric values, such as accuracy, are left unspecified as they will depend on actual model performance. The source for the dataset is specified as 'user-link', indicating that the user may provide a URL to download the dataset. Some preprocessing steps are mentioned based on the given complexities. The confidence score is set to a high level given the clear understanding of the task from the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project lead for the cutting-edge financial sentiment analysis platform, direct the ML team to design and implement a state-of-the-art Reuters-21578 news classification system using advanced deep learning algorithms (BERT and convolutional neural networks). Strive for an exceptional accuracy of 90%, ensuring sector-specific (aviation, tech, and energy) precision, and include an innovative noise reduction mechanism that discerns authentic breaking news from rumors with a stringent downtime requirement of 5 milliseconds. Additionally, devise a comprehensive evaluation strategy to benchmark performance against industry competitors.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"financial services\",\n        \"description\": \"Design and implement a state-of-the-art Reuters-21578 news classification system using BERT and convolutional neural networks, focusing on aviation, tech, and energy sectors. Target accuracy is 90% and includes a noise reduction mechanism to differentiate between authentic breaking news and rumors with a 5ms downtime requirement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"sector-specific precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 5,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"news classification\"],\n            \"specification\": {\n                \"data_split\": [\"train\", \"validation\", \"test\"],\n                \"sector_labels\": [\"aviation\", \"tech\", \"energy\"]\n            },\n            \"description\": \"A news dataset for sentiment analysis, with a focus on sector-specific classification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"BERT and Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": [\"BERT\", \"CNN\"],\n                \"noise_reduction_techniques\": [\"innovative mechanism for rumor detection\"]\n            },\n            \"description\": \"A state-of-the-art text classification model utilizing BERT and CNNs for financial news analysis with sector-specific and noise reduction capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The dataset should be pre-processed to cater to sector-specific requirements and BERT's architecture for capturing context.\",\n        \"The noise reduction mechanism is a key differentiator, but specifics may need further details from the team.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Develop a system demonstration showcasing high accuracy, sector-specific precision, and the noise reduction mechanism.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent (build) and high expertise level. It reflects the NLP problem, task, and domain, along with the requested performance and complexity metrics. The dataset, model architecture, and evaluation strategy are outlined based on the provided information. However, sector-specific precision and the exact noise reduction mechanism need further clarification from the ML team.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "As the project director for an avant-garde AI research team, we're challenged with enhancing a highly specialized, BERT-Enhanced Q&A system using SQuAD 2.0. Our mandate is to push the boundaries of performance with a stringent F1 score of 97% in the domain of expert-level question analysis, maintaining an impressively low latency of 1.4 seconds for seamless interaction with our innovative Transformer-based RoBERTa v3 model. Delve into groundbreaking techniques, precision-focused fine-tuning, and innovative model pruning methods, while ensuring peak efficiency, scalability, and a significant leap beyond current leaderboard standards in the field.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"research\",\n        \"description\": \"Enhance a BERT-Enhanced Q&A system using SQuAD 2.0 with a focus on expert-level question analysis. Target F1 score of 97% and maintain a latency of 1.4 seconds for seamless interaction with a RoBERTa v3 Transformer-based model. Include groundbreaking techniques, precision-focused fine-tuning, and innovative model pruning for improved performance, scalability, and leaderboard advancement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 1.4,\n                \"unit\": \"seconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"efficiency\",\n                \"value\": null,\n                \"unit\": \"relative\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"relative\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0\",\n            \"modality\": [\"text\"],\n            \"target_variables\": null,\n            \"specification\": {\n                \"format\": \"structured questions and answers\",\n                \"domain\": \"expert-level\"\n            },\n            \"description\": \"A dataset for enhancing the Q&A system with expert-level questions and answers\",\n            \"preprocessing\": [\"BERT tokenization\", \"SQuAD 2.0-specific preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-Enhanced Q&A System (RoBERTa v3)\",\n            \"family\": \"Transformer-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"RoBERTa v3\",\n                \"latency\": {\n                    \"inference_time\": 1.4\n                },\n                \"performance\": {\n                    \"F1_score_target\": 0.97\n                }\n            },\n            \"description\": \"A cutting-edge Q&A system based on BERT with precision-focused fine-tuning and pruning for high performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Include pioneering NLP techniques for performance enhancement\",\n        \"Fine-tune with precision-oriented objectives to push F1 score\",\n        \"Investigate model pruning for efficiency and scalability\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"precision-focused fine-tuning\", \"model pruning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Transformer-based\", \"RoBERTa v3 optimized\"],\n        \"demonstration\": \"Include interactive demo showcasing latency and expert-level performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON accurately reflects the information given in the instruction, with the user's intent to build a model, high expertise, focus on NLP question answering, target F1 score and latency, along with specifics on the BERT and SQuAD 2.0 datasets, model enhancements, and service requirements. However, some metrics like efficiency and scalability are left unspecified as they are open-ended and require further clarification.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a specialized project manager for a boutique data analytics firm, you've been tasked with overseeing a high-impact project for a premium blueberry farming consortium. Using the proprietary \"Blue Haven Ridge\" dataset, containing minute sensor data on microclimate, soil nutrients, and genetic varietals, analyze and preprocess the train, validation, and test subsets independently. Your mission is to create a sophisticated machine learning algorithm, specifically a hybrid ensemble regression model, that optimally integrates deep learning and traditional statistical techniques. Strive for top-tier performance with a focus on Root Mean Squared Logarithmic Error (RMSLE) to distinguish your work from competing models, and secure a coveted partnership with the cooperative by achieving a 5% improvement over current industry benchmark.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Analyze and preprocess the proprietary Blue Haven Ridge dataset for a high-impact project on a blueberry farming consortium. Create a hybrid ensemble regression model that integrates deep learning and statistical techniques to optimize for RMSLE. Aim to achieve a 5% improvement over the current industry benchmark.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Root Mean Squared Logarithmic Error (RMSLE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Blue Haven Ridge\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"output variable representing blueberry yield\"],\n            \"specification\": {\n                \"sensor data structure\": \"minute-level data on microclimate, soil nutrients, and genetic varietals\",\n                \"subset division\": {\n                    \"train\": \"\",\n                    \"validation\": \"\",\n                    \"test\": \"\"\n                }\n            },\n            \"description\": \"A proprietary dataset containing detailed sensor information on the blueberry farming environment.\",\n            \"preprocessing\": [\n                \"Data cleaning\",\n                \"Feature normalization\",\n                \"Feature engineering\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Ensemble Regression Model\",\n            \"family\": \"Combines deep learning and statistical techniques\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"integration technique\": \"optimal fusion of deep learning and statistical methods\",\n                \"target performance\": \"5% improvement over industry benchmark\"\n            },\n            \"description\": \"A sophisticated model designed for tabular regression, leveraging a hybrid approach to minimize RMSLE.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The ensemble model is expected to outperform competing models due to its integration of deep learning and statistical techniques.\",\n        \"Striving for a 5% improvement over the industry benchmark is key to securing the partnership.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\",\n            \"interactions and feature engineering\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"TensorFlow Serving\",\n            \"Apache Spark MLLib\"\n        ],\n        \"demonstration\": \"Demonstration of the model's performance with improved RMSLE compared to industry standards.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the user's intent to build a complex project, leveraging their high expertise in data analytics. The specific task of creating a hybrid ensemble regression model and optimizing for RMSLE is accurately captured. Performance metrics are listed, with the value for RMSLE left null for optimization, and additional details regarding dataset preprocessing, ensemble features, and potential deployment tools are included. However, specific values for performance improvement or industry benchmark comparison are not available and may be needed for a complete assessment.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "For a specialized hydrocarbons and petrochemicals conglomerate, develop a state-of-the-art Question Answering (QA) system that leverages the niche Salsbury Oils industry extension of TriviaQA. Implement a highly-optimized BERT fusion model, fine-tuned intensively on an extensive database of intricate process plant intricacies and arcane drilling trivia. The system must guarantee real-time responsiveness (<20 ms), employing advanced load balancing algorithms for a surge of 20,000 concurrent users, ensuring ultra-reliability amidst extreme operational hazards in severe weather conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"hydrocarbons and petrochemicals\",\n        \"description\": \"Develop a state-of-the-art Question Answering system for the Salsbury Oils industry extension of TriviaQA. The system should use a BERT fusion model, fine-tuned on a large database of specific process plant details and drilling trivia. Real-time responsiveness of <20ms and advanced load balancing for 20,000 concurrent users with ultra-reliability in extreme weather conditions is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time responsiveness\",\n                \"value\": 20\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 20,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"load balancing\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Salsbury Oils Industry TriviaQA Extension\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"complexity\": null\n            },\n            \"description\": \"An extensive database containing intricate process plant details and arcane drilling trivia specific to the Salsbury Oils industry.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT Fusion Model\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"BERT\",\n                \"fine-tuning_details\": \"intensively on Salsbury Oils trivia\",\n                \"complexity\": \"highly-optimized\"\n            },\n            \"description\": \"A state-of-the-art QA system using a BERT model tailored for the Salsbury Oils domain.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced load balancing and high reliability are critical in a petrochemicals QA system due to extreme operational conditions and surge demands.\",\n        \"Fine-tuning on intricate process plant details and drilling trivia increases the system's accuracy and domain-specificity.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time and scalable\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for concurrent processing\"],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the provided instruction, with the user's intent to build a model and high expertise level clearly stated. The problem area and task are for question answering in NLP, focusing on the Salsbury Oils industry. Performance metrics include real-time responsiveness and load balancing, matching the requirements. The BERT fusion model and fine-tuning details are accurately incorporated. However, exact preprocessing steps or load balancing strategies are not explicitly specified, leaving room for further clarification.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "In the realm of AI-driven linguistic innovation, your advanced machine learning development team has been handed a challenging yet groundbreaking assignment. Your mission is to design a novel, hybrid transformer-based model that pushes the boundaries of translation technology within the niche of endangered and structurally complex languages found in the Amazon rainforest region, using the recently compiled 'Amazonian Linguistics Translator Corpus' (ALTC). The model should seamlessly connect five such underrepresented languages with Spanish, while achieving a staggering semantic congruence rating of 90% as measured by the highly selective Kullback-Leibler Divergence Index (KLD). \r\n\r\nThis project mandates the model to exhibit a superior level of contextual intuition, as demonstrated by successfully resolving cultural references and nuances unique to the Amazonian societies. To guarantee transparency and trust in the translation process, you must incorporate interactive reinforcement learning techniques, allowing users to provide granular feedback and influencing the model's continuous improvement. As part of the project, compile a detailed blueprint outlining the interpretable model architecture, including a rigorous evaluation of its privacy-preserving capabilities, and conduct a comparative analysis with previously unseen models, highlighting the algorithm's resilience to computational and linguistic complexities.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics and indigenous communities\",\n        \"description\": \"Design a novel, hybrid transformer-based model for translating endangered and structurally complex languages in the Amazon rainforest region (Amazonian Linguistics Translator Corpus, ALTC) into Spanish. Aim for a 90% semantic congruence using Kullback-Leibler Divergence Index (KLD) and emphasize contextual understanding and cultural references. Incorporate interactive reinforcement learning for continuous improvement and evaluate privacy-preserving capabilities and resilience to linguistic and computational complexities.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Kullback-Leibler Divergence\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Amazonian Linguistics Translator Corpus (ALTC)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Spanish translations\"],\n            \"specification\": {\n                \"languages\": [\"Endangered Amazonian languages\"],\n                \"size\": \"recently compiled\",\n                \"contextual_complexity\": \"high\"\n            },\n            \"description\": \"A language corpus for translating Amazonian languages into Spanish, emphasizing endangered and structurally complex languages.\",\n            \"preprocessing\": [\"Cultural reference annotation\", \"Data cleaning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Transformer Model for Amazonian Linguistics\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"contextual_intuition\": \"high\",\n                \"reinforcement_learning\": true,\n                \"privacy_features\": \"evaluated\"\n            },\n            \"description\": \"A novel model designed for translating Amazonian languages into Spanish, incorporating hybrid transformers, cultural context handling, and interactive reinforcement learning for improved performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model should handle linguistic intricacies of Amazonian languages and their cultural nuances.\",\n        \"Incorporating reinforcement learning ensures the model adapts to user feedback for enhanced translation quality.\",\n        \"A comparison with previous models will showcase algorithmic resilience to linguistic and computational challenges.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Interpretable model architecture\", \"Privacy-preserving analysis\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Context-sensitive user feedback loop\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the given user's instruction by specifying the build intent, the challenging task in the AI/ML space, and the requirements. The expertise level, area, and performance metrics align with the project's ambition. However, some details are left vague (like target device and deployment specifics), as they are not directly provided in the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "In your role as a visionary space AI project supervisor, task your interdisciplinary team with constructing a sophisticated galaxy imaging classifier. Mandate the procurement of the 'GalaxyZoo2' data set from multiple verified archives, ensuring data integrity. Conduct an in-depth comparative analysis of cutting-edge CNN architectures, such as VGG16's feature extraction, ResNet50's depth, and InceptionV3's inception modules, each subjected to advanced image preprocessing strategies like adaptive normalization and color channel enhancement. Devise a nuanced strategy to optimize model selection based on F1 score, precision, and recall, while tackling the complexities of class distribution imbalances and noise mitigation using ensemble techniques and data augmentation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"space science\",\n        \"description\": \"Construct a sophisticated galaxy imaging classifier using the GalaxyZoo2 dataset, sourced from multiple verified archives for data integrity. Compare and analyze state-of-the-art CNN architectures (VGG16, ResNet50, InceptionV3) with advanced preprocessing techniques like adaptive normalization and color channel enhancement. Optimize model selection based on F1 score, precision, and recall, addressing class imbalance and noise mitigation through ensemble techniques and data augmentation.\",\n        \"performance_metrics\": [\n            {\"name\": \"F1 score\"},\n            {\"name\": \"precision\"},\n            {\"name\": \"recall\"}\n        ],\n        \"complexity_metrics\": [\n            {\"name\": \"inference time\"},\n            {\"name\": \"memory usage\"}\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"GalaxyZoo2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"galaxy classification\"],\n            \"specification\": {\n                \"data_integrity\": \"verified from multiple archives\",\n                \"preprocessing\": [\"adaptive normalization\", \"color channel enhancement\"]\n            },\n            \"description\": \"A dataset for galaxy image classification in the space science domain, sourced from multiple verified archives.\",\n            \"preprocessing\": [\n                \"adaptive normalization\",\n                \"color channel enhancement\"\n            ],\n            \"augmentation\": [\n                \" TBD based on expert analysis\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": [\"VGG16\", \"ResNet50\", \"InceptionV3\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_extraction\": [\"VGG16\", \"ResNet50\", \"InceptionV3\"],\n                \"preprocessing\": \"adaptive normalization and color channel enhancement\"\n            },\n            \"description\": \"Comparative study of VGG16, ResNet50, and InceptionV3 CNN architectures for galaxy image classification\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced preprocessing techniques to enhance image quality and features\",\n        \"Handling class imbalance and noise mitigation through ensemble methods and data augmentation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"ensemble techniques\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Description of the demonstration methods\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intention to build a machine learning model with a specific application domain (space science) and task (galaxy imaging classification). It covers the user's expertise level and the problem's constraints like imbalanced classes and noise mitigation. The comparison of advanced CNN architectures and preprocessing techniques are included, as well as the performance metrics. However, a specific model is not mentioned, and it's assumed that the team will select the best based on the criteria provided. The source of the dataset is specified, and the confidence score is set to high given the clear instructions.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Create a highly specialized, distributed time series forecasting model for the in-app user activity (DAU) of our niche mobile gaming application. Utilize a cutting-edge ensemble of deep learning models like Hierarchical Attention LSTM, Facebook's Prophet with custom resolution adjustments, and ARIMA with exogenous variables. Implement advanced feature extraction methods like TBATS and wavelet analysis to capture complex seasonality and outliers. Introduce L1 and L2 regularization, along with a hybrid dropout strategy, to enhance generalization in low-data scenarios. Design a metric, \"Weighted Horizon Mean Absolute Percentage Error\" (WHMAPE), which penalizes errors differently based on prediction horizon. Compare the novel model's performance with earlier methodologies using a fine-grained, game-specific historical dataset and present a comprehensive, visually insightful report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"mobile gaming\",\n        \"description\": \"Create a highly specialized, distributed time series forecasting model for the in-app user activity (DAU) of a niche mobile gaming application. Use advanced deep learning techniques like Hierarchical Attention LSTM, Facebook's Prophet with custom resolution, and ARIMA with exogenous variables. Apply feature extraction methods such as TBATS and wavelet analysis, and regularization with L1 and L2 and hybrid dropout for better generalization in low-data scenarios. Develop a custom performance metric, 'Weighted Horizon Mean Absolute Percentage Error' (WHMAPE), and compare the new model against previous methods using a detailed, game-specific dataset. Produce a comprehensive and visually insightful report for the project.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"WHMAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"in-app user activity (DAU)\"],\n            \"specification\": {\n                \"time_frequency\": \"daily\",\n                \"game-specific\": true\n            },\n            \"description\": \"A fine-grained historical dataset containing the user activity data of a niche mobile gaming app, with emphasis on game-specific attributes for model comparisons.\",\n            \"preprocessing\": [\n                \"TBATS decomposition\",\n                \"Wavelet analysis\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Interpreting seasonal patterns\",\n                \"Error distribution\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": [\n                \"Hierarchical Attention LSTM\",\n                \"Facebook Prophet\",\n                \"ARIMA with exogenous variables\"\n            ],\n            \"type\": \"ensemble\",\n            \"specification\": {\n                \"ensemble_method\": \"Distributed Learning\",\n                \"regularization\": [\n                    \"L1\",\n                    \"L2\"\n                ],\n                \"dropout_strategy\": \"Hybrid\"\n            },\n            \"description\": \"Advanced time series forecasting model incorporating deep learning techniques for improved in-app user activity prediction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Use of ensemble models for better forecasting\",\n        \"Custom performance metric development for accuracy with varying horizons\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature extraction\",\n            \"L1, L2 regularization and hybrid dropout implementation\"\n        ],\n        \"target_device\": \"Mobile, Cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Comparison charts, error analysis, and detailed report\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the provided instruction, with the user's intent to build a model, high expertise level, and specific requirements for a niche mobile gaming application. The area of time-series analysis is identified, and the tasks of creating an advanced ensemble model, feature extraction, and custom performance metric are included. The user's request for a comprehensive report and visualization is addressed in the 'knowledge' and 'service' sections. However, specific details like the WHMAPE value and some model specifications are left to be filled based on the model's performance and characteristics.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "The project manager of an elite machine learning team is tasked with advancing the ImageClassifier-SLD for Stanford Cars Dataset v2.0, focusing on the innovative Inception-v4+ design. The goal is not only to optimize for accuracy, but to achieve at least a 97% mAP while maintaining per-class precision and recall above 95% in various challenging conditions, such as partial object visibility, nuanced color shifts, and diverse weather patterns. The team must conduct extensive data augmentation, investigate transfer learning strategies, and document the model's robustness with real-world simulations. A compelling presentation must showcase the augmented testing outputs, accompanied by a meticulous side-by-side comparison with previous top-tier models, highlighting key performance improvements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"automotive\",\n        \"description\": \"Advance the ImageClassifier-SLD for Stanford Cars Dataset v2.0 using Inception-v4+ design, optimizing for accuracy with a minimum of 97% mAP. Maintain per-class precision and recall above 95% in challenging conditions like partial object visibility, nuanced color shifts, and diverse weather patterns. Conduct extensive data augmentation, explore transfer learning, and document robustness with real-world simulations. The final output must include a compelling presentation with augmented testing results and a comparison with top-tier models.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean average precision (mAP)\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": {\n                    \"min\": 0.95,\n                    \"max\": null\n                }\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": {\n                    \"min\": 0.95,\n                    \"max\": null\n                }\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Dataset v2.0\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"image class labels\"],\n            \"specification\": {\n                \"complexity\": \"challenging with diverse conditions\",\n                \"size\": \"Stanford Cars specific\"\n            },\n            \"description\": \"Stanford Cars Dataset version 2.0 for image classification with a focus on car models.\",\n            \"preprocessing\": [\"data augmentation for challenging conditions\"],\n            \"augmentation\": [\"partial object visibility, color shifts, weather patterns\"],\n            \"visualization\": [\"model performance on augmented data\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ImageClassifier-SLD (Inception-v4+)\",\n            \"family\": \"Inception-v4\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Inception-v4\",\n                \"target_performance\": \"optimized for accuracy and specific constraints\"\n            },\n            \"description\": \"Advanced ImageClassifier-SLD using Inception-v4, designed for the challenging conditions of the Stanford Cars Dataset.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"transfer learning\",\n            \"real-world simulations\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"augmented testing outputs and comparison with top-tier models\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the provided instruction, including the use of Inception-v4+ design and specific performance requirements. It reflects the project manager's role, the area of computer vision, and the focus on image classification. The dataset, model, and service requirements are inferred from the task. The metric values are set according to the performance targets mentioned, and the confidence score is high due to the direct mapping of the instruction to the JSON structure.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "link prediction", "instruction": "Construct an advanced, deep-learning-based link prediction model specifically tailored for Tencent Weibo, leveraging a novel Attention-enhanced Graph Neural Network with Hybrid Attention Layers and Temporal Context Integration. The objective is to not only detect latent emotional connections and influencer dynamics, but also decipher evolving sentiments and behavior patterns in the massive real-time user engagement graph, ensuring high precision and interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"social network analysis\",\n        \"description\": \"Develop an advanced deep-learning-based link prediction model tailored for Tencent Weibo, using an Attention-enhanced Graph Neural Network with Hybrid Attention Layers and Temporal Context Integration. The model aims to analyze latent emotional connections, influencer dynamics, evolving sentiments, and behavior patterns in real-time user engagement data, prioritizing high precision and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tencent Weibo\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"real-time user engagement graph\",\n                \"dimensions\": {\n                    \"nodes\": \"users, influencers\",\n                    \"edges\": \"emotional connections, influencer dynamics\"\n                },\n                \"size\": {\n                    \"vertices\": null,\n                    \"edges\": null\n                }\n            },\n            \"description\": \"A massive, real-time graph containing user engagement data from Tencent Weibo, capturing emotional connections and behavior patterns.\",\n            \"preprocessing\": [\n                \"Attention-enhanced node embedding\",\n                \"Temporal feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention-enhanced GNN\",\n            \"family\": \"Deep Graph Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"Hybrid Attention Layers\": {},\n                    \"Temporal Context Integration\": {}\n                },\n                \"interpretability\": true\n            },\n            \"description\": \"A deep-learning model utilizing Attention-enhanced Graph Neural Networks for link prediction, specifically designed for Tencent Weibo data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Attention mechanisms improve node representation learning for complex graph data.\",\n        \"Temporal context is crucial for capturing evolving sentiment and behavior patterns.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"novel Attention-enhanced layer design\", \"temporal fusion\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"model interpretability demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is derived from the given instruction, assuming a high level of expertise. The problem area is graph machine learning, downstream task is link prediction, and the focus on high precision and interpretability is clear. Performance metric of precision is not explicitly defined, so it is set to null. The dataset is described in detail based on the real-time engagement graph and user-specified search method. The model name, family, and type are specified with attention and temporal context integration in mind. The model's interpretability is highlighted, which aligns with the goal. The service requirements and confidence score are also computed based on the given information.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project director overseeing an elite team focused on high-resolution urban sprawl and biodiversity research, we require a domain specialist in deep learning satellite image analytics. Specifically, seek a candidate proficient in customizing and fine-tuning state-of-the-art algorithms like EfficientNet-based land use segmentation (incorporating B5 backbone), with emphasis on differentiating intricate features like tidal estuaries, primary tropical forests, and intricate built-up areas. The task involves developing a streamlined workflow to achieve at least 95% accuracy within a stringent 2-hour processing constraint, while ensuring the model's ability to generalize across the expansive and dynamically changing Landsat 8 archive, including seasonal variations. Propose a tailored model architecture, a data augmentation strategy, and conduct a rigorous runtime analysis including edge computing considerations for real-time deployments on resource-constrained devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning and biodiversity research\",\n        \"description\": \"Seek a domain expert in deep learning satellite image analytics to develop a land use segmentation model using EfficientNet-B5 backbone for high-resolution urban sprawl research. The model should target at least 95% accuracy within a 2-hour processing constraint, and generalize to Landsat 8 archive, including seasonal variations. Focus on differentiating intricate features like tidal estuaries, primary tropical forests, and complex built-up areas. Additionally, propose a tailored model architecture, data augmentation strategy, and analyze runtime for edge computing deployment on resource-constrained devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"processing time\",\n                \"value\": 120,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"edge computing performance\",\n                \"value\": \"real-time\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Landsat 8\",\n            \"modality\": [\"image\", \"time series\"],\n            \"target_variables\": [\"land use labels\"],\n            \"specification\": {\n                \"archive_size\": \"large\",\n                \"seasonal_variations\": true\n            },\n            \"description\": \"A high-resolution satellite image archive with multiple time steps for land use segmentation research.\",\n            \"preprocessing\": [\n                \"incorporating temporal information\"\n            ],\n            \"augmentation\": [\n                \"time-aware data augmentation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet-B5 based land use segmentation\",\n            \"family\": \"EfficientNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"customized, possibly including feature pyramid networks (FPN)\",\n                \"runtime_constraints\": \"2-hour processing window\"\n            },\n            \"description\": \"An advanced deep learning model for image segmentation using EfficientNet-B5 backbone, specifically tailored for detecting tidal estuaries, primary tropical forests, and intricate built-up areas.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Deep learning expertise in satellite imagery, especially in satellite image segmentation\",\n        \"Domain knowledge in urban sprawl and biodiversity\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"including seasonality and temporal dependencies\"\n        ],\n        \"target_device\": \"resource-constrained\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for edge computing platforms\"\n        ],\n        \"demonstration\": \"streamlined workflow for real-time performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is a detailed mapping of the instruction to the provided schema, capturing the project requirements for the domain specialist, model architecture, performance, and deployment considerations. The specific expertise level and application domain were inferred from the context. The performance metrics and constraints are clearly defined, and the dataset and model specifications reflect the requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a distinguished project lead, request the AI development team to design and implement a sophisticated plant species classifier using the 'Advanced Plant Seedlings Dataset with Variable Environmental Factors.' Incorporate not just a state-of-the-art convolutional neural network, but also consider transfer learning with multiple databases for enhanced recognition. Demonstrate the ability to achieve a consistent accuracy of over 98% under various illumination conditions, and present a comprehensive plan for hardware-efficient optimizations that guarantee at least 150ms latency for seamless, low-power mobile deployment on diverse devices. Additionally, include strategies for real-world data augmentation and model interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Design and implement a sophisticated plant species classifier using the 'Advanced Plant Seedlings Dataset with Variable Environmental Factors.' Employ a state-of-the-art convolutional neural network and incorporate transfer learning from multiple databases to enhance recognition. The classifier should consistently achieve an accuracy of over 98% under varying illumination conditions. The objective is to guarantee at least 150ms latency for mobile deployment on diverse devices with low power consumption. Include strategies for real-world data augmentation and model interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.150,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Advanced Plant Seedlings Dataset with Variable Environmental Factors\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"illumination_conditions\": \"variable\",\n                \"mobile_compatibility\": true\n            },\n            \"description\": \"A dataset for plant species classification with diverse environmental factors and considering mobile deployment scenarios.\",\n            \"preprocessing\": [\"transfer learning from multiple databases\"],\n            \"augmentation\": [\n                \"real-world data augmentation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"state_of_the_art\": true,\n                \"efficient_for_mobile\": true\n            },\n            \"description\": \"A sophisticated CNN for plant species classification using transfer learning, designed for high accuracy and hardware efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating transfer learning for improved recognition across multiple databases\",\n        \"Strategies for low-latency mobile deployment and low-power consumption\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"mobile hardware-efficient optimizations\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A comprehensive demonstration showcasing performance under varying illumination and mobile device conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the essence of the given instruction, including the user's intent to build a sophisticated classifier, their high expertise level, the key features of the dataset and model, performance goals, and additional requirements. The specific implementation details, such as the state-of-the-art CNN, transfer learning, latency constraints, and data augmentation, are incorporated, making the response valid and complete. However, the exact model name, family, and some specific performance metrics were left unspecified to allow flexibility in the final solution.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a seasoned project manager for a top-tier precision advertising expenditure forecast project, you've been tasked with tackling a sophisticated data challenge. The extensive Media Campaign Cost Dataset, enriched with advanced temporal and categorical variables, has been meticulously stratified into non-adjacent training, validation, and testing partitions, each with distinct seasonality patterns. Your goal is to create an innovative regression model, incorporating cutting-edge feature interactions and temporal lags, aiming for the tiniest possible mean absolute percentage error (MAPE) in addition to delivering interpretability. The project must include a comprehensive whitepaper on feature extraction methods, model selection rationale, and nuanced explanations for influential factors. Convey the nuances of sparse data handling and address potential model biases.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"As a seasoned project manager for a precision advertising expenditure forecast project, develop an innovative regression model using the stratified Media Campaign Cost Dataset with advanced temporal and categorical variables. The model should achieve the lowest possible mean absolute percentage error (MAPE), incorporate feature interactions and temporal lags, and provide interpretability. A whitepaper is required on feature extraction methods, model selection rationale, and insights on influential factors, including sparse data handling and addressing potential model biases.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Media Campaign Cost Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"advertising expenditure\"],\n            \"specification\": {\n                \"stratification\": \"non-adjacent partitions with distinct seasonality patterns\",\n                \"feature_types\": [\"temporal\", \"categorical\"],\n                \"sparse_data_handling\": true\n            },\n            \"description\": \"An extensive dataset with advanced temporal and categorical variables, stratified for training, validation, and testing.\",\n            \"preprocessing\": [\n                \"feature extraction\",\n                \"stratified splitting\",\n                \"seasonality handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"innovative regression\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_interactions\": true,\n                \"temporal_lags\": true\n            },\n            \"description\": \"An advanced regression model designed for precision advertising expenditure prediction with feature interactions, temporal lags, and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Handling sparse data with model regularization and feature engineering strategies is crucial.\",\n        \"Explaining model biases and feature importance will contribute to model transparency.\",\n        \"Innovative feature extraction methods cater to the unique temporal and categorical nature of the data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"sparse data techniques\",\n            \"influential factor analysis\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"whitepaper on feature extraction, model selection, and influential factors\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is tailored to the instruction, considering the user's project management role and the specifics of the data challenge. It captures the expert-level expertise and the precise requirements for performance (MAPE), interpretability, and whitepaper on feature extraction, model selection, and data handling. Some details like specific model selection, certain feature engineering steps, and exact performance value are left unspecified as per the instruction, but these can be filled with appropriate details based on domain knowledge.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "Act as a project manager for the elite ML team, faced with the challenge of designing an advanced time series classifier for a competing pharmaceutical R&D project. The task involves processing the intricate Heartbeat dataset with extra complexity from non-uniform sampling rates (INPUT_SEQ_LEN dynamically varying), 750 features derived from both time and frequency domains (INPUT_DIM=750). Implement an ensemble model, accounting for class imbalance using SMOTE, and rigorously employ nested cross-validation with 5 folds. Optimize not only for AUC-ROC and accuracy, but also for F1 score and model interpretability. The project must excel in unseen datasets, withstand concept drift, and deliver a comprehensive interpretability report for each class (labels: {healthy, anomalous}). The ultimate goal is to outperform industry benchmarks by 5% and secure a fast inference speed without sacrificing performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"pharmaceuticals\",\n        \"description\": \"Design an advanced time series classifier for a competitive pharmaceutical R&D project using the Heartbeat dataset with non-uniform sampling rates and 750 features from time and frequency domains. Implement an ensemble model, handle class imbalance with SMOTE, employ nested cross-validation with 5 folds, optimize for AUC-ROC, accuracy, and F1 score. Focus on interpretability, ensure outperformance of industry benchmarks by 5%, and aim for fast inference while maintaining high performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"ms\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Heartbeat\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"healthy\", \"anomalous\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": {\n                    \"type\": \"dynamic\",\n                    \"description\": \"Non-uniform sampling rates\"\n                },\n                \"INPUT_DIM\": 750,\n                \"feature_domains\": [\"time\", \"frequency\"]\n            },\n            \"description\": \"Dataset with 750 features from time and frequency domains, including non-uniform sampling rates.\",\n            \"preprocessing\": [\"SMOTE for class imbalance\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model interpretability\": \"high\"\n            },\n            \"description\": \"An advanced ensemble model designed for time-series classification, addressing class imbalance with SMOTE and optimized for AUC-ROC, accuracy, and F1 score.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Class imbalance is managed using SMOTE.\",\n        \"Nested cross-validation with 5 folds is employed for model selection.\",\n        \"Interpretability is prioritized for each class.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"interpretability reports for each class\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the project manager's responsibilities, the task description, and requirements specified by the user. The area, downstream task, and performance metrics are clearly outlined. The ensemble model, SMOTE, cross-validation, and interpretability are all included according to the instruction. However, specific model details and performance targets (like a 5% improvement over benchmarks) are not quantified, so their values are left as null. The confidence score is moderately high due to the clarity of the request but may be lower if more specific information was required.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "visual question answering", "instruction": "As the project manager, you're tasked with developing a state-of-the-art TextVQA system for an educational institution. The system should not only accurately interpret text within images but also analyze context, visual clues, and handle complex multimodal inputs. Additionally, research and integrate cutting-edge model architectures like M6 or LXMERT, ensuring high precision and scalability for large datasets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"As a project manager, the task is to develop a state-of-the-art TextVQA system for an educational institution, focusing on accurate interpretation of text within images, context analysis, visual clues, and handling complex multimodal inputs. The system should incorporate cutting-edge models like M6 or LXMERT for high precision and scalability for large datasets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\", \"image\", \"multimodal\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"Large, diverse multimodal datasets for training and evaluation, suitable for complex visual question answering tasks.\",\n            \"preprocessing\": [\n                \"data cleaning and augmentation\",\n                \"multi-modal alignment\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6 or LXMERT\",\n            \"family\": \"Advanced multimodal models\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"State-of-the-art deep learning architecture, either M6 or LXMERT, for TextVQA with emphasis on precision and scalability.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"value\": \"Relevant to TextVQA: latest advancements in multimodal learning, context fusion techniques, and benchmarking results for M6 and LXMERT.\"\n        },\n        {\n            \"value\": \"Scaling considerations: efficient hardware and software optimizations for large-scale deployment.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual embeddings, attention mechanisms\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-accelerated inference framework\"\n        ],\n        \"demonstration\": \"Interactive platform showcasing TextVQA's performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the instruction by specifying a high-level build intent, high expertise level, the multimodal learning area for the task, and the educational domain. Performance metrics include precision and scalability requirements, while the need for state-of-the-art models like M6 or LXMERT is included, along with the necessity to handle complex inputs. The dataset expectations and model specifications align with the task, acknowledging preprocessing and augmentation techniques, and the software requirements for deployment. The confidence is high due to the clear mapping of the instruction to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a celestial exploration project manager, you demand the implementation of a highly specialized Image Categorization AI for the purpose of detecting diverse astrophysical entities in the extensive Galaxy Zoo dataset. Emphasize on deploying a state-of-the-art MViT (Multi-modal Vision Transformer) enhanced with self-attention mechanisms, which not only enhances accuracy but also demonstrates transfer learning resilience and interpretability. This system must exhibit exceptional performance in dealing with the intricate nuances and unforeseen celestial phenomena, while maintaining real-time efficiency for live research support.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Implement a specialized Image Categorization AI for detecting diverse astrophysical entities in the Galaxy Zoo dataset. The model must be a state-of-the-art MViT (Multi-modal Vision Transformer) with self-attention mechanisms, focusing on accuracy, transfer learning resilience, interpretability, and real-time efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time efficiency\",\n                \"value\": null,\n                \"unit\": \"seconds per image\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"astrophysical entity\"],\n            \"specification\": null,\n            \"description\": \"A comprehensive dataset for astrophysical entity detection, containing diverse celestial objects.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MViT with self-attention\",\n            \"family\": \"Multi-modal Vision Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"enhanced\",\n                \"transfer_learning\": \"resilient\"\n            },\n            \"description\": \"A state-of-the-art model designed for image classification, specifically tailored for detecting diverse astrophysical entities in the Galaxy Zoo dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"MViT's self-attention mechanisms improve accuracy and interpretability\",\n        \"Model should handle complex celestial phenomena and maintain real-time performance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"dedicated for real-time research support\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Real-time AI performance showcasing on live research scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes the user's intention to build a model (expertise level high), problem context (image classification in computer vision), the target dataset, the desired model (MViT with self-attention), and key performance indicators. The description of the model and its requirements are derived directly from the user's instructions. However, specific performance values are not given, they are set to null to indicate the need for actual model evaluation. Missing information includes the expected accuracy and interpretability values, which would need to be derived from model testing or fine-tuning.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "The task for the machine learning development team is to design and implement a real-time, cloud-based system that not only analyzes high-resolution aerial images of extensive crop fields but also differentiates between various stages of growth and stress conditions, with a minimum accuracy of 95%. This system must integrate with our existing farm management software and generate actionable reports within a 2-hour window for on-field response.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Design and implement a real-time, cloud-based system that analyzes high-resolution aerial images for crop growth stage and stress detection. The minimum accuracy required is 95%, and the system must integrate with existing farm management software with a response time of 2 hours for on-field action.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 120,\n                \"unit\": \"minutes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High-Resolution Crop Field Images\",\n            \"modality\": [\"image\", \"time series\"],\n            \"target_variables\": [\"crop growth stage\", \"stress condition\"],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"image_size\": {\"width\": \"variable\", \"height\": \"variable\"},\n                \"time_interval\": {\n                    \"type\": \"daily\",\n                    \"frequency\": null\n                }\n            },\n            \"description\": \"A dataset containing high-resolution aerial images capturing different growth stages and stress conditions in crop fields.\",\n            \"preprocessing\": [\"image resizing\", \"image normalization\"],\n            \"augmentation\": [\"data augmentation for time series\"],\n            \"visualization\": [\"image segmentation maps\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (CNN) for Image Classification\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"CNN architecture suitable for real-time and cloud deployment\",\n                \"inference_speed\": \"optimized for fast response times\"\n            },\n            \"description\": \"A real-time cloud-based image analysis model specifically for crop growth and stress detection, meeting the specified accuracy requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The high-resolution and time series nature of the dataset necessitate a CNN architecture for image analysis and recognition of patterns over time.\",\n        \"Integrating with farm management software and meeting a 2-hour response time will require efficient model deployment and data processing algorithms.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for growth and stress indicators\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"existing farm management software integration\",\n        \"inference_engine\": [\"optimized for real-time inference in cloud environment\"],\n        \"demonstration\": \"Interactive dashboards showcasing live image analysis and action plans\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction, mapping user intent (build), expertise level, and the details of the problem and requirements. Performance metric (accuracy), response time, and integration with existing software are included. Dataset information is deduced from the context. The specified model is appropriate for the task and the system's design considers the constraints. However, a specific CNN architecture is assumed, and more detailed specifications could be requested.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the BraTS dataset's advanced medical image analysis, we seek a state-of-the-art, explainable V-Net architecture that surpasses benchmark accuracy with a targeted Dice coefficient of 0.90. The model should undergo rigorous multi-step feature extraction, interpretable attention maps, and continuous validation using cross-validation strategies while maintaining interpretability for key decision-making.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"medical imaging\",\n        \"description\": \"Seeking a state-of-the-art, explainable V-Net architecture for advanced medical image analysis on the BraTS dataset, targeting benchmark accuracy with a Dice coefficient of 0.90. The model requires multi-step feature extraction, interpretable attention maps, and continuous validation using cross-validation methods while prioritizing interpretability for decision-making.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Dice coefficient\",\n                \"value\": 0.90\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BraTS\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": \"variable (adapted for V-Net)\",\n                \"modalities\": \"multi-modal medical images\"\n            },\n            \"description\": \"A benchmark medical image dataset for brain tumor segmentation, with specific emphasis on BraTS.\",\n            \"preprocessing\": [\"multi-modal fusion\", \"normalization\"],\n            \"augmentation\": [\"data augmentation specific to medical images\"],\n            \"visualization\": [\"attention maps for interpretability\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"V-Net (explainable)\",\n            \"family\": \"V-Net architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": \"enabled\",\n                \"attention mechanisms\": \"interpretable\"\n            },\n            \"description\": \"An advanced V-Net designed for medical image segmentation, tailored to the BraTS dataset and aiming for a Dice coefficient of 0.90, with emphasis on explainability through attention maps.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art techniques in deep learning, especially for medical image segmentation, should be considered.\",\n        \"Interpretablility is crucial for decision-making in medical applications.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"multi-step feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretation tools for actionable insights\"],\n        \"demonstration\": \"Interpretable model outputs with attention maps\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's request, including the requirement for a state-of-the-art V-Net, explainability, and the target Dice coefficient. It reflects the user's high expertise and the application domain, medical imaging. Specific dataset details and model characteristics are included, but more granular details, such as exact pre-processing steps and training details, are not included since they might vary depending on the implementation strategy. The confidence score is high given the clarity of the instruction and the structure of the response.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create a specialized emotion-preserving machine translation system using the highly specialized ParaCrawl linguistic dataset, focusing on English to Dutch conversion. Implement a siamese dual-encoder architecture with sentiment analysis integration, ensuring not only precise word-to-word correspondence but also capturing nuanced emotional nuances in the source content.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Develop a specialized emotion-preserving machine translation system focusing on English to Dutch conversion. This system should use a siamese dual-encoder architecture with sentiment analysis integration, maintaining precise word-to-word translation and capturing emotional nuances.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"BLEU score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"emotional accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl linguistic dataset (English to Dutch)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"valid\": null,\n                    \"test\": null\n                },\n                \"language_pair\": \"English to Dutch\",\n                \"specialization\": \"highly specialized\"\n            },\n            \"description\": \"A dataset for training a machine translation system, specifically focused on preserving emotions and tailored for the English to Dutch language pair.\",\n            \"preprocessing\": [\n                \"emotion-annotated data preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Specialized Siamese Dual-Encoder for Emotion-Preserving MT\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"siamese dual-encoder\",\n                \"feature_encoder\": \"contextual word embeddings\",\n                \"sentiment_analysis_module\": true\n            },\n            \"description\": \"A deep learning model that leverages siamese architecture with embedding-based encoders and sentiment analysis for precise and emotionally nuanced translation between English and Dutch.\"\n        }\n    ],\n    \"knowledge\": [\n        \"ParaCrawl's linguistic specialization helps preserve emotional nuances.\",\n        \"Siamese dual-encoder architecture aids in matching semantic and emotional meaning across languages.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual embeddings customization\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU/CPU parallel processing\"],\n        \"demonstration\": \"sentence-level emotion-preserving translation demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction accurately reflects in the JSON format, capturing the intent to build a specialized machine translation system. Expertise level is assumed to be high based on the complexity of the task. Performance metrics for precision, recall, BLEU score, and emotional accuracy are not provided, as they can be inferred but not specified. The dataset is clearly defined as ParaCrawl linguistic (English to Dutch) and highlights the emotional preservation requirement. The model details are specified, incorporating siamese dual-encoder and sentiment analysis. A few potential feature engineering steps and inference engine requirements are mentioned, and a demonstration idea is outlined.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the AI-driven coral reef monitoring initiative, outline a comprehensive strategy to integrate a state-of-the-art modified PSPNet architecture, incorporating deep learning techniques like multi-exposure fusion and adaptive normalization. The model should handle not only water clarity disparities but also account for time-varying lighting conditions and real-time underwater scene complexities, targeting a 20% increase in F1-score compared to the current industry benchmark, and include a detailed evaluation plan with cross-validation experiments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"environmental science\",\n        \"description\": \"As a project manager for an AI-driven coral reef monitoring initiative, develop a strategy to integrate a state-of-the-art modified PSPNet architecture, incorporating deep learning techniques like multi-exposure fusion and adaptive normalization. The model should handle water clarity disparities, time-varying lighting conditions, and real-time underwater scene complexities, targeting a 20% increase in F1-score compared to current industry benchmark.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 1.20\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"seconds\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory usage\",\n                \"unit\": \"GB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coral Reef Monitoring Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"underwater scene segmentation mask\"],\n            \"specification\": {\n                \"image_size\": \"variable\",\n                \"color_space\": \"RGB\",\n                \"data_division\": [\"train\", \"validation\", \"test\"]\n            },\n            \"description\": \"A dataset containing underwater images with varying water clarity and lighting conditions for coral reef monitoring.\",\n            \"preprocessing\": [\n                \"multi-exposure fusion\",\n                \"adaptive normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"real-time monitoring results\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Modified PSPNet\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"PSPNet with deep learning enhancements\",\n                \"techniques\": [\"multi-exposure fusion\", \"adaptive normalization\"]\n            },\n            \"description\": \"An advanced image segmentation model specifically tailored for coral reef monitoring, handling real-world challenges in underwater scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art PSPNet with fusion and normalization techniques helps enhance image clarity.\",\n        \"Cross-validation experiments will evaluate model's performance increment of 20% over industry benchmark.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            {\n                \"technique\": \"custom preprocessing steps\"\n            }\n        ],\n        \"target_device\": \"underwater monitoring devices\",\n        \"deployment_endpoint\": \"remote cloud or edge server\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"Real-time monitoring results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the given instruction, including the project context (AI-driven coral reef monitoring), the modified PSPNet architecture, and the performance and complexity requirements. The intent to build the model, the high level of expertise, and the specific evaluation plan (cross-validation) are covered. The missing metric values (inference time and memory usage) are placeholders for optimization, acknowledging the ambiguity in specifying these for an underwater scene segmentation task.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series anomaly detection", "instruction": "As a project manager for the machine learning team, you need to develop an anomaly detection system for a decade-long dataset of hourly power consumption in a metropolis. Mandate the use of deep learning methods, accounting for seasonality, weather variables, and cross-check with satellite data to ensure accuracy, and present visualizations to support findings.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"energy management\",\n        \"description\": \"Develop an anomaly detection system for a decade-long hourly power consumption dataset of a metropolis. Use deep learning methods, account for seasonality and weather variables, and incorporate cross-check with satellite data for improved accuracy. Visualizations for findings are required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Decade-long Power Consumption Dataset\",\n            \"modality\": [\n                \"time series\",\n                \"tabular\"\n            ],\n            \"target_variables\": [\"hourly power consumption\"],\n            \"specification\": {\n                \"duration\": \"10 years\",\n                \"frequency\": \"hourly\"\n            },\n            \"description\": \"A time-series dataset with hourly power consumption data for a metropolis over a decade, including other potentially relevant features like weather and satellite data.\",\n            \"preprocessing\": [\n                \"Seasonality adjustment\",\n                \"Feature engineering (weather and satellite data integration)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Time-series plots\",\n                \"Anomaly detection visualizations\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Autoencoder or Time-Series CNN/LSTM\",\n                \"explained_components\": \"Seasonality, weather variables, and satellite data\"\n            },\n            \"description\": \"A deep learning-based anomaly detection model for power consumption data, leveraging on seasonal patterns, weather variations, and cross-check with satellite data for improved performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Deep learning methods are needed for the complex and dynamic nature of the power consumption data.\",\n        \"Incorporating seasonality and weather variables will increase the model's accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonality adjustment\",\n            \"satellite and weather data integration\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"TensorFlow Serving\",\n            \"Apache Spark MLlib\"\n        ],\n        \"demonstration\": \"Visualizations to showcase anomaly detection results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project management instruction, reflecting the need for a build (expertise level is high), and the requirements in the area of time-series analysis for anomaly detection. Performance metrics, seasonality, and deep learning specifics are included. However, specific metric values or deployment endpoint are not mentioned, leaving them as placeholders. High confidence in understanding is assumed due to the clear instruction provided.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a highly scalable node classification model on the NELL dataset employing a state-of-the-art Hypergraph Neural Network, with an emphasis on fault tolerance. Demand an accuracy of at least 90% under varying edge weight distributions and perform exhaustive hyperparameter tuning with A/B testing, documenting the impact on model performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic\",\n        \"description\": \"Develop a highly scalable node classification model on the NELL dataset using a state-of-the-art Hypergraph Neural Network, focusing on fault tolerance and achieving an accuracy of at least 90%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {},\n            \"description\": \"A dataset for node classification tasks, with emphasis on varying edge weight distributions in the NELL graph.\",\n            \"preprocessing\": [\"handle varying edge weight distributions\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-Art Hypergraph NN\",\n            \"family\": \"Hypergraph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"method\": \"state-of-the-art\",\n                \"features\": \"fault tolerance\"\n            },\n            \"description\": \"A highly advanced model for node classification on hypergraphs, designed for scalability and robustness against different edge weight distributions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hyperparameter tuning and A/B testing are crucial for achieving high performance and fault tolerance in the model.\",\n        \"The model must be able to handle diverse edge weight distributions in the NELL dataset.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"hyperparameter tuning with A/B testing\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Document the impact of different hyperparameters on model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the main aspects of the instruction, including the high-level intent, expertise level, and the specifics of the problem, model, and dataset. The performance metric of 90% accuracy is incorporated. The knowledge and service sections reflect the emphasis on fault tolerance and extensive testing, while the preprocessing and feature engineering steps are adapted from the instructions. However, due to the absence of specific details on hyperparameter settings and distribution of edge weights, these elements are left somewhat open to interpretation.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project manager for our advanced multilingual AI initiative, you need to direct the team in fine-tuning a transformer model on the extensive Common Crawl dataset for English-Japanese translation. Emphasize the requirement of exceptional handling of dialects, slang, and informal speech, aiming for a remarkable 98% accuracy in recognizing and translating colloquial expressions in real-world scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"AI and language services\",\n        \"description\": \"Fine-tune a transformer model on the Common Crawl dataset specifically for English to Japanese translation, with a focus on handling dialects, slang, and informal speech. Target accuracy should be at least 98% for recognizing and translating colloquial expressions in real-world scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl (English-Japanese)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source_type\": \"extensive\",\n                \"language_pair\": \"English-Japanese\"\n            },\n            \"description\": \"A large-scale dataset for English and Japanese text, including a wide variety of dialects, slang, and informal speech.\",\n            \"preprocessing\": [\n                \"Text cleaning, normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer model (e.g., T5, M4)\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine-tuning_details\": \"Emphasis on dialects, slang, and informal speech handling\"\n            },\n            \"description\": \"A state-of-the-art neural network model, specifically designed for multilingual machine translation with a focus on handling informal language.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Achieving 98% accuracy for colloquial expressions requires extensive data and robust model capacity.\",\n        \"Transformer models are known to excel in handling natural language variability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Domain-specific fine-tuning for translation of colloquial expressions\"\n        ],\n        \"target_device\": \"Cloud\",\n        \"deployment_endpoint\": \"Model-as-a-Service API\",\n        \"inference_engine\": [\"GPU-based inference\"],\n        \"demonstration\": \"End-to-end demo showcasing translation of colloquial English to Japanese\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been generated based on the project manager's instructions, focusing on the build aspect with high expertise. The problem area (NLP), task (MT), and application domain are identified, along with the performance metric (accuracy) and target value. The Common Crawl dataset is specified with a focus on dialects and informal speech, and source is mentioned to be a link that would need to be provided. The model is described as a transformer model with a focus on informal language. The confidence score is high since most information is clearly specified from the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "community detection", "instruction": "Utilize the Sunbelt dataset for the CiteSeer citation network, where the task is to design a scalable Label Propagation algorithm that discerns fine-grained, dynamically evolving research communities based on co-authorship patterns. Emphasize resistance to concept drift and the ability to seamlessly integrate new researchers with minimal computational overhead.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Design a scalable Label Propagation algorithm for the CiteSeer citation network in Sunbelt dataset, focusing on fine-grained, dynamically evolving research communities based on co-authorship patterns. Ensure resistance to concept drift and efficient integration of new researchers with minimal computational overhead.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"convergence speed\",\n                \"value\": null\n            },\n            {\n                \"name\": \"concept drift resistance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational overhead\",\n                \"value\": null,\n                \"unit\": \"CPU cycles\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sunbelt CiteSeer\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_nodes\": null,\n                \"average_degree\": null,\n                \"dynamic nature\": \"evolving\"\n            },\n            \"description\": \"A co-authorship network dataset for detecting fine-grained research communities in CiteSeer.\",\n            \"preprocessing\": [\n                \"dataset pruning to remove noise\",\n                \"concept drift detection methods\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"community structure analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Label Propagation Algorithm\",\n            \"family\": \"graph clustering\",\n            \"type\": \"statistical models\",\n            \"specification\": {\n                \"scalability\": \"high\",\n                \"dynamic handling\": \"true\"\n            },\n            \"description\": \"A statistical model for community detection based on the Label Propagation algorithm, designed to be resilient to concept drift and efficient in handling new researchers.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Fine-grained communities require a tailored algorithm to capture intricate co-authorship patterns.\",\n        \"Resistance to concept drift calls for adaptability in the algorithm design.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"noise reduction methods\", \"dynamic adaptation techniques\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"demonstration of community evolution and new researcher integration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the task provided. The user's intent is to build, and their expertise is assumed high. The problem domain and task are well defined, as are the desired performance metrics and complexity constraints. The dataset is specifically mentioned as the Sunbelt CiteSeer, and the required characteristics and steps are included in the dataset properties. The model type, algorithm specification, and additional knowledge required for the task are also present. However, specific target values for performance metrics are not provided as they may vary depending on the implementation. The confidence score is high due to the clear mapping of the instruction to the JSON schema.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for a specialized deep-learning team, your goal is to develop an advanced ConvLSTM-Transformer hybrid model for hourly multivariate urban weather forecasting using the UrbanWeather v2.0 dataset. The data has been meticulously partitioned into train, validation, and test sets with non-overlapping time windows of 96 timestamps each, containing detailed measurements of 21 meteorological variables (INPUT_SEQ_LEN=96, INPUT_DIM=21). The model must predict the subsequent 96-hour weather patterns (PRED_SEQ_LEN=96, PRED_DIM=21) with accuracy, optimizing for both mean squared logarithmic error (MSLE) and mean absolute percentage error (MAPE). Additionally, design an efficient early stopping strategy and incorporate rolling window cross-validation for robust model validation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate science or environmental management\",\n        \"description\": \"Develop an advanced ConvLSTM-Transformer hybrid model for hourly multivariate urban weather forecasting using the UrbanWeather v2.0 dataset. The dataset is partitioned with non-overlapping time windows of 96 timestamps each, with 21 meteorological variables as inputs and another 96-hour prediction sequence of 21 variables. The model should optimize for both mean squared logarithmic error (MSLE) and mean absolute percentage error (MAPE). Implement an efficient early stopping strategy and incorporate rolling window cross-validation for robust validation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared logarithmic error (MSLE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute percentage error (MAPE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UrbanWeather v2.0\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 21,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 21\n            },\n            \"description\": \"A multivariate dataset for urban weather forecasting with hourly measurements of 21 meteorological variables over non-overlapping 96-timestamp windows.\",\n            \"preprocessing\": [\"Time series normalization, feature extraction\"],\n            \"augmentation\": [\"none (time-series specific)\"],\n            \"visualization\": [\"time series plots, feature importance\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ConvLSTM-Transformer hybrid\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"A combination of Convolutional LSTM and Transformer layers for sequence forecasting\"\n            },\n            \"description\": \"A state-of-the-art model for hourly urban weather prediction, combining ConvLSTM and Transformer blocks to capture short and long-term dependencies.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider model parallelism to handle large input sequences\",\n        \"Custom loss functions for MSLE and MAPE can be used during training\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"rolling window cross-validation, feature engineering\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient serving for time-series data\"],\n        \"demonstration\": \"Forecasting visualizations for different time periods\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is designed based on the given instruction, capturing the user's high level of expertise and intent to build a model. The problem area, task, and dataset specifics are clearly specified, including performance metrics (MSLE and MAPE) and complexities (inference time, memory). The requirements for dataset preprocessing, model architecture, early stopping strategy, and model validation are included, reflecting a well-defined project scope. The confidence score is high due to the clear mapping of the instruction to the JSON structure.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a highly-optimized, real-time LSTM-based energy consumption forecasting model using hourly data for the next 72 hours. Demand not only accuracy but also sub-second inference (<100 milliseconds), necessitating efficient hardware integration and parallel processing techniques for rapid and scalable predictions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy management\",\n        \"description\": \"Develop a real-time, highly-optimized LSTM-based energy consumption forecasting model for the next 72 hours, focusing on sub-second inference (<100 milliseconds) with efficient hardware integration and parallel processing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": 100\n                },\n                \"optimization\": \"sub-second\"\n            },\n            {\n                \"name\": \"hardware integration\",\n                \"value\": \"efficient\",\n                \"unit\": null\n            },\n            {\n                \"name\": \"parallel processing\",\n                \"value\": \"required for rapid and scalable predictions\",\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hourly Energy Consumption Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"energy consumption\"],\n            \"specification\": {\n                \"time_horizon\": 72,\n                \"temporal_frequency\": \"hourly\"\n            },\n            \"description\": \"Dataset containing hourly energy consumption data for the forecasting task.\",\n            \"preprocessing\": [\"resampling to hourly data\", \"feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"LSTM-based Energy Consumption Model\",\n            \"family\": \"LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"real-time\",\n                \"parallel_inference\": true\n            },\n            \"description\": \"A state-of-the-art LSTM model designed for energy consumption forecasting, requiring minimal latency (<100 ms).\"\n        }\n    ],\n    \"knowledge\": [\n        \"The target is real-time forecasting and hardware efficiency.\",\n        \"Parallel processing is crucial for scalability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance computing (HPC), embedded devices (IoT)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"custom optimized for hardware acceleration\"],\n        \"demonstration\": \"Include live, real-time predictions with the model\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response accurately captures the user's intent to build a model, their high expertise level, and the specific requirements for a real-time LSTM model with sub-second inference. The problem area, task, and domain are identified, as well as the metrics and complexity constraints. The dataset is defined with the necessary hourly data and preprocessing steps, and the model's details and service requirements are specified. The confidence score is moderately high due to the clear instruction, but some specific details like target values and additional preprocessing techniques are not provided.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "For the exclusive \"Majestic Lexicon Quest\" project, our team must fine-tune a specialized BERT model, focusing on the RoBERTa-base variant, leveraging the Hierarchical Question-Answering (HQA) subset of SQuAD 2.0. Striving for unparalleled precision, we aim for an F1 score above 93% in the biomedical domain, ensuring efficiency while maintaining a response time constraint of 1.5 milliseconds on a state-of-the-art server infrastructure.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"biomedical\",\n        \"description\": \"Fine-tune a specialized BERT model using the RoBERTa-base variant for the 'Majestic Lexicon Quest' project. The task is to leverage the Hierarchical Question-Answering subset of SQuAD 2.0. The goal is to achieve an F1 score above 93% in the biomedical domain while maintaining a response time of 1.5 milliseconds on a state-of-the-art server infrastructure.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.93\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 1.5,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0 - Hierarchical Question-Answering subset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"biomedical\",\n                \"subset\": \"HQA\"\n            },\n            \"description\": \"A specialized subset of SQuAD 2.0 for fine-tuning a BERT model for hierarchical question answering in the biomedical context.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Specialized BERT (RoBERTa-base)\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"RoBERTa\",\n                \"task\": \"question answering\",\n                \"domain\": \"biomedical\"\n            },\n            \"description\": \"A fine-tuned BERT model optimized for Hierarchical QA in the biomedical domain, targeting precision over 93%.\"\n        }\n    ],\n    \"knowledge\": [\n        \"BERT's ability to handle text-based QA is leveraged for the biomedical domain.\",\n        \"Efficiency is crucial for a response time of 1.5 milliseconds on state-of-the-art infrastructure.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"state-of-the-art server\"\n        ],\n        \"demonstration\": \"Performance on a biomedical server with a 1.5 ms response time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been generated based on the specified task of fine-tuning a RoBERTa-base BERT model for biomedical domain QA with an F1 score goal and response time constraint. The essential elements have been mapped to the JSON structure, and the confidence score is high due to the clarity of the task. Details such as the exact preprocessing steps and model architecture specifics are not provided as they were not specified in the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "The art team recently received an influx of user-generated content through a dedicated in-app feature, capturing intricate lifestyle scenarios utilizing niche product variations. The diverse set of images demand a sophisticated image classifier, differentiating them into high-quality testimonials, requiring product improvement feedback, and unique content for an exclusive 'customer stories' exhibit. The objective is to develop a state-of-the-art machine learning model, trained on a proprietary dataset of 5000+ images with nuanced captions, ensuring 95% accuracy in categorization and seamless integration with our current content management system.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"Develop a state-of-the-art image classifier for user-generated content, differentiating between high-quality testimonials, product improvement feedback, and exclusive 'customer stories' content. The dataset consists of 5000+ images with nuanced captions, targeting 95% accuracy in classification.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"image_category\"],\n            \"specification\": {\n                \"size\": 5000,\n                \"captioned\": true\n            },\n            \"description\": \"A proprietary dataset containing over 5000 user-generated images with detailed captions, designed for image classification tasks.\",\n            \"preprocessing\": [\n                \"caption_to_embedding\"\n            ],\n            \"augmentation\": [\n                \"image_translation\",\n                \"geometric_transformations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art image classifier\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Advanced deep learning architecture\",\n                \"training_details\": {\n                    \"loss_function\": \"Cross-entropy loss\",\n                    \"optimizer\": \"Adam\"\n                }\n            },\n            \"description\": \"A highly sophisticated machine learning model designed to classify intricate lifestyle scenarios and niche product variations with high accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The dataset contains nuanced captions that enrich the learning process for the model.\",\n        \"State-of-the-art models require extensive training on diverse and high-quality data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"caption extraction\", \"pre-processing for classification\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"in-app content management system integration\",\n        \"inference_engine\": [\"TensorFlow serving\", \"real-time image processing\"],\n        \"demonstration\": \"An interactive tool showcasing classification results within the app\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response provides a detailed representation of the user's instruction, capturing their intent to build a high-quality image classifier. The task requirements, including data, model characteristics, and integration, are well-defined, allowing for accurate implementation. However, missing specific model architecture or pre-trained model name, as well as minor details such as preprocessing techniques, leaves room for slight improvement.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "node classification", "instruction": "Develop a state-of-the-art deep learning system for deciphering and classifying neural subtypes in the Canine Hippocampus using a novel fusion of Hierarchical Temporal Convolutional Graph Networks (HTCGN) and Deep Graph Infomax (DGI) on the recently released Feline Cerebral Cortex Connectivity Atlas. The model must not only achieve exceptional AUC-ROC and F1 scores, but also demonstrate interpretability by visualizing influential node interactions and adapting to dynamically changing brain connectivity patterns in real-time.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Develop a state-of-the-art deep learning system for classifying neural subtypes in the Canine Hippocampus, using a fusion of Hierarchical Temporal Convolutional Graph Networks (HTCGN) and Deep Graph Infomax (DGI). The model should achieve exceptional AUC-ROC and F1 scores, and also provide interpretability by visualizing influential node interactions, and adapt to real-time changes in brain connectivity patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Feline Cerebral Cortex Connectivity Atlas\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"recent release\",\n                \"connectivity_type\": \"novel fusion\"\n            },\n            \"description\": \"A novel dataset for neural subtype classification using HTCGN and DGI in the Canine Hippocampus, based on the Feline Cerebral Cortex Connectivity Atlas.\",\n            \"preprocessing\": [\"novel fusion technique\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"influential node interactions visualization\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HTCGN and DGI Fusion\",\n            \"family\": \"Hierarchical Temporal Convolutional Graph Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"visualizing node interactions\",\n                \"adaptivity\": \"real-time brain connectivity patterns\"\n            },\n            \"description\": \"A deep learning system combining HTCGN and DGI for neural subtype classification, designed for the Canine Hippocampus with emphasis on interpretability and real-time adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Fusing HTCGN and DGI in a deep learning system is cutting-edge for neural subtype classification.\",\n        \"Innovative model requirements for interpretability and real-time adaptability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"model interpretability and real-time performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is well-formatted based on the schema, with relevant information extracted from the instruction. The user's intent to build a model, high expertise level, and the problem's research area are clear. The model is specified as a fusion of HTCGN and DGI, with requirements for AUC-ROC, F1 scores, interpretability, and real-time adaptation. However, the exact performance metrics values were not provided, so they are set to null. The source of the dataset is expected to be a link provided by the user. Some assumptions, like the user's link to the dataset and novel fusion technique, are made due to missing specific details in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As a machine learning project manager, you require the development team to deploy an unsupervised time-series anomaly detection system using at least three distinct lightweight transformer models (e.g., EfficientTS, TinyTimeformer, and MiniTrans). Each model must surpass 70% average accuracy on the diverse 250 Hexagon ML/UCR datasets, and the overall performance should exhibit low computational overhead for real-time industrial monitoring.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"industrial monitoring\",\n        \"description\": \"Deploy an unsupervised time-series anomaly detection system using at least three lightweight transformer models, specifically EfficientTS, TinyTimeformer, and MiniTrans. The system must achieve an average accuracy of over 70% on the 250 Hexagon ML/UCR datasets and demonstrate low computational overhead for real-time industrial monitoring purposes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average accuracy\",\n                \"value\": 0.70\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational overhead\",\n                \"value\": null,\n                \"unit\": \"real-time response latency\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [],\n            \"target_variables\": [],\n            \"specification\": {\n                \"datasets\": [\"Hexagon ML/UCR\"],\n                \"number_of_datasets\": 250,\n                \"data_distribution\": \"diverse\"\n            },\n            \"description\": \"A collection of 250 diverse datasets from the Hexagon ML/UCR repository for time-series anomaly detection benchmarking.\",\n            \"preprocessing\": [\"unsupervised pre-processing\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model validation curves\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientTS\",\n            \"family\": \"Lightweight Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A lightweight transformer model for time-series analysis.\"\n        },\n        {\n            \"name\": \"TinyTimeformer\",\n            \"family\": \"Lightweight Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"Another lightweight transformer model designed for efficient time-series tasks.\"\n        },\n        {\n            \"name\": \"MiniTrans\",\n            \"family\": \"Lightweight Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A smaller transformer model optimized for real-time industrial monitoring.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Using at least three distinct transformer models to improve anomaly detection robustness.\",\n        \"Focusing on lightweight models for low computational overhead in industrial settings.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": null,\n        \"target_device\": \"real-time industrial devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Real-time inference, low-latency processing\"],\n        \"demonstration\": \"Anomaly detection results and system performance on real-time monitoring\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the project requirements as specified. The user's intent to build an anomaly detection system, high expertise level, and domain focus (industrial monitoring) are captured. The performance metrics (average accuracy and computational overhead) are clearly defined. The use of multiple lightweight transformer models and the benchmark datasets are appropriately outlined. However, specific model specifications are not provided and should be included based on actual model evaluations. The confidence score is high due to the structure and clarity of the parsed data.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for the elite ML development team, you need to design a cutting-edge, mobile-friendly QA model based on TinyBERT. Target the NewsQA dataset while maintaining state-of-the-art accuracy of above 90%, with an emphasis on optimizing efficiency, achieving sub-200 ms response times, and ensuring compatibility across various device platforms with limited computing resources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Design a cutting-edge QA model based on TinyBERT, targeting the NewsQA dataset. The model should achieve state-of-the-art accuracy above 90%, with a focus on efficiency, having sub-200 ms response times, and ensuring compatibility across mobile devices with limited computing resources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 200,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NewsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"not specified\",\n                \"format\": \"suitable for TinyBERT\"\n            },\n            \"description\": \"A natural language processing dataset for question answering, focusing on news articles.\",\n            \"preprocessing\": [\"TinyBERT-compatible data preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": \"mobile-friendly\",\n                \"training_time\": \"optimized\",\n                \"model_size\": \"lightweight\"\n            },\n            \"description\": \"A cutting-edge, TinyBERT-based QA model designed for efficient, high-accuracy question answering on mobile devices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"TinyBERT's efficiency on mobile devices is crucial for this project.\",\n        \"Compatibility across multiple platforms is achieved through device-agnostic optimization.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Mobile-specific feature extraction\"],\n        \"target_device\": [\"mobile\", \"IoT\", \"limited computing resources\"],\n        \"deployment_endpoint\": \"cross-platform compatible\",\n        \"inference_engine\": [\"TensorFlow Lite\", \"PyTorch on low-end devices\"],\n        \"demonstration\": \"Mobile app demo showcasing real-time QA performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction, using the template structure. The user's intent to build a model, expertise level, and problem area are clear from the context. The NewsQA dataset and TinyBERT are specified. Performance metrics (accuracy) and efficiency requirements (sub-200 ms response times) are included. However, the exact model size and specific preprocessing steps are not provided as they may vary depending on the initial implementation details.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "Utilize the extensive 'Worldwide Hotel Trends and User Behaviors' dataset, combining historical and real-time data, to design a sophisticated model. The system must employ Hierarchical Graph Attention Networks to capture spatial-temporal patterns in user movements. Demand not only predictive accuracy but also a highly efficient model with real-time adaptive recommendations for personalized in-app experience, ensuring seamless user engagement and minimal latency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"travel and hospitality\",\n        \"description\": \"Design a sophisticated model using the 'Worldwide Hotel Trends and User Behaviors' dataset that combines historical and real-time data. The model should employ Hierarchical Graph Attention Networks for capturing spatial-temporal patterns in user movements and provide real-time adaptive recommendations for personalized in-app experience with minimal latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"predictive accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"minimal\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model efficiency\",\n                \"value\": {\n                    \"efficiency\": \"high\",\n                    \"real-time\": true\n                },\n                \"unit\": \"computational resources\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Worldwide Hotel Trends and User Behaviors\",\n            \"modality\": [\"time series\", \"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_types\": [\"historical\", \"real-time\"],\n                \"data_sources\": [\"user movements\"],\n                \"spatial_temporal_dimensions\": [\"global\", \"real-time\"]\n            },\n            \"description\": \"A dataset combining historical and real-time data on hotel trends and user behaviors for capturing sophisticated spatial-temporal patterns.\",\n            \"preprocessing\": [\n                \"spatiotemporal feature extraction\",\n                \"data fusion\"\n            ],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [\"data exploration and insights\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hierarchical Graph Attention Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"spatial-temporal graph model\",\n                \"attention mechanism\": \"dynamic\"\n            },\n            \"description\": \"A neural network model designed to capture user movements' spatial-temporal patterns using HGANs, aiming for real-time adaptive recommendations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The combination of historical and real-time data is crucial for dynamic recommendations.\",\n        \"Hierarchical graph structures aid in understanding user movement patterns across time.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"personalized features\"],\n        \"target_device\": \"mobile, cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time streaming inferencing\"],\n        \"demonstration\": \"A user-centric in-app experience with seamless recommendations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response adheres to the schema and reflects the user's intent to build a model, their high expertise, and the details provided in the instruction. The area of recommendation systems and the task of sequential recommendation are aligned with the dataset and model. Performance metrics such as predictive accuracy and latency, along with efficiency in real-time, have been specified. The dataset includes historical and real-time data fusion and specific preprocessing steps. The model type and its attention mechanism have been clearly described. The service requirements for personalized, real-time experience are incorporated.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series classification", "instruction": "Develop a deep neural network integrated with DTW, for the complex Sign Language Time Series dataset, which not only necessitates high precision and recall but also differentiates between nuanced hand gestures with real-time temporal adaptation. Optimize the model for low latency and robustness, ensuring user-friendly performance for seamless communication between the hearing impaired and the sign language proficient.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \" accessibility\",\n        \"description\": \"Develop a deep neural network integrated with Dynamic Time Warping (DTW) for the Sign Language Time Series dataset. The model should have high precision and recall, support nuanced gesture differentiation, and be optimized for low latency and robustness to ensure real-time and user-friendly communication between the hearing impaired and sign language proficient.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sign Language Time Series\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dimensionality\": null,\n                \"length\": null,\n                \"time_resolution\": null\n            },\n            \"description\": \"A dataset of sign language time series that requires nuanced gesture recognition with real-time temporal adaptation\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"time warping\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep Neural Network with DTW Integration\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": null,\n                \"parameters\": {\n                    \"depth\": null,\n                    \"hidden_units\": null\n                },\n                \"optimization\": {\n                    \"optimization_algorithm\": null,\n                    \"learning_rate\": null\n                }\n            },\n            \"description\": \"A deep neural network architecture combining DTW for sign language time series analysis with emphasis on high precision, recall, low latency, and robustness\"\n        }\n    ],\n    \"knowledge\": [\n        \"Sign Language Time Series requires a model that can handle complex time series data with subtle gesture distinctions.\",\n        \"DTW's real-time temporal adaptation is crucial for seamless communication.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"low-latency and user-friendly\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": \"live demonstration of seamless communication\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key details from the instruction, including the high-level intent (building a model), problem area (time-series analysis), and user's expertise level. Performance metrics like precision and recall are expected to be high, but their values are not specified since they are typically dependent on the model's performance. The low-latency requirement is reflected in the complexity metrics and target device. The dataset's preprocessing and augmentation methods are listed based on the description provided. The deep neural network with DTW integration is a suitable choice based on the user's requirement for nuanced gesture recognition and real-time adaptability.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "As a machine learning project manager for a biomedical sentiment analysis team, you must fine-tune a state-of-the-art transformer model using the WebMD Review dataset. Ensure interpretability by implementing explainable AI techniques, while maintaining a minimum F1-score of 0.9 and a balanced positive-negative ratio in the training set to enhance model robustness.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"biomedical\",\n        \"description\": \"Fine-tune a state-of-the-art transformer model for biomedical sentiment analysis using the WebMD Review dataset. Incorporate explainable AI techniques and maintain a minimum F1-score of 0.9 with a balanced positive-negative ratio in the training set for improved robustness.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": \"required\",\n                \"unit\": \"yes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WebMD Review Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\"],\n            \"specification\": null,\n            \"description\": \"A dataset for biomedical sentiment analysis, containing web-based medical reviews.\",\n            \"preprocessing\": [\n                \"balance positive-negative ratio\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"State-of-the-art transformer model (e.g., BERT, RoBERTa)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"implemented\"\n            },\n            \"description\": \"A fine-tuned transformer model for biomedical sentiment analysis, with a focus on explainability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating explainable AI techniques ensures interpretability for stakeholders.\",\n        \"Balancing the dataset improves model robustness, especially in medical contexts.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"custom explainable AI techniques\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Include a clear explanation of model interpretability features and a model explanation method\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response closely follows the provided instruction, mapping the user's intent (build) and high expertise level. The problem area is NLP, specifically sentiment analysis with a biomedical focus, and the downstream task is text classification. The WebMD Review dataset is correctly identified, and the requirements for interpretability, F1-score, and balanced training set are included. However, missing details like the specific transformer model, training procedure, or interpretability method are expected as they would depend on the team's preference. The confidence score is high, assuming that the instruction is clear enough to guide the response.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Can you develop a real-time predictive model, integrating historical data and external factors, to forecast peak service demand with precision? The model should also automatically adjust for seasonal variations and unexpected events, aiming to minimize staff fluctuations and improve overall operational efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"business operations\",\n        \"description\": \"Develop a real-time predictive model for forecasting peak service demand, integrating historical data and external factors, while considering seasonality and handling unexpected events for staff optimization and improved operational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"response_time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Historical Service Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_range\": \"\",\n                \"data_length\": \"\"\n            },\n            \"description\": \"Time-series data containing historical service demand trends and external factors to be integrated.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"seasonality extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"anomaly detection plots\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Real-time Time-Series Forecasting\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"including external factors\",\n                \"handling Seasonality\": \"yes\"\n            },\n            \"description\": \"A neural network model designed for real-time service demand forecasting, capable of automatic adjustment for seasonal variations and handling unexpected events.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integration of historical and external data is crucial for accurate predictions.\",\n        \"Models need to consider seasonality for realistic forecasting.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection based on real-time data\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"API for real-time predictions\",\n        \"inference_engine\": [\"on-demand\", \"real-time\"],\n        \"demonstration\": \"Live demo showcasing peak demand predictions and staff adjustments.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response closely follows the schema and template, capturing the essence of the instruction. It reflects the user's intent to build a model, high expertise, and the area (time-series analysis) and application domain (business operations). The user's requirement for precision forecasting and real-time processing is indicated, though no specific metric values are provided. The dataset and model expectations are derived from the task, with preprocessing, augmentation, and model specifications mentioned accordingly. The confidence score is high due to a clear understanding of the task, but missing details like the specific model structure or target values are subject to a lower confidence.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "node classification", "instruction": "For the High School Dynamic Contact Network project, design and implement a scalable Temporal Graph Neural Network (TGN) with attention mechanisms. The model must account for evolving ties, node attributes, and temporal hierarchies to accurately classify key actors, track their influence, and analyze emergent social dynamics across multiple academic years. Ensure the system is robust to missing data and can handle large volumes efficiently.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"education\",\n        \"description\": \"For the High School Dynamic Contact Network project, design and implement a scalable Temporal Graph Neural Network (TGN) with attention mechanisms. The model should account for evolving ties, node attributes, and temporal hierarchies for node classification, influence tracking, and social dynamics analysis across multiple academic years. Ensure the system is robust to missing data and handles large volumes efficiently.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness to missing data\",\n                \"value\": null\n            },\n            {\n                \"name\": \"efficiency for large volumes\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High School Dynamic Contact Network\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [\"actor type\", \"influence score\"],\n            \"specification\": {\n                \"node attributes\": [\"node attributes\"],\n                \"edge attributes\": [\"temporal relationships\"],\n                \"time resolution\": \"academic years\"\n            },\n            \"description\": \"A dataset containing dynamic contact network data with evolving ties, node attributes, and timestamps over multiple academic years.\",\n            \"preprocessing\": [\"data cleaning, data normalization\"],\n            \"augmentation\": [\"temporal data augmentation\"],\n            \"visualization\": [\"temporal node/edge visualization\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Scalable TGN with Attention\",\n            \"family\": \"Temporal Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"attention-based\",\n                \"scalability techniques\": [\"efficient inference\", \"parallel processing\"]\n            },\n            \"description\": \"A deep learning model employing TGN with attention mechanisms specifically designed for the High School Dynamic Contact Network project.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Temporal hierarchies and attention mechanisms enhance the model's ability to capture dynamic social patterns.\",\n        \"Robustness to missing data requires imputation techniques and noise-resistant algorithms.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature engineering for node attributes\"],\n        \"target_device\": \"high-performance computing infrastructure\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Distributed computing\"],\n        \"demonstration\": \"interactive dashboard showcasing model performance and network analytics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the complexities of the High School Dynamic Contact Network project with a scalable TGN. The area, downstream tasks, and the user's high expertise level are clear from the instruction. Performance metrics like accuracy and efficiency are expected but not directly stated, so their values are set to null. The model type, its family, and specific architecture are derived from the instruction. The TGN's attention mechanisms and scalability are emphasized. However, some missing aspects include the exact expected accuracy target and specific feature engineering details.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For a cutting-edge urban greening initiative, develop a specialized SegNet++ model, utilizing the challenging Vaihingen dataset with high-resolution images capturing diverse vegetation species, including subtle variations in health and adaptation to microclimates. The model should demonstrate robust performance in low light, dawn, and twilight scenarios, while incorporating real-time seasonal adjustments, to guide precision urban forestry policies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"Develop a specialized SegNet++ model for a cutting-edge urban greening project using the Vaihingen dataset with high-resolution images showcasing diverse vegetation species and subtle variations in health and adaptation to microclimates. The model must perform robustly in low light, dawn, and twilight conditions, and incorporate real-time seasonal adjustments for precision urban forestry policies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean Intersection over Union (IoU)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity (low light, dawn, twilight)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen\",\n            \"modality\": [\"image\", \"multimodal\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": {\n                    \"width\": \"high\",\n                    \"height\": \"high\"\n                },\n                \"data_distribution\": \"challenging with diverse vegetation and microclimate variations\"\n            },\n            \"description\": \"A high-resolution dataset capturing diverse vegetation species and their variations in health and adaptation to microclimates, suitable for low light, dawn, and twilight scenarios.\",\n            \"preprocessing\": [\"data augmentation for low light, dawn, and twilight\"],\n            \"augmentation\": [\"brightness adjustment\", \"color shifts\", \"noise injection\"],\n            \"visualization\": [\"high-resolution vegetation segmentation maps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SegNet++\",\n            \"family\": \"convolutional neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specialized for urban greening with real-time seasonal adjustments\",\n                \"adaptability\": \"low light, dawn, and twilight conditions\"\n            },\n            \"description\": \"A deep learning model designed for image segmentation, specifically tailored to the Vaihingen dataset and its requirements for precision urban forestry policies.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Use of high-resolution images and advanced segmentation methods like SegNet++ are crucial for capturing fine details in vegetation health and microclimates.\",\n        \"Incorporating seasonal adjustments is essential for adapting to varying conditions in an urban greening project.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"extracting features for low light, dawn, and twilight\"],\n        \"target_device\": \"optimized for real-time applications\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-based for faster processing\"],\n        \"demonstration\": \"model-generated segmentation maps and adaptive performance visuals\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intention to build a model (SegNet++), the required performance and conditions, the specific dataset and its modalities, and the target application domain. The performance metrics, complexity metrics, and required model features are suitably detailed, reflecting a thorough understanding of the instruction. The confidence is high, though exact metric values would be needed for higher accuracy, and some preprocessing steps are suggested based on the task.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "For our upcoming machine learning initiative, we need the project manager to direct a team in designing an advanced image classifier based on the Fashion-MNIST dataset. The focus should be on implementing Capsule Networks with a rigorous focus on achieving state-of-the-art accuracy of at least 95%, while maintaining exceptional resilience to affine transformations. In addition, the project must include a comprehensive analysis comparing the performance, efficiency, and interpretability of Capsule Networks against conventional Convolutional Neural Networks (CNNs), and discuss potential architectural enhancements. The report must showcase a deeper understanding of the theoretical foundations and practical implications of using Capsule Networks for this task.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion technology\",\n        \"description\": \"Design an advanced image classifier using Capsule Networks on the Fashion-MNIST dataset. The focus should be on achieving state-of-the-art accuracy (at least 95%) while maintaining resilience to affine transformations. The project must include a comparative analysis with Convolutional Neural Networks (CNNs), discussing the theoretical foundations and practical implications of Capsule Networks for this task.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": \"28x28\",\n                \"number_of_classes\": 10\n            },\n            \"description\": \"A fashion industry-oriented image dataset suitable for advanced image classification tasks, focusing on resilience against affine transformations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Capsule Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"capsule networks\",\n                \"resilience\": \"affine transformations\"\n            },\n            \"description\": \"A state-of-the-art image classifier built on Capsule Networks, designed for the Fashion-MNIST dataset with targeted performance of 95% accuracy.\"\n        },\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"comparison\": \"with Capsule Networks\"\n            },\n            \"description\": \"A reference Convolutional Neural Network for performance comparison and discussion in the project.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Understanding of Capsule Networks' theoretical foundations and practical implications for image classification tasks.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"The analysis and comparison must be presented in a clear, interpretable report.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the user's intention to build an advanced image classifier, their expertise level, the focus on Capsule Networks and state-of-the-art accuracy, along with the dataset and model requirements. It also accounts for the need for comparison and analysis. However, specific model details and certain complexity metrics are left unspecified, expecting the project manager to fill in these details based on the project requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a machine learning project manager, your challenge is to develop an advanced time-series forecasting model for the Electricity dataset, which requires intricate handling of a complex, hierarchical partitioning into train (80%), validation (10%), and test sets (10%). The dataset boasts a high-dimensional input sequence (INPUT_SEQ_LEN=96, INPUT_DIM=321), necessitating effective feature extraction and dimensionality reduction techniques. Your objective is not only to achieve state-of-the-art performance in predicting future sequences (PRED_SEQ_LEN=96, PRED_DIM=321) but also to minimize mean squared error (MSE) and mean absolute error (MAE) with precision and interpretability. Implement an innovative deep learning architecture, incorporate regularization, and compare it with multiple baselines to demonstrate superior generalization capabilities.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy or utilities\",\n        \"description\": \"Develop an advanced time-series forecasting model for the Electricity dataset with a complex, hierarchical partitioning. Focus on handling a high-dimensional input sequence (INPUT_SEQ_LEN=96, INPUT_DIM=321) and employ effective feature extraction and dimensionality reduction techniques. Aim for state-of-the-art performance, minimize MSE and MAE, and ensure precision and interpretability. Use innovative deep learning architecture, regularization, and compare it with multiple baselines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321\n            },\n            \"description\": \"A high-dimensional time series dataset requiring complex hierarchical partitioning for training, validation, and testing.\",\n            \"preprocessing\": [\n                \"dimensionality reduction techniques\",\n                \"feature extraction for high-dimensional inputs\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"architecture\": \"innovative, unspecified\",\n                \"regularization\": \"incorporated\"\n            },\n            \"description\": \"A deep learning model specifically designed for time-series forecasting, addressing the high input dimensionality and hierarchical partitioning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using techniques such as LSTM or Transformers for handling long sequences and hierarchical data.\",\n        \"Incorporating regularization helps prevent overfitting.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"dimensionality reduction techniques\",\n            \"feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"comparison with multiple baselines and visualizations of performance metrics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response parses the user's instruction into a JSON structure, reflecting the need for a high-level project manager, the area of time-series analysis, the advanced time-series forecasting task, and the specific dataset requirements, such as input dimensions and dataset partitioning. Performance metrics (MSE and MAE) are not provided with specific target values, so they are marked as null. The user's request for precision, interpretability, and comparison with baselines is captured in the 'description' field. The missing specific deep learning architecture is expected to be filled by a domain expert, as indicated by the 'expertise' level.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "As a video analysis specialist, you're interested in applying advanced MViT (Multimodal Vision Transformer) models for fine-grained forgery detection in the Celeb-DF dataset, specifically targeting face swapping and deepfake detection. Can you design an architecture that outperforms existing state-of-the-art algorithms by at least 5% in terms of precision and AUC-ROC? Include benchmarking against recent SOTA techniques in your proposal.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"cybersecurity\",\n        \"description\": \"Design a state-of-the-art MViT-based architecture for fine-grained forgery detection in the Celeb-DF dataset, with a focus on face swapping and deepfake detection. The goal is to surpass existing SOTA techniques by at least 5% in precision and AUC-ROC.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": 1.05\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\"video\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"specific_split\": \"fine-grained forgery detection\",\n                \"data_types\": [\"face swapping\", \"deepfake\"]\n            },\n            \"description\": \"Multimodal dataset for detecting face swapping and deepfakes in celebrity faces, designed for MViT models.\",\n            \"preprocessing\": [\n                \"face detection\",\n                \"feature extraction\"\n            ],\n            \"augmentation\": [\n                \"temporal augmentation\",\n                \"geometric transformations\"\n            ],\n            \"visualization\": [\"model input and output examples\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced MViT\",\n            \"family\": \"Multimodal Vision Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"custom MViT with fine-grained forgery detection enhancements\",\n                \"performance comparison\": \"outperforms SOTA with 5% improvement in precision and AUC-ROC\"\n            },\n            \"description\": \"A state-of-the-art MViT model specifically designed for fine-grained forgery detection, targeting face swapping and deepfake detection in the Celeb-DF dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"MViT models excel in analyzing multimodal data, providing unique insights for forgery detection.\",\n        \"Achieving a 5% improvement in precision and AUC-ROC is a challenging but achievable benchmark.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"targeted feature extraction for forgery patterns\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for video and image input\"\n        ],\n        \"demonstration\": \"comparing results with baseline SOTA techniques\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent, focusing on building a state-of-the-art MViT model for forgery detection. The user's expertise level and the task's domain are appropriately included. Performance metrics are clearly specified with target improvements. The Celeb-DF dataset and its relevant processing are described. The architecture and benchmarking against SOTA are well-justified within the response.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "text classification", "instruction": "You're now required to develop a robust text classification model using the BoolQ dataset, focusing on enhancing interpretability by integrating feature importance analysis. Ensure the model not only achieves high accuracy but also demonstrates clear reasoning behind its predictions, with a target F1-score of at least 0.9 and a BERT-base model fine-tuned with a limited training budget of 5 epochs.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Develop a robust and interpretable text classification model using the BoolQ dataset. Focus on feature importance analysis while maintaining high accuracy with a target F1-score of at least 0.9 and a limited training budget of 5 epochs.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": \"limited\",\n                \"training_epochs\": 5\n            },\n            \"description\": \"A dataset for text classification, emphasizing interpretability with feature importance analysis.\",\n            \"preprocessing\": [\"BERT fine-tuning\"],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"base_model\": \"BERT-base\",\n                \"training_budget\": 5\n            },\n            \"description\": \"A fine-tuned BERT model with enhanced interpretability, specifically designed for text classification on the BoolQ dataset with a 5-epoch training constraint.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Feature importance analysis is essential for interpretability.\",\n        \"Limited training budget requires efficient model performance under constraints\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"clear reasoning behind predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a model, their high level of expertise, the natural language processing domain, and the specific requirements of the text classification task on the BoolQ dataset. The performance metric of F1-score with a target value of 0.9 is clearly specified, as is the use of a BERT-base model with a training budget of 5 epochs. The dataset's source and the feature importance analysis are included based on the instruction. However, it's assumed that the user wants to achieve high accuracy (but not the exact value) which is not specified here. The confidence score is set to 0.9 based on the clear instruction and the well-formatted structure.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As a project manager, you're overseeing a sophisticated machine learning project for time-series forecasting. The ILI dataset, a real-world collection of time-stamped data split into training (80%), validation (10%), and testing (10%) subsets, presents sequences of past observations (INPUT_SEQ_LEN=36, 7-dimensional) for analysis. The objective is to develop a multi-layered predictive model that forecasts the subsequent 24-step sequence (PRED_SEQ_LEN=24, PRED_DIM=7) with precision, using advanced techniques like Seasonality Decomposition and ARIMA. Implement multiple feature engineering steps and optimize the model using GridSearchCV for optimal hyperparameters, ensuring to report both mean squared error (MSE) and mean absolute error (MAE) for comprehensive performance evaluation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"public health\",\n        \"description\": \"A sophisticated machine learning project for time-series forecasting using the ILI dataset. The dataset is split into training, validation, and testing subsets with 7-dimensional, 36-step input sequences and a goal of forecasting the next 24-step sequence with advanced techniques like Seasonality Decomposition and ARIMA. The project requires multiple feature engineering steps and optimization using GridSearchCV for optimal hyperparameters. Performance is to be evaluated using mean squared error (MSE) and mean absolute error (MAE).\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 36,\n                \"DIM\": 7,\n                \"split_ratios\": [\"80%\", \"10%\", \"10%\"]\n            },\n            \"description\": \"A real-world time-stamped dataset with 36-step input sequences and 7 dimensions, split into training, validation, and testing subsets.\",\n            \"preprocessing\": [\"Seasonality Decomposition\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks (deep learning)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-layered, includes ARIMA components\"\n            },\n            \"description\": \"A deep learning model with ARIMA components for time-series forecasting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced techniques like Seasonality Decomposition and ARIMA are key components of the model.\",\n        \"GridSearchCV for hyperparameter optimization is crucial to achieve high precision.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"detrending\",\n            \"rolling averages\",\n            \"lag features\",\n            \"seasonality analysis\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"auto\"],\n        \"demonstration\": \"Include predicted sequences and error metrics in visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction, reflecting the project's high-level requirements such as the dataset, task, and performance metrics. The user's expertise is assumed to be high as a project manager overseeing a sophisticated project. The model family is inferred as neural networks (deep learning), but the specific model architecture is left unspecified for optimization. The GridSearchCV and feature engineering steps are clearly indicated. The confidence score is high, but may need to be lower if there's ambiguity in the provided requirements.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "As a marine surveillance system, we seek a tailored state-of-the-art U-Net model with a fusion of Sentinel-1 and Sentinel-2 satellite imagery for enhanced vessel detection in complex coastal scenes. The model should discriminate among types (including vessel classes like container ships, tankers, and sailboats), estimate dimensions (length and beam), and estimate relative headings under various weather patterns like storms and fog. Achieve an impressive 95% F1-score within 1 second per frame for offloading on AWS's P4 instances, ensuring optimal balance between speed and accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"marine surveillance\",\n        \"description\": \"Seek a state-of-the-art U-Net model for marine vessel detection in complex coastal scenes using Sentinel-1 and Sentinel-2 satellite imagery. The model should differentiate between vessel classes (e.g., container ships, tankers, sailboats), estimate dimensions (length and beam), and predict relative headings. Target is a 95% F1-score, with a real-time requirement of 1 second per frame for offloading on AWS P4 instances, optimizing speed and accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 1.0,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-1 and Sentinel-2 Fusion\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vessel type\", \"length\", \"beam\", \"relative heading\"],\n            \"specification\": {\n                \"image_modalities\": [\"satellite\", \"multi-spectral\"],\n                \"image_size\": \"variable\",\n                \"data_distribution\": [\"complex coastal scenes\", \"weather patterns (storms, fog)\"]\n            },\n            \"description\": \"Satellite imagery fusion for marine vessel detection and characterization\",\n            \"preprocessing\": [\"fusion\", \"resampling\"],\n            \"augmentation\": [\"data augmentation for weather variations\"],\n            \"visualization\": [\"model performance on coastal scenes\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom U-Net\",\n            \"family\": \"Semantic Segmentation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"U-Net\",\n                \"backbone\": \"state-of-the-art\",\n                \"fusion_method\": \"multi-modal fusion\",\n                \"target_platform\": \"AWS P4 instances\"\n            },\n            \"description\": \"A real-time U-Net designed for Sentinel-1 and Sentinel-2 imagery for enhanced vessel detection and attribute estimation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time operation and resource constraints on AWS P4 instances\",\n        \"Impact of weather patterns on model performance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"image preprocessing and enhancement\"],\n        \"target_device\": \"AWS P4 instances\",\n        \"deployment_endpoint\": \"AWS Object Detection API\",\n        \"inference_engine\": [\"AWS Inferentia or optimized GPU for real-time inference\"],\n        \"demonstration\": \"Enhanced vessel detection visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a model, the U-Net architecture requirement, and the user's expertise level. The problem area, downstream task, and details are derived from the instruction. The metrics (F1-score and inference time) and the requirements for achieving optimal balance are clear. The dataset, model, and service sections reflect the details provided in the instruction. However, some specifications are not provided, such as the model's precise specifications or the dataset's exact size, as these weren't part of the provided instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager must design a high-performance e-commerce translation system, employing the cutting-edge U-Net architecture. Targeting not just general Chinese to English translation, but specifically tailored to the intricate nuances of Taobao's product catalog in retail and fashion. The model must meet stringent standards, boasting a BLEU score of 40 or above, and exhibit expertise in preserving product jargon and promotional language. Additionally, the system must be scalable and future-proof, seamlessly integrating with diverse e-commerce platforms for real-time, context-aware translation of millions of product listings, ensuring a seamless global customer experience without sacrificing conversions or brand consistency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Design a high-performance e-commerce translation system using the U-Net architecture, focused on Chinese to English translation, specifically for Taobao's retail and fashion product catalog. The system must achieve a BLEU score of 40 or above, preserve product jargon and promotional language, and be scalable and future-proof, supporting real-time, context-aware translation for millions of product listings across diverse platforms.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 40\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"product descriptions\"],\n            \"specification\": {\n                \"domain\": \"Taobao retail and fashion catalog\",\n                \"language_pair\": [\"Chinese\", \"English\"]\n            },\n            \"description\": \"A domain-specific corpus of Chinese and English product descriptions from Taobao's retail and fashion categories.\",\n            \"preprocessing\": [\n                \"language preprocessing\",\n                \"genre-specific normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net-based E-commerce Translation Model\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"U-Net\",\n                \"target_corpus\": \"Taobao product catalog\",\n                \"focus_areas\": [\"product jargon\", \"promotional language\"]\n            },\n            \"description\": \"A specialized U-Net architecture designed to handle translation tasks in the e-commerce context with a focus on Taobao product translation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cutting-edge U-Net architecture for enhanced performance\",\n        \"Strategic focus on preserving product-specific language\",\n        \"Scalability and future-proof design\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual understanding\",\n            \"real-time adaptation\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"context-aware translation in action\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the requirements stated in the instruction, including the e-commerce application, U-Net architecture, target BLEU score, and focus on product jargon and promotional language preservation. The dataset and model specifics were derived from the context, while allowing for inference from the given details. However, exact model specification details and preprocessing steps might be missing due to the vague nature of the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project lead for a cutting-edge machine learning team, your mandate is to design a sophisticated deep-learning architecture that combines a state-of-the-art ConvLSTM-Transformer fusion model, specifically tailored for ultra-precise hourly multivariate urban climate prognosis. Utilizing the comprehensive UrbanWeather v2.1 dataset, which now comprises an expanded array of 25 complex meteorological variables (expanding INPUT_SEQ_LEN to 112 to accommodate additional data), the model must forecast a challenging 192-hour weather forecast horizon (up from PRED_SEQ_LEN=96). The model's performance must be evaluated on two crucial metrics: mean squared logarithmic error with a customized penalty for extreme values (MSLE+) and a novel MAPE variant focusing on percentile error bands (MAPE-P). Implement an advanced early stopping mechanism incorporating auto-adaptive validation frequency and nested rolling window cross-validation to ensure optimal generalization and adaptability across various seasons and microclimates. Additionally, devise a strategy for real-time model updating using transfer learning and continuous monitoring of external climate impact factors.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate science\",\n        \"description\": \"Design a deep-learning architecture combining ConvLSTM-Transformer fusion for ultra-precise hourly multivariate urban climate prognosis, using the UrbanWeather v2.1 dataset with 25 meteorological variables. The model should forecast a 192-hour horizon with custom metrics MSLE+ and a novel MAPE-P. Implement advanced early stopping and real-time model updating via transfer learning and continuous monitoring of external climate factors.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSLE+\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE-P\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UrbanWeather v2.1\",\n            \"modality\": [\"time series\", \"multimodal\"],\n            \"target_variables\": [\"weather variables\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 112,\n                \"PRED_SEQ_LEN\": 192\n            },\n            \"description\": \"Comprehensive urban weather dataset with an expanded set of 25 meteorological variables, suitable for 192-hour forecasting.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ConvLSTM-Transformer Fusion\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"architecture_description\": \"Combines state-of-the-art ConvLSTM and Transformer layers for climate prognosis.\"\n                },\n                \"hyperparameters\": {\n                    \"input_shape\": \"[None, 112, num_variables]\",\n                    \"forecast_horizon\": 192,\n                    \"custom_metrics\": [\n                        {\n                            \"name\": \"MSLE+\",\n                            \"penalty_for_extreme_values\": true\n                        },\n                        {\n                            \"name\": \"MAPE-P\"\n                        }\n                    ]\n                }\n            },\n            \"description\": \"A deep learning model designed for multi-modal, ultra-precise 192-hour urban climate prognosis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating ConvLSTM and Transformers for handling time-varying and sequence data in weather forecasting.\",\n        \"Utilizing MSLE+ for robustness against extreme values and MAPE-P for percentile accuracy.\",\n        \"Early stopping with auto-adaptive validation and nested rolling window cross-validation for generalization across seasons and microclimates.\",\n        \"Real-time model updates and continuous monitoring of external factors through transfer learning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Adaptive data preprocessing to handle new meteorological variables\",\n            \"Transfer learning for quick updates\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Creation of detailed, interactive forecasts to showcase results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction indicates a complex and sophisticated project with high technical requirements. I have parsed the information accurately based on the schema, reflecting the problem area, dataset expansion, target task, and advanced performance metrics. The need for transfer learning and real-time updates is also captured. However, specific details on auto-adaptive validation frequency and exact hyperparameter values are not provided, making the confidence score slightly lower.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "For a vertically integrated energy and chemicals giant, design an advanced QA system using a tailored Salsbury Oils enhanced TriviaQA. Implement a cutting-edge BERT fusion model enhanced with domain-specific knowledge from detailed process plant complexities and specialized drilling trivia. Ensure real-time (<20 ms) response with automatic scaling for peak loads of 20,000 simultaneous users, while maintaining 99.99% uptime during extreme conditions involving hazardous operations and fluctuating weather resilience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"industry (energy and chemicals)\",\n        \"description\": \"Design an advanced QA system for a vertically integrated energy and chemicals company using Salsbury Oils enhanced TriviaQA. Incorporate a BERT fusion model enhanced with domain-specific knowledge from process plant complexities and drilling trivia. Target real-time response times (<20 ms), automatic scaling for 20,000 simultaneous users, and 99.99% uptime during hazardous operations and weather fluctuations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 20\n            },\n            {\n                \"name\": \"mean time to respond\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response latency (average)\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"user concurrency support\",\n                \"value\": 20000,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"uptime during extreme conditions\",\n                \"value\": 0.9999,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Salsbury Oils enhanced TriviaQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A tailored question-answering dataset for the energy and chemicals industry with process plant and drilling trivia.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT Fusion Model (enhanced with domain knowledge)\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT Fusion\",\n                \"domain_knowledge_integration\": true,\n                \"specific_models\": [\"BERT\", \"domain-specific model\"]\n            },\n            \"description\": \"A BERT-based QA model customized for an energy and chemicals company's context, integrating process plant and drilling domain-specific knowledge.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating domain-specific BERT knowledge for plant complexities and drilling trivia.\",\n        \"Support for real-time and scalable response during peak loads.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\", \"GPU-accelerated\"],\n        \"demonstration\": \"automated scaling and response-time demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the provided instruction, including the area of NLP (QA system), domain specifics, the targeted model (BERT fusion with domain knowledge), and the performance metrics for real-time response and scalability. The expert level is set to high due to the requirement for advanced QA system. The response time is specified as <20ms, but specifics on preprocessing and model architecture customization are not provided as these might require additional information. The confidence score is high based on the clarity of the task but may be lower due to the lack of granular details.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "For the intricate Enron Email dataset, design a specialized RGCN architecture incorporating homomorphic encryption for multi-level secure communication. The model should employ hierarchical clustering and fine-grained job role taxonomy to achieve >98% precision in distinguishing executives, engineers, and support staff. Mandate the model to deliver real-time analysis with_tensorpipe_ communication on a distributed Kubernetes cluster, guaranteeing N+1 fault tolerance, <10ms latency, and auto-scaling within a microservices infrastructure. Ensure seamless operation for datasets expanding to exabytes daily, accommodating massive growth with minimal performance degradation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Design a specialized RGCN architecture with homomorphic encryption for multi-level secure communication on the Enron Email dataset. The model should employ hierarchical clustering and fine-grained job role taxonomy to achieve >98% precision in differentiating executives, engineers, and support staff.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time analysis\",\n                \"value\": null, \n                \"unit\": \"enabled\"\n            },\n            {\n                \"name\": \"N+1 fault tolerance\",\n                \"value\": 1, \n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 10,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"auto-scaling\",\n                \"value\": \"yes\"\n            }\n        ],\n        \"data_size_requirement\": {\n            \"growth_type\": \"exabytes\",\n            \"minimal_performance_degradation\": \"yes\"\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"job roles\"],\n            \"specification\": null,\n            \"description\": \"A large graph dataset containing email communication data for executives, engineers, and support staff in the Enron organization.\",\n            \"preprocessing\": [\"homomorphic encryption\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Specialized RGCN\",\n            \"family\": \"Relational Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"encryption\": \"homomorphic\",\n                \"hierarchical_clustering\": \"yes\",\n                \"taxonomy\": \"fine-grained job role\"\n            },\n            \"description\": \"An RGCN architecture designed specifically for the Enron Email dataset, utilizing encryption and advanced clustering techniques for multi-level security and precision.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model must handle large-scale datasets by leveraging Kubernetes cluster and microservices infrastructure.\",\n        \"TensorPipe for real-time, distributed communication on the cluster is crucial.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": null,\n        \"target_device\": \"distributed Kubernetes cluster\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorPipe\"],\n        \"demonstration\": \"microservices infrastructure\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON accurately captures the user's intent to build a model, their expertise level, and the specific requirements regarding the Enron dataset and the RGCN model. The performance metric of precision is set based on the user's expectation, and complex requirements like real-time analysis, fault tolerance, latency, and auto-scaling are represented in the complexity metrics. The scalable nature of the setup for handling massive data growth is noted. However, some values in complexity metrics are not specified as they were not directly provided in the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design an efficient, real-time RNN-Transformer hybrid model optimized for English to Russian conversational slang and idiomatic expressions translation. Utilize the specialized OPUS-Xtra dataset, targeting a BLEU score of 38 and achieving sub-200 milliseconds latency per sentence, concurrently maintaining user engagement and preserving cultural nuances in informal chats.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"conversational AI\",\n        \"description\": \"Design an efficient, real-time RNN-Transformer hybrid model for translating English to Russian conversational slang and idiomatic expressions, using the OPUS-Xtra dataset. Target a BLEU score of 38 and aim for sub-200 milliseconds latency per sentence while maintaining user engagement and preserving cultural nuances.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 38\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency per sentence\",\n                \"value\": 200,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS-Xtra\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source_language\": \"English\",\n                \"target_language\": \"Russian\",\n                \"specific_domain\": \"conversational slang and idiomatic expressions\"\n            },\n            \"description\": \"A specialized dataset for translating informal chats between English and Russian, focusing on slang and idiomatic expressions.\",\n            \"preprocessing\": [\"language preprocessing for conversational data\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model input-output and attention mechanism\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-Transformer hybrid\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"RNN components integrated with Transformers\",\n                \"latency_reduction_techniques\": \"sub-200ms for real-time performance\"\n            },\n            \"description\": \"A cutting-edge model designed for real-time translation, focusing on conversational slang and idiomatic expressions in English to Russian.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"importance\": \"real-time and cultural nuances preservation\",\n            \"insight\": \"Utilizing specialized dataset for colloquial expressions, domain-specific modeling\"\n        },\n        {\n            \"importance\": \"hybrid architecture\",\n            \"insight\": \"Combining RNNs with Transformers for translation efficiency and performance\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-aware feature extraction\"],\n        \"target_device\": \"suitable for edge computing or cloud\",\n        \"deployment_endpoint\": \"API-based deployment for real-time translations\",\n        \"inference_engine\": [\"GPU-accelerated for faster inference\"],\n        \"demonstration\": \"user-friendly chat interface showing translations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent (build), expertise, problem area, task (translation), performance metric (BLEU), and dataset specifics (OPUS-Xtra) as per the instruction. The hybrid model's description and requirements (RNN-Transformer, latency, cultural preservation) are extracted. Some additional insights on real-time and cultural nuances are provided, but specific model parameters and exact preprocessing steps are not detailed, as the instruction is more descriptive. The confidence score is high, given the clear instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for a groundbreaking multilingual tech firm, design a highly scalable translation app catering specifically to Quechua-Spanish dialects. Employ the scarce JW300 dataset, but require an architecture that combines convolutional neural networks (CNN) with Transformer models, ensuring cross-platform compatibility. Mandate minimal error rate (<5%) for colloquial expressions, and push for a BLEU score of 40 or above in low-resource scenarios, while also considering energy efficiency for IoT devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual tech\",\n        \"description\": \"Design a highly scalable translation app catering to Quechua-Spanish dialects, using the JW300 dataset and combining CNN and Transformer models for cross-platform compatibility. Target an error rate below 5% for colloquial expressions and aim for a BLEU score of 40 or higher in low-resource scenarios, with energy efficiency for IoT devices in mind.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"error rate\",\n                \"value\": 0.05\n            },\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 40\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"specific energy consumption for IoT devices\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"JW300\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"description\": \"Scarce Quechua-Spanish dialect dataset for machine translation\"\n            },\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"CNN-Transformer Hybrid\",\n            \"family\": \"Convolutional Neural Networks (CNN) combined with Transformer models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"compatibility\": \"cross-platform\",\n                \"architecture_description\": \"Combines CNN and Transformer models for translation tasks\"\n            },\n            \"description\": \"A scalable model architecture designed for multilingual machine translation, focusing on Quechua-Spanish dialects and energy efficiency for IoT devices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The combination of CNN and Transformer models adds strength to handle sparse data in JW300 and cross-platform scenarios.\",\n        \"Minimizing error rate and achieving a high BLEU score are crucial performance indicators.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"IoT devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for IoT\"],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is well-structured to reflect the user's intent, requirements for the problem, and specifications. High expertise level is assumed for the user due to the project manager role. The application domain, performance metrics (error rate and BLEU score), and the inclusion of energy efficiency for IoT devices are captured. However, additional information about preprocessing steps or specific models would be beneficial, but the instruction didn't provide these details.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "Instruct the ML team to design a cutting-edge financial sentiment analysis platform, utilizing domain-specific fastText for Reuters-21578 news classification. Strive for 92% accuracy with a 5% increase in precision for niche sector news, while integrating an advanced algorithm that distinguishes genuine breaking news from false rumors in under 25 milliseconds, ensuring seamless market integrity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Design a financial sentiment analysis platform using domain-specific fastText for Reuters-21578 news classification. Target 92% accuracy with a 5% increase in precision for niche sector news. Additionally, incorporate an advanced algorithm for real-time detection of genuine breaking news from false rumors within 25 milliseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.92\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 1.05,\n                \"description\": \"Increased by 5% for niche sector news\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time (for breaking news detection)\",\n                \"value\": 0.025,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment classification\"],\n            \"specification\": {\n                \"subset\": \"financial news\"\n            },\n            \"description\": \"A dataset for financial news sentiment analysis, focusing on the Reuters-21578 corpus.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"fastText (domain-specific)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Financial domain-specific\"\n            },\n            \"description\": \"A state-of-the-art text classification model using fastText, tailored for financial sentiment analysis of Reuters-21578 news.\"\n        }\n    ],\n    \"knowledge\": [\n        \"fastText's ability to handle domain-specific text data is critical for achieving high accuracy.\",\n        \"Advanced algorithm is needed to differentiate breaking news from rumors in real-time.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Fast and efficient inference mechanism\"\n        ],\n        \"demonstration\": \"Real-time demonstration of high-speed breaking news detection and financial sentiment analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's instruction, which is to build an advanced sentiment analysis platform with specified accuracy and precision improvements. The area of expertise is high, and the specific Reuters-21578 dataset, preprocessing steps, and the model family are all derived from the details provided. The performance metrics, including the complex algorithmic requirement, are clearly stated, and the confidence score is high due to the clear direction and the inferred details.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "As the lead project director for a boutique AI research firm, your mandate is to develop an ultra-specific medical domain BERT-variant QA system using SQuAD 2.0 in the life sciences field. The system must exhibit state-of-the-art precision, achieving a stringent F1 score of 97% in oncology research mode, and guarantee sub-1.4-second response times for bioinformaticians utilizing the streamlined BioBERT model. Explore advanced gradient-based transfer learning, adaptive learning rate optimization, and efficient model quantization techniques to push boundaries in precision without sacrificing real-time responsiveness, targeting to surpass Oxford's latest benchmark by 3%.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"life sciences\",\n        \"description\": \"Develop an ultra-specific medical domain BERT-variant QA system using SQuAD 2.0. Targeted for oncology research with a precision goal of 97% F1 score and sub-1.4-second response times for bioinformaticians, employing advanced techniques like gradient-based transfer learning, adaptive learning rate optimization, and efficient model quantization to surpass Oxford's latest benchmark by 3%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 Score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Response Time\",\n                \"value\": 1.4,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"specific_domain\": \"medical, oncology\"\n            },\n            \"description\": \"A large-scale, question answering dataset in the life sciences domain, focusing on medical domain knowledge.\",\n            \"preprocessing\": [\"BERT fine-tuning for oncology\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-variant for Medical QA\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"precision\": \"state-of-the-art\",\n                \"optimized\": [\"gradient-based transfer learning\", \"adaptive learning rate\", \"model quantization\"],\n                \"quantization_method\": \"\"\n            },\n            \"description\": \"A highly specialized and optimized BERT model for medical domain question answering, designed for ultra-fast response times.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced transfer learning and optimization techniques are essential for achieving high precision in a highly specialized domain like oncology.\",\n        \"Model efficiency and real-time responsiveness are critical in a bioinformatics context.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"customized BERT adaptation for oncology\",\n            \"optimized for F1 score and response time\"\n        ],\n        \"target_device\": \"optimized for bioinformaticians' use\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Efficient inference with minimal processing time\"\n        ],\n        \"demonstration\": \"Exceeding Oxford's benchmark by 3%\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the schema, reflecting the given instruction's requirements. The user's intent to build is clear, and the expertise level is high given the complexity of the task. The area of NLP and downstream task of QA are identified. The system's performance metrics are specified, including the F1 score and response time. The dataset is SQuAD 2.0 linked by user, and the model's specifications are described in detail. The instructions for gradient-based transfer learning and other optimizations are incorporated, and the user's goal to surpass Oxford's benchmark is included in the 'demonstration' service.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "Act as a project manager for a cutting-edge AI team, tasked with creating a text classification system for the 20 Newsgroups dataset. Choose a model with state-of-the-art performance, transparency, and energy efficiency. Implement an innovative federated learning technique, ensuring data privacy while reducing training time, and optimize for a specific edge device architecture.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Create a text classification system for the 20 Newsgroups dataset, targeting state-of-the-art performance, transparency, and energy efficiency. Implement federated learning with a focus on data privacy and reduced training time, while optimizing for a specific edge device architecture.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"f1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"energy consumption per inference\",\n                \"value\": null,\n                \"unit\": \"mJ\"\n            },\n            {\n                \"name\": \"model size for edge device\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"category\"],\n            \"specification\": null,\n            \"description\": \"A large text dataset for multi-class text classification, split into train, validation, and test sets.\",\n            \"preprocessing\": [\n                \"tokenization\",\n                \"stopword removal\",\n                \"lemmatization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art NLP model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"explainable AI\",\n                \"energy efficiency techniques\": [\"efficient inference\", \"low-precision training\"]\n            },\n            \"description\": \"A transparent text classification model designed for the 20 Newsgroups dataset, incorporating energy efficiency and state-of-the-art performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transparency is essential for explainable AI, which builds trust in the system.\",\n        \"Federated learning enhances data privacy while reducing training time on edge devices.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"federated pre-processing\"],\n        \"target_device\": \"specific edge device\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Edge AI accelerator\"],\n        \"demonstration\": \"A demonstration showcasing real-time classification and model explainability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the user's instruction, mapping the problem to the NLP area and focusing on text classification. The text model's performance and transparency are highlighted as key requirements, as well as energy efficiency through federated learning and optimization for edge devices. The dataset, model, and service-related details are inferred from the context. The metrics are left unspecified since they need actual performance data. The confidence score is high due to the clear task description and mapping.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As the lead data science project manager, your task has evolved into a challenging multidomain endeavor. Your team must now delve into the Wild Blueberry Yield Dataset, a vast, intricately connected database containing a plethora of quantitative variables. Conduct meticulous feature extraction, normalization, and data imputation for the segregated train, validation, and test sets, each requiring unique preprocessing strategies. The mission is to engineer an avant-garde regression model that integrates advanced feature selection techniques and ensemble methods, guaranteeing state-of-the-art predictive prowess. The model must consistently achieve a superior Mean Absolute Error (MAE) compared to current benchmark models, while considering the dataset's temporal and spatial correlations. Don't forget to document the iterative approach and justify hyperparameter tuning decisions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Work on the Wild Blueberry Yield Dataset, a large and interconnected database with quantitative variables. Perform feature extraction, normalization, and data imputation for train, validation, and test sets. Apply advanced feature selection techniques and ensemble methods to develop a state-of-the-art regression model. Target is to achieve superior Mean Absolute Error (MAE) compared to benchmarks, considering temporal and spatial correlations. Document the iterative approach and hyperparameter tuning decisions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Absolute Error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"iterative approach\",\n                \"value\": 1,\n                \"unit\": \"iterations\"\n            },\n            {\n                \"name\": \"hyperparameter tuning\",\n                \"value\": 0.9,\n                \"unit\": \"optimized\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wild Blueberry Yield Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"yield\"],\n            \"specification\": {\n                \"size\": null, \n                \"dimensions\": null, \n                \"temporal_correlations\": true,\n                \"spatial_correlations\": true\n            },\n            \"description\": \"An extensive, interconnected database with quantitative variables for blueberry yield prediction, requiring unique preprocessing techniques for train, validation, and test sets.\",\n            \"preprocessing\": [\n                \"feature extraction\",\n                \"normalization\",\n                \"data imputation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble methods\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced_feature_selection\": true,\n                \"ensemble_technique\": null\n            },\n            \"description\": \"An advanced regression model integrating feature selection techniques and ensemble methods, designed to predict blueberry yields with improved accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Temporal and spatial correlations need to be accounted for in model development.\",\n        \"Iterative approach and thorough hyperparameter tuning are critical for state-of-the-art performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal feature engineering\",\n            \"spatial feature engineering\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Detailed documentation of the iterative approach and hyperparameter tuning process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's task, with a high-level description, target dataset, advanced model requirements, and desired performance metrics. However, there are some details missing, such as the specific ensemble technique, model architecture, and exact performance targets. A low-confidence score is given due to the lack of these critical details.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "visual question answering", "instruction": "As a project manager for the middle school exhibition, you are tasked with designing a state-of-the-art Visual Question Answering (VQA) system that incorporates cutting-edge transformers, grounded in the TextVQA dataset. Demanding not just high accuracy but also explainable AI, please outline a hybrid architecture that integrates cross-modal feature extraction, context-aware attention mechanisms, and deep visual-language alignment methods. In addition, prepare a comprehensive technical paper detailing the rigorous evaluation metrics, step-by-step algorithmic processes, and empirical justification for your approach to showcase profound understanding and differentiability from existing solutions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Design a state-of-the-art Visual Question Answering (VQA) system using cutting-edge transformers, grounded in the TextVQA dataset. The system must have high accuracy and be explainable, incorporating a hybrid architecture with cross-modal feature extraction, context-aware attention mechanisms, and deep visual-language alignment methods.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explainability\",\n                \"value\": {\n                    \"type\": \"explainable\"\n                }\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TextVQA\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source_size\": null,\n                \"image_resolution\": null,\n                \"question_length\": null\n            },\n            \"description\": \"A benchmark dataset for visual question answering, providing a basis for the system.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-based VQA\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"cross-modal\": true,\n                    \"context-aware_attention\": true,\n                    \"visual_language_alignment\": true\n                },\n                \"explainability\": \"methods like attention maps or layer-wise relevance propagation\"\n            },\n            \"description\": \"A hybrid VQA model combining transformers for deep feature extraction and understanding, designed for high accuracy and explainability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Relevant research: Transformers and their success in multimodal tasks, TextVQA dataset for model benchmarking.\",\n        \"Challenges: Ensuring explainability while maintaining high accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Cross-modal feature extraction\",\n            \"Context-aware attention\",\n            \"Visual-language alignment\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explained model outputs and reasoning process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is derived from the given instruction, capturing the user's intent to build a model, their high level of expertise, and the focus on state-of-the-art VQA. The hybrid architecture is detailed, along with the requirement for explainability using explainable AI techniques. The TextVQA dataset and the comprehensive technical paper components are included. However, specific performance targets are not provided, leaving them open for the actual development process.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for an exclusive biomedical innovation, you must supervise a team in developing a deep learning model for ECG Monitoring. The dataset, now comprising 10 million global patient records with rare, high-resolution (1000Hz) signals, has been stratified into train, val, and test subsets featuring imbalanced classes with temporal patterns spanning 10 minutes. The challenge is to design an RNN-based model that not only outperforms current benchmarks with AUC-PRC as the key evaluation, but also imparts interpretability through partial dependence plots and feature importance analysis, ensuring ethical deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"biomedical\",\n        \"description\": \"Develop a deep learning model, specifically an RNN-based approach, for ECG Monitoring using a large dataset of 10 million global patient records with 1000Hz signals. The dataset is stratified into train, val, and test subsets with imbalanced classes and temporal patterns lasting 10 minutes. The goal is to achieve better performance than current benchmarks using AUC-PRC as the primary metric and ensure interpretability through partial dependence plots and feature importance analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-PRC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Exclusive Biomedical ECG Dataset (10M records)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"ECG signals\"],\n            \"specification\": {\n                \"size\": \"10 million records\",\n                \"sampling_rate\": 1000,\n                \"imbalance\": \"true\",\n                \"duration\": \"10 minutes\"\n            },\n            \"description\": \"A large, high-resolution ECG dataset with stratified subsets for training, validation, and testing.\",\n            \"preprocessing\": [\"normalization\", \"resampling\"],\n            \"augmentation\": [],\n            \"visualization\": [\"partial dependence plots\", \"feature importance analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"RNN-based deep learning model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"true\",\n                \"target_interpretation\": [\"partial dependence plots\", \"feature importance\"]\n            },\n            \"description\": \"A deep learning model using RNNs designed for ECG monitoring, targeting interpretability and improved AUC-PRC performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Large and complex ECG data require advanced deep learning models, especially RNNs, to capture temporal patterns.\",\n        \"Ethical deployment requires interpretability aspects for model decisions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"ECG monitoring system with interpretability features\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is tailored to the provided instruction, capturing the project manager's role, the high level of expertise, the biomedical context, and the specific requirements of the deep learning model. The dataset description includes crucial details such as size, resolution, and imbalance, as well as requested preprocessing and visualization techniques. The model family and performance metric are specified, indicating a clear understanding of the problem. The confidence score is high, but absent specific model architecture details or performance targets, it's set to 0.95.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, you're now required to design a gamified question-answering platform for primary school kids, integrating the exclusive KidsQA+ dataset. Implement a custom, simplified BERT with visual storytelling and adaptive language complexity. Ensure not only educational value but also track cognitive growth and maintain a GUES-score of at least 85% for age-appropriate, error-free responses within a 2-second latency constraint.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Design a gamified question-answering platform for primary school kids, integrating the KidsQA+ dataset. The platform should use a custom, simplified BERT with visual storytelling and adapt to the language complexity of its users.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"GUES-score\",\n                \"value\": 0.85\n            },\n            {\n                \"name\": \"Latency\",\n                \"value\": 2\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Language Complexity Adaptation\",\n                \"value\": 1.0,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA+\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": \"Exclusive for primary school kids\",\n                \"language_level\": \"Age-appropriate\"\n            },\n            \"description\": \"An exclusive dataset for a gamified question-answering platform targeting primary school kids.\",\n            \"preprocessing\": [\n                \"Language simplification\",\n                \"Visual storytelling data preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Performance tracking\",\n                \"Cognitive growth visualization\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Simplified BERT for Kids\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_complexity\": \"Reduced for primary school-level understanding\",\n                \"inference_speed\": null\n            },\n            \"description\": \"A custom, simplified version of BERT tailored for question answering in a gamified environment with visual storytelling.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Language complexity should be adapted to the target audience (primary school kids)\",\n        \"Maintaining a GUES-score of 85% and low latency is crucial for educational value and platform effectiveness.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\" Educational content creation\"],\n        \"target_device\": \"PC and mobile-friendly\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Real-time response processing\",\n            \"Lightweight for fast performance\"\n        ],\n        \"demonstration\": \"Gamification elements and AI-driven feedback\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers the provided instruction, reflecting the user's intent to build a platform, the requirements for custom BERT, dataset, and performance metrics. It assumes the user's high level of expertise. Specifics like dataset size and language level, and model's reduced complexity are inferred from the context. The cognitive growth tracking and GUES-score target are clearly stated. The possible feature engineering, deployment, and inference engine requirements are also included. However, the exact model complexity reduction isn't detailed, as it's a subjective aspect, and the latency metric could be more specific, but it's generally in the expected range.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project lead, you are required to design an enhanced image recognition system for the AI team. This task necessitates the following distinct steps:\r\n\r\n1. Collect and preprocess a comprehensive 'Plant Seedlings Dataset' (50 species plus sub-species) with diverse environmental variations for improved robustness.\r\n2. Implement a deep learning model, utilizing state-of-the-art architectures, for multi-class classification, considering fine-grained distinctions among species.\r\n3. Employ transfer learning, optimizing for a resource-limited, mobile device, ensuring at least 95% accuracy, while maintaining real-time performance (1 second per image).\r\n4. Conduct extensive testing with cross-validation, addressing potential deployment challenges and evaluating performance trade-offs.\r\n5. Develop a lightweight deployment strategy that minimizes computational overhead without compromising accuracy, targeting mobile platforms.\r\n6. Document the entire process, including model explainability and edge-case handling, for future maintenance and auditing.\r\n\r\nRemember to provide detailed project milestones, performance metrics, and a clear plan for iterative improvement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"artificial intelligence\",\n        \"description\": \"Design an enhanced image recognition system for plant seedlings, considering 50 species and sub-species with diverse environmental variations. The model should be multi-class, deep learning-based, and optimized for real-time performance on resource-limited mobile devices with at least 95% accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"seconds\",\n                \"value\": 1\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational overhead\",\n                \"unit\": \"CPU cycles or memory usage\",\n                \"value\": 0.1\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Seedlings Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\"],\n            \"specification\": {\n                \"number_of_species\": 50,\n                \"diversity_index\": \"environmental variations\"\n            },\n            \"description\": \"A comprehensive dataset with 50 plant species and sub-species, featuring diverse environmental conditions for improved model robustness.\",\n            \"preprocessing\": [\n                \"data collection\",\n                \"image resizing\",\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\n                \"random cropping\",\n                \"rotation\",\n                \"gaussian noise\"\n            ],\n            \"visualization\": [\n                \"data distribution analysis\",\n                \"quality control\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art deep learning architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"e.g., ResNet, EfficientNet, MobileNetV2\"\n            },\n            \"description\": \"A multi-class classification model optimized for fine-grained distinctions among plant seedlings.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning to leverage existing pre-trained models\",\n        \"Model architecture choices for mobile optimization\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"lightweight deployment techniques\",\n            \"on-device computations\"\n        ],\n        \"target_device\": \"mobile platforms\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"TensorFlow Lite, Mobile NNAPI, or other lightweight backends\"\n        ],\n        \"demonstration\": \"Use case demonstration, edge-case handling\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response provides a detailed breakdown of the user's instruction, including the requirement for a high-performing image recognition system, data collection and preprocessing, model design, transfer learning, testing, and deployment strategies. Performance metrics, complexity constraints, and future improvement plans are all covered. The only potential missing information could be specific model architecture choices, which are left unspecified to accommodate various options.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "node classification", "instruction": "Construct a tailored GWNN architecture, specifically designed for the `Intracellular Spatial Transcriptomics of Layer 5 Neurons in the Mouse Brain's Entorhinal Cortex`, using the unique MindSpore Pro toolbox. Target a precision boost in the recognition of both ultrarare and highly prevalent subtypes with at least 95% specificity, while accounting for signal nuances in data from Synaptophysin-GFP labeled neurons. Ensure model adaptability for handling 100,000+ nodes on a state-of-the-art GPU cluster, implementing a distributed training strategy with at least 80% compute resource utilization efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Construct a custom GWNN architecture using the MindSpore Pro toolbox for the Intracellular Spatial Transcriptomics of Layer 5 Neurons in the Mouse Brain's Entorhinal Cortex. Focus on precision in identifying both ultrarare and prevalent subtypes with at least 95% specificity, accounting for signal nuances in Synaptophysin-GFP labeled neurons. Ensure the model can handle 100,000+ nodes on a state-of-the-art GPU cluster with a distributed training strategy targeting 80% compute resource utilization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"specificity\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference scalability\",\n                \"value\": \"100,000+ nodes\",\n                \"unit\": \"nodes\"\n            },\n            {\n                \"name\": \"compute resource utilization\",\n                \"value\": 0.80,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Intracellular Spatial Transcriptomics of Layer 5 Neurons in the Mouse Brain's Entorhinal Cortex\",\n            \"modality\": [\"graph\", \"text\"],\n            \"target_variables\": [\"node classifications\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"total_count\": \"100,000+\",\n                    \"labeled_subtypes\": \"ultrarare and highly prevalent\"\n                },\n                \"data_types\": {\n                    \"Synaptophysin-GFP\": \"signal nuances\"\n                }\n            },\n            \"description\": \"Graph dataset with spatial transcriptomics data from Synaptophysin-GFP labeled neurons in Layer 5 of the Mouse Brain's Entorhinal Cortex.\",\n            \"preprocessing\": [\"MindSpore Pro toolbox specific\"],\n            \"augmentation\": [],\n            \"visualization\": [\"data from Synaptophysin-GFP neurons\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom GWNN (Graph Wavelet Neural Network)\",\n            \"family\": \"MindSpore Pro Architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"node_type\": \"MindSpore compatible\",\n                \"distributed_training\": true\n            },\n            \"description\": \"A tailored Graph Wavelet Neural Network designed for spatial transcriptomics data from Layer 5 neurons with emphasis on precision and resource efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"MindSpore Pro toolbox offers custom graph neural network solutions\",\n        \"Ultrarare and highly prevalent subtypes pose a challenge for recognition\",\n        \"100,000+ nodes call for scalable distributed training\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"custom signal processing for Synaptophysin-GFP data\"],\n        \"target_device\": \"state-of-the-art GPU cluster\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"MindSpore's distributed inference\"],\n        \"demonstration\": \"optimized for recognizing subtypes in spatial transcriptomics data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a model (high expertise) and problem domain. The area is graph machine learning and the task is node classification. Specifics on the dataset and a custom GNN architecture (GWNN) are derived from the instruction. Performance metrics and complexity metrics are included, accounting for precision, scalability, and resource efficiency. The provided knowledge pieces and service requirements are directly related to the task description. The missing 'accuracy' in performance metrics is inferred to be a requirement but no value is provided, allowing for a default null value. The confidence score is not 1.0 due to potential missing details in the specific preprocessing steps and demonstration instructions.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "For the cutting-edge astrophotography initiative, develop a robust classification system that distinguishes not only between nebulae, pulsars, and exoplanets but also smaller subcategories within each. The massive, unlabeled data collection should include high-resolution, dynamically varying celestial images with realistic noise. As a project manager, identify the optimal fusion of CNN and transformer architectures, considering transfer learning, data augmentation strategies, and propose a detailed plan for noise reduction and feature extraction, aiming to surpass current benchmark accuracy in the highly competitive field of astronomical deep learning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Develop a robust classification system for astrophysical objects, including subcategories of nebulae, pulsars, and exoplanets. The project requires dealing with large, unlabeled, high-resolution, and dynamically varying celestial images with realistic noise.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Unlabeled Astrophysical Images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"category\", \"subcategories\"],\n            \"specification\": {\n                \"image_resolution\": null,\n                \"dynamic_range\": null,\n                \"noise_distribution\": \"realistic\"\n            },\n            \"description\": \"A massive dataset containing high-resolution celestial images with dynamic variations and realistic noise.\",\n            \"preprocessing\": [\n                \"data augmentation\",\n                \"noise reduction\"\n            ],\n            \"augmentation\": [\n                \"CNN-based\",\n                \"transformer-based\"\n            ],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"fusion of CNN and transformer architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": true,\n                \"architectural_details\": \"\",\n                \"performance_benchmark\": {\n                    \"accuracy\": null\n                }\n            },\n            \"description\": \"A state-of-the-art deep learning model combining CNN and transformer architectures for advanced celestial object classification, with a focus on noise reduction and feature extraction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The project requires advanced techniques like transfer learning, fusion of CNN and transformers, and specialized data augmentation strategies for high-accuracy results.\",\n        \"Considering realistic noise in images and dynamic variations will be key to distinguish subcategories in the competitive field of astronomical deep learning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"custom feature extraction\",\n            \"feature fusion\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-optimized\",\n            \"cloud-based\"\n        ],\n        \"demonstration\": \"real-world performance demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the information from the instruction accurately, with high level of detail regarding the requirements for image classification in astrophysics. It includes the user's intent to build a model, high level of expertise, and a focus on performance metrics and complexity optimization. The details of the dataset, model, and service are derived from the specific requirements in the given instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a seasoned project manager for an elite media planning initiative, you've been tasked with a challenging cost estimation problem. The expansive Media Campaign Cost Dataset, enriched with sophisticated temporal patterns and numerous categorical variables, has been partitioned into strictly non-overlapping subsets: a rigorous training set, a validation set with temporal anomalies, and a test set with outliers. Your mission now entails constructing an advanced, interpretable regression model that captures intricate feature interactions, time-varying effects, and handles missing data effectively. The performance benchmark shifts to a hybrid RMSLE metric, incorporating domain-specific weightings. Demonstrate superior performance not just on average but also resilience to concept drift. Concomitantly, prepare a comprehensive report detailing your innovative feature selection, model optimization techniques, and sophisticated model explainability methods, ensuring business stakeholders fully appreciate your model's decision-making rationale.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"media planning\",\n        \"description\": \"Conduct a cost estimation for a Media Campaign Cost Dataset with sophisticated temporal patterns, categorical variables, and partitioned into train, validation (with anomalies), and test sets. Develop an interpretable regression model that captures feature interactions, time-varying effects, and handles missing data. Target a hybrid RMSLE metric with domain-specific weightings, ensuring resilience to concept drift.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSLE (hybrid)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Media Campaign Cost Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"cost\"],\n            \"specification\": {\n                \"temporal_features\": true,\n                \"categorical_features\": true,\n                \"missing_data_rate\": null,\n                \"anomalies\": {\n                    \"validation_set\": true\n                },\n                \"outliers\": {\n                    \"test_set\": true\n                }\n            },\n            \"description\": \"A dataset for media campaign cost estimation with temporally evolving patterns, categorical variables, and strictly non-overlapping subsets.\",\n            \"preprocessing\": [\n                \"handle missing data\",\n                \"handle temporal anomalies\",\n                \"transform categorical variables\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Interpretable Regression Model (e.g., Lasso, Random Forest Regressor with feature importance)\",\n            \"type\": \"classical machine learning\",\n            \"specification\": {\n                \"feature_interactions\": true,\n                \"time_varying\": true,\n                \"interpretability_methods\": [\"SHAP values\", \"partial dependence plots\"]\n            },\n            \"description\": \"An advanced regression model for cost estimation that includes feature interactions and time-varying effects, while being amenable to explanation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Temporal patterns and feature interactions must be captured to account for the dataset's complexity.\",\n        \"Model should handle missing data and temporal anomalies in the validation set.\",\n        \"Resilience to concept drift is crucial due to the dynamic nature of media campaigns.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Anomaly Detection (e.g., Z-score, ISolation Forest)\",\n            \"Time-series Feature Extraction (e.g., lag features)\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"explanatory.visualizations\": [\"Explained variance plot\", \"Performance vs. concept drift metrics\"]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the user's project requirements, including the dataset characteristics, model specifications, and performance metrics. The hybrid RMSLE metric implies a focus on both overall and domain-specific performance, while the requirement for explainability aligns with the need for stakeholders to understand the model's decisions. However, specific algorithms or hyperparameters were not provided, leaving room for further clarification.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a project manager, lead a team to develop a real-time, high-traffic video platform recommendation system using the 'Bilibili Danmu' dataset. Mandate implementation of a hybrid deep learning model that integrates both convolutional and recurrent neural networks for complex user behavior analysis, while ensuring GDPR compliance, achieving at least 90% accuracy, and maintaining sub-millisecond latency for seamless user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"media streaming\",\n        \"description\": \"As a project manager, develop a real-time, high-traffic video platform recommendation system using the Bilibili Danmu dataset. The system should employ a hybrid deep learning model combining convolutional and recurrent neural networks for complex user behavior analysis. Ensure GDPR compliance, aim for at least 90% accuracy, and maintain sub-millisecond latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.001,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Bilibili Danmu\",\n            \"modality\": [\"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": { \"train\": \"high\", \"validation\": \"high\", \"test\": \"high\" },\n                \"time_series\": true,\n                \"contextual_data\": true\n            },\n            \"description\": \"A dataset from the Bilibili video platform containing user-generated Danmu data for real-time recommendation system development.\",\n            \"preprocessing\": [\n                \" GDPR compliant data cleaning\",\n                \"Data anonymization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid CNN-RNN\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"CNN + RNN\",\n                \"complexity\": \"optimized for real-time and high-traffic\"\n            },\n            \"description\": \"A hybrid deep learning model combining Convolutional Neural Networks (CNN) for visual features and Recurrent Neural Networks (RNN) for sequential data, tailored for Bilibili Danmu analysis to achieve real-time user behavior predictions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Combining CNNs and RNNs enhances model's ability to capture both spatial and temporal patterns in user interactions.\",\n        \"GDPR compliance is crucial for handling user data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"user behavior feature extraction\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"real-time video platform\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"Live, real-time user experience demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the project requirements as per the instruction. It covers the user's intent to build a model, the high level of expertise needed, and the problem area, application domain, and task description. Metrics and constraints, such as accuracy and latency, are defined. The dataset, model, and GDPR compliance aspects are derived from the instruction. However, the specific details for preprocessing, like GDPR compliant techniques, are assumed based on common practices. The confidence score is moderate due to the assumption of GDPR techniques.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create a sophisticated translation system utilizing the ParaCrawl dataset, employing a dual encoder model with sentiment analysis integration. The system must distinguish between formal and informal language, preserving not only grammatical accuracy but also the nuanced emotional nuances in English-to-Dutch conversations, considering cross-cultural context.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Develop a sophisticated translation system using the ParaCrawl dataset with a dual encoder model and sentiment analysis integration. The system must be able to distinguish between formal and informal language, maintaining grammatical accuracy and capturing nuanced emotional nuances in English-to-Dutch conversations, with a focus on cross-cultural context.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sentence-level BLEU\",\n                \"value\": null\n            },\n            {\n                \"name\": \"情感识别准确率\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": \"large\",\n                \"data_division\": \"train-test-validation\"\n            },\n            \"description\": \"A large-scale multilingual dataset for translation, including English-to-Dutch conversations with a focus on cross-cultural context.\",\n            \"preprocessing\": [\n                \"sentence segmentation\",\n                \"tokenization\",\n                \"language detection\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"cross-lingual data analysis\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dual Encoder Translation Model\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"encoder-decoder architecture\",\n                \"encoder_modules\": \"dual encoders\",\n                \"language_models\": \"sentiment-aware\"\n            },\n            \"description\": \"A transformer model with dual encoders for simultaneous translation and sentiment analysis in English-to-Dutch conversations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"ParaCrawl's large size and diverse data provide a good foundation for training a complex translation model.\",\n        \"Sentiment analysis integration enhances the model's ability to capture nuanced emotions in cross-cultural context.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual embeddings\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"customized translation API\",\n        \"inference_engine\": [\"GPU-based\"],\n        \"demonstration\": \"interactive translation demonstrations with sentiment feedback\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response closely reflects the user's instruction, indicating a build intent with a high level of expertise. The problem area, dataset, model, and metrics are well-defined based on the task at hand. However, the exact performance expectations and values are not specified, so they are left as null. The specific preprocessing, augmentation, and feature engineering steps are inferred from the given requirements.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, demand not only a machine translation model with low-resource Swahili-English support but also ensure compatibility with dialectal variations. Utilize the Masakhane dataset, integrate Transformer and RNN models, and mandate a minimum 10% improvement in TER metrics compared to existing benchmarks. Constrain the model to optimize for real-world scenarios with minimal training data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual\",\n        \"description\": \"As a project manager, require a machine translation model with low-resource Swahili-English support, including dialectal variations. Use the Masakhane dataset and combine Transformer and RNN models. Aim for at least a 10% improvement in TER metrics compared to existing benchmarks, and focus on optimizing for real-world scenarios with limited training data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"TER (Translation Edit Rate)\",\n                \"value\": 0.1\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Masakhane Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Swahili\", \"English\"],\n            \"specification\": {\n                \"source_language\": \"Swahili\",\n                \"target_language\": \"English\",\n                \" dialect_variations\": true\n            },\n            \"description\": \"A low-resource dataset designed for Swahili-English machine translation, considering dialectal variations.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": [\"Transformer\", \"RNN\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"combined\",\n                \"training_data_efficiency\": \"low-resource\"\n            },\n            \"description\": \"A machine translation model incorporating both Transformer and RNN components, optimized for low-resource and dialectal Swahili-English scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consideration of dialectal variations is crucial for real-world Swahili-English translation.\",\n        \"Model should improve upon existing TER benchmarks by at least 10%.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"customizable for real-world scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's requirements for a machine translation model with a focus on low-resource Swahili-English, dialectal variations, and the Masakhane dataset. The performance metric (10% improvement in TER), model combination (Transformer and RNN), and the real-world scenarios are appropriately included. However, there might be room for more details on specific feature engineering techniques or optimal hyperparameters for low-resource settings.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for a machine learning team, you're tasked with a specialized assignment. Explain the necessary steps to develop a transfer learning model for fine-grained cat species classification, considering a diverse dataset with subtle breed variations, using convolutional neural networks (CNNs), and incorporating feature extraction and ensemble techniques for improved accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal recognition, possibly agriculture or wildlife conservation\",\n        \"description\": \"Develop a transfer learning model for fine-grained cat species classification on a diverse dataset with subtle breed variations, using convolutional neural networks (CNNs) and incorporating feature extraction and ensemble techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"image_resolution\": null,\n                \"number_of_classes\": null\n            },\n            \"description\": \"A diverse dataset with subtle breed variations for cat species classification.\",\n            \"preprocessing\": [\n                \"Data normalization\"\n            ],\n            \"augmentation\": [\n                \"Random cropping\",\n                \"Horizontal flipping\"\n            ],\n            \"visualization\": [\n                \"Sample image statistics\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (CNNs)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"base_model\": null,\n                \"feature_extractor_layer\": null,\n                \"additional_layers\": null\n            },\n            \"description\": \"A transfer learning model employing CNNs for fine-grained cat species classification.\"\n        },\n        {\n            \"name\": \"\",\n            \"family\": \"Feature Extraction\",\n            \"description\": \"Incorporating pre-trained features to enhance performance.\"\n        },\n        {\n            \"name\": \"\",\n            \"family\": \"Ensemble Techniques\",\n            \"description\": \"Combining multiple models for improved classification accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider leveraging large-scale pre-trained models like VGG16, ResNet, or Inception for feature extraction.\",\n        \"Fine-tuning is crucial for adapting to the subtle breed variations in the dataset.\",\n        \"Ensemble models can compensate for potential overfitting and boost accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Fine-tuning and transfer learning for domain-specific features\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow serving\", \"Docker\"],\n        \"demonstration\": \"Classification results and performance visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the task's requirements, including the use of CNNs, feature extraction, and ensemble techniques. The problem area, downstream task, and application domain are derived from the context. Performance metrics like accuracy, precision, and recall are included without specific values (left null). Time and memory complexity requirements are also mentioned but not fully specified. The dataset and model-related details are based on common practices in the computer vision for fine-grained classification, and the source is user-upload for a custom dataset. Confidence is high due to the clarity of the task and application of established machine learning methods.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Utilize a cutting-edge, adaptive time series forecasting model that integrates an auto-encoder and state-space techniques for our mobile app's daily active users data. The model must exhibit resilience to concept drift, handle seasonality intricately, and enforce regularization to ensure robust generalization across multiple future periods.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"mobile\",\n        \"description\": \"Develop a cutting-edge time series forecasting model for daily active users data using an auto-encoder and state-space techniques. The model should be resilient to concept drift, handle seasonality effectively, and enforce regularization for strong generalization across multiple future periods.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute percentage error\"\n            },\n            {\n                \"name\": \"seasonal decomposition\",\n                \"value\": \"automatically adapt to seasonality\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resilience to concept drift\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mobile App Daily Active Users\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"daily active users\"],\n            \"specification\": {\n                \"length\": \"available for the past year\",\n                \"frequency\": \"daily\"\n            },\n            \"description\": \"Contains historical data for daily active users in a mobile app\",\n            \"preprocessing\": [\"handling missing values\", \"normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time series decomposition\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Adaptive Time Series Forecasting with Auto-Encoder and State-Space\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"auto-encoder + state-space model\",\n                \"regularization method\": \"enforced during training\"\n            },\n            \"description\": \"A deep learning model that combines auto-encoders and state-space methods for robust time series forecasting in a mobile app context.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptive models help combat concept drift\",\n        \"State-space techniques enhance handling of seasonality\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"handling concept drift\", \"seasonality extraction\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"yet to specify\",\n        \"inference_engine\": [\"GPU-optimized for speed\"],\n        \"demonstration\": \"Include visual forecast results and model resilience demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's instruction, reflecting the requirements for a state-of-the-art time series model and emphasizing the need for adaptability, seasonality handling, and regularization. The dataset is specified as time series data uploaded by the user. The model details are based on the instruction and the suggested architecture. The confidence score is high, but missing details like target MAE value or specific deployment endpoint could be filled based on additional user inputs.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager, you require the ML team to develop a highly efficient and interpretable time-series forecasting model for the ETTm2 dataset. They must employ advanced feature extraction techniques and ensemble learning, while adhering to a strict constraint of interpretability and explainability. The model should not only minimize mean squared error (MSE) and mean absolute error (MAE) but also demonstrate superior performance in handling sudden anomalies and seasonal variations within the fixed sequence lengths of INPUT_SEQ_LEN=96 and PRED_SEQ_LEN=96, with INPUT_DIM=7 and PRED_DIM=7. Additionally, the model must be scalable and able to handle real-time updates with minimal computational overhead.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"business or finance\",\n        \"description\": \"Develop a highly efficient and interpretable time-series forecasting model for the ETTm2 dataset using advanced feature extraction and ensemble learning. Emphasis on interpretability, anomaly detection, seasonal variations, and real-time scalability with minimal computational overhead.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error (MSE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error (MAE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational overhead for real-time updates\",\n                \"value\": \"minimal\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"PRED_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 7,\n                \"PRED_DIM\": 7\n            },\n            \"description\": \"A time-series dataset with input and prediction sequence lengths of 96 each, and 7-dimensional inputs and outputs.\",\n            \"preprocessing\": [\"advanced feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"anomaly_detection\": \"enhanced\",\n                \"seasonality_handling\": \"advanced\"\n            },\n            \"description\": \"A highly interpretable time-series forecasting model using ensemble techniques with optimized handling of anomalies and seasonal variations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced feature extraction is crucial for performance improvement.\",\n        \"Interpretability constraints demand model transparency and explanation-friendly design.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time updating\", \"minimal computational impact\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"include support for sudden anomalies and seasonal patterns in performance visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key aspects of the user's requirements, such as the need for a time-series forecasting model, specific dataset details, and constraints on interpretability. Performance metrics and complexity constraints are included, and knowledge-related points are highlighted. However, specific model architecture details or preprocessing techniques are not explicitly mentioned, as they may vary based on the team's approach. The confidence score is high due to the clarity of the project manager's instructions.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "As a project manager, request a specialized PubMed-driven CiteGraphSAGE model for biomedical node classification, optimized for transfer learning in niche subfields. Mandate efficient model pruning for under 10MB size and guarantee sub-10ms inference on state-of-the-art GPU architecture for live, real-time deployments in a high-precision healthcare monitoring system.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"biomedical\",\n        \"description\": \"Request a specialized PubMed-driven CiteGraphSAGE model for biomedical node classification, optimized for transfer learning in niche subfields. The model should have efficient pruning for a size under 10MB and guarantee sub-10ms inference on state-of-the-art GPU architecture for real-time, high-precision healthcare monitoring system.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"transfer learning performance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.01\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model size\",\n                \"value\": 10,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"inference time (GPU)\",\n                \"value\": 0.01,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PubMed-Driven CiteGraphSAGE\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A graph dataset generated from PubMed data, used for biomedical node classification.\",\n            \"preprocessing\": [\n                \"PubMed data preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"CiteGraphSAGE\",\n            \"family\": \"PubMed-Driven Graph Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pruning\": true,\n                \"target_size\": 10,\n                \"architecture\": {\n                    \"GPU-friendly\": true,\n                    \"latency\": \"sub-10ms\"\n                }\n            },\n            \"description\": \"A specialized graph neural network for biomedical node classification, designed for transfer learning in niche subfields and optimized for real-time inference.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning can improve performance in niche biomedical domains with limited data\",\n        \"Model pruning for size constraints on state-of-the-art GPUs is crucial for real-time healthcare monitoring\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"PubMed data feature extraction\",\n            \"Transfer learning adaptation\"\n        ],\n        \"target_device\": \"state-of-the-art GPU\",\n        \"deployment_endpoint\": \"high-precision healthcare monitoring system\",\n        \"inference_engine\": [\"GPU-optimized inference\"],\n        \"demonstration\": \"Real-time node classification in live healthcare monitoring\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response closely follows the provided instruction, capturing the specific requirements for the CiteGraphSAGE model, performance metrics, and deployment constraints. The model's transfer learning capabilities, pruning for size, and GPU performance are clearly stated, and the knowledge and service components are based on the details given in the user's request. The confidence score is high due to the clear mapping from the instruction to the JSON structure.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for the AI development team, design an advanced multilingual neural machine translation system specifically tailored for TEDx talks. Utilize the recently released 'TEDx Multilingual Corpus 2.0' dataset, employing the cutting-edge M6 transformer architecture with auniket compression technique. Mandate a minimum of 30% reduction in FLOPs for handheld devices running on Android versions below 9, while ensuring compatibility with devices like the Pixel 3 with a maximum memory footprint of 2GB.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"conference and public speaking\",\n        \"description\": \"Design an advanced multilingual neural machine translation system for TEDx talks, using the TEDx Multilingual Corpus 2.0 dataset and M6 transformer architecture with auniket compression technique.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"translation quality\",\n                \"value\": null\n            },\n            {\n                \"name\": \"compression ratio\",\n                \"value\": 0.3\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"FLOPs reduction\",\n                \"value\": 0.3,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": \"max 2GB\",\n                \"unit\": \"memory\"\n            },\n            {\n                \"name\": \"compatibility\",\n                \"value\": \"devices with Android versions below 9\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TEDx Multilingual Corpus 2.0\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A multilingual dataset for neural machine translation, specifically designed for TEDx talks.\",\n            \"preprocessing\": [\"M6 transformer preprocessing\", \"uniket compression\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Multilingual Translation System\",\n            \"family\": \"M6 with Auniket compression\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"M6 transformer\",\n                \"compression_technique\": \"uniket\"\n            },\n            \"description\": \"A tailored multilingual neural machine translation system for TEDx talks, leveraging M6 and compression techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Use of M6 transformer and Auniket compression to enhance translation accuracy and efficiency.\",\n        \"Emphasis on handheld device compatibility and memory optimization\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"model optimization for handheld devices\"],\n        \"target_device\": [\"Android versions below 9, Pixel 3 and similar devices\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Lite for efficient mobile execution\"],\n        \"demonstration\": \"Translation quality performance on various TEDx talks\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the given instruction, capturing the user's intent to build a system, their high level of expertise, and the specifics of the problem - multilingual translation, TEDx talks, and performance constraints. The areas of machine translation, NLP, and natural language understanding are clearly defined. The dataset, model, and target device restrictions are derived from the details provided. The compression technique, performance metrics, and compatibility are taken into account. However, specific model performance indicators are not specified as they would depend on the model implementation and evaluation, which is beyond the scope of the JSON description.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for a high-demand grocery retail solution, mandate the development of a cutting-edge YOLO-based inventory management model trained on the \"Extended Shelf Recognition Dataset\" containing over 2,000 distinct product categories. The model should not only detect but also accurately segment and rank products in real-time, maintaining a minimum precision and recall of 95% across diverse lighting and clutter conditions. In addition, prioritize achieving sub-second response times (ideally below 0.5 seconds) for seamless inventory updates throughout multiple store locations, while ensuring the system is scalable and adaptable to future product additions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"grocery retail\",\n        \"description\": \"Develop a cutting-edge YOLO-based inventory management model trained on the Extended Shelf Recognition Dataset with over 2,000 unique product categories. The model should detect, segment, and rank products in real-time with precision and recall of at least 95%. Target sub-second response times (under 0.5 seconds) for seamless inventory updates across multiple store locations, and ensure scalability for future product additions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extended Shelf Recognition Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"images\": {\n                        \"total\": 2000,\n                        \"categories\": 2000\n                    }\n                },\n                \"data_distribution\": \"diverse lighting and clutter conditions\"\n            },\n            \"description\": \"A large dataset for training an inventory management model, containing a wide range of product categories for real-world scenarios.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"Real-time lighting adjustments\",\n                \"Data augmentation to handle diverse conditions\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO-based Inventory Management Model\",\n            \"family\": \"YOLO (You Only Look Once)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art object detection\",\n                \"processing_speed\": {\n                    \"training_time\": \"optimized for efficient training\",\n                    \"inference_speed\": \"sub-second\"\n                },\n                \"accuracy\": \"segmentation and ranking\"\n            },\n            \"description\": \"A YOLO model tailored for rapid inventory detection, segmentation, and ranking in challenging retail environments.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time and high accuracy are critical in a high-demand retail inventory system.\",\n        \"Scalability and adaptability are essential for future product expansion.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Fine-tuning for low-latency response\",\n            \"Enhancements for clutter handling\"\n        ],\n        \"target_device\": \"multi-store locations\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for real-time processing\"\n        ],\n        \"demonstration\": \"Demonstration of seamless inventory updates under various conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's request for a YOLO-based inventory management model with the specified performance metrics, dataset details, and requirements for real-time, scalable, and adaptable functionality. The confidence score is high as the parsed information covers key aspects of the problem and solution.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "object detection", "instruction": "As the project manager, you are now tasked with enhancing the machine learning smart city parking solution. The objective is to integrate M2Det into a scalable, robust system that not only analyzes parking space occupancy from the 'PKLot' dataset but also predicts future availability patterns. The model must demonstrate advanced adversarial resistance, handling various imaging challenges such as low light, heavy rain, and occlusions. Additionally, the system should implement real-time anomaly detection and optimize for near-zero latency, targeting a 95% detection accuracy rate with a stringent 100 milliseconds response time. Provide a detailed roadmap, architecture diagram, and a comprehensive evaluation plan, including A/B testing results.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city\",\n        \"description\": \"Enhance the machine learning smart city parking solution by integrating M2Det into a scalable, robust system. The focus is on analyzing parking space occupancy from the PKLot dataset, predicting future availability patterns, and ensuring advanced adversarial resistance, handling imaging challenges like low light, heavy rain, and occlusions. The system must have real-time anomaly detection and low latency, with a target of 95% detection accuracy and 100 milliseconds response time.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"detection accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": 0.1\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 100,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PKLot\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"parking space occupancy\"],\n            \"specification\": {\n                \"data_format\": \"images\",\n                \"type\": \"static and possibly dynamic\",\n                \"challenges\": [\"low light\", \"heavy rain\", \"occlusions\"]\n            },\n            \"description\": \"A dataset for parking space occupancy analysis with imaging challenges.\",\n            \"preprocessing\": [\"adversarial preprocessing\"],\n            \"augmentation\": [\"low light simulation\", \"rain simulation\", \"occlusion techniques\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M2Det\",\n            \"family\": \"Single Shot Multi-box Detector\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"M2Det variations, e.g., M2Det++ or M2Det-Light\",\n                \"adversarial resistance\": \"Diverse adversarial training\"\n            },\n            \"description\": \"M2Det integration with emphasis on advanced resilience and imaging challenges.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"anomaly detection algorithms\", \"real-time optimization\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"A/B testing results and detailed roadmap, architecture diagram\"\n    },\n    \"knowledge\": [\n        \"Advanced image processing techniques to handle imaging challenges\",\n        \"Strategies for scaling and robustness\"\n    ],\n    \"confidence\": {\n        \"rationale\": \"The JSON response is derived from the given instructions, mapping the requirements to the schema. The user's intent to build is indicated, along with high expertise. The problem description covers the objective, datasets, model integration, and performance metrics. Challenges and latency goals are incorporated, and the service requirements include real-time optimization and A/B testing. The confidence is high due to the clear instructions and accurate interpretation.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a financial analyst for a prestigious bank, your task is to develop a predictive model for the intricate value of customer transactions. Utilize advanced data mining techniques to cleanse and transform raw data, derive intricate features, and apply multiple regression models. Conduct an extensive exploratory analysis, justify feature selection, and evaluate performance using RMSLE on a confidential test dataset. Present a comprehensive report outlining your methodology and findings.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Develop a predictive model for customer transaction value using advanced data mining techniques, focusing on data cleansing, feature extraction, and multiple regression models. Conduct an exploratory analysis, justify feature selection, and evaluate performance using RMSLE on a confidential test dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"confidential customer transactions\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"transaction_value\"],\n            \"specification\": {\n                \"size\": \"confidential\",\n                \"dimensions\": \"inconsistent with instructions\",\n                \"length\": \"various\"\n            },\n            \"description\": \"Raw data from customer transactions requiring advanced data mining and transformation for analysis.\",\n            \"preprocessing\": [\"data cleansing\", \"feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"multiple regression models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_types\": [\"linear regression\", \"logistic regression\", \"ensemble models\"],\n                \"feature_importance\": \"unknown\"\n            },\n            \"description\": \"A predictive model using advanced regression techniques for customer transaction value prediction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced data mining and feature engineering techniques are expected to improve model accuracy.\",\n        \"Exploration of inter-feature correlations and feature interactions is crucial for selecting the best set of predictors.\",\n        \"Performance evaluation with RMSLE ensures robustness against outliers.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"methodology and findings report\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the instruction, detailing the user's intention to build a predictive model, high level of expertise, and the financial domain. The problem area is set to tabular data analysis for transaction value prediction. Performance metric (RMSLE) is included, but the exact value is not specified. The dataset is assumed confidential, which is consistent with the context. However, specific details of preprocessing, feature extraction, or model types are not provided as they are not specified in the instruction. A comprehensive report is expected as a final output, which covers the methodology and findings.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As the project manager for a machine learning team, you aim to implement a real-time, energy-efficient anomaly detection system using the Hexagon ML/UCR repository's exclusive subset of 250 time series datasets, specifically focusing on manufacturing and financial sector data. The target is to achieve a minimum precision of 75% with lightweight, autoregressive causal transformer models, taking into account computational efficiency and interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"industrial (manufacturing and financial sectors)\",\n        \"description\": \"As a project manager, you want to build a real-time, energy-efficient anomaly detection system using a subset of 250 time series datasets from the Hexagon ML/UCR repository, with a focus on manufacturing and financial data. The system must have a minimum precision of 75% and use lightweight, autoregressive causal transformer models while prioritizing computational efficiency and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.75\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"ease of interpretation\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hexagon ML/UCR time series subset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_datasets\": 250,\n                \"source_domain\": [\"manufacturing\", \"financial\"],\n                \"repository\": \"Hexagon ML/UCR\"\n            },\n            \"description\": \"A collection of 250 time series datasets from manufacturing and financial sectors, exclusive to the Hexagon ML/UCR repository.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Lightweight Autoregressive Causal Transformer\",\n            \"family\": \"Transformer-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_complexity\": \"lightweight\",\n                \"architecture\": \"autoregressive, causal\"\n            },\n            \"description\": \"Anomaly detection model that leverages lightweight transformer structures, optimized for real-time performance and interpretability in the manufacturing and financial domains.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"energy-efficient, real-time\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": \"real-world, interpretable results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the key points from the project manager's requirements. The user's intent is to build, high level of expertise is assumed, and the task is focused on time series analysis for anomaly detection. The performance metric (minimum precision) is specified, along with the need for computational efficiency and interpretability. The complexity metrics have placeholders for numerical values as they are not specified in the instruction. The dataset and model choices align with the specified repository, target domains, and model type.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "For the 'Hierarchical Network-Anonymized dataset of E-commerce Fraud' involving high-frequency, multi-factor transactions, the project manager is required to develop a robust One-Class SVM model using deep feature extraction. Emphasize on AUC-ROC optimization, real-time adaptability, and a precision threshold of 95% to guarantee minimal false positives in a privacy-preserving, big-data environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Develop a robust One-Class SVM model on the Hierarchical Network-Anonymized E-commerce Fraud dataset with high-frequency, multi-factor transactions. Focus on optimizing AUC-ROC, achieving real-time adaptability, and maintaining a precision threshold of 95% to minimize false positives in a privacy-preserving, big-data environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time adaptability\",\n                \"value\": \"required\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hierarchical Network-Anonymized E-commerce Fraud\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_type\": \"high-frequency, multi-factor\",\n                \"privacy\": \"privacy-preserving\"\n            },\n            \"description\": \"A dataset for time-series analysis with high-frequency transactions in an e-commerce fraud context.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"One-Class SVM\",\n            \"type\": \"statistical models\",\n            \"specification\": {\n                \"feature_extraction_technique\": \"deep\"\n            },\n            \"description\": \"A One-Class SVM model with deep feature extraction for anomaly detection in high-frequency, privacy-sensitive e-commerce fraud data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Privacy preservation is crucial for the Hierarchical Network-Anonymized dataset.\",\n        \"AUC-ROC optimization is necessary for robust fraud detection.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time, minimal false positives\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the given instruction, reflecting the project manager's requirements. The user's intent, problem area, and target metrics are clearly defined. The high-level expertise is inferred. However, the AUC-ROC value is not specified, which might need to be obtained through model training. Also, the project manager might need to elaborate on the exact deep feature extraction technique and the real-time adaptability requirements in the model's performance or implementation.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "tabular regression", "instruction": "As a seasoned project manager for a machine learning team, your objective is to develop an advanced regression model using the Crab Age Dataset with high dimensionality and mixed feature types. The dataset has been meticulously stratified into train, validation, and test sets, each with distinct data characteristics. The challenge lies in optimizing performance by minimizing mean absolute error (MAE) while accounting for non-linear relationships and potential interactions. The project should also include feature engineering, model tuning, and thorough experimentation with various algorithms, all to achieve state-of-the-art predictive accuracy in the age prediction task.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"data science\",\n        \"description\": \"Develop an advanced regression model using the Crab Age Dataset with high dimensionality and mixed feature types. The dataset has been stratified into train, validation, and test sets with unique characteristics. The focus is on minimizing mean absolute error (MAE), accounting for non-linear relationships and interactions, through feature engineering, model tuning, and testing with multiple algorithms for state-of-the-art accuracy in age prediction.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"optimized\",\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": \"minimal\",\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Crab Age Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"age\"],\n            \"specification\": {\n                \"dimensions\": {\n                    \"number_of_features\": \"high\",\n                    \"feature_combinations\": \"possible\"\n                },\n                \"data_distribution\": \"stratified\"\n            },\n            \"description\": \"A high-dimensional dataset with mixed feature types, divided into train, validation, and test sets each with distinct characteristics.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"handling missing values\",\n                \"one-hot encoding for categorical features\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature distribution\",\n                \"correlation analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks or ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"non-linear\",\n                \"tunable_parameters\": [\n                    \"interactions handling\",\n                    \"regularization\"\n                ]\n            },\n            \"description\": \"A highly optimized model, incorporating advanced techniques for non-linear relationships and feature interactions, specifically designed for the Crab Age Dataset regression task.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Challenge lies in handling high dimensionality, mixed feature types, and stratified data characteristics.\",\n        \"State-of-the-art accuracy requires thorough experimentation with various algorithms and fine-tuning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"PCA for dimensionality reduction\",\n            \"Interaction features\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A comprehensive report showcasing performance improvements and model explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the project requirements, including the advanced regression model needed, the dataset's characteristics, and the need for feature engineering and model optimization. Confidence is high due to the clear instruction and direct mapping to the JSON schema. However, specific model algorithms and hyperparameters are left open for tuning, as the instruction does not provide them.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "As a seasoned project manager, your mandate is to oversee the development of a robust text classification model for the Textual Entailment task. You've been tasked with enhancing a machine learning system that not only distinguishes between neutral (0), contradiction (1), and entailment (2) relationships but also exhibits exceptional performance in handling diverse linguistic nuances and context. The model must undergo multi-stage training, including feature extraction, fine-tuning, and evaluation using standard benchmark datasets for precise results. Ensure a detailed analysis of model interpretability and continuous monitoring for possible improvements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"nlp and artificial intelligence\",\n        \"description\": \"Oversee the development of a robust text classification model for the Textual Entailment task, focusing on distinguishing neutral, contradiction, and entailment relationships while handling diverse linguistic nuances and context. The model must go through multi-stage training and feature extraction, followed by fine-tuning and evaluation using standard benchmark datasets for precise results. Emphasis is on model interpretability and continuous monitoring for improvements.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"model interpretability metric (e.g., SHAP, LIME)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Standard Benchmark Datasets (e.g., SNLI, MNLI, etc.)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"neutral\", \"contradiction\", \"entailment\"],\n            \"specification\": null,\n            \"description\": \"Diverse benchmark datasets for Textual Entailment, capturing various linguistic nuances and contexts for model evaluation.\",\n            \"preprocessing\": [\"feature extraction\", \"tokenization\", \"text cleaning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-based (BERT, RoBERTa, etc.)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer architecture with fine-tuning capabilities\",\n                \"training stages\": [\"feature extraction\", \"fine-tuning\"],\n                \"languages/tuning\": \"Multilingual or domain-specific\"\n            },\n            \"description\": \"A deep learning model designed for Textual Entailment with emphasis on handling diverse linguistic nuances and context.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Standard benchmark datasets help in understanding model performance across various contexts.\",\n        \"Multi-stage training aids in model robustness and nuanced understanding.\",\n        \"Model interpretability is crucial for NLP tasks to gain insights and improve.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"TF-IDF\",\n            \"Word embeddings (Word2Vec, GloVe)\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"RESTful API\",\n            \"Integrated with existing ML platform\"\n        ],\n        \"demonstration\": \"Demonstrations on validation sets, highlighting performance on nuanced examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction by detailing the user's intent, high level of expertise, the problem domain (NLP), task (text classification for entailment), and the model's requirements. Performance metrics include accuracy, F1 score, and AUC-ROC, which are commonly used in text classification. Standard benchmark datasets and multi-stage training, as well as interpretability and continuous monitoring, are incorporated. However, specific target values for metrics are not provided as they would be determined during model development.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "The project for enhanced financial news classification involves preprocessing the extensive Reuters-21578 archive, necessitating a multi-step approach. The goal is to develop a state-of-the-art machine learning model, leveraging fastText for optimal efficiency, while achieving at least 87% precision. Strive for a tailored algorithm that differentiates market-sensitive news with an error rate <2%, ensuring minimal false alarms in critical sectors.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"The project involves preprocessing the Reuters-21578 archive in a multi-step approach. A state-of-the-art machine learning model should be developed using fastText for optimal efficiency. The target is to achieve at least 87% precision, with a focus on distinguishing market-sensitive news, aiming for an error rate <2% to minimize false alarms in critical sectors.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.87\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"archive_size\": {\n                    \"type\": \"string\",\n                    \"value\": \"extensive\"\n                },\n                \"preprocessing_steps\": {\n                    \"type\": \"array\",\n                    \"value\": [\"multi-step\"]\n                }\n            },\n            \"description\": \"An extensive financial news archive for text classification.\",\n            \"preprocessing\": [\"multi-step preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"fastText\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization\": \"state-of-the-art\",\n                \"efficiency\": \"optimized\"\n            },\n            \"description\": \"A machine learning model using fastText for financial news classification, targeting at least 87% precision and low false alarms in market-sensitive news.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"preprocessing_nature\": \"multi-step to handle extensive data\",\n            \"model_technique\": \"leveraging fastText for efficiency\",\n            \"accuracy_threshold\": \"87% precision\",\n            \"false_alarm_goal\": \"error rate < 2% to minimize false alarms\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"strive for minimal false alarms in critical sectors during model demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction by capturing the project's intent, the high level of expertise, and the specific area of natural language processing with a focus on text classification in finance. The Reuters-21578 dataset is accurately represented, as are the required model development approach, performance metrics, and the emphasis on minimizing false alarms. However, exact preprocessing steps and the modeling algorithm's fine-tuning are left unspecified as these details are not provided in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Revise the Enron Email dataset project for node classification using RGCN, focusing on Latent Feature Embeddings and Multi-scale Graph Pooling. The model must distinguish employees by communication patterns, ensuring interpretability and handling diverse message volumes, while maintaining real-time analysis for large graphs with at least 10,000 nodes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"organizational analysis\",\n        \"description\": \"Revise the Enron Email dataset project to classify employees based on communication patterns using RGCN, focusing on Latent Feature Embeddings and Multi-scale Graph Pooling. The model should be interpretable, handle varying message volumes, and support real-time analysis for large graphs with at least 10,000 nodes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"real-time\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": \"optimization for large graphs with 10,000+ nodes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email Dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"Employee\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"count\": \">10,000\"\n                }\n            },\n            \"description\": \"A large graph dataset containing email communication patterns among Enron employees, considering diverse message volumes.\",\n            \"preprocessing\": [\n                \"Message volume normalization\",\n                \"Node attribute extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Interpretable node embeddings visualization\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RGCN with Latent Feature Embeddings and Multi-scale Graph Pooling\",\n            \"family\": \"Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretablity\": \"high\",\n                \"message_volume_handling\": \"diverse\",\n                \"real_time_analysis\": true\n            },\n            \"description\": \"A deep learning model using RGCN, focusing on feature extraction and scalability for employee classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"RGCN enhances node representation learning in the context of email communication.\",\n        \"Latent Feature Embeddings improve model interpretability.\",\n        \"Multi-scale Graph Pooling enables efficient analysis of large graphs.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Latent feature extraction\",\n            \"Message volume normalization\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"edge computing platform\",\n        \"inference_engine\": [\"Real-time graph analytics framework\"],\n        \"demonstration\": \"Data-driven employee segmentation and network analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's instruction for a machine learning project, describing the task as node classification on the Enron Email dataset using RGCN. It highlights interpretability, diverse message handling, and real-time analysis for large graphs. The performance metric is not specified, but the model's properties and handling of specific techniques are explained. The dataset source and preprocessing steps, as well as model characteristics, are included. The user's high expertise level and the target deployment environment are also incorporated.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "For the upcoming fashion-forward tech project, we need a high-precision image classifier using the exclusive Fashion-MNIST_Extended dataset, which includes 3D rotated and scaled variations. Implement a state-of-the-art CapsuleNet model, targeting above 95% accuracy, while maintaining efficiency. Compare its performance against Enhanced ResNet-CNNs, evaluating the impact on computational efficiency and robustness under diverse real-world conditions, and report intricate trade-offs between capsule and convolutional architectures.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion and tech\",\n        \"description\": \"Develop a high-precision image classifier for a fashion project using the exclusive Fashion-MNIST_Extended dataset featuring 3D rotated and scaled variations. Implement a CapsuleNet model with a target accuracy above 95% and prioritize efficiency. Compare the model to Enhanced ResNet-CNNs, evaluating computational efficiency and robustness under real-world conditions, highlighting intricate trade-offs between capsule and convolutional architectures.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST_Extended\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"3D_rotations\": true,\n                \"scaling_variations\": true\n            },\n            \"description\": \"A fashion-focused image dataset with 3D rotated and scaled variations of the original MNIST dataset.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"CapsuleNet\",\n            \"family\": \"Capsule Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": \"optimized for speed and precision\",\n                \"state_of_the_art\": true\n            },\n            \"description\": \"A state-of-the-art image classifier using CapsuleNet architecture specifically tailored for the Fashion-MNIST_Extended dataset.\"\n        },\n        {\n            \"name\": \"Enhanced ResNet-CNNs\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"performance comparison\": true\n            },\n            \"description\": \"A comparison reference model using Enhanced ResNet-CNNs architecture\"\n        }\n    ],\n    \"knowledge\": [\n        \"3D rotations and scaling variations in the dataset require robustness and high precision from the model\",\n        \"Efficiency is a key consideration for a fashion-forward tech project\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Report with detailed performance analysis and trade-offs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's intent to build a high-precision image classifier, the expertise level, and the project's fashion-forward tech context. The problem area, dataset, and models are defined based on the instructions. Performance metrics are included, with a clear target accuracy. The comparison to Enhanced ResNet-CNNs and emphasis on efficiency and robustness are also captured. However, specific preprocessing or feature engineering steps are not provided, as these were not detailed in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a cutting-edge project manager, you are directing a specialized machine learning team to tackle a complex text classification challenge. The extensive ChatGPT Prompt Diverse Dataset, containing a plethora of intricate sentences with unseen nuances, necessitates the design of a high-precision, multi-label classifier. The team must not only optimize for accuracy but also demonstrate superior performance in handling domain-specific language and mitigating bias. Your objective is to develop a state-of-the-art model that surpasses benchmark metrics while maintaining interpretability, all within a stringent timeline.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI research or industry with complex language requirements\",\n        \"description\": \"Develop a high-precision, multi-label classifier for the ChatGPT Prompt Diverse Dataset with unseen nuances, focusing on domain-specific language, bias mitigation, and interpretability within a tight timeline.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"model interpretability\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ChatGPT Prompt Diverse Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"classification labels\"],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"unseen nuances\": true,\n                \"domain specificity\": true,\n                \"bias mitigation aspects\": true\n            },\n            \"description\": \"A complex text dataset with nuanced sentences, targeting domain-specific language and bias reduction\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true\n            },\n            \"description\": \"A high-performing, multi-label text classifier designed for the ChatGPT Prompt Diverse Dataset with focus on domain adaptation, bias mitigation, and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The dataset's complexity and unseen nuances require a robust and adaptable model architecture.\",\n        \"Superior performance in handling domain-specific language is critical for success.\",\n        \"Mitigating bias is an essential aspect for credibility and fairness.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"require interpretability in model decisions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the project's requirements, including the user's intent to build a model, their high expertise, the problem domain, and the diverse dataset. Performance metrics (accuracy, precision, recall) and complexity constraints (training time, interpretability) are inferred based on the goal of state-of-the-art performance and interpretability. The dataset and model descriptions are adapted to the provided context. The confidence score is set to high given the clear task description and direct mapping to the schema.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager overseeing the development of a cutting-edge AI-driven time-series forecasting system, you must now address the following multi-faceted challenge for the machine learning team:\n\nUsing the Electricity dataset, which exhibits intricate temporal dependencies and contains real-world anomalies, partition it into non-overlapping train, validation, and test sets with stringent 50% sequence overlap for smooth transition. Each sequence input (INPUT_SEQ_LEN=96, with high-dimensional features, INPUT_DIM=321) should capture historical patterns. The objective is to engineer a novel forecasting architecture that not only predicts the subsequent 96 steps (PRED_SEQ_LEN=96) but also accounts for seasonality and trends. It's crucial to optimize for both Mean Squared Error (MSE) and Mean Absolute Error (MAE), ensuring robustness across various horizons. Additionally, design a detailed experimental plan for hyperparameter tuning and ensemble methods, and document the trade-offs between complexity and performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy or utilities\",\n        \"description\": \"Develop a cutting-edge AI-driven time-series forecasting system using the Electricity dataset, addressing temporal dependencies and real-world anomalies. Partition the data with 50% sequence overlap, and design a model to predict the next 96 steps (96-step horizon) with a focus on seasonality and trends. Optimize for Mean Squared Error (MSE) and Mean Absolute Error (MAE), and include a plan for hyperparameter tuning and ensemble methods.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Squared Error (MSE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Error (MAE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"temporal_overlap\": 50,\n                \"anomalies\": true,\n                \"seasonality\": true,\n                \"trends\": true\n            },\n            \"description\": \"Intricate with temporal dependencies and real-world anomalies, high-dimensional features (321), and a 96-step historical context (INPUT_SEQ_LEN=96).\",\n            \"preprocessing\": [\n                \"Sequence splitting with 50% overlap\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"novel forecasting architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"prediction_window\": 96,\n                \"sequence_length\": 96,\n                \"account_for\": [\"seasonality\", \"trends\"],\n                \"architecture\": \"custom designed for time-series analysis\"\n            },\n            \"description\": \"A tailored model that forecasts 96-step ahead, considering seasonality and trends.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Optimize for MSE and MAE simultaneously, trading-off between complexity and performance.\",\n        \"A detailed experimental plan for hyperparameter tuning and ensemble methods.\",\n        \"Account for real-world anomalies in the Electricity dataset.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Custom-designed sequence handling\",\n            \"Hyperparameter tuning\",\n            \"Ensemble techniques\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"description\": \"A visually and quantitatively interpretable demonstration of the forecasting system's performance across different horizons and under varying scenarios.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the detailed instruction provided, capturing the user's intent to build a machine learning model, high expertise level, and the problem statement covering the Electricity dataset, task, and performance metrics. The dataset specifications are accurate based on the instruction, and the model family and architecture are inferred from the requirements of the time-series forecasting task. The need for hyperparameter tuning and ensemble methods is also noted. The user's confidence is high due to the comprehensive representation of the project requirements.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For our upcoming precision agriculture initiative, develop a lightweight, DeepLabV3+ model specifically tailored for the proprietary AgriSeg-Advanced dataset. The model must exhibit exceptional segmentation accuracy above 90% in recognizing crop types and minimizing false positives for weeds, while ensuring real-time performance with a minimum frame rate of 30 FPS on resource-constrained embedded devices operating under diverse environmental conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Develop a lightweight DeepLabV3+ model for the proprietary AgriSeg-Advanced dataset in precision agriculture. The model should have high segmentation accuracy above 90% for crop types and minimize false positives for weeds, while achieving real-time performance with a minimum frame rate of 30 FPS on embedded devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"segmentation accuracy\",\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AgriSeg-Advanced\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"crop types\", \"weeds\"],\n            \"specification\": {\n                \"size\": {\n                    \"width\": null,\n                    \"height\": null,\n                    \"depth\": null\n                },\n                \"data_distribution\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                }\n            },\n            \"description\": \"A proprietary dataset for precision agriculture, focused on crop type and weed segmentation.\",\n            \"preprocessing\": [\n                \"resize\",\n                \"normalization\"\n            ],\n            \"augmentation\": [\n                \"random cropping\",\n                \"data augmentation for crops and weeds\"\n            ],\n            \"visualization\": [\n                \"semantic segmentation maps\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"DeepLabV3+\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"parameters\": {\n                        \"number_of_layers\": null,\n                        \"encoder\": \"efficient or mobileNet\",\n                        \"decoder\": \"Depthwise Separable Convolution\"\n                    },\n                    \"performance_tradeoff\": {\n                        \"accuracy\": \"above 90%\",\n                        \"inference_speed\": \"30 FPS\"\n                    }\n                }\n            },\n            \"description\": \"A lightweight DeepLabV3+ model optimized for high segmentation accuracy and real-time performance on resource-constrained devices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"AgriSeg-Advanced's challenging environmental conditions demand robustness in model design.\",\n        \"Real-time performance on embedded devices requires careful architecture and optimizations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained embedded devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient on-device inference\"],\n        \"demonstration\": \"runtime segmentation performance and frame rate demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is structured according to the JSON schema, focusing on user's intent, expertise, and the precision agriculture context. The area and downstream task are derived from the model's purpose (image segmentation). Accuracy requirement, real-time performance, and resource constraints are explicitly defined in the problem and performance metrics. Dataset details and model specifics, including preprocessing and augmentation, are incorporated. The confidence score is high due to the clear mapping to the given instructions.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "After reviewing recent advancements, can you assess the feasibility of integrating transformer models for multi-modal analysis, specifically detecting high-resolution video forgeries in the diverse 'Celeb-DF' dataset? Elaborate on the required adaptations and performance benchmarks to ensure advanced accuracy beyond random predictions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"computer vision\",\n        \"description\": \"Assess the feasibility of integrating transformer models for high-resolution video forgery detection in the diverse 'Celeb-DF' dataset, considering recent advancements and emphasizing the need for advanced accuracy beyond random predictions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_type\": \"high-resolution video\",\n                \"diversity\": \"diverse\"\n            },\n            \"description\": \"A multimodal dataset for detecting high-resolution video forgeries, featuring the 'Celeb-DF' dataset as the focus.\",\n            \"preprocessing\": [\"adaptations required for transformer models\"],\n            \"augmentation\": [\"potentially required for improved performance\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based video forgery detection model\",\n            \"family\": \"Transformer models (specifically for video analysis)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-modal architecture\",\n                \"adaptations\": \"fine-tuning or architectural modifications for video data\"\n            },\n            \"description\": \"A novel approach to multimodal forgery detection, leveraging transformers for high-resolution video analysis and aiming for advanced accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Recent advancements in transformer models, particularly for video understanding, may bring promising results in video forgery detection.\",\n        \"Performance benchmarking against state-of-the-art methods is essential to ensure accuracy beyond random predictions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"require explanation on how to demonstrate model performance and detect forgeries\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intention to consult about integrating transformer models for a specific multi-modal problem. The high expertise level, research area (computer vision), and application domain (video forgery detection) are derived from the context. Performance metrics, complexity, and required adaptations are derived directly from the instruction. However, the exact performance values for accuracy and other metrics are not provided as they depend on the user's prior knowledge or a literature review. Similarly, the deployment details and demonstration requirements are not specified but hinted at.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image segmentation", "instruction": "In a multi-phased assignment for the PlantVillage dataset, you are tasked with developing an advanced attention-guided DenseNet architecture for high-resolution segmentation of both subtle and distinct disease manifestations in plant leaves. Not only must the model achieve exceptional accuracy, but also provide in-depth, interactive visual explanations for non-technical stakeholders to effectively interpret and understand plant health conditions, thereby enabling early intervention and promoting sustainable agriculture practices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Develop an advanced attention-guided DenseNet architecture for high-resolution segmentation of plant diseases in the PlantVillage dataset. The model should achieve excellent accuracy and provide interactive visual explanations for non-technical stakeholders.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PlantVillage dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"data_split\": \"multi-phased\"\n            },\n            \"description\": \"A dataset for plant leaf disease segmentation with high-resolution images and a multi-phased assignment requirement.\",\n            \"preprocessing\": [\"image resizing, normalization\"],\n            \"augmentation\": [\"data augmentation for plant leaf images\"],\n            \"visualization\": [\n                \"interactive visual explanations for disease manifestations\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Attention-guided DenseNet\",\n            \"family\": \"DenseNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"attention-guided\",\n                \"explainability\": true\n            },\n            \"description\": \"A deep learning model using DenseNet architecture with attention mechanism for plant leaf disease segmentation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Attention-guided DenseNets enhance detail detection in segmentation tasks.\",\n        \"The need for visual explanations for non-technical stakeholders highlights the importance of interpretability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for plant leaf health\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretable models for stakeholders\"],\n        \"demonstration\": \"Interactive visualizations and model interpretation walk-through\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is structured based on the provided instruction, including the user's intention to build, the advanced DenseNet architecture for plant disease segmentation, the application in agriculture, and the focus on interpretability. Performance metrics like accuracy are left unspecified as they usually need a validation phase. Preprocessing and augmentation steps are listed according to the requirement, and the dataset details and model description match the given task. The lack of technical details like specific accuracy target, detailed model specifications, or exact deployment environment is due to the nature of the high-level instruction.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "multimodal emotion recognition", "instruction": "As the project manager for a cutting-edge AI research team, I need a comprehensive, in-depth plan for a state-of-the-art emotion recognition system. The proposal should entail a novel fusion of transformer and CNN architectures, specifically designed for the Multimodal EmotionLines Dataset (MELD). Aim for an impressive F1 score of 0.9 or above in all emotion categories, while exploring cross-modal synchronization and resistance to variance in real-world scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal emotion recognition\",\n        \"application_domain\": \"artificial intelligence research\",\n        \"description\": \"As a project manager for an AI research team, a comprehensive plan is required for a state-of-the-art emotion recognition system. The system should integrate a novel fusion of transformer and CNN architectures, specifically tailored for the Multimodal EmotionLines Dataset (MELD). The target is to achieve an F1 score of 0.9 or above across all emotion categories, with a focus on cross-modal synchronization and resilience to real-world variance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-world variance resilience\",\n                \"value\": null,\n                \"unit\": \"percent (%)\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multimodal EmotionLines Dataset (MELD)\",\n            \"modality\": [\"text\", \"image\", \"audio\"],\n            \"target_variables\": [\"emotions\"],\n            \"specification\": {\n                \"description\": \"A large-scale multimodal dataset for emotion recognition, including synchronised annotations for multiple modalities.\"\n            },\n            \"preprocessing\": [\n                \"Cross-modal synchronization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Inter- and intra-modal feature analysis\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-CNN Fusion Architecture\",\n            \"family\": \"Transformers and Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"description\": \"A novel architecture combining transformers for handling multimodal data and CNNs for specific emotion recognition tasks.\"\n            },\n            \"description\": \"A cutting-edge approach for emotion recognition, combining the strengths of transformers and CNNs.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Innovative fusion architecture is crucial for the MELD dataset's unique multi-modal nature.\",\n        \"Resilience to real-world variance requires careful fine-tuning and robustness mechanisms.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Adaptive cross-modal fusion strategy\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"Interactive visualizations of emotion recognition performance across various scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the high-level project requirements, including the fusion of transformer and CNN for multimodal emotion recognition, the target F1 score, and the focus on cross-modal synchronization. The MELD dataset's details are extracted, along with preprocessing and augmentation steps, and the specific novel model architecture is specified. However, the confidence score is not complete due to the lack of real-world variance resilience threshold, which would require more context to determine.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a transportation planner, request a sophisticated deep learning architecture, implementing a transfer learning-driven ResNet-CuDNNS Bidirectional GRU model on the sparse and high-resolution traffic data. The task is to predict minute-level traffic volume for the subsequent three-month period, differentiating performance in hourly spikes during rush hours and discernible trends on public holidays and weekends. Also, evaluate the impact of meteorological factors and compare it with the baseline CNN-LSTM model's efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"transportation\",\n        \"description\": \"Request a deep learning model to predict minute-level traffic volume for the next three months, differentiating performance during rush hours, and considering public holidays and weekends. The architecture should be based on transfer learning using a ResNet-CuDNNS Bidirectional GRU model on sparse and high-resolution traffic data. Additionally, evaluate the impact of meteorological factors and compare it to a baseline CNN-LSTM model.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Absolute Error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE (Mean Absolute Percentage Error)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Accuracy during rush hours\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Accuracy on public holidays\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Accuracy on weekends\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sparse and High-Resolution Traffic Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"traffic_volume\"],\n            \"specification\": {\n                \"resolution\": \"minutes\",\n                \"data_length\": \"sparse and covers the next three months\",\n                \"time_interval\": \"one minute steps\"\n            },\n            \"description\": \"Transportation data with high resolution and sparsity, covering the next three months for predicting minute-level traffic volume.\",\n            \"preprocessing\": [\n                \"Data normalization\",\n                \"Data imputation\",\n                \"Feature engineering (meteorological factors)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNet-CuDNNS Bidirectional GRU\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pretrained_model\": \"transfer learning\",\n                \"architecture\": {\n                    \"ResNet\": {\n                        \"type\": \"ResNet\",\n                        \"variant\": \"CuDNNS\"\n                    },\n                    \"GRU\": {\n                        \"direction\": \"bidirectional\"\n                    }\n                },\n                \"hyperparameters\": {\n                    \"dropout_rate\": null,\n                    \"learning_rate\": null,\n                    \"optimizer\": null\n                }\n            },\n            \"description\": \"A transfer learning-based deep learning model using ResNet-CuDNNS and Bidirectional GRU for traffic forecasting, considering time-series characteristics and impact of meteorological factors.\"\n        },\n        {\n            \"name\": \"CNN-LSTM Baseline Model\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"CNN\": {},\n                    \"LSTM\": {}\n                },\n                \"hyperparameters\": {\n                    \"dropout_rate\": null,\n                    \"learning_rate\": null,\n                    \"optimizer\": null\n                }\n            },\n            \"description\": \"A comparison CNN-LSTM model for the traffic forecasting task, for evaluating efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning enables efficient adaptation of existing model weights.\",\n        \"Performance differentiation is important for rush hours, public holidays, and weekends.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Meteorological data integration\",\n            \"Hourly and holiday/weekend-based feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Performance comparison with baseline model\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a deep learning model, their expertise level, and the specific requirements for the transportation domain. The description is based on the task details, including ResNet-CuDNNS and bidirectional GRU. The performance metrics and complexity requirements are relevant and missing values are set to null as they would typically need to be determined through model tuning and evaluation. The dataset and model properties are based on the given information, and the user's expectations for feature engineering and demonstrating model performance are included.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a project manager for the AI-driven Bilibili Danmu enhancement, seek a high-performance, scalable Bert4Rec-based sequential recommendation system. Emphasize on advanced natural language processing of complex user comment interactions, user behavior patterns, and cross-platform integration for an unprecedented accuracy boost in video suggestions. Incorporate real-time feedback analysis and a novel feature extraction layer to stay ahead of competitors.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"media streaming platform\",\n        \"description\": \"As a project manager for the AI-driven Bilibili Danmu enhancement, seek a high-performance, scalable Bert4Rec-based sequential recommendation system. Emphasis on advanced natural language processing of complex user comment interactions, user behavior patterns, and cross-platform integration for an unprecedented accuracy boost in video suggestions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean average precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Bilibili Danmu Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"video suggestions\"],\n            \"specification\": {\n                \"data_format\": \"sequence of comment interactions and user behavior data\",\n                \"size\": \"cross-platform and real-time data\",\n                \"min_length\": null,\n                \"max_length\": null\n            },\n            \"description\": \"A dataset capturing complex user comment interactions, behavior patterns across platforms for Bilibili video suggestions.\",\n            \"preprocessing\": [\n                \"advanced NLP processing (Bert4Rec)\",\n                \"cross-platform integration\"\n            ],\n            \"augmentation\": [\n                \"real-time feedback analysis\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Bert4Rec\",\n            \"family\": \"Transformer-based sequential recommendation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"incorporating NLP processing for comment interactions\",\n                \"integration\": \"cross-platform\",\n                \"feature_extraction_layer\": \"novel for improved accuracy\"\n            },\n            \"description\": \"A high-performance Bert4Rec model optimized for sequential recommendation, emphasizing NLP processing and cross-platform integration\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced NLP for complex interactions and behavior patterns\",\n        \"Cross-platform integration for wider data scope\",\n        \"Real-time feedback analysis for adaptive recommendations\",\n        \"Incorporating a novel feature extraction layer for a competitive edge\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"NLP-based comment and behavior analysis\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"Bilibili recommendation service\",\n        \"inference_engine\": [\n            \"real-time inference using optimized Bert4Rec model\"\n        ],\n        \"demonstration\": \"interactive video suggestions with improved accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the requirements of a high-performance sequential recommendation system, focusing on Bert4Rec, NLP for comment interactions, cross-platform integration, and real-time feedback. Performance metrics like accuracy, precision, recall, and MAAP are not explicitly provided, so values are set to null for now. The model's NLP and feature extraction layers are captured accurately, indicating the project manager's goals for the model's performance. Confidence score is high, but some details might need refining if more specific metrics were given.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager requires a cutting-edge e-commerce translation system, employing advanced U-Net architecture, specifically tailored for Alibaba's Taobao dataset. The focus should be on precision, retail and fashion terminologies, and seamless interoperation. Target a BLEU score of 40 or above for outstanding readability, and ensure the model can effortlessly integrate with mainstream platforms for real-time, optimized translation of product narratives, meeting the most stringent quality standards in the competitive market.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"e-commerce (retail and fashion)\",\n        \"description\": \"Develop a cutting-edge e-commerce translation system using the U-Net architecture, specifically tailored for Alibaba's Taobao dataset. Focus on precision with retail and fashion terminologies and ensure seamless interoperation. Aim for a BLEU score of 40 or above for excellent readability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 40\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Taobao\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"train\": \"(number of training instances)\",\n                    \"validation\": \"(number of validation instances)\",\n                    \"test\": \"(number of test instances)\"\n                },\n                \"linguistic details\": {\n                    \"multilingual\": true,\n                    \"focus_on_terms\": [\"retail\", \"fashion\"]\n                }\n            },\n            \"description\": \"Advanced e-commerce dataset for machine translation, containing product narratives for the Taobao platform.\",\n            \"preprocessing\": [\"lexicon customization for retail/fashion terms\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net for E-commerce Translation\",\n            \"family\": \"U-Net architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"(specifics of U-Net adapted for e-commerce translation)\",\n                \"precision Focus\": true,\n                \"interoperability\": \"seamless\"\n            },\n            \"description\": \"An advanced NLP model specifically designed for precision-oriented e-commerce translations with a focus on retail and fashion terminologies.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Precision is crucial for retail and fashion translations in e-commerce context.\",\n        \"Customizing for targeted terminologies can improve the model's performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"linguistic customization for retail/fashion\", \"domain adaptation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time translation engines\"],\n        \"demonstration\": \"Achieving high-quality, real-time translation of product narratives\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes all the required information as per the schema, with a high focus on the U-Net architecture for the Alibaba Taobao dataset. The expert level of the user and the intent to build a model are clearly defined. The description of the problem and requirements, such as precision, interoperation, and target BLEU score, are detailed and aligned with the instruction. However, actual numbers for dataset instances are not provided, so they can be researched based on the Taobao dataset size. The confidence score is high due to a clear understanding of the instructions.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create a cutting-edge neural machine translation system that employs the Bilingual Encoders and Decoders (BraidedNet) architecture for translating real-time social media content between English and Spanish. The model should focus on not only formal and colloquial text, but also be highly adept at translating emojis, memes, and regional slang specific to teenage online conversations. The system must surpass a SacreBLEU benchmark of 40, with a special emphasis on preserving context and maintaining the humor in sarcastic remarks. To guarantee seamless user engagement, the model's end-to-end latency must be optimized to an average of 100 milliseconds per post, demanding efficient parallel processing and memory management.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"social media\",\n        \"description\": \"Develop a cutting-edge neural machine translation system using the Bilingual Encoders and Decoders (BraidedNet) architecture for real-time English to Spanish translation. The system should handle both formal and informal text, as well as emojis, memes, and regional slang in teenage online conversations. It must surpass a SacreBLEU benchmark of 40 and preserve context and humor in sarcastic comments. The system should have an average end-to-end latency of 100 milliseconds per post, prioritizing efficient parallel processing and memory management.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"SacreBLEU\",\n                \"value\": 40\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Average End-to-End Latency\",\n                \"value\": 0.1, \n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\", \"multimodal\"], \n            \"target_variables\": [], \n            \"specification\": {\n                \"multimodal_data_types\": [\"emojis\", \"memes\", \"text\"],\n                \"languages\": [\"English\", \"Spanish\"]\n            },\n            \"description\": \"A comprehensive dataset containing diverse text and multimodal content for training the Bilingual Encoders and Decoders (BraidedNet) model.\",\n            \"preprocessing\": [\n                \"Multilingual Text Preprocessing\",\n                \"Multimodal Data Preprocessing\"\n            ],\n            \"augmentation\": [\n                \"Emotion-Aware Augmentation\",\n                \"Slang-aware Synonym Replacement\"\n            ],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BraidedNet\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Bilingual Encoders and Decoders\",\n                \"target_lang_code\": \"ES\",\n                \"specialization\": \"Real-time social media translation, including emojis, memes, and regional slang\"\n            },\n            \"description\": \"An advanced neural machine translation model employing the BraidedNet architecture, tailored for English to Spanish translation with emphasis on informal and contextual nuances.\"\n        }\n    ],\n    \"knowledge\": [\n        \"BraidedNet architecture's resilience to noisy and informal content\",\n        \"Importance of emoji and meme translation in social media context\",\n        \"Challenge of preserving humor in sarcasm detection\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Context-Aware Attention Mechanism\",\n            \"Parallelization for real-time processing\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\", \"TensorFlow Serving\"],\n        \"demonstration\": \"Translation of sarcastic posts with preserved humor\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response accurately reflects the user's instruction, considering their intent to build a high-expertise machine translation system using BraidedNet architecture. The problem area, downstream task, performance metric, and complexity requirements are clearly captured. The Bilingual Encoders and Decoders model is specified, along with its specialization and the importance of handling diverse content and latency constraints. However, some aspects such as specific dataset preprocessing steps or feature engineering are not specified as part of the user's instruction, so those may not be included in the response.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "For the innovative fashion tech initiative, develop a sophisticated image recognition system utilizing the expanded and challenging Fashion-MNIST_Extended dataset, featuring 3D rotations and scale variations. Mandate the implementation of a cutting-edge CapsuleNet model, targeting not only at least 95% accuracy but also achieving energy-efficient design. Conduct a rigorous comparison with Enhanced ResNet-CNNs, analyzing their performance in both computational efficiency and resilience across various realistic scenarios, and provide a detailed analysis on the subtle nuances and trade-offs between capsule and convolutional architectures in terms of accuracy, speed, and robustness.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion tech\",\n        \"description\": \"Develop a sophisticated image recognition system for the Fashion-MNIST_Extended dataset with 3D rotations and scale variations. Use a CapsuleNet model, aiming for at least 95% accuracy while ensuring energy efficiency. Compare its performance to Enhanced ResNet-CNNs, evaluating computational efficiency, resilience in realistic scenarios, and analyze the trade-offs between capsule and convolutional architectures in accuracy, speed, and robustness.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null, \n                \"unit\": \"measured in watt-hours per classification\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST_Extended\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"3D rotations and scale variations\": true\n            },\n            \"description\": \"A challenging fashion image dataset with added 3D transformations.\",\n            \"preprocessing\": [\"3D rotation and scaling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"CapsuleNet\",\n            \"family\": \"Capsule Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy target\": 0.95,\n                \"energy efficiency\": true\n            },\n            \"description\": \"A state-of-the-art image classification model designed for Fashion-MNIST_Extended dataset with focus on accuracy and energy efficiency.\"\n        },\n        {\n            \"name\": \"Enhanced ResNet-CNNs\",\n            \"family\": \"Residual Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"comparison against CapsuleNet\": true\n            },\n            \"description\": \"A competitor model used for comparison in terms of performance, efficiency, and resilience.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The importance of energy efficiency in the fashion tech context\",\n        \"Challenges and nuances associated with capsule vs convolutional architectures\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A detailed analysis of performance and trade-offs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the instruction provided, incorporating the user's intent to build a model, the high-level requirement for accuracy and energy efficiency, the specific datasets and model families, and the requested performance comparison and analysis. The complexity of the task is reflected in the need for rigorous analysis and comparison, but some specific metric targets or preprocessing details are left open for a more comprehensive response based on further research or consultation.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Elaborate on the Enron Email dataset analysis by integrating a stacked RGCN architecture for node classification, emphasizing Hierarchical Latent Feature Exploration and Progressive Graph Coarsening. The objective is to detect subtle communication networks, achieving high precision in classifying employees based on intricate interaction dynamics, volume disparities, and real-time analysis of massive datasets with over 100,000 nodes, while ensuring model explainability and scalable performance for large-scale data inputs.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"finance (specifically, Enron email data)\",\n        \"description\": \"Analyze the Enron Email dataset using a stacked RGCN (Recurrent Graph Convolutional Network) architecture with a focus on Hierarchical Latent Feature Exploration and Progressive Graph Coarsening. The goal is to detect subtle communication networks among employees, achieve high precision in classification, handle intricate interaction dynamics, volume disparities, and perform real-time analysis on large datasets with over 100,000 nodes. Additionally, the model must prioritize explainability and scalable performance for large-scale data inputs.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email Dataset\",\n            \"modality\": [\"text\", \"graph\"],\n            \"target_variables\": [\"employee\"],\n            \"specification\": {\n                \"nodes_count\": 100000,\n                \"edge_count\": null,\n                \"data_size\": null\n            },\n            \"description\": \"Dataset of email communications among Enron employees, including textual data and network structure for analysis.\",\n            \"preprocessing\": [\n                \"Tokenization\",\n                \"Text normalization\",\n                \"Graph construction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Graph structure visualization\",\n                \"Communication patterns\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Stacked RGCN\",\n            \"family\": \"Recurrent Graph Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"hidden_layers\": null,\n                \"Latent Feature Exploration\": {\n                    \"description\": \"Hierarchical and progressive exploration of employee features\"\n                },\n                \"Graph Coarsening\": {\n                    \"description\": \"Real-time analysis technique for scalability\"\n                }\n            },\n            \"description\": \"An advanced model that utilizes stacked RGCN for classifying employees based on email interactions in the Enron dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hierarchical feature exploration and progressive coarsening enable detailed analysis of employee interactions.\",\n        \"Model scalability is crucial for handling over 100,000 nodes.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"word embeddings\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"API-based\",\n        \"inference_engine\": [\n            \"GPU-based\"\n        ],\n        \"demonstration\": \"Visual explanations of classified employee clusters\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a model (high expertise), with the problem focusing on NLP and node classification. The dataset is described in detail, as well as the specific RGCN architecture, and emphasis on explainability and scalability. However, the exact precision value, inference time, and memory usage are unknown and should be filled based on the actual model performance. Performance metrics and complexity metrics are left as null placeholders.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "As the project manager for an innovative grocery retail platform, task the machine learning team to design a state-of-the-art YOLO-enhanced inventory control system, leveraging the extensive \"Adaptive Retail Environment Insights Dataset\" (AREID). This dataset encompasses a staggering 5,000 exclusive product categories, demanding not only precise and real-time object recognition, but also advanced semantic segmentation and ranking with a high standard of 98% average precision and recall. Emphasize the need for consistent sub-millisecond response times (<0.1 seconds) across all locations, while ensuring system resilience, expansion potential, and seamless integration with dynamic product updates. Moreover, mandate the model to adapt and optimize its performance in rapidly changing environmental conditions, such as seasonal variations and temporary store layouts, with minimal manual intervention.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"Design a state-of-the-art YOLO-enhanced inventory control system for an innovative grocery platform using the Adaptive Retail Environment Insights Dataset. The system should have 98% average precision and recall, sub-millisecond response times (<0.1 seconds), and be able to adapt to changing environmental conditions with minimal manual intervention.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average precision\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"average recall\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": 0.001,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Adaptive Retail Environment Insights Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"product_categories\": 5000\n            },\n            \"description\": \"A comprehensive dataset for object recognition, semantic segmentation, and ranking in the retail context with diverse product categories and real-time requirements.\",\n            \"preprocessing\": [\n                \"dynamic image augmentation\"\n            ],\n            \"augmentation\": [\"YOLO-based\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO-enhanced Inventory Control System\",\n            \"family\": \"YOLO (You Only Look Once) architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"resilience\": \"built-in\",\n                \"adaptation\": \"dynamic\"\n            },\n            \"description\": \"A machine learning model leveraging YOLO for object detection with advanced features like semantic segmentation and the ability to adapt under variable retail environments.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model must address the high complexity of 5,000 product categories and maintain real-time performance.\",\n        \"Adaptive behavior to changing conditions is crucial for the system's effectiveness.\",\n        \"Seamless integration with dynamic product updates is a requirement.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud with fast processing speed\",\n        \"deployment_endpoint\": \"retail platform API\",\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": \"live product recognition and inventory update demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction, including the high-level user intent, the expertize level, and the specific problem details like the dataset and the YOLO-enhanced object detection system requirements. Performance metrics are specified, along with response time and complexity constraints. The system's adaptation potential and integration with dynamic updates are also covered. However, the model's exact architecture and implementation details are left to the machine learning team to design, as it wasn't specified in the instruction.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "time-series forecasting", "instruction": "Develop a specialized, deep learning hybrid model, specifically combining a long short-term memory (LSTM) auto-encoder and a vector autoregressive (VAR) model, for the precise forecasting of our app's daily active users. Emphasize the implementation of a tailored concept drift detection mechanism, an advanced seasonal decomposition approach accounting for complex patterns, and a hybrid regularization strategy that balances between Bayesian priors and early stopping to guarantee optimal performance across an extensive range of scenarios and future data fluctuations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"mobile apps\",\n        \"description\": \"Develop a specialized deep learning hybrid model combining an LSTM auto-encoder and a VAR model for precise forecasting of daily active users in an app, including a concept drift detection mechanism, advanced seasonal decomposition, and a hybrid regularization strategy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"App Daily Active Users\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"daily active users\"],\n            \"specification\": {\n                \"length\": null,\n                \"time_freq\": \"daily\",\n                \"features\": {\n                    \"LSTM\": null,\n                    \"VAR\": null\n                }\n            },\n            \"description\": \"Contains historical app daily active users data for model training and forecasting.\",\n            \"preprocessing\": [\n                \"seasonal decomposition\",\n                \"concept drift detection\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid LSTM+VAR Model\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"lstm_layers\": null,\n                \"var_model\": null,\n                \"customization\": \"concept drift detection, hybrid regularization\"\n            },\n            \"description\": \"A deep learning model combining LSTM auto-encoder and VAR model, with custom adaptations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Combining LSTM and VAR for robust time-series forecasting, using tailored concept drift detection and advanced seasonal decomposition.\",\n        \"Hybrid regularization balances Bayesian priors and early stopping for optimal performance across various scenarios.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"advanced feature extraction from combined models\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"real-time forecasting dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction. The user's intent to build a model, high expertise, and specific task in time-series analysis are all covered. The description of the problem and model is detailed, including the hybrid approach, model components, and required preprocessing steps. The metrics (MAE, accuracy, inference time, and memory size) are included with the understanding that they are typically used in time-series forecasting. The confidence score is set to high due to the clear task description and comprehensive interpretation within the schema.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "Incorporate a time-bound milestone for the project, specifying that the transfer learning model must achieve at least 95% accuracy within three months. Additionally, ensure the model's explainability by integrating a technique for visualizing feature importance and present a detailed report on overfitting mitigation strategies during the development process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"image recognition\",\n        \"description\": \"Incorporate a time-bound milestone for the project, aiming for at least 95% accuracy within three months using transfer learning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ImageNet (Transfer Learning Dataset)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A pre-trained dataset for transfer learning, suitable for image classification tasks\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transfer Learning Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"training_time\": \"3 months\",\n                \"initial_accuracy\": null\n            },\n            \"description\": \"A transfer learning model for image classification with a goal of achieving at least 95% accuracy within the given time frame\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model must incorporate explainability through feature importance visualization.\",\n        \"Mitigation strategies for overfitting will be detailed in the project report.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A report on overfitting mitigation strategies and feature importance visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent for building a model with a high expertise level. The project is specified in computer vision with image classification as the downstream task and an accuracy target of 95% within a time-bound milestone of three months. The model type, dataset, and performance metric are based on the requirements. However, the initial accuracy of the transfer learning model is not mentioned, as it depends on the starting point. Overfitting mitigation and feature importance visualization are key requirements for model explainability. The confidence score is high due to the clarity of the requirements in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a seasoned project manager, you've been elevated to spearhead a cutting-edge media planning strategy. Your assignment involves developing an advanced regression model for the complex Media Campaign Cost Dataset, which boasts high-dimensional temporal patterns, rare anomalies, and a diverse mix of categorical features. The data is divided into three distinct subsets: a meticulously curated training set, a validation set harboring temporal irregularities, and an elusive test set packed with extreme outliers. Your objective now encompasses not only building a model that exhibits exceptional predictive power and handles feature interactions, temporal dependencies, and missing data gracefully, but also achieving state-of-the-art performance using a domain-specific hybrid RMSLE metric. \n\nTo demonstrate resilience against concept drift and maintain interpretability, you must devise innovative feature engineering techniques, rigorously optimize the model using evolutionary algorithms, and incorporate explainable AI methods. Remember to create a sleek and visualized report, explaining the model's decision-making process in layman's terms for non-technical business stakeholders, while also showcasing the nuances of your model's performance across varying market conditions. This project demands both technological prowess and persuasive storytelling for successful implementation and adoption.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"marketing or advertising\",\n        \"description\": \"Develop an advanced regression model for the Media Campaign Cost Dataset, which includes high-dimensional temporal patterns, rare anomalies, and diverse categorical features. The model should exhibit exceptional predictive power, handle feature interactions, temporal dependencies, and missing data, while adhering to a domain-specific hybrid RMSLE metric and demonstrating resilience against concept drift.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"hybrid RMSLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model explainability\",\n                \"value\": null,\n                \"unit\": \"interpretable AI methods\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Media Campaign Cost Dataset\",\n            \"modality\": [\"tabular\", \"time series\"],\n            \"target_variables\": [\"cost\"],\n            \"specification\": {\n                \"dimensions\": \"high-dimensional\",\n                \"temporal_patterns\": \"unique\",\n                \"anomalies\": \"present\",\n                \"feature_types\": \"mixed categorical and numeric\"\n            },\n            \"description\": \"Contains complex temporal patterns, rare anomalies, and diverse feature characteristics, split into training, validation, and test subsets with varying levels of data challenges.\",\n            \"preprocessing\": [\"feature engineering for temporal and categorical features\", \"handling missing data\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"report on model performance across market conditions\",\n                \"explainable model decision-making\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hybrid Regression Model\",\n            \"type\": \"neural networks or ensemble techniques\",\n            \"specification\": {\n                \"feature_interactions\": true,\n                \"temporal_dependencies\": true,\n                \"handling_missing_values\": true\n            },\n            \"description\": \"A domain-specific, hybrid model designed to tackle the complexities of the Media Campaign Cost Dataset and achieve state-of-the-art performance with a focus on interpretability and resilience.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilize evolutionary algorithms for model optimization\",\n        \"Implement explainable AI methods for model interpretability\",\n        \"Create a visual, non-technical report for stakeholders\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"temporal and categorical feature interactions\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"potentially specific for domain or hybrid RMSLE metric\"],\n        \"demonstration\": \"explaining decision-making process and performance across market conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the provided instruction, capturing the high-level project objective, specific dataset characteristics, requirements for the model (including feature interactions, temporal dependencies, and explainability), and the need for innovative preprocessing techniques and visual communication of results. However, the exact model type, hybrid RMSLE metric target, and specific feature engineering methods are not detailed, leaving room for these in the actual implementation. The confidence score is high due to the clear task description but low for specific details that need further clarification.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "Design a custom Graph Wavelet Neural Network (GWNN) architecture leveraging MindSpore Pro's specialized toolbox for `Intracellular Spatial Transcriptomics`. Emphasize detecting subtypes in Layer 5 Mouse Entorhinal Cortex neurons, with a focus on both ultrarare and high-frequency instances, maintaining 99% accuracy. Optimize for sensitivity to Synaptophysin-GFP fluorescence nuances and build a scalable model capable of training on a massive dataset with over 1.5 million nodes on cutting-edge GPUs, ensuring a distributed strategy that maximizes 90% parallel efficiency across multiple nodes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"biomedical research\",\n        \"description\": \"Design a custom Graph Wavelet Neural Network (GWNN) using MindSpore Pro's toolkit for Intracellular Spatial Transcriptomics in Layer 5 Mouse Entorhinal Cortex neurons. Focus on detecting subtypes, including ultrarare and high-frequency instances, with an accuracy target of 99%. Optimize for sensitivity to Synaptophysin-GFP fluorescence and build a scalable model capable of training on large datasets (1.5 million nodes) on cutting-edge GPUs with a parallel efficiency of at least 90%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null, \n                \"unit\": \"minutes\" \n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null, \n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"parallel efficiency\",\n                \"value\": 0.9,\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Intracellular Spatial Transcriptomics (Layer 5 Mouse Entorhinal Cortex)\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node subtype\"],\n            \"specification\": {\n                \"node_count\": 1500000,\n                \"synaptic_gfp_data\": {\n                    \"type\": \"fluorescence measurements\"\n                }\n            },\n            \"description\": \"Large dataset capturing the intracellular structure of Layer 5 neurons with Synaptophysin-GFP annotations.\",\n            \"preprocessing\": [\n                \"Normalize Synaptophysin-GFP fluorescence\",\n                \"Extract relevant node features\"\n            ],\n            \"augmentation\": [\"None, due to domain-specific data nature\"],\n            \"visualization\": [\n                \"Graph structure and node subtype distribution\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom GWNN with MindSpore Pro\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"GWNN\",\n                \"toolkit\": \"MindSpore Pro\",\n                \"optimized_for\": \"Synaptophysin-GFP\",\n                \"target_nodes\": \"Mouse Entorhinal Cortex neurons\"\n            },\n            \"description\": \"A Graph Wavelet-based neural network designed to detect subtypes in Layer 5 mouse neurons with MindSpore Pro support.\"\n        }\n    ],\n    \"knowledge\": [\n        \"MindSpore Pro's specialized toolbox for Intracellular Spatial Transcriptomics offers domain-specific optimizations.\",\n        \"Ultra-rare and high-frequency instances require careful consideration in model design.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Target node extraction\",\n            \"Synaptophysin-GFP encoding\"\n        ],\n        \"target_device\": \"cutting-edge GPUs\",\n        \"deployment_endpoint\": \"high-performance computing cluster\",\n        \"inference_engine\": [\"MindSpore, distributed with GPU support\"],\n        \"demonstration\": \"Visualize instance results, confusion matrix, and parallel training progress\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent (to build a model), expertise level, and the requirements specific to the GNN architecture and task in the Intracellular Spatial Transcriptomics domain. The details are faithful to the given instruction, including the model type (GWNN), dataset characteristics, metrics, and the target platform. Some complexity metrics (like inference time and memory size) have assumed values since they are not specified in the instruction. The confidence score is 0.95 due to the comprehensiveness of the structure and the assumption of some unspecified details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "As the project lead for a prestigious middle school's groundbreaking exhibition, you are commissioned to develop a sophisticated VQA system using the latest M6 transformer, anchored in the extensive and diverse CLEVR-TextVQA dataset. Emphasize on not just unassailable precision but also algorithmic transparency, design a hybrid architecture that ingeniously combines advanced multi-modal fusion, fine-grained adaptive context-based attention, and sophisticated visual-linguistic matching techniques. Moreover, draft a 50-page in-depth research paper outlining meticulously defined evaluation criteria, a step-by-step elucidation of intricate model design, and exhaustive empirical evidence to substantiate its novelty and performance, demonstrating your team's unparalleled expertise in differentiating your solution from competitors.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"education, research\",\n        \"description\": \"Develop a sophisticated VQA system using the latest M6 transformer, anchored in the CLEVR-TextVQA dataset. Focus on high precision and algorithmic transparency, with a hybrid architecture combining multi-modal fusion, fine-grained adaptive context-based attention, and advanced visual-linguistic matching.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"algorithmic transparency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CLEVR-TextVQA\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"multimodal\",\n                \"size\": null,\n                \"features\": [\"contextual\", \"diverse\"],\n                \"description\": \"Extensive and diverse dataset for visual question answering\"\n            },\n            \"description\": \"A benchmark dataset for VQA tasks, containing questions grounded in visual scenes.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced M6 Transformer VQA Model\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"multi-modal fusion\": true,\n                    \"adaptive context-based attention\": true,\n                    \"visual-linguistic matching\": true\n                },\n                \"parameters\": {\n                    \"transparency\": \"high\",\n                    \"inference speed\": null,\n                    \"memory size\": null\n                }\n            },\n            \"description\": \"A state-of-the-art VQA model combining multiple advanced techniques for improved performance and transparency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Latest M6 transformer technology, algorithmic transparency, and competitor differentiation\",\n        \"Advanced multi-modal fusion and visual-linguistic matching\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"description\": \"A detailed step-by-step explanation and performance demonstration of the VQA system.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a cutting-edge VQA system and reflects the high level of expertise required. The problem area, task, and application domain are clearly specified. The performance metrics (precision and algorithmic transparency) are mentioned but left unspecified for values as they are dependent on the model's implementation. The dataset and model are based on the provided details. However, some details are left vague, such as specific architecture details, model's memory size, and deployment options, as the instruction did not specify those.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "question answering", "instruction": "Develop a highly specialized QA system for the energy and chemicals industry, leveraging Salsbury Oils' TriviaQA as a foundation. Mandate the implementation of a BERT fusion model, enriched with intricate domain expertise from intricate plant dynamics and intricate drilling specifics. The system must achieve sub-millisecond response times (<10 ms), scalable to handle 20,000 users simultaneously, ensuring 99.99% uptime under extreme conditions, including emergencies and adverse weather resilience, while incorporating machine learning to continuously adapt to new hazards and regulations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"energy and chemicals industry\",\n        \"description\": \"Develop a highly specialized QA system using Salsbury Oils' TriviaQA as a base, requiring a BERT fusion model enhanced with deep domain knowledge from plant dynamics and drilling specifics. The system must have sub-millisecond response times, be scalable for 20,000 users with 99.99% uptime, and integrate machine learning for real-time hazard and regulation adaptation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 10\n            },\n            {\n                \"name\": \"system scalability\",\n                \"value\": 20000\n            },\n            {\n                \"name\": \"uptime\",\n                \"value\": 0.9999\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\n                    \"value\": \"sub-millisecond\",\n                    \"unit\": \"milliseconds\"\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Salsbury Oils' TriviaQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"original_dataset\": {\n                    \"source\": \"user-link\",\n                    \"description\": \"TriviaQA foundation dataset for the energy and chemicals domain\"\n                },\n                \"domain_expert_data\": {\n                    \"type\": \"custom\",\n                    \"description\": \"Custom domain knowledge from plant dynamics and drilling specifics\"\n                }\n            },\n            \"description\": \"The foundation for the specialized QA system, incorporating Salsbury Oils' TriviaQA and domain-specific data.\",\n            \"preprocessing\": [\"BERT fine-tuning\", \"domain knowledge integration\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"BERT fusion\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT with domain-specific embeddings\",\n                \"adaptation\": \"continuous learning for new hazards and regulations\"\n            },\n            \"description\": \"A BERT-based model designed for a specialized QA system, integrating intricate domain expertise.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Leveraging BERT fusion, plant dynamics, and drilling specifics in the QA system.\",\n        \"Real-time hazard and regulation adaptation with machine learning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time updates\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"high-performance, low-latency\"],\n        \"demonstration\": \"Emergency and adverse weather resilience demonstrated through response time and system stability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the schema, reflecting the user's intent to build a QA system, high expertise, and specific requirements for the task. The problem area, domain, and downstream task are clearly defined based on the instructions. Metrics like response time and system scalability are incorporated. The BERT fusion model and domain-specific enrichment are specified. However, to fully capture the system's adaptability, a more detailed description of the continuous learning process would be beneficial. The target device and deployment information is not specified in this example.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project director for a high-performance AI lab, your directive is to conceive a groundbreaking deep-learning architecture that seamlessly merges a cutting-edge ConvLSTM-Transformer fusion model. This design must be tailored for pinpoint hourly climate predictions in dense urban areas, handling an expanded dataset, UrbanWeather v2.1+ (25 variables, INPUT_SEQ_LEN at 120 for extended data complexity). The task involves forecasting a demanding 192-hour weather outlook (12-day horizon), with a focus on enhanced MSLE+ incorporating an innovative penalty for extreme conditions, and a tailored MAPE-P metric capturing percentile errors within dynamic bands. The model must demonstrate robustness through early stopping with auto-adaptive validation every two weeks, employing nested rolling window CV, and seasonality-aware optimization. Furthermore, devise a real-time model upgrade strategy using transfer learning, constant monitoring of external climate influencers, and frequent fine-tuning for optimal predictive prowess in a constantly evolving urban climate ecosystem.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate science\",\n        \"description\": \"Conceive a groundbreaking deep-learning architecture using ConvLSTM-Transformer fusion for hourly climate predictions in dense urban areas with the UrbanWeather v2.1+ dataset (25 variables, 120-hour INPUT_SEQ_LEN). The model should forecast a 192-hour outlook (12-day horizon) and focus on enhanced MSLE+ with a custom MAPE-P metric for percentile errors. Ensure robustness through early stopping, auto-adaptive validation every two weeks, nested rolling window CV, and seasonality-aware optimization. Implement a real-time upgrade strategy using transfer learning, monitoring external influencers, and frequent fine-tuning for optimal performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSLE+\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE-P\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"early stopping\",\n                \"value\": null,\n                \"unit\": \"epochs\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UrbanWeather v2.1+\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"variables\": 25,\n                \"sequence_length\": 120,\n                \"complexity\": \"dense urban areas\"\n            },\n            \"description\": \"A dataset for time-series forecasting in dense urban areas with 25 climate variables and a 120-hour INPUT_SEQ_LEN.\",\n            \"preprocessing\": [\n                \"Resampling (hourly data)\",\n                \"Handling missing data\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ConvLSTM-Transformer Fusion\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"ConvLSTM-Transformer fusion\",\n                \"target horizon\": 192,\n                \"seasonality-aware\": true,\n                \"transfer learning\": true\n            },\n            \"description\": \"A deep-learning architecture specifically designed for climate forecasting in urban environments using ConvLSTM-Transformer fusion.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning from pre-trained climate models\",\n        \"External climate influencer monitoring\",\n        \"Seasonality-aware adjustments\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"nested rolling window CV\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"auto-adaptive\"],\n        \"demonstration\": \"12-day weather forecast demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is derived from the user's directive, capturing key components such as the problem's area and task, target metrics, model requirements, and dataset details. Performance metrics and complexity constraints are noted where applicable. The expert level of expertise and the nuances of the forecasting process are incorporated. However, actual metric values and specific implementation details are not provided, reflecting a potential lack of those details in the given instruction.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "node classification", "instruction": "For the High School Contact Network project, develop a distributed TGN with multi-level attention and anomaly detection. Integrate a fault-tolerant mechanism to handle sudden data surges, and optimize performance on heterogeneous hardware. Validate the model with a time-sensitive accuracy threshold, considering evolving attributes, inter-year dependencies, and a minimum F1-score of 0.9 over varying node degrees.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"social network analysis\",\n        \"description\": \"Develop a distributed Temporal Graph Neural Network (TGN) for the High School Contact Network project, with multi-level attention and anomaly detection capabilities. Ensure the model includes a fault-tolerant mechanism for handling sudden data surges and optimize performance for heterogeneous hardware.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"time-sensitive accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High School Contact Network\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"heterogeneous_nodes\": true,\n                \"evolving_attributes\": true,\n                \"inter-year_dependencies\": true,\n                \"node_degrees\": true\n            },\n            \"description\": \"A dynamic graph dataset capturing high school contact network with evolving attributes and dependencies across years.\",\n            \"preprocessing\": [\n                \"node and edge feature engineering\",\n                \"time-series data preprocessing\"\n            ],\n            \"augmentation\": [\n                \"temporal data augmentation\"\n            ],\n            \"visualization\": [\n                \"dynamic network visualization\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Distributed TGN with Multi-Level Attention\",\n            \"family\": \"Temporal Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"distributed\",\n                \"attention_layers\": \"multi-level\",\n                \"anomaly_detection\": true,\n                \"fault_tolerant\": true\n            },\n            \"description\": \"A distributed TGN designed for the High School Contact Network, featuring multi-level attention and integrated anomaly detection with fault tolerance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporate evolving attributes and inter-year dependencies during model training and validation.\",\n        \"Address the heterogeneous nature of the node degrees for performance optimization.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"node degree normalization\",\n            \"timestamp-based feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for heterogeneous hardware\"\n        ],\n        \"demonstration\": \"required to be time-sensitive\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a model, the high expertise level, and the specific requirements of the High School Contact Network project. The problem area, task, and domain are in line with the project. The performance metrics and complexity constraints are included as per the instruction. However, some specific performance targets (time-sensitive accuracy) and values (e.g., F1-score, inference time) are still unspecified and need to be confirmed.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a cutting-edge AI consultant for a medical diagnostics startup, your mandate is to develop a deep learning ensemble model that not only classifies extremely rare and nuanced patient outcomes based on intricate Electronic Health Records (EHRs) but also discovers hidden correlations between specific treatments and patients' long-term health. You must integrate dynamic transfer learning, generate visual explanations with the SHapley Additive exPlanations (SHAP) framework, and target a remarkable AUC-ROC score of 0.99, while adhering to strict privacy regulations and ensuring a 5% improvement in the model's predictive accuracy over the previous iteration.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"medical diagnostics\",\n        \"description\": \"Develop a deep learning ensemble model for classifying rare and nuanced patient outcomes based on EHRs. The model should also discover hidden correlations between treatments and long-term health, integrating dynamic transfer learning and visual explanations using SHAP. Target an AUC-ROC score of 0.99 while adhering to strict privacy regulations and aim for a 5% accuracy improvement over the previous iteration.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"EHRs for Medical Diagnostics\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"patient_outcomes\", \"treatment_relations\"],\n            \"specification\": {\n                \"data_size\": \"large\",\n                \"privacy_sensitive\": true,\n                \"imbalanced_classes\": true\n            },\n            \"description\": \"A comprehensive EHR dataset for rare and nuanced patient outcomes classification, including EHR data and potential treatment-related information.\",\n            \"preprocessing\": [\"data anonymization\", \"imputation for missing values\"],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [\"SHAP interaction visualizations\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Learning Ensemble Model\",\n            \"family\": \"ensemble\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep learning\",\n                \"transfer_learning\": true,\n                \"解釋ability Techniques\": \"SHAP\"\n            },\n            \"description\": \"A deep learning ensemble model designed for classifying rare patient outcomes and detecting correlations between treatments and long-term health.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Privacy-preserving techniques for handling sensitive EHR data\",\n        \"Ensemble learning and transfer learning for improving model accuracy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction from EHRs\", \"hidden correlation identification\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"private cloud-based platform\",\n        \"inference_engine\": [\"GPU-optimized\", \"TPU\"],\n        \"demonstration\": \"custom dashboard with SHAP explanations and model performance updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key aspects of the user's instruction, such as the need for a deep learning ensemble model, target performance metrics, and privacy requirements. The expert level of the user, application domain, and downstream task are derived directly from the context. The dataset details include the use of transfer learning and SHAP for explanations, while also addressing the need for privacy and data anonymization. The service requirements, like deployment environment and inference engines, reflect the complexities of a medical AI project. However, the specific implementation details for the model architecture and feature engineering are not provided as they might vary, and the user did not specify them.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "The art team now faces a stringent challenge, needing not just an advanced image classifier, but one capable of hierarchical analysis. They must classify user-generated content into four refined categories: premium endorsements, improvement suggestions, distinctive 'customer tales,' and curator-worthy pieces. The model must handle detailed, artistic styles and subtle product context, achieving a minimum of 98% precision on a proprietary dataset of at least 7000 high-resolution images with intricate captions. In addition, the model must be deployable as a real-time API, maintaining responsiveness and adapting dynamically to new content, all while seamlessly integrating with a complex, multi-language CMS system with strict data privacy regulations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"art and user-generated content\",\n        \"description\": \"The task is to develop an advanced image classifier that can perform hierarchical analysis and classify user-generated content into four refined categories: premium endorsements, improvement suggestions, distinctive 'customer tales,' and curator-worthy pieces. The model should handle artistic styles, subtle product context, and achieve at least 98% precision on a proprietary dataset of 7000 high-resolution images with intricate captions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time inference\",\n                \"value\": true,\n                \"unit\": \"API calls per second\"\n            },\n            {\n                \"name\": \"adaptability to new content\",\n                \"value\": true\n            },\n            {\n                \"name\": \"data privacy integration\",\n                \"value\": true,\n                \"unit\": \"compliant with data privacy regulations\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"proprietary_image_dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"classification_labels\"],\n            \"specification\": {\n                \"size\": {\n                    \"total_images\": 7000,\n                    \"image_resolution\": \"high\"\n                },\n                \"data_types\": {\n                    \"images\": \"artistic, high-resolution\",\n                    \"captions\": \"intricate\"\n                }\n            },\n            \"description\": \"A dataset with 7000 high-resolution images, each with detailed artistic styles and intricate captions, for the fine-grained classification task.\",\n            \"preprocessing\": [\n                \"的艺术风格转换\",\n                \"图像增强\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Image Classifier\",\n            \"family\": \"Convolutional Neural Networks (CNN) or advanced deep learning architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": {\n                    \"model_complexity\": \"high\",\n                    \"number_of_classes\": 4\n                },\n                \"flops\": \"optimized for real-time inference\"\n            },\n            \"description\": \"A highly accurate and specialized deep learning model designed for complex image classification with support for hierarchical analysis\"\n        }\n    ],\n    \"knowledge\": [\n        \"Handling artistic styles requires specialized feature extraction\",\n        \"Responsiveness and real-time API integration are crucial\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"artistic style transfer\"],\n        \"target_device\": \"real-time API\",\n        \"deployment_endpoint\": \"multi-language CMS system\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"demonstrating accuracy, responsiveness, and integration in a live setting\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the task, including advanced image classification, hierarchical analysis, and specific performance metrics. The model requirements, dataset properties, and integration details are based on the provided instructions. However, without specific details about the model's architecture or any preprocessing techniques used, the precision value is set to null, as the actual precision might be achieved through a specific model choice or tuning.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for the elite ML team, devise a state-of-the-art time series classifier for a pharmaceutical R&D competition. The classifier must handle the intricate Heartbeat dataset with non-uniform sampling rates, varying from 50 to 200 samples (INPUT_SEQ_LEN), and 750 domain-optimized features (combining time and frequency domains, INPUT_DIM=750). Implement an ensemble model using SMOTE for balancing classes (10% imbalance), conduct advanced nested cross-validation with stratified 10-folds. Focus on optimizing AUC-ROC, accuracy, F1 score, and model explainability, ensuring real-time performance. Additionally, develop interpretability reports for each class (labels: healthy, anomalous), withstand concept drift, and consistently surpass industry benchmarks by at least 10% in unseen datasets. Prioritize performance stability and speed, while maintaining explainability as a key differentiator in your submission.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"pharmaceutical R&D\",\n        \"description\": \"Design a state-of-the-art time series classifier for an R&D competition targeting the Heartbeat dataset with non-uniform sampling rates between 50 to 200 samples (INPUT_SEQ_LEN) and 750 domain-optimized features (INPUT_DIM=750) from both time and frequency domains. Implement an ensemble model with SMOTE for class balancing, conduct advanced nested cross-validation with stratified 10-folds. Focus on optimizing AUC-ROC, accuracy, and F1 score, ensuring real-time performance. Develop interpretability reports for healthy and anomalous classes. The model should withstand concept drift and consistently surpass industry benchmarks by at least 10% on unseen datasets, prioritizing performance stability, speed, and explainability as key differentiators.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Heartbeat\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"healthiness\"],\n            \"specification\": {\n                \"min_samples\": 50,\n                \"max_samples\": 200,\n                \"input_dim\": 750,\n                \"non_uniform_sampling_rate\": true\n            },\n            \"description\": \"A pharmaceutical R&D time series dataset with non-uniform sampling rates and a 10% class imbalance.\",\n            \"preprocessing\": [\n                \"SMOTE for class balancing\",\n                \"Handling non-uniform sampling rates\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_method\": \"ensemble of state-of-the-art time series models\",\n                \"smote_strategy\": \"SMOTE\",\n                \"classes_balance\": \"10%\"\n            },\n            \"description\": \"An advanced time series classifier that combines multiple models and uses SMOTE for balancing classes\"\n        }\n    ],\n    \"knowledge\": [\n        \"Concept drift adaptation is essential for maintaining performance\",\n        \" Explainable AI techniques for individual class interpretations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature extraction from time and frequency domains\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Real-time processing\"\n        ],\n        \"demonstration\": \"Interpretable model reports for each class\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project requirement, indicating a high-level 'build' intent with a 'high' expertise level. The 'problem' area, task, application domain, and the dataset properties are translated directly from the instruction. Performance metrics are mentioned as null to indicate they need to be optimized, and complexity metrics are included with their values as placeholders. The key differentiators and emphasis on explainability are clearly stated in the model description.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "image classification", "instruction": "Challenge the AI team to develop a highly-accurate (99%) plant species classifier using the 'Advanced Plant Seedlings Dataset with Variable Environmental Factors,' incorporating not just ResNet50 and DenseNet121, but also a multi-database ensemble for enhanced generalization. Ensure night vision compatibility, and optimize for Google Coral edge TPUs with minimal energy consumption (10mW). Propose a real-time, mobile-first deployment strategy with a focus on achieving sub-100ms latency on budget-friendly smartphones and IoT devices, while maintaining explainable AI for user transparency in decision-making.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"botany\",\n        \"description\": \"Develop a highly accurate (99%) plant species classifier using the 'Advanced Plant Seedlings Dataset with Variable Environmental Factors.' Incorporate ResNet50 and DenseNet121 models, along with a multi-database ensemble for enhanced generalization. Ensure night vision compatibility and optimize for Google Coral edge TPUs with a low power consumption of 10mW. Propose a real-time, mobile-first deployment strategy targeting sub-100ms latency on budget-friendly smartphones and IoT devices, while maintaining explainable AI for user transparency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy consumption\",\n                \"value\": 10,\n                \"unit\": \"mW\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 100,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Advanced Plant Seedlings Dataset with Variable Environmental Factors\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"plant species\"],\n            \"specification\": {\n                \"night_vision_compatibility\": true,\n                \"diversity\": \"variable environmental factors\"\n            },\n            \"description\": \"A challenging dataset for plant species classification with diverse environmental conditions.\",\n            \"preprocessing\": [\"night vision augmentation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": [\"ResNet50\", \"DenseNet121\"],\n            \"family\": \"neural networks\",\n            \"type\": \"image recognition\",\n            \"specification\": {\n                \"architecture\": [\"ResNet50, DenseNet121\"],\n                \"ensemble\": true\n            },\n            \"description\": \"An ensemble of ResNet50 and DenseNet121 for plant species classification.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"mobile optimization\",\n            \"energy-efficient\"\n        ],\n        \"target_device\": \"Google Coral edge TPU\",\n        \"deployment_endpoint\": \"mobile-first, IoT\",\n        \"inference_engine\": [\"Coral TPU inference\"],\n        \"demonstration\": \"real-time, explainable AI for mobile devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction, including the user's intent to build, high expertise level, task area (computer vision), and the specific requirements for model accuracy, multi-database ensemble, night vision, power consumption, and deployment strategy. The performance metric (accuracy) is set to the desired 99% and corresponding complexity metrics are provided. The dataset name, modality, and requirements are accurately reflected, as is the model selection and associated specifications. The service requirements and target devices are specified accordingly. The confidence score is high, given the clear instruction, but is not 1.0 due to missing details about specific feature engineering techniques or remaining ambiguity in achieving the sub-100ms latency.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a seasoned project manager, you are tasked with assembling a high-performance team to design a cutting-edge neural machine translation model. The team must utilize the OPUS-Xtra dataset, specialized for English to Russian translations of colloquial slang and idiomatic expressions. The model should not only achieve a minimum BLEU score of 35 but also demonstrate real-time efficiency, translating at least 4 sentences per second while maintaining a latency of under 200 milliseconds. Emphasize user experience by ensuring natural and engaging translations in fast-paced, informal chat conversations without hindering interaction flow.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Design a high-performance neural machine translation model, targeting English to Russian translation of colloquial slang and idiomatic expressions using the OPUS-Xtra dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 35.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Real-time efficiency\",\n                \"value\": 4.0,\n                \"unit\": \"sentences/s\"\n            },\n            {\n                \"name\": \"Latency\",\n                \"value\": 200,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS-Xtra\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"English to Russian\",\n                \"linguistic focus\": \"colloquial slang and idiomatic expressions\"\n            },\n            \"description\": \"A dataset specialized for English to Russian translations with informal slang and idiomatic content.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Neural Machine Translation (NMT)\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time performance\": {\n                    \"translations_per_second\": 4.0,\n                    \"latency\": 200.0\n                }\n            },\n            \"description\": \"A state-of-the-art, real-time NMT model optimized for English to Russian translation of colloquial content.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Emphasis on user experience requires natural and engaging translations without disrupting the conversation flow.\",\n        \"Targeting colloquial slang and idiomatic expressions needs a model with strong language understanding.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Real-time or Serverless\"],\n        \"demonstration\": \"Fast, seamless translation of informal English to Russian in chat conversations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been crafted based on the given instruction, indicating the user's intent to build a high-performance NMT model, focusing on the OPUS-Xtra dataset and performance metrics. Expertise level is set to high. The dataset source, model specifications, and real-time performance constraints are included. However, a specific implementation plan for feature engineering and deployment endpoint is not included due to the lack of detail provided in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "The project manager now seeks a novel ChebNet++ enhancement, targeting high-precision node classification in Amazon's massive co-purchase graph. Prioritize community-specific feature extraction at sub-attribute levels, guaranteeing an F1-score above 97%, while incorporating streaming ETL and a self-healing mechanism that ensures continuous performance amidst structural volatility with less than 5-minute recovery time.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"recommendation systems\",\n        \"description\": \"Seek a novel ChebNet++ enhancement for high-precision node classification in Amazon's co-purchase graph, focusing on community-specific feature extraction at sub-attribute levels with an F1-score target of 97%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"recovery time\",\n                \"value\": 5,\n                \"unit\": \"minutes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Amazon's co-purchase graph\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"structure\": \"massive, co-purchase graph\",\n                \"data_type\": \"streaming ETL\"\n            },\n            \"description\": \"A large co-purchase graph for node classification, requiring streaming data processing and self-healing functionality.\",\n            \"preprocessing\": [\"community-specific feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ChebNet++\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"target\": \"node classification\",\n                \"enhancements\": [\"novel ChebNet++\"],\n                \"task_requirement\": \"high-precision\",\n                \"feature_extraction\": \"sub-attribute level\"\n            },\n            \"description\": \"A ChebNet++-based model enhanced for high-precision node classification, incorporating community-specific feature extraction and self-healing capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"For Amazon's co-purchase graph, extracting community-specific features at sub-attribute levels is crucial for achieving high precision.\",\n            \"application\": \"High-precision node classification\"\n        },\n        {\n            \"text\": \"Incorporating streaming ETL is necessary due to the dynamic nature of the graph and the requirement for continuous performance.\",\n            \"application\": \"Streaming data processing\"\n        },\n        {\n            \"text\": \"Self-healing mechanism is necessary to recover from structural changes with a recovery time of less than 5 minutes.\",\n            \"application\": \"Self-healing for structural volatility\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstrate model's performance on sub-community classification tasks and recovery process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is tailored to the instruction, covering user intent, expertise, and details related to the problem, including the area, task, and performance metrics. The specific feature extraction, model name, and streaming ETL requirements are extracted from the instruction. The suggested model enhancement is in line with the user's request, and the recovery time is specified. However, specific feature engineering and deployment details are not provided, as they would depend on the manager's preferences.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "The project manager is now mandated to develop a multi-tiered AI solution for luxury retail, leveraging a fine-tuned Faster R-CNN architecture on the 'LuxuryRetail-110K' dataset. The objective is to achieve a stringent 97% average precision across five sub-genres (fashion, accessories, jewelry, watches, and art), with a requirement for sub-genre-specific performance analysis and an end-to-end error rate below 2%. This must significantly improve accuracy and streamline inventory management in elite retail environments, ensuring seamless synchronization with existing systems.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury retail\",\n        \"description\": \"Develop a multi-tiered AI solution for luxury retail using a fine-tuned Faster R-CNN architecture on the 'LuxuryRetail-110K' dataset. Aim for 97% average precision across five sub-genres (fashion, accessories, jewelry, watches, and art) with a focus on sub-genre-specific performance analysis. End-to-end error rate should be below 2% to enhance inventory management and seamless synchronization with existing systems.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average precision\",\n                \"value\": 0.97,\n                \"sub-genres\": [\n                    {\n                        \"genre\": \"fashion\",\n                        \"expected_value\": 0.97\n                    },\n                    {\n                        \"genre\": \"accessories\",\n                        \"expected_value\": 0.97\n                    },\n                    {\n                        \"genre\": \"jewelry\",\n                        \"expected_value\": 0.97\n                    },\n                    {\n                        \"genre\": \"watches\",\n                        \"expected_value\": 0.97\n                    },\n                    {\n                        \"genre\": \"art\",\n                        \"expected_value\": 0.97\n                    }\n                ]\n            },\n            {\n                \"name\": \"end-to-end error rate\",\n                \"value\": 0.02\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LuxuryRetail-110K\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"sub-genres\": [\"fashion\", \"accessories\", \"jewelry\", \"watches\", \"art\"],\n                \"data_size\": \"110,000 images\"\n            },\n            \"description\": \"A large-scale dataset for luxury retail-specific object detection, focusing on five sub-genres.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Fine-tuned Faster R-CNN\",\n            \"family\": \"CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_improvement_target\": \"significantly\",\n                \"accuracy_percentages\": \"97%\"\n            },\n            \"description\": \"An advanced deep learning model specifically fine-tuned for object detection in luxury retail, targeting sub-genre-specific performance and low error rate.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Performance metrics need to cover sub-genre-specific analysis to optimize inventory management.\",\n        \"Significant accuracy improvement is required to justify the AI solution's implementation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or edge (for optimized inventory management)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time or batch processing, depending on the system\"],\n        \"demonstration\": \"sub-genre-specific performance dashboards and error rate analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's build intent, high expertise level, and the main requirements of the project, focusing on the area, task, and metrics. Specific sub-genre details, source of dataset, and the advanced model architecture are well represented. The service and confidence aspects have been inferred, including the importance of sub-genre-specific performance and end-to-end error rate.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project manager for a cutting-edge startup, task the machine learning development team with the creation of an AI-powered neural translation system that can not only translate between highly nuanced languages like Japanese, Mandarin, and Swahili in real-time, but also dynamically adapts to dialects and regional variations. The team must employ a novel fusion of Transformer-XL and Dual encoder architectures, ensuring energy efficiency for use on wearable tech with limited computational resources, while simultaneously enhancing the system's ability to preserve cultural nuances and idiomatic expressions. Additionally, devise a method to integrate a machine learning-based power management module that predicts and optimizes the system's energy consumption based on usage patterns, guaranteeing a seamless and prolonged experience for users in resource-constrained environments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-cultural communication for wearable tech\",\n        \"description\": \"Develop an AI-powered neural translation system capable of real-time translation between nuanced languages (e.g., Japanese, Mandarin, and Swahili) with dialect and regional variations. The system must incorporate a Transformer-XL and Dual encoder architecture, prioritize energy efficiency for wearable tech with limited resources, and maintain cultural nuances and idiomatic expressions. Additionally, include a power management module that predicts and optimizes energy consumption based on usage patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time translation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cultural preservation score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"power consumption optimization\",\n                \"value\": null,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [\n                \"\"\n            ],\n            \"specification\": {},\n            \"description\": \"A dataset for training and fine-tuning the neural translation system, likely including large collections of parallel texts in the specified languages.\",\n            \"preprocessing\": [\n                \"cleaning, tokenization, normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"AI Neural Translation System\",\n            \"family\": \"Transformer-XL and Dual encoder fusion\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-XL and Dual encoder\",\n                \"energy-efficient\": true,\n                \"computational_resource_adaptation\": true\n            },\n            \"description\": \"A real-time neural machine translation system optimized for energy efficiency on wearable devices, preserving cultural nuances and idiomatic expressions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transformer-XL for long-term dependency handling\",\n        \"Dual encoder for handling dialects and regional variations\",\n        \"Power management module for resource-constrained environments\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"adaptation to user usage patterns\"\n        ],\n        \"target_device\": \"wearable tech\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"energy-efficient inference\"],\n        \"demonstration\": \" Seamless, real-time, and culturally aware translation for users in resource-limited environments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's requirements, such as the focus on advanced NLP and AI concepts like Transformer-XL and Dual encoders. The system's requirements for energy efficiency and cultural preservation are also incorporated. However, specific performance metrics and complexity constraints are left unspecified due to the lack of numerical targets in the instruction. The model's name and some preprocessing steps are placeholders awaiting explicit details. The confidence score is high since the structure is well mapped to the given schema, but accuracy would improve with more precise instructions.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "question answering", "instruction": "As the project lead for a premier AI research and development team, we're now mandated to develop an exceptional BERT-based hybrid model that marries Question-Answering prowess with natural language processing, leveraging the advanced SQuAD 2.0+ corpus. The objective is to push the boundaries of performance by consistently achieving an F1 score of 97% or above in the most rigorous expert mode assessments, while maintaining the stringent real-time response criteria of sub-1.4-second latency for users interacting with the ultra-fast,next-gen M6 RoBERTa variant. In pursuit of this challenge, delve into groundbreaking algorithmic innovations, fine-tuning methodologies, and advanced compression techniques, ensuring peak efficiency without compromising benchmark advancements in the highly competitive AI landscape.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"AI research and development\",\n        \"description\": \"Develop a BERT-based hybrid model integrating Question-Answering abilities with natural language processing, targeting SQuAD 2.0+ corpus. The objective is to achieve an F1 score of 97% or above in expert mode assessments with a sub-1.4-second latency for the M6 RoBERTa variant, requiring algorithmic innovations, fine-tuning methodologies, and advanced compression techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 1.4,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0+\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"corpus size\": \"large\",\n                \"expert mode\": \"true\"\n            },\n            \"description\": \"Advanced question-answering dataset suitable for BERT-based models, with expert level questions.\",\n            \"preprocessing\": [\"BERT tokenization\", \"question answering-specific preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-Hybrid\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT + QA-specific architecture\",\n                \"RoBERTa variant\": \"M6\",\n                \"latency\": \"sub-1.4 seconds\"\n            },\n            \"description\": \"An advanced BERT model tailored for Question-Answering, leveraging SQuAD 2.0+ corpus, optimized for F1 score of 97%+, and real-time response\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilize SQuAD 2.0+ corpus to improve QA performance\",\n        \"Employ fine-tuning to adapt model for expert mode assessments\",\n        \"Explore compression techniques for real-time response and efficiency\",\n        \"Incorporate algorithmic advancements for benchmark improvement\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"BERT-specific feature engineering\", \"real-time response optimization\"],\n        \"target_device\": \"ultra-fast, next-gen\",\n        \"deployment_endpoint\": \"AI research and development platform\",\n        \"inference_engine\": [\"RoBERTa's M6 variant\"]\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response incorporates the user's intent to build a model, expert-level expertise, and the requirements for the problem (BERT-based QA, SQuAD 2.0+, performance metrics, and latency). Key details from the instruction were captured, such as fine-tuning methodology, algorithmic innovations, and compression techniques. The model name, architecture, and M6 variant are specified. However, some specific details like the compression technique or the link for SQuAD 2.0+ corpus are not included. The confidence score is slightly lower due to the lack of these details.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, design and implement an innovative Transformer-XL architecture, incorporating state-of-the-art edge computing capabilities, for a multilingual real-time translation system. The model must excel in translating TED Talks from five underrepresented languages into English, with exceptional accuracy (98.5%+), ultra-low latency (<1% real-time), and minimal delay for resource-constrained IoT devices. The project should prioritize hardware acceleration using domain-specific accelerators, low-bit quantization (4-6 bits), and dynamic vocabulary pruning for at least 80% model compression. Additionally, ensure a highly energy-efficient design (50% energy reduction) with a user-friendly, battery life-enhancing UI, optimized for seamless performance on portable devices with limited power resources, and accommodate for different device form factors. Regular benchmarking and A/B testing must be conducted to maintain performance standards and user satisfaction.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual real-time translation\",\n        \"description\": \"Design and implement a Transformer-XL architecture with edge computing capabilities for multilingual real-time translation, targeting TED Talks from five underrepresented languages into English. Aim for accuracy above 98.5%, ultra-low latency (<1% real-time), and 80% model compression through domain-specific accelerators, low-bit quantization, and dynamic vocabulary pruning. Prioritize energy efficiency, user-friendly UI for portable devices with limited resources, and conduct benchmarking and A/B testing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.985\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 0.01,\n                \"unit\": \"% real-time\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": 0.5,\n                \"unit\": \"50% energy reduction\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TED Talks (underrepresented languages)\",\n            \"modality\": [\"text\", \"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"languages1\", \"languages2\", \"languages3\", \"languages4\", \"languages5\"]\n            },\n            \"description\": \"A collection of TED Talks from five underrepresented languages, intended for multilingual translation into English.\",\n            \"preprocessing\": [\"edge computing\", \"low-bit quantization\", \"dynamic vocabulary pruning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL with Edge Computing\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model compression\": 0.8,\n                \"hardware_accelerators\": [\"domain-specific accelerators\"],\n                \"quantization_bits\": {\n                    \"integer\": 4,\n                    \"float\": 6\n                },\n                \"dynamic_vocabulary_pruning\": 0.8\n            },\n            \"description\": \"An innovative Transformer-XL designed for real-time multilingual translation, optimized for resource-constrained IoT devices.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"low-bit quantization\", \"dynamic vocabulary pruning\"],\n        \"target_device\": \"IoT, portable devices with limited power resources\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"domain-specific accelerators\"],\n        \"demonstration\": {\n            \"UI_optimization\": \"user-friendly, battery life-enhancing\",\n            \"device_form_factors\": \"seamless performance across various devices\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was constructed based on the user's instructions. It reflects the user's intent to build a model, their high expertise, and the project requirements for multilingual translation, target performance, hardware acceleration, and energy efficiency. Details such as data preprocessing, model characteristics, and service requirements were derived directly from the instruction. However, specific metrics like latency and energy reduction could not be directly provided as the instruction was more qualitative. The confidence score is high due to the clarity of the project description but might be lower without specific quantitative targets.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project lead, your responsibility is to orchestrate a multi-disciplinary team to develop a cutting-edge Textual Entailment system. This involves refining a pre-existing model using a massive, domain-specific and dynamically diverse dataset. Strive for superior out-of-the-box performance by integrating an ensemble of no less than three state-of-the-art models, each achieving a benchmarked F1-score of 0.95 or higher in a multiclass evaluation. Prioritize model interpretability, efficient resource utilization, and rigorously implement bias mitigation techniques, taking into account real-world scenarios and constant monitoring for fairness. Remember to document the rationale behind each design decision and conduct regular bias audits throughout the project life cycle.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"artificial intelligence research\",\n        \"description\": \"As the project lead, orchestrate a multi-disciplinary team to develop a cutting-edge Textual Entailment system. The task includes refining a pre-existing model using a large, domain-specific, and dynamically diverse dataset. Target an out-of-the-box performance by integrating an ensemble of at least three state-of-the-art models, each with an F1-score of 0.95 or higher in a multiclass evaluation. Prioritize interpretability, resource efficiency, and bias mitigation techniques with real-world considerations and continuous monitoring.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": \"high\",\n                \"unit\": \"Ease of Understanding\"\n            },\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": 0.9,\n                \"unit\": \"Efficiency Score\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Large Domain-Specific Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"massive\",\n                \"diversity\": \"dynamically diverse\"\n            },\n            \"description\": \"A large and dynamically diverse dataset for refining a Textual Entailment model.\",\n            \"preprocessing\": [\"data cleaning, tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model performance heatmaps\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ensemble\",\n            \"family\": \"state-of-the-art models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_size\": 3,\n                \"model_f1_score\": {\n                    \"min\": 0.95,\n                    \"requirement\": \"benchmarked\"\n                }\n            },\n            \"description\": \"An ensemble of top-performing models with a focus on achieving at least 0.95 F1-score.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Need to use ensemble models for superior performance\",\n        \"Interpretability and bias mitigation are critical aspects\",\n        \"Emphasis on real-world scenarios and fairness\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"model interpretation\",\n            \"bias mitigation techniques\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time monitoring for fairness\"],\n        \"demonstration\": \"Rationale and bias audit documents\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the provided instruction and matches the JSON schema. The problem is defined as Textual Entailment in NLP with a focus on ensemble models, interpretability, and bias mitigation. The performance metrics and model requirements are clearly specified. The team's responsibilities and project considerations are incorporated, including the need for documentation. The confidence score is high given the direct mapping of the instruction to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project lead for the pioneering AI research team, your mandate is to design an advanced text classifier using the diverse BoolQ dataset. In addition to differentiating between two nuanced entailment categories (0: non-connection, 1: absolute agreement), incorporate a sophisticated metric that gauges model resilience to semantic fluctuations and syntactic anomalies. Mandate a comprehensive analysis, calculating both F1 score and precision/recall for a holistic performance benchmark, while considering the impact of domain adaptation in a cross-modal scenario.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"research\",\n        \"description\": \"Design an advanced text classifier on the BoolQ dataset for two nuanced entailment categories (0: non-connection, 1: absolute agreement). Incorporate a metric for model resilience to semantic fluctuations and syntactic anomalies. Comprehensively evaluate the performance using F1 score, precision, and recall, and consider domain adaptation in a cross-modal scenario.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resilience to semantic fluctuations\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            },\n            {\n                \"name\": \"resilience to syntactic anomalies\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"entailment categories\"],\n            \"specification\": {\n                \"entailment_categories\": [\n                    {\n                        \"category\": \"non-connection\",\n                        \"label\": 0\n                    },\n                    {\n                        \"category\": \"absolute agreement\",\n                        \"label\": 1\n                    }\n                ]\n            },\n            \"description\": \"A diverse dataset for text classification, particularly focusing on entailment detection with two distinct categories.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Text Classifier\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"cross-modal capability\": true,\n                \"adaptation_strategy\": \"domain adaptation\"\n            },\n            \"description\": \"A state-of-the-art neural network model designed for text classification, designed to handle the diverse BoolQ dataset and its nuanced entailment categories.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model must handle semantic variations and syntactic anomalies to ensure resilience.\",\n        \"Comprehensive evaluation of F1 score, precision, and recall is necessary for a thorough benchmark.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"cross-modal adaptation evaluation\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intent to build an advanced text classifier, their high expertise level, and the specific requirements. The problem details, including the task, dataset, performance metrics, and resilience requirements, are clearly specified. However, the performance target values for F1 score, precision, and recall are left unspecified, which might need further clarification from the user. The model specifics leave room for selection based on the user's choices.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "As the project lead for the \"Cosmological Research Query Optimization and Analysis\" (CRQOA) initiative, direct the interdisciplinary team to design and implement a novel BERT model, employing a Hugging Face's custom RoBERTaX variant. This model must be fine-tuned exclusively on arXiv's highly specialized Astrophysics and Space Science Question-Answering Subset, focusing on achieving a state-of-the-art F1 score of at least 97% while ensuring real-time responsiveness with a strict latency constraint of 100 milliseconds to streamline advanced astronomical research inquiries.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Design and implement a novel BERT model, specifically a Hugging Face's RoBERTaX variant, for the CRQOA initiative. Fine-tune the model on arXiv's Astrophysics and Space Science Question-Answering Subset to achieve a state-of-the-art F1 score of at least 97%, with a real-time response latency of 100 milliseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv's Astrophysics and Space Science Question-Answering Subset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"Astrophysics and Space Science\",\n                \"dataset_split\": [\"train\", \"validation\", \"test\"]\n            },\n            \"description\": \"A specialized dataset for question answering in the field of astrophysics and space science, derived from arXiv.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"CRQOA_Novel_BERT_RoBERTaX\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"custom RoBERTaX\",\n                \"pretraining\": \"BERT\",\n                \"fine_tuning_dataset\": \"arXiv's Astrophysics and Space Science\",\n                \"use_case\": \"Question Answering\"\n            },\n            \"description\": \"A state-of-the-art BERT-based model with a RoBERTaX variant, specifically tailored for advanced astronomical research inquiries.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time, low-latency\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Hugging Face's inference framework\"],\n        \"demonstration\": \"focus on real-time responsiveness and advanced inquiries\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been structured according to the provided instruction, with the user's intent to build a model (high expertise) and the problem area (natural language processing), downstream task (question answering), and specific requirements (RoBERTaX fine-tuning, F1 score of 97%, and 100ms latency) all clearly defined. The model's name, family, and type were derived from the information given, and related details for the dataset and service were appropriately added. The confidence score is high due to the clarity of the instruction and the accuracy of the JSON structure.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "text classification", "instruction": "Demand a project roadmap for the AI team to address this complex text classification problem with the DAIGT dataset, mandating a tripartite strategy: 1) refine feature extraction through deep learning techniques, 2) analyze and integrate multiple generation styles for improved model diversity, and 3) conduct rigorous A/B testing with nested cross-validation, targeting pinnacle accuracy and transparent model interpretability in line with ethical standards.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"language understanding or NLP research\",\n        \"description\": \"Project roadmap for an AI team to address a complex text classification problem using the DAIGT dataset. The strategy involves refining feature extraction with deep learning, integrating multiple generation styles for diversity, and conducting rigorous A/B testing with nested cross-validation, focusing on pinnacle accuracy and transparent interpretability while upholding ethical standards.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"A/B testing\",\n                \"value\": \"rigorous\",\n                \"unit\": \"methodology\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"classification target\"],\n            \"specification\": null,\n            \"description\": \"A complex text dataset for the text classification problem, requiring advanced feature extraction techniques.\",\n            \"preprocessing\": [\"deep learning feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep Learning for Text Classification\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_extraction\": \"Advanced DL techniques\"\n            },\n            \"description\": \"A deep learning model designed for complex text classification, utilizing refined feature extraction and diverse generation styles.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The DAIGT dataset necessitates advanced feature extraction methods.\",\n        \"Integrating multiple generation styles is crucial for diversity in the model.\",\n        \"Ethical considerations and model interpretability are critical in the project.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Include model explanations and performance visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intention to build a project roadmap, including the high level of expertise. The NLP area is derived from the text classification task. The description and strategy points directly correspond to the instruction, while performance metrics like accuracy and interpretability are not specified, leaving them as null values. Complexity metrics related to rigorous A/B testing and nested cross-validation are included. The DAIGT dataset details are assumed based on the requirement for advanced feature extraction. The model details and knowledge are kept consistent with the project's requirements.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "In a high-stakes data analytics competition, tackle the Wild Blueberry Yield Dataset, a complex, interdependent data mine. Develop an innovative model that splits into exclusive training, validation, and test sets. Design a top-notch predictive algorithm that forecasts yield with nanometer-level accuracy, outperforming past records by at least 5%. Strive for a mean absolute percentage error (MAPE) under one-tenth of a percent, pushing the boundaries of regression analysis in this sophisticated challenge.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture or data analytics competition\",\n        \"description\": \"Tackle the Wild Blueberry Yield Dataset with complex, interdependent data, developing an innovative model with exclusive training, validation, and test sets. Aim for nanometer-level accuracy in yield prediction, surpassing past records by at least 5%, while targeting a mean absolute percentage error (MAPE) below one-tenth of a percent to push the limits of regression analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.0001\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": 0.001\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wild Blueberry Yield Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"yield\"],\n            \"specification\": {\n                \"data_structure\": \"complex and interdependent\",\n                \"split_distribution\": \"exclusive training, validation, and test sets\"\n            },\n            \"description\": \"A dataset for a high-level data analytics competition with sophisticated predictive requirements.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy_goal\": \"nanometer-level\",\n                \"improvement_target\": \"past records + 5%\",\n                \"performance_metric\": \"MAPE\",\n                \"target_value\": 0.0001\n            },\n            \"description\": \"An innovative predictive algorithm designed for regression analysis, targeting nanometer-level accuracy and low MAPE.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent, need for a high-level model, and the specific requirements of the Wild Blueberry Yield Dataset. The metrics, particularly MAPE and the accuracy target, are clearly specified. The model family (neural networks) is chosen to meet the sophistication of the challenge, but the exact model name and family remain unspecified to accommodate various options. The confidence score is high due to the clear mapping of the task to the schema. However, further details like the specific preprocessing steps, data exploration, or chosen neural architecture are not provided as the instruction doesn't mention them.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for the elite deep-tech temporal data analysis team, your challenge is to design a top-tier predictive model system for the intricate Atmospheric Phenomena dataset. This dataset comprises a diverse array of meteorological patterns across 12 distinct regions, divided into train, validation, and test sets with no temporal overlap. Each data point consists of 144 hourly observations, featuring 1500 unique climate indicators (INPUT_SEQ_LEN=144, INPUT_DIM=1500). Your team must develop a hybrid neural architecture combining convolutional and recurrent layers, targeting not only a 144-step ahead forecast (PRED_SEQ_LEN=144, PRED_DIM=1500) with exceptional skill in capturing patterns, but also withstand irregularities and short-term volatility. To showcase advanced analytics, incorporate a multi-resolution forecasting strategy and a tailored loss function integrating rolling-window performance and trend-sensitive error metrics. Prepare a meticulous research paper outlining the rationale behind the innovative model design, rigorous evaluation against specialized benchmarks, and any unconventional methods utilized for unparalleled forecasting reliability in the face of complex weather dynamics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate science\",\n        \"description\": \"Design a predictive model system for the Atmospheric Phenomena dataset, with a hybrid neural architecture combining convolutional and recurrent layers, capable of a 144-step ahead forecast while handling irregularities and short-term volatility. The model should incorporate multi-resolution forecasting and a custom loss function sensitive to both rolling-window performance and trend-related error metrics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Atmospheric Phenomena\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"144-hour forecast (1500 dimensions)\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 144,\n                \"INPUT_DIM\": 1500,\n                \"TRAIN_SIZE\": null,\n                \"VALID_SIZE\": null,\n                \"TEST_SIZE\": null,\n                \"temporal_split\": \"no temporal overlap\"\n            },\n            \"description\": \"A diverse meteorological dataset with 12 regions, hourly observations, and no temporal overlap across train, validation, and test sets.\",\n            \"preprocessing\": [\n                \"Normalization of climate indicators\",\n                \"Sequence splitting\"\n            ],\n            \"augmentation\": [\"None (due to complex and sequential nature of data)\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Neural Architecture (CNN+RNN)\",\n            \"family\": \"time-series forecasting\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"PRED_SEQ_LEN\": 144,\n                \"PRED_DIM\": 1500,\n                \"architecture_details\": \"Combination of convolutions and recurrent layers for multi-resolution forecasting\",\n                \"loss_function\": \"Custom (rolling-window + trend-sensitive)\"\n            },\n            \"description\": \"A state-of-the-art model for complex weather forecasting, leveraging both CNN and RNN for accurate long-term predictions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hybrid architecture enhances capture of complex patterns in weather data\",\n        \"Multi-resolution forecasting aids in handling short-term volatility\",\n        \"Custom loss function addresses specific meteorological error considerations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Feature engineering for climate indicators\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Research paper outlining model design, evaluation, and methods\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the project, including the expert level, dataset, model requirements, and performance metrics. The dataset details are based on the information provided, while the specific values for performance metrics are left null since they are not directly specified in the instruction. The confidence score is 0.95, reflecting the high confidence in understanding the task but acknowledging the need for more detailed model specifications.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "As a project manager for an advanced visual comprehension system, you are mandated to develop an AI that bridges the gap between Optical Character Recognition (OCR) and Visual Question Answering, using the extensive TextVQA dataset. The model should exhibit exceptional precision, handle context, and demand multilayered reasoning to decode visual content and respond accurately to complex queries.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"computer vision and natural language processing\",\n        \"description\": \"Develop an AI for an advanced visual comprehension system that bridges OCR and Visual Question Answering using the TextVQA dataset. The model must exhibit high precision, handle context, and require multilayered reasoning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TextVQA\",\n            \"modality\": [\n                \"text\", \n                \"image\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"contextual_questions\": true,\n                \"complexity_range\": \"multilayered\"\n            },\n            \"description\": \"A comprehensive dataset for Visual Question Answering, combining OCR and complex queries.\",\n            \"preprocessing\": [\"OCR text extraction\", \"question-context pairing\"],\n            \"augmentation\": [\"generative text variations\", \"saliency-based image augmentation\"],\n            \"visualization\": [\"dataset distribution analysis\", \"error analysis for precision\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"OCR and Visual Question Answering fusion model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"bridging OCR outputs and visual features\",\n                \"reasoning_layers\": \"deep or complex\"\n            },\n            \"description\": \"A state-of-the-art model designed to integrate OCR and VQA, capable of high precision, context handling, and multilayered reasoning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The project requires a deep integration of OCR and VQA techniques.\",\n        \"Model must be robust against variations in visual complexity and question complexity.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual feature extraction\", \"question-specific feature augmentation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\", \"Caffe or TensorFlow framework\"],\n        \"demonstration\": \"Interactive visualizations and error analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is generated based on the instruction, mapping it to the provided schema. High-level information, such as the user's intent, problem area, and key requirements, are included. The model's high precision is specified based on the advanced nature of the project. However, some aspects like the model's exact specifications and certain complexity metrics are not explicitly given, depending on the specifics of the technologies and techniques to be used. The confidence score is set at a high level, assuming the given instruction is clear, but leaving room for potential ambiguity in the implementation choices.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "As a seasoned project manager for an interdisciplinary team, you are tasked with overseeing the design and implementation of an advanced image recognition system. The objective is to develop a state-of-the-art deep learning model, leveraging the Vision Transformer (ViT) architecture, for the precise identification of a diverse array of celestial objects in the highly intricate Galaxy Zoo dataset. The system must not only demonstrate exceptional accuracy on existing images but also exhibit outstanding transfer learning capabilities, capable of adapting to novel astronomical phenomena. Rigorous evaluation metrics, including precision, recall, and AUC-ROC, should be incorporated to ensure the model meets stringent scientific requirements for object classification in the dynamic and unpredictable cosmos.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Design and implement an advanced image recognition system, utilizing a Vision Transformer (ViT) architecture, for the precise identification of celestial objects in the Galaxy Zoo dataset. The system should have high accuracy, strong transfer learning capabilities, and meet stringent evaluation with metrics like precision, recall, and AUC-ROC.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"celestial object type\"],\n            \"specification\": null,\n            \"description\": \"A complex dataset containing diverse celestial objects for image classification using the Vision Transformer (ViT) architecture.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art ViT model\",\n            \"family\": \"Vision Transformer (ViT)\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A deep learning model based on the Vision Transformer architecture for advanced celestial object recognition in astronomy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"ViT's attention mechanism is crucial for capturing global context in complex astronomical images.\",\n        \"Transfer learning is essential for adapting to novel astronomical phenomena.\",\n        \"Rigorous evaluation with precision, recall, and AUC-ROC is necessary for scientific credibility.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"cloud-based\"],\n        \"demonstration\": \"Examples of object identification and transfer learning results on unseen astronomical phenomena.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build an advanced image recognition system. The high level of expertise is indicated, along with the specific problem domain (astronomy) and task (image classification). The metrics, such as accuracy, precision, and AUC-ROC, are included, with their values initially set to null as they would be determined during the model development process. The source of the dataset is specified as a link, which suggests the user may need to access it from an external source.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the machine learning team, I require you to develop a robust sentiment analysis model using the large-scale IMDB dataset. The task involves preprocessing, feature extraction, and fine-tuning a transformer-based model. Aim for at least 95% Macro-F1 score and provide detailed AUC-ROC and PR-curves for evaluation. Don't forget to consider class imbalance and perform hyperparameter optimization for optimal performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"social media analysis\",\n        \"description\": \"Develop a sentiment analysis model using the large-scale IMDB dataset, focusing on preprocessing, feature extraction, and fine-tuning a transformer-based model. Target at least 95% Macro-F1 score and provide AUC-ROC and PR-curves for performance evaluation, while addressing class imbalance and conducting hyperparameter optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Macro-F1 score\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"PR-curve\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"IMDB\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\"],\n            \"specification\": {\n                \"size\": \"large-scale\",\n                \"imbalance\": true\n            },\n            \"description\": \"A large movie review dataset for sentiment analysis, consisting of movie reviews and their associated sentiment labels.\",\n            \"preprocessing\": [\n                \"sentence tokenization\",\n                \"lowercasing\",\n                \"removal of stopwords\",\n                \"lemmatization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"fine-tuning\",\n                \"attention mechanism\": true,\n                \"class imbalance handling\": true\n            },\n            \"description\": \"A transformer-based model specifically designed for text classification, targeting sentiment analysis with IMDB dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"A high Macro-F1 score is required, indicating a focus on overall model performance across both classes, while considering class imbalance.\",\n        \"Hyperparameter optimization is crucial for achieving optimal performance in a transformer model.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"preprocessing steps for text data\",\n            \"handling class imbalance\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"visual AUC-ROC and PR-curves for model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the task, including the user's intent, high expertise level, the nature of the problem in NLP, the specific task of text classification, and the details for preprocessing and model architecture. Performance metrics are included, with the Macro-F1 score specified and the AUC-ROC and PR-curves' values set to null as they will be computed during model evaluation. The importance of handling class imbalance and hyperparameter optimization is emphasized. Some aspects are left unspecified (like the target model name and deployment details) as they would depend on the team's choice.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the cutting-edge medical imaging project, you're seeking a state-of-the-art V-Net++ architecture with advanced attention mechanisms for BraTS dataset analysis. The model must not only surpass a Dice similarity coefficient of 0.90, but also generate saliency maps and perform real-time interpretable feature attribution, reflecting nuanced pathological patterns for reliable and explainable AI-assisted diagnosis in oncology.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"medical imaging\",\n        \"description\": \"A state-of-the-art V-Net++ model with advanced attention mechanisms for BraTS dataset analysis. The model must achieve a Dice similarity coefficient of at least 0.90, generate saliency maps, and provide real-time interpretable feature attribution for AI-assisted diagnosis in oncology.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Dice similarity coefficient\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"FPS (frames per second)\"\n            },\n            {\n                \"name\": \"interpretability of feature attribution\",\n                \"value\": null,\n                \"unit\": \"percentage of relevant features\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BraTS\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"segmentation mask\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": \"240\",\n                    \"height\": \"240\",\n                    \"depth\": 3\n                },\n                \"label_classes\": 4\n            },\n            \"description\": \"A medical imaging dataset for brain tumor segmentation with BraTS annotations.\",\n            \"preprocessing\": [\" skull stripping\", \"registration\", \"normalization\"],\n            \"augmentation\": [\"Gaussian noise\", \"intensity-based augmentation\"],\n            \"visualization\": [\"slices, 3D rendering\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"V-Net++ with Attention Mechanisms\",\n            \"family\": \"V-Net family\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"V-Net++\",\n                \"attention_layers\": [\"Squeeze-and-Excitation\", \"Self-Attention\"]\n            },\n            \"description\": \"A deep learning model leveraging V-Net++ with advanced attention mechanisms for BraTS analysis and AI-assisted oncology diagnosis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"BraTS dataset demands high accuracy and interpretability for reliable medical applications.\",\n        \"Attention mechanisms enhance performance by focusing on relevant features.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific feature extraction\"],\n        \"target_device\": \"cloud or high-performance GPU server\",\n        \"deployment_endpoint\": \"cloud-based AI platform\",\n        \"inference_engine\": [\"TensorFlow, PyTorch\"],\n        \"demonstration\": \"Interactive visualization of saliency maps and feature attributions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a cutting-edge model for a specific medical imaging project. The Dice similarity coefficient is set as a clear performance metric, and the V-Net++ architecture is chosen based on the requirement. The complex nature of the project and dataset preprocessing requirements are covered in the knowledge and service sections, respectively. However, the exact real-time performance or interpretability scores are not provided and should be filled based on the model's actual performance.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager, I require the development team to investigate the suitability of cutting-edge deep learning semantic segmentation models for processing high-resolution Sentinel-2 satellite data. Emphasize on efficient algorithms that efficiently distinguish between intricate land classes like fresh and saltwater bodies, dense versus sparse forests, and dense urban areas, ensuring accuracy within a 24-hour timeframe for real-time analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"remote sensing\",\n        \"description\": \"Investigate the suitability of cutting-edge deep learning semantic segmentation models for processing high-resolution Sentinel-2 satellite data, focusing on efficient algorithms that accurately distinguish between land classes like fresh and saltwater bodies, dense versus sparse forests, and dense urban areas, with a requirement for real-time analysis within a 24-hour timeframe.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"24 hours\",\n                \"unit\": \"time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2 Satellite Data\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"spectrum_dimensions\": \"multi-spectral\"\n            },\n            \"description\": \"High-resolution satellite imagery from Sentinel-2 for land use classification analysis.\",\n            \"preprocessing\": [\n                \"data normalization\",\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\n                \"geospatial transformations\",\n                \"lighting and weather adjustments\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning (e.g., U-Net, Mask R-CNN, SegNet)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"complexity\": \"low-to-moderate\",\n                \"efficient_inference\": true\n            },\n            \"description\": \"Investigated deep learning models that emphasize real-time segmentation of intricate land classes from Sentinel-2 data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Recent advancements in deep learning, like transfer learning and lightweight architectures, may enhance model performance.\",\n        \"Research on domain adaptation for satellite imagery is essential to adapt models to the Sentinel-2 data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or edge for real-time analysis\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-accelerated\"\n        ],\n        \"demonstration\": \"A demonstration of segmentation results on real-time Sentinel-2 data and explanation of chosen models.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's intent is to consult on the best deep learning models for the task, indicating a consult-build scenario. Their high level of expertise is applicable to this complex problem. The focus is on real-time segmentation within a 24-hour timeframe, which is reflected in the performance metric and complexity constraint. The dataset's nature and the desired model characteristics are derived from the project manager's requirements.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for a specialized deep learning team, you've been tasked with optimizing a niche regression problem using the \"Lithium-Ion Battery Performance\" dataset. This dataset contains 50 distinct chemical composition features and is stratified into train, validation, and test sets with non-uniform distributions. Your goal is to develop a cutting-edge regression model that minimizes the mean absolute percentage error (MAPE) for predicting electrode materials' specific capacity. Ensure the model's explainability and provide a detailed analysis of feature importance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"materials science\",\n        \"description\": \"Optimize a niche regression problem using the 'Lithium-Ion Battery Performance' dataset with 50 chemical composition features. The dataset is stratified and non-uniformly distributed among train, validation, and test sets. The target is specific capacity prediction, and the goal is to develop a model with minimal MAPE, ensuring explainability and a detailed feature importance analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Lithium-Ion Battery Performance\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"specific capacity\"],\n            \"specification\": {\n                \"num_features\": 50,\n                \"data_distribution\": \"non-uniform\"\n            },\n            \"description\": \"A dataset containing 50 chemical composition features for lithium-ion battery electrodes, stratified into train, validation, and test sets.\",\n            \"preprocessing\": [\"feature scaling\", \"imputation\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature correlations\",\n                \"distribution plots\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"explainable\",\n                \"model_performance\": \"minimizing MAPE\"\n            },\n            \"description\": \"A cutting-edge deep learning regression model for lithium-ion battery performance prediction, prioritizing explainability and target-specific capacity MAPE minimization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The non-uniform feature distribution might necessitate advanced sampling techniques or tailored loss functions.\",\n        \"Explaining complex models is crucial for understanding and optimization.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature engineering for explainability\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"AI platform\",\n        \"inference_engine\": [\"TensorFlow-serving\"],\n        \"demonstration\": \"Interactive dashboard showcasing model predictions and feature importance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON response reflects the project manager's role, the problem's details, and the user's high expertise level. The area and downstream task are derived from the domain (materials science) and the specific problem (battery performance). The dataset properties (50 features, stratified distribution, and source) are clearly stated, along with preprocessing and augmentation steps. The model's target (MAPE minimization) and explainability are also captured. However, the expected MAPE value is not provided, which might require further discussion with the user.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "For the comprehensive upgrade of our cutting-edge fitness mobile application, design an intelligent adaptive system that not only suggests personalized workout routines following a dynamic algorithm, considering individual user history, real-time performance, preferences, and physiological data, but also incorporates gamification elements to enhance user engagement and retention, and analyze its effectiveness in different user segments for continuous optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"fitness and wellness\",\n        \"description\": \"Design an adaptive system for a fitness mobile app that suggests personalized workout routines based on user history, real-time performance, preferences, and physiological data. The system should also incorporate gamification elements to boost engagement and retention, and assess its effectiveness across different user segments for continuous improvement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"recommendation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"user engagement\",\n                \"value\": null\n            },\n            {\n                \"name\": \"retention rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"User History & Performance Data\",\n            \"modality\": [\"tabular\", \"time series\"],\n            \"target_variables\": [\"workout suggestions\", \"engagement metrics\"],\n            \"specification\": null,\n            \"description\": \"Contains user historical workout data, real-time performance data, and physiological metrics for model input.\",\n            \"preprocessing\": [\"data normalization\", \"feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"Gamification Data\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"engagement levels\", \"usage patterns\"],\n            \"specification\": null,\n            \"description\": \"User engagement and gamification elements usage data to evaluate system effectiveness.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Intelligent Adaptive Workout Suggester\",\n            \"family\": \"Personalized Recommendation Engine\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"type\": \"deep learning\",\n                    \"layers\": [\"history encoder\", \"real-time processor\", \"user preference model\", \"physiological model\", \"gamification fusion\"]\n                },\n                \"parameters\": null\n            },\n            \"description\": \"A dynamic deep learning model to generate personalized workout routines and integrate gamification for enhanced user experience.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporates ML algorithms like collaborative filtering and time-series analysis for adaptive workout suggestions.\",\n        \"Uses reinforcement learning for gamification design and personalization.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"user profiling\",\n            \"contextual ranking\"\n        ],\n        \"target_device\": \"mobile and wearable devices\",\n        \"deployment_endpoint\": \"backend server or app API\",\n        \"inference_engine\": [\"on-device processing\", \"cloud-based\"],\n        \"demonstration\": \"A detailed analytics dashboard for various user segments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intention to build an advanced system, the system's requirements for high performance and personalized routines, and gamification for engagement. The high level of expertise indicates a deep understanding of ML models and application to fitness apps. However, specific performance targets and machine learning models' complexities have not been provided, and are marked as null for optimization. The system's requirements for data, such as pre-processing, have been outlined.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for a cutting-edge AI team, we need a multifaceted machine translation system that caters to underrepresented languages, focusing on Swahili-to-English translation. Mandate the utilization of Masakhane's diverse dataset, incorporating both cutting-edge Transformer and recurrent neural network architectures. Strive for a substantial leap of 15% over the current state-of-the-art BLEU scores, ensuring superior accuracy and efficiency in niche language pairs.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-cultural communication\",\n        \"description\": \"Develop a multifaceted machine translation system specifically for Swahili-to-English translation, targeting underrepresented languages. Emphasize on utilizing Masakhane's diverse dataset and integrating Transformer and recurrent neural network architectures. Aim for a significant improvement of 15% over current state-of-the-art BLEU scores, prioritizing accuracy and efficiency in niche language pairs.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 0.15\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Masakhane's Diverse Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"multilingual\",\n                \"source_languages\": [\"Swahili\"],\n                \"target_languages\": [\"English\"],\n                \"diversity\": \"inclusive of underrepresented languages\"\n            },\n            \"description\": \"A comprehensive dataset for Swahili-to-English translation, leveraging Masakhane's efforts in supporting diverse languages\",\n            \"preprocessing\": [\n                \"text cleaning\",\n                \"tokenization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer, Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"models\": [\"Transformer\", \"Recurrent Neural Network\"],\n                \"architecture_integration\": \"both\"\n            },\n            \"description\": \"Advanced NLP model combining Transformer and RNN architectures for high-accuracy Swahili-to-English machine translation\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating Masakhane's dataset ensures a more inclusive and representative model for Swahili.\",\n        \"Utilizing both Transformer and RNN architectures can boost performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"cross-lingual transfer learning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"accurate and efficient Swahili-to-English translations for underrepresented languages\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers all aspects of the user's instruction, including the intended model build, dataset selection, and requirements for accuracy and language diversity. It assumes a high level of expertise for the machine learning engineer. The chosen architectures, preprocessing steps, and the goal of improving over existing BLEU scores are accurately reflected. The confidence score is high, given the clear direction from the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Elaborate on a large-scale project to monitor coastal zones with the COCO-Sea dataset, utilizing an advanced Attention U-Net architecture for segmentation. Ensure pixel-level precision between land, water, and intricate structures, while optimizing for real-time performance. Target 4K drone footage capturing, accommodating varying lighting scenarios, and maintaining a minimal latency of 1 frame per second. Conduct rigorous testing with diverse datasets and benchmark the model's scalability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"environmental monitoring\",\n        \"description\": \"Develop a large-scale project to monitor coastal zones using the COCO-Sea dataset with an Attention U-Net architecture, focusing on pixel-level precision for land, water, and intricate structures. The project must prioritize real-time performance, particularly for 4K drone footage, while maintaining a latency of 1 frame per second and accommodating varying lighting scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"pixel-level precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"frames per second\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 1,\n                \"unit\": \"frames\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO-Sea\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": \"4K\",\n                    \"height\": \"4K\"\n                },\n                \"variations\": \"varying lighting scenarios\"\n            },\n            \"description\": \"A large-scale dataset for coastal zone monitoring with 4K drone footage capturing complex lighting scenarios.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention U-Net\",\n            \"family\": \"advanced deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"optimizing for pixel-level precision\",\n                \"real-time performance\": \"true\",\n                \"inference_speed\": \"in line with 1 frame per second\",\n                \"lighting_adaptability\": \"adequate for varying lighting scenarios\"\n            },\n            \"description\": \"An advanced U-Net architecture tailored for the segmentation task of coastal zone monitoring, emphasizing precision and real-time capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Attention U-Net models are suitable for handling intricate structures in coastal images.\",\n        \"Pixel-level precision is essential for effective coastal zone monitoring.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time processing compatible\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"rigorous testing and benchmarking of scalability with diverse datasets\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the user's intent to build a complex coastal monitoring project. The problem area and downstream task are defined as computer vision (image segmentation). The user's expertise level is set to high. Performance metrics such as pixel-level precision and latency are specified, though values are left open for actual benchmarking. The COCO-Sea dataset is described based on the given conditions, and the Attention U-Net model is selected for its advanced capabilities. The confidence score is high as the information is derived directly from the instruction, but details like specific preprocessing steps and inference engine requirements are speculative.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a seasoned project manager, you are tasked with overseeing the development of a cutting-edge time-series classification model for the 'Enhanced Real-World Handwriting Dynamics Dataset'. This dataset, now expanded with intricate features (INPUT_SEQ_LEN=200 and INPUT_DIM=10), consists of highly diverse, multi-modal time-series data. The challenge lies in not only improving predictive accuracy but also accounting for sequential patterns and handling class imbalance, using windowed cross-validation for evaluation. Your goal is to design a robust model that outperforms state-of-the-art methods and delivers at least 95% validation accuracy, while maintaining interpretability and efficiency. Remember to document the model's explainability and optimization strategies in the final report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"signal processing\",\n        \"description\": \"Develop a cutting-edge time-series classification model for the 'Enhanced Real-World Handwriting Dynamics Dataset', which includes complex, multi-modal data with a sequence length of 200 and input dimension of 10. The focus is on improving accuracy, handling sequential patterns, class imbalance, and using windowed cross-validation for evaluation. The model should achieve at least 95% validation accuracy, be interpretable, and efficient with a documented explainability strategy and optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"validation accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enhanced Real-World Handwriting Dynamics Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"class_labels\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 200,\n                \"INPUT_DIM\": 10\n            },\n            \"description\": \"An expanded dataset with intricate features and multi-modal time-series data.\",\n            \"preprocessing\": [\"data normalization\", \"sequence windowing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art, interpretable\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"e.g., deep LSTM or Convolutional LSTM\",\n                \"explainability_techniques\": \"e.g., SHAP, LIME\",\n                \"optimization_strategies\": \"e.g., early stopping, model pruning\"\n            },\n            \"description\": \"A robust time-series classification model designed for the enhanced dataset, targeting high accuracy, interpretability, and efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Handle class imbalance with techniques such as SMOTE or class weighting\",\n        \"Maintaining interpretability with explainable AI techniques\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"windowed feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Final model performance demonstration and interpretability explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the requirements from the given instruction. The user's intent is to build a model (expertise is high), focusing on the specified area (time-series analysis) and task (classification). The dataset details and model requirements (high accuracy, interpretability, and efficiency) are extracted from the instruction. Performance metric value is set at 95% as specified. Important details such as source of the dataset and windowing strategies are included.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a specialized project manager for an elite machine learning development team, you are tasked with creating an advanced time-series forecasting model for the niche ETTm2 dataset. The dataset consists of intricate real-world sensor data, divided into three partitions: train, validation, and test. Each input sequence of historical readings has a distinct length (96 timestamps with 7 distinct features, INPUT_SEQ_LEN=96, INPUT_DIM=7). The challenge is to design and train an algorithm that forecasts future sequences with equal complexity, predicting 96 time steps ahead (PRED_SEQ_LEN=96, PRED_DIM=7). To demonstrate superior performance, the model must minimize both mean squared error (MSE) and mean absolute error (MAE) and exhibit exceptional accuracy in handling the dynamic patterns within the data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"data analytics, predictive maintenance, or a similar industry\",\n        \"description\": \"Create an advanced time-series forecasting model for the ETTm2 dataset with intricate sensor data. The model must forecast 96 time steps ahead and handle input sequences with 7 distinct features and different lengths.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 7,\n                \"partitioning\": {\n                    \"train\": \"\",\n                    \"validation\": \"\",\n                    \"test\": \"\"\n                }\n            },\n            \"description\": \"A complex real-world sensor data with 96 timestamps and 7 features, divided into train, validation, and test subsets.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"advanced time-series forecasting algorithm\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": {\n                    \"sequence_length\": \"INPUT_SEQ_LEN\",\n                    \"input_dim\": \"INPUT_DIM\",\n                    \"prediction_length\": \"PRED_SEQ_LEN\",\n                    \"output_dim\": \"PRED_DIM\"\n                },\n                \"model_selection\": [\"efficient in handling dynamic patterns\"]\n            },\n            \"description\": \"A specialized model for time-series forecasting that takes into account the dataset's unique characteristics and complexity.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"exemplify superior performance on handling dynamic patterns within the provided data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been parsed based on the provided user's instruction, capturing the intended model creation, dataset details, and performance metrics. Since the task involves a specialized project manager, the 'expertise' is set to 'high'. The model type is not explicitly specified, but assumed to be a neural network given the complexity of the task. Performance metrics' values are not given, as they typically require model training and evaluation to determine. The confidence score is set to a high value, assuming the information is accurate but may be incomplete without model details.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for a cutting-edge machine learning team, you've been tasked with developing a novel text classification system that not only differentiates between human-written and AI-generated content but also discerns the specific level of AI sophistication, from simple rule-based engines to advanced deep learning models. The KAGGLE-Turing Test dataset, containing a vast collection of encrypted messages and chat logs, has been provided. Your objective is to create a model that not only predicts the AI origin (0 for human, 1-5 for AI complexity scale, with 5 being the most advanced) but also estimates the probability of being authored by a particular AI architecture type. The team must achieve an average F1-score of at least 90% across all categories and provide a comprehensive explanation for their chosen architecture and feature selection. Additionally, the project should include a user-friendly visualization tool to present the classification results in an intuitive manner. Remember to consider ethical implications and bias mitigation techniques during the development process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"cybersecurity and AI ethics\",\n        \"description\": \"Develop a text classification system that can distinguish between human-written and AI-generated content, with a grading system for AI sophistication level. Target KAGGLE-Turing Test dataset with a minimum F1-score of 90% and explanations for chosen architecture and feature selection. Include a user-friendly visualization tool for results and emphasize ethical considerations and bias mitigation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average F1-score\",\n                \"value\": 0.90\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KAGGLE-Turing Test\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"AI origin\", \"AI sophistication level\"],\n            \"specification\": null,\n            \"description\": \"A collection of encrypted messages and chat logs for distinguishing human-written from AI-generated content, with AI sophistication levels.\",\n            \"preprocessing\": [\"decryption\", \"tokenization\", \"feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [\"confusion matrix\", \"ROC curves\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"explainable AI model (e.g., interpretability techniques incorporated)\",\n                \"accuracy requirements\": \"Minimum 90% F1-score\"\n            },\n            \"description\": \"A deep learning-based text classification model for detecting AI origin and sophistication level with high interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Ethical implications in AI-generated content detection\",\n        \"Bias mitigation strategies in text classification\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Data preprocessing for NLP tasks (removal of noise, stopwords, and tokenization)\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom API or web-based platform\",\n        \"inference_engine\": [\"state-of-the-art text processing libraries\"],\n        \"demonstration\": \"Interactive dashboard with clear visualization of classification results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's task of developing a text classification model with high-level requirements on F1-score, ethical implications, and visualization. The user's high expertise level has been assumed. The given dataset (KAGGLE-Turing Test) and model family (deep learning) are inferred. The model specification addresses the performance metrics. The service requirements for deployment, engine, and user interface are clearly outlined. However, specific model architecture details are not included as they might be context-dependent, leaving room for the team's expertise to choose.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "For the Coauthorship Science project, develop a state-of-the-art node classification model using a pruned spectralHGNN architecture, which incorporates citation networks and temporal dynamics. The objective is to fine-tune the model on multiple sub-domains within the broader scientific disciplines, ensuring high granularity author disambiguation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"scholarly research\",\n        \"description\": \"Develop a state-of-the-art node classification model for the Coauthorship Science project. The model should be based on a pruned spectralHGNN architecture, incorporating citation networks and temporal dynamics. The objective is to achieve high granularity author disambiguation by fine-tuning on multiple sub-domains within broader scientific disciplines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coauthorship Science\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"citation_network_data\": {},\n                \"temporal_data\": {}\n            },\n            \"description\": \"Dataset containing citation networks and temporal dynamics for node classification across various scientific sub-domains.\",\n            \"preprocessing\": [\"prune citation network\", \"extract temporal features\"],\n            \"augmentation\": [],\n            \"visualization\": [\"visualize node clusters by sub-domains\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"pruned spectralHGNN\",\n            \"family\": \"spectralHGNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"pruned_layers\": {},\n                    \"citation_integration\": true,\n                    \"temporal_inclusion\": true\n                }\n            },\n            \"description\": \"A deep learning model using a pruned spectralHGNN architecture for fine-grained author disambiguation in the Coauthorship Science dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art techniques require careful pruning and fine-tuning for best performance\",\n        \"Incorporating temporal dynamics and citation networks is crucial for accurate author disambiguation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain adaptation\", \"transfer learning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU\", \"optimized for specific framework\"],\n        \"demonstration\": \"Highlight granular classification results by sub-domains\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's request for a state-of-the-art node classification model for the Coauthorship Science project, considering the fine-tuning on multiple sub-domains and the specific architecture details. Performance metrics are kept as null for now since they will be filled during model development or after testing. The dataset and preprocessing steps, as well as the model's description and engineering requirements, are derived from the given instruction. Some missing details could include specific pruning strategies and performance values of existing models, which are assumed to be discussed during the consultation phase.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "For the 'AerospaceTelemetry' project, design a fault detection system using Enhanced Multi-layered RRCF (eML-RRCF) on high-frequency telemetry data. Implement not only real-time processing but also optimize for low-latency decision support, demanding accurate feature extraction, ensemble modeling, and performance validation under strict mission reliability constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"aerospace engineering\",\n        \"description\": \"Design a fault detection system for the 'AerospaceTelemetry' project using Enhanced Multi-layered RRCF (eML-RRCF) on high-frequency telemetry data, with a focus on real-time processing, low-latency decision support, accurate feature extraction, ensemble modeling, and performance validation under mission reliability constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"mission reliability\",\n                \"value\": null,\n                \"unit\": \"%\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"response time (decision support)\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AerospaceTelemetry\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"sampling_rate\": null,\n                \"data_dimensions\": null,\n                \"time_series_length\": null\n            },\n            \"description\": \"High-frequency telemetry data for real-time fault detection in aerospace systems.\",\n            \"preprocessing\": [\"accurate feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Multi-layered RRCF (eML-RRCF)\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"number_of_layers\": null,\n                \"number_of_trees\": null,\n                \"depth\": null,\n                \"feature_distribution\": null\n            },\n            \"description\": \"A fault detection system using ensemble of eML-RRCF trees for real-time processing on high-frequency telemetry data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The project emphasizes low-latency decision support, indicating the need for efficient algorithms and optimized data processing.\",\n        \"Mission reliability constraints call for rigorous performance testing and robust anomaly detection.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time processing\", \"low-latency decision support\"],\n        \"target_device\": \"edge computing or aerospace device\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"Anomaly detection dashboard or live system demo with latency metrics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a fault detection system for the AerospaceTelemetry project. The area, downstream task, and expertise level are derived from the project's context. Performance metrics and complexity metrics are added to reflect the requirements such as accuracy, low-latency, and mission reliability. Specific details on data preprocessing and model configuration were based on the instruction but might not be fully specified due to ambiguity, such as the sampling rate and the exact specifications of eML-RRCF.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager, I need you to develop a highly scalable and fault-tolerant deep learning architecture specifically designed for multi-class object segmentation in real-world images, with the added challenge of recognizing various breeds of cats under varying lighting conditions and background complexities.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"computer vision\",\n        \"description\": \"Develop a highly scalable and fault-tolerant deep learning architecture for multi-class object segmentation in real-world images, with a focus on recognizing various breeds of cats under varying lighting conditions and complex backgrounds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean Intersection over Union (mIoU)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"resource efficiency (e.g., memory footprint)\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Real-world Cat Images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_shape\": {\n                    \"width\": null,\n                    \"height\": null,\n                    \"channels\": 3\n                },\n                \"breeds_dataset\": \"varied\",\n                \"lighting_conditions\": \"varied\",\n                \"background_complexities\": \"high\"\n            },\n            \"description\": \"A collection of real-world images containing various breeds of cats under different lighting conditions and complex backgrounds.\",\n            \"preprocessing\": [\"data augmentation for lighting and background diversity\"],\n            \"augmentation\": [\"random brightness, contrast, and noise\"],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep Learning Architecture for Multi-class Object Segmentation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"type\": \"convolutional neural network (CNN) with encoder-decoder structure\",\n                    \"depth\": null,\n                    \"width\": null\n                },\n                \"backbone\": \"suitable for real-world object recognition with attention mechanisms or transformers\"\n            },\n            \"description\": \"A scalable and fault-tolerant deep learning model for segmenting multiple classes of objects in images, designed specifically for cat breeds recognition.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using transfer learning from ImageNet-pretrained models to improve performance under varying lighting conditions.\",\n        \"Use techniques like data augmentation and domain adaptation to handle background complexities.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or GPU-enabled environment\",\n        \"deployment_endpoint\": \"a distributed platform for large-scale deployment\",\n        \"inference_engine\": [\"TensorFlow, Keras, or PyTorch\"],\n        \"demonstration\": \" Provide visual demonstration of model performance on a variety of test cases\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the key elements provided in the instruction, including the user's intent to build a deep learning model, high-level expertise, and the focus on real-world image segmentation with specific challenges. The area and downstream task, as well as the domain, are derived from the context. Performance metrics like accuracy and mIoU are included, though without specific values (set to null). Complexity metrics like inference time and resource efficiency are mentioned as essential. The dataset's details, such as preprocessing and augmentation, are adapted from the requirement to handle challenging conditions. The model's high scalability, fault tolerance, and architecture targeting multi-class segmentation are captured. Some domain-specific knowledge is included. However, the exact model specifications, such as backbone architecture or specific architecture details, are not provided since they are not clearly specified in the instruction.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "node classification", "instruction": "Using the Mouse Brain Connectivity dataset, develop a state-of-the-art GWNN model for node classification. Mandate precision and recall above 95%, while accounting for multi-scale spatial dependencies and outliers in the data. Additionally, integrate a novel feature extraction technique to enhance the model's discriminative power.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Develop a state-of-the-art GWNN model for node classification using the Mouse Brain Connectivity dataset. Mandate precision and recall to be above 95% and account for multi-scale spatial dependencies and outliers.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Connectivity\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"graph_size\": \"unknown\",\n                \"edge_types\": \"multi-scale\",\n                \"outliers_percentage\": \"unknown\",\n                \"node_features\": \"unknown\"\n            },\n            \"description\": \"A dataset with mouse brain connectivity information, containing multi-scale spatial dependencies and outliers.\",\n            \"preprocessing\": [\n                \"handling outliers\",\n                \"multi-scale feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Graph Wavelet Neural Networks (GWNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_depth\": \"state-of-the-art\",\n                \"novel_feature_engineering\": \"unknown (novel technique)\"\n            },\n            \"description\": \"A Graph Wavelet Neural Network designed to achieve high precision and recall for node classification, with focus on multi-scale dependencies and outlier resistance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art GWNN models require careful feature engineering to capture multi-scale dependencies.\",\n        \"A novel feature extraction technique is needed to enhance model discriminative power.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"novel feature extraction technique\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Descriptive documentation or results demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was derived from the given instruction, adapting it to the schema's structure. The problem area, downstream task, and expert level are clear from the context. Performance metrics (precision and recall) are mentioned with required values. However, specific values for graph size, edge types, outliers, and novel feature extraction technique are unknown and assumed to be unknown or left to be determined by the user. The confidence score is slightly lower due to some unspecified details.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "In a cross-platform, high-traffic retail environment, design an advanced YOLO-based inventory management system. Utilizing the \"StoreShelf Dataset\" with over 10,000 diverse product categories, the model must exhibit exceptional object detection and recognition (minimum 95% mAP), distinguishing between at least 150 distinct product types. Implement an efficient, real-time pipeline that not only classifies products but also performs skimming and out-of-stock detection. Ensure that each shelf image analysis and inventory update are sub-second transactions, optimizing for a maximum latency of 1 second, while maintaining scalability and robustness under heavy load.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"Design an advanced YOLO-based inventory management system for a high-traffic, cross-platform retail environment. The system must use the StoreShelf Dataset with >10,000 diverse product categories, achieving at least 95% mAP for object detection and recognition with a focus on 150 specific product types. The pipeline should perform real-time classification, skimming, and out-of-stock detection with sub-second transaction time, targeting a maximum latency of 1 second. Scalability and robustness under heavy load are essential.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean average precision (mAP)\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 1,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"StoreShelf Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"total_samples\": 10000,\n                    \"product_categories\": 150\n                },\n                \"diversity\": \"extensive\"\n            },\n            \"description\": \"A dataset with over 10,000 shelf images containing diverse product categories for object detection and inventory management.\",\n            \"preprocessing\": [\"data augmentation\"],\n            \"augmentation\": [\"random resized crops\", \"horizontal flips\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO-based inventory management system\",\n            \"family\": \"You Only Look Once (YOLO)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"real-time object detection and recognition\",\n                \"accuracy\": \"optimized for high performance\",\n                \"detection_classes\": 150\n            },\n            \"description\": \"An advanced YOLO model tailored for the specific requirements of a scalable inventory management system in a high-traffic retail environment.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Scalability and real-time performance are critical for a high-traffic retail environment.\",\n        \"Sub-second transaction time and low latency are key performance indicators.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"product type categorization\",\n            \"transaction pipeline optimization\"\n        ],\n        \"target_device\": \"cross-platform (cloud, IoT devices)\",\n        \"deployment_endpoint\": \"optimized for high-traffic and real-time operations\",\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"demonstrate end-to-end shelf image analysis and inventory update\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was built based on the given instruction, including the core requirements for a machine learning project like model architecture, performance metrics, and operational aspects. The user's intent is to build a high-performance YOLO model with specific constraints and details about the dataset, model's capabilities, and target environment are accurately captured. However, the expert's confidence score is not explicitly specified, so it's left as a decimal point.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a seasoned project manager for a cross-cultural literary collaboration, your team must develop a tailored neural machine translation system. The requirements include: a) employing multiple-source parallel Korean and English web novel datasets for diverse domains, b) optimizing for real-time translation speed without sacrificing fluency, c) considering low-resource techniques for Korean, d) implementing attention mechanisms for context preservation, and e) conducting a user study to ensure readability and naturalness for global readers. Can you devise a roadmap for this high-accuracy, scalable solution?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-cultural literary collaboration\",\n        \"description\": \"Develop a tailored neural machine translation system for a cross-cultural literary collaboration. Key requirements are: 1) using multiple-source parallel Korean and English web novel datasets covering diverse domains, 2) optimizing for real-time translation speed without loss of fluency, 3) incorporating low-resource techniques for Korean, 4) implementing attention mechanisms to preserve context, and 5) conducting a user study for readability and naturalness.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"translation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time speed\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency for real-time translation\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multiple-source Korean-English Web Novel Datasets\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain_coverage\": \"diverse\",\n                \"parallel_corpus\": true\n            },\n            \"description\": \"Large, diverse web novel datasets for Korean and English in multiple domains\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"sentence segmentation\"\n            ],\n            \"augmentation\": [\n                \"domain-specific translation augmentation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Neural Machine Translation\",\n            \"family\": \"Transformer with attention mechanism\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"low_resource_techniques\": \"used for Korean\",\n                \"attention_mechanisms\": true\n            },\n            \"description\": \"A neural machine translation model tailored for real-time performance, fluency, and context preservation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Exploiting parallel corpora for low-resource languages\",\n        \"Balancing translation speed and fluency using efficient architectures\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware optimization\"\n        ],\n        \"target_device\": \"real-time and resource-constrained\",\n        \"deployment_endpoint\": \"cloud-based for scalability\",\n        \"inference_engine\": [\n            \"GPU-accelerated for speed\"\n        ],\n        \"demonstration\": \"user studies, comparative evaluations with existing systems\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents a structured representation of the user's instruction, including the high-level intent for model building, the target area of NLP (natural language processing), and specific requirements for the model. Metrics like accuracy and real-time speed are included, but actual values are left unspecified as they would require further research and experimentation. The roadmap is well-structured, including dataset details, preprocessing steps, model design, and user study requirements. However, specific details such as exact user study design and performance of low-resource techniques are not specified, as they would vary depending on the implementation.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "For the upcoming wildlife conservation project, we require a specialized object detection model, specifically an Enhanced EfficientDet, trained on the diverse and challenging iWildCam dataset. The model must exhibit exceptional species classification and real-time tracking on resource-constrained edge devices with limited battery capacity, operating in extreme environmental conditions and remotely situated base stations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"Design and develop a specialized Enhanced EfficientDet object detection model for a wildlife conservation project. The model must be trained on the iWildCam dataset, delivering exceptional species classification and real-time tracking on resource-constrained edge devices with limited battery capacity, operability in extreme environmental conditions, and remote base stations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"species classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time tracking performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"iWildCam dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A diverse and challenging dataset for wildlife conservation, containing images suitable for training an Enhanced EfficientDet model for object detection.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced EfficientDet\",\n            \"family\": \"EfficientDet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"constraints\": {\n                    \"device\": \"edge devices\",\n                    \"battery\": \"limited\",\n                    \"environment\": \"extreme\",\n                    \"computational_resources\": \"low\"\n                }\n            },\n            \"description\": \"A computationally efficient object detection model optimized for real-time performance and deployment on resource-constrained edge devices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"iWildCam dataset's diversity and complexity challenge the model's performance.\",\n        \"Model must be tailored for resource-constrained environments and remote base stations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"remote base stations\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"accurate species classification and tracking in real-time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the user's intent to build a model, emphasizes the expert level of expertise, and provides a clear problem description. The area of interest, downstream task, and the dataset are specified. Performance metrics are expected to focus on classification accuracy and real-time tracking, while complexity metrics center on inference time and memory usage for edge devices. The model type and constraints are in line with the requirements. However, specific performance targets are not provided, and the system may require additional information to fully define the model's efficiency and performance.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for a high-accuracy medical sentiment analysis team, you must develop a state-of-the-art text classifier using the intricate WebMD Review dataset. This dataset consists of fine-grained, multi-aspect customer feedback that demands advanced sentiment understanding. Your goal is to achieve not just decent, but exceptional performance with a focus on precision, recall, and F1-score. Ensure the model's explainability and interpretability while maintaining real-time efficiency for healthcare professionals to analyze patient experiences.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Develop a state-of-the-art text classifier for fine-grained, multi-aspect medical sentiment analysis using the WebMD Review dataset. The classifier should achieve exceptional performance in precision, recall, and F1-score, with explainability, interpretability, and real-time efficiency for healthcare professionals.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WebMD Review Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"feature_dimensions\": null,\n                \"aspect_types\": null\n            },\n            \"description\": \"A dataset containing fine-grained, multi-aspect customer feedback for medical sentiment analysis, demanding advanced understanding.\",\n            \"preprocessing\": [\"data cleaning\", \"tokenization\", \"lemmatization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"data exploration\", \"feature distribution\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Text Classifier\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-based (e.g., BERT, RoBERTa)\",\n                \"parameters\": {\n                    \"number_of_layers\": null,\n                    \"hidden_units\": null,\n                    \"attention_heads\": null\n                },\n                \"interpretability\": \"Explainable AI techniques like SHAP, LIME, or Integrated Gradients\"\n            },\n            \"description\": \"A state-of-the-art, explainable text classifier for healthcare sentiment analysis with a focus on precision, recall, and F1-score.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Understanding the intricacies of multi-aspect sentiment analysis in medical context is crucial.\",\n        \"Maintaining real-time efficiency and explainability are essential for healthcare professionals.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature engineering for sentiment analysis\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"Interactive demo for healthcare professionals to analyze patient experiences\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was created based on the provided instruction. The high level of expertise, problem area (NLP), task (text classification), and the focus on precision, recall, and F1-score are captured. The specific dataset and its complexity are noted, as well as the requirement for explainability and real-time efficiency. However, performance metric values are left as null as they are not provided in the instruction. The model family and architecture are assumed based on the problem's complexity and need for advanced sentiment understanding.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "node classification", "instruction": "For the High School Adaptable Social Dynamics Research Project, design a state-of-the-art Temporal Graph Neural Network (TGN) with attention mechanisms, which not only classifies influential nodes in ever-evolving contact networks but also analyzes temporal dependencies and community formations. The model must handle scalability, high sparsity, and outperform existing methods while maintaining interpretability for educators and researchers.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"social sciences\",\n        \"description\": \"Design a state-of-the-art Temporal Graph Neural Network (TGN) with attention mechanisms for the High School Adaptable Social Dynamics Research Project. The model should classify influential nodes in dynamic contact networks, analyze temporal dependencies, and detect community formations. It must handle scalability, high sparsity, and outperform existing methods while maintaining interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability_score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": {\n                    \"unit\": \"efficiency per node\"\n                }\n            },\n            {\n                \"name\": \"sparsity handling\",\n                \"value\": {\n                    \"unit\": \"%\"\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High School Adaptable Social Dynamics Dataset\",\n            \"modality\": [\n                \"graph\",\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"nodes\": {\n                    \"size\": null,\n                    \"features\": null\n                },\n                \"edges\": {\n                    \"type\": null,\n                    \"quantity\": null\n                },\n                \"time_steps\": null,\n                \"sparsity\": null\n            },\n            \"description\": \"A dynamic contact network dataset for high school social dynamics research, with temporal and evolving node interactions.\",\n            \"preprocessing\": [\n                \"time series alignment\",\n                \"node embedding generation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"network evolution visualization\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-Art TGN with Attention Mechanisms\",\n            \"family\": \"Temporal Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"params\": {\n                    \"hidden_layer_size\": null,\n                    \"attention_heads\": null,\n                    \"interpretability_techniques\": [\n                        \"attention visualization\",\n                        \"explainable node importance\"\n                    ]\n                },\n                \"inference_speed\": null,\n                \"training_speed\": null\n            },\n            \"description\": \"A TGN designed for analyzing social dynamics in high school contact networks, incorporating attention mechanisms and striving for interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model should handle sparsity with graph convolution techniques.\",\n        \"Interpretability is crucial to help educators and researchers understand the results.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"node features engineering\",\n            \"temporal feature extraction\"\n        ],\n        \"target_device\": null,\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"TensorFlow Serving\"\n        ],\n        \"demonstration\": \"Visualize predicted influential nodes and network changes over time.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been crafted based on the given instruction. It covers the user's intent for model development, expertise, specific GNN with attention for node classification, temporal dependencies, community analysis, and interpretability requirements. Performance metrics are not specified for performance comparison but acknowledged as a requirement. Complexity metrics and dataset preprocessing steps align with the challenges mentioned. However, exact model parameters and dataset specifics (such as number of nodes, edges, features) are left unspecified as they would typically be determined during the design and experimentation phase.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the advanced financial news classification project, you must now develop a machine learning pipeline using the Reuters-21578 dataset. Mandate a fastText model for its efficiency, but demand a state-of-the-art architecture that surpasses 85% accuracy, particularly in high-risk sectors like market manipulation and earnings reports, while meticulously ensuring a stringent 2% false positive rate across all categories.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Develop a machine learning pipeline for advanced financial news classification using the Reuters-21578 dataset. Mandate a fastText model for efficiency, target at least 85% accuracy in high-risk categories (market manipulation, earnings reports), and maintain a stringent 2% false positive rate across all categories.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.85,\n                \"targeted_categories\": [\"market manipulation\", \"earnings reports\"]\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": 0.02,\n                \"global\": true\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"financial news category\"],\n            \"specification\": {\n                \"data_size\": \"size as per dataset\",\n                \"categorical_labels\": true\n            },\n            \"description\": \"A comprehensive financial news dataset used for text classification.\",\n            \"preprocessing\": [\"text normalization\", \"tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"fastText\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": \"high\",\n                \"architecture\": \"state-of-the-art\"\n            },\n            \"description\": \"A fast, efficient, and state-of-the-art neural network model for text classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The importance of high accuracy in high-risk categories\",\n        \"Requirement for low false positive rate\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time inference support\"],\n        \"demonstration\": \"A demonstration of financial news classification with sample outputs\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response has been parsed according to the provided requirements. The user intent, high expertise level, and the problem description are clear, focusing on the financial news classification and the specific performance metrics. The fastText model is mandated, and state-of-the-art performance is demanded. The false positive rate is strictly set, particularly in high-risk sectors. Preprocessing steps are specified, and the model requirements are adequately detailed. The confidence score is 0.98 as the information is derived directly from the instruction, and there's a clear mapping to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "For the DBLP citation network, develop a cutting-edge EdgeConv-based node classification model that not only differentiates papers by subject with precision >=0.85 but also demonstrates robustness to label noise and incorporates interpretability techniques. Optimize for hardware efficiency on limited-resource GPUs while ensuring scalability for large datasets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic\",\n        \"description\": \"Develop a cutting-edge EdgeConv-based node classification model for the DBLP citation network, differentiating papers by subject with precision >=0.85, being robust to label noise, and incorporating interpretability techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"GPU efficiency\",\n                \"value\": null,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DBLP Citation Network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"edges\": {\n                    \"total_count\": null,\n                    \"max_neighbors\": null\n                },\n                \"nodes\": {\n                    \"total_count\": null,\n                    \"max_attributes_count\": null\n                }\n            },\n            \"description\": \"A large graph dataset for node classification with the DBLP citation network.\",\n            \"preprocessing\": [\"label noise reduction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"interpretability visualization\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"EdgeConv\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": null,\n                \"flop_rate\": null,\n                \"interpretability_techniques\": [\"attention mechanisms\", \"explainable AI components\"]\n            },\n            \"description\": \"An advanced EdgeConv model tailored for node classification on the DBLP network, optimized for hardware efficiency on resource-limited GPUs and scalability for large datasets.\"\n        }\n    ],\n    \"knowledge\": [\n        \"EdgeConv architecture enhances node representation for graph-based learning.\",\n        \"Incorporating interpretability techniques helps with model transparency and identifying influential factors.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Lightweight feature extraction for resource constraint\"\n        ],\n        \"target_device\": \"limited-resource GPUs\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient inference libraries like TensorFlow-Lite\"],\n        \"demonstration\": \"Model explainability through feature importance highlighting\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's request to build a high-performing EdgeConv-based model, considering precision, label noise robustness, interpretability, and hardware efficiency. However, specific metric values or model size requirements are not provided, so they are left as null for optimization. The description and details of the dataset, preprocessing, and model specifications are based on the given instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for the AI translation team, your objective is to develop a cutting-edge transformer-based model using the WMT19 multilingual news commentary corpus. Emphasize on enhancing cross-lingual capabilities and targeting parallel corpus adaptation. Mandate a stepwise refinement process, evaluating BLEU scores with a minimum requirement of 45, while also considering METEOR, TER, and ROUGE-L for a comprehensive assessment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"Develop a cutting-edge transformer-based model for AI translation using the WMT19 multilingual news commentary corpus, focusing on cross-lingual capabilities and parallel corpus adaptation. Implement a stepwise refinement process with a BLEU score target of 45 and evaluate using METEOR, TER, and ROUGE-L for comprehensive assessment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU\",\n                \"value\": 45\n            },\n            {\n                \"name\": \"METEOR\",\n                \"value\": null\n            },\n            {\n                \"name\": \"TER\",\n                \"value\": null\n            },\n            {\n                \"name\": \"ROUGE-L\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 Multilingual News Commentary Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"corpus_size\": \"multilingual\",\n                \"data_format\": \"news commentary\"\n            },\n            \"description\": \"A multilingual dataset for training and evaluating advanced machine translation models.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"cutting-edge\",\n                \"cross-lingual_capabilities\": true,\n                \"adaptation_technique\": \"parallel corpus\"\n            },\n            \"description\": \"A transformer model designed for state-of-the-art machine translation, with an emphasis on multilingual and parallel corpus improvements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transformer-based models excel in natural language understanding and translation tasks.\",\n        \"Cross-lingual capabilities are critical for multilingual translation.\",\n        \"Parallel corpus adaptation can improve translation performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Stepwise refinement process\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \" BLEU, METEOR, TER, and ROUGE-L evaluations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project manager's objective, with focus on model development, the dataset, and performance metrics. The user's intent to build is clear, and the model family is inferred as neural networks, specifically a transformer. The user's expertise level is assumed to be high. The performance metrics (BLEU, METEOR, TER, and ROUGE-L) are included as expected, although target values for the latter two are unspecified as they're not mentioned in the instruction. The dataset and source are accurately represented, as is the desired pre-processing and evaluation methodology. The confidence score is high based on the level of detail provided, though missing specific model configuration details.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, I require a sophisticated deep learning architecture for image classification, integrating CIFAR-10 dataset. Mandate a state-of-the-art model with at least 99% accuracy, ensuring transfer learning, data augmentation, and fine-tuning for optimal performance, while maintaining efficiency and generalization across multiple subcategories.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"industrial or research\",\n        \"description\": \"Seeking a sophisticated deep learning architecture for image classification using the CIFAR-10 dataset, with a target accuracy of at least 99%. Requires transfer learning, data augmentation, and fine-tuning for optimal performance, while ensuring efficiency and generalization across multiple subcategories.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CIFAR-10\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": \"32x32\",\n                \"number_of_classes\": 10\n            },\n            \"description\": \"A popular benchmark dataset for image classification with 10 subcategories and varying image intensities.\",\n            \"preprocessing\": [\n                \"image normalization\",\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\n                \"random cropping\",\n                \"horizontal flipping\",\n                \"color jitter\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"e.g., ResNet, EfficientNet, or Vision Transformer\",\n                \"transfer_learning\": true,\n                \"fine-tuning\": true,\n                \"efficiency\": \"considered while model design\"\n            },\n            \"description\": \"A deep learning model tailored for CIFAR-10 image classification, achieving high accuracy with transfer learning, data augmentation, and fine-tuning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating transfer learning helps to leverage pre-trained models for better performance.\",\n        \"Data augmentation enhances model robustness against variations in the dataset.\",\n        \"Fine-tuning ensures the model adapts to the specific CIFAR-10 task.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Illustrate performance on unseen samples or confusion matrix\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project manager's requirements, including the focus on a state-of-the-art model, high accuracy, and the mentioned techniques for enhancing performance. Specific model details are left intentionally vague to allow for choice based on current advancements in the field. Missing elements include the exact architecture, as it depends on current research, but the description is clear enough to guide the selection process.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "You are now embarking on a comprehensive data science project involving multiple regression techniques. Using the diverse Mohs Hardness Dataset, which contains intricate numerical features, you must perform an efficient train-test-validation split (80-10-10%). Your objective is to develop a highly-accurate predictive model, focusing on minimizing the median absolute error (MedAE) as the key performance indicator. Conduct feature engineering, fine-tune hyperparameters, and compare at least three advanced regression algorithms before presenting your final results.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"materials science\",\n        \"description\": \"Conduct a data science project on the Mohs Hardness Dataset using multiple regression techniques. Split the dataset into an 80-10-10% train-test-validation split. The objective is to develop a highly-accurate predictive model, with a focus on minimizing median absolute error (MedAE) as the primary performance indicator. Implement feature engineering, fine-tune hyperparameters, and compare at least three advanced regression algorithms.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"median absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mohs Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"hardness\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": 0.8,\n                    \"validation\": 0.1,\n                    \"test\": 0.1\n                },\n                \"feature_distribution\": {\n                    \"intricate numerical features\": \"all\"\n                }\n            },\n            \"description\": \"A dataset with detailed numerical features for the analysis and prediction of Mohs hardness.\",\n            \"preprocessing\": [\"feature normalization\", \"missing value imputation\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature correlations\",\n                \"data distribution plots\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": {\n        \"items\": [\n            {\n                \"name\": \"Linear Regression\",\n                \"family\": \"Linear\",\n                \"type\": \"classical machine learning\"\n            },\n            {\n                \"name\": \"Random Forest Regressor\",\n                \"family\": \"Ensemble\",\n                \"type\": \"classical machine learning\"\n            },\n            {\n                \"name\": \"Gradient Boosting Regressor\",\n                \"family\": \"Ensemble\",\n                \"type\": \"classical machine learning\"\n            }\n        ]\n    },\n    \"knowledge\": [\n        \"Efficient feature selection and preprocessing are crucial for intricate numerical datasets.\",\n        \"Fine-tuning hyperparameters is needed for optimal model performance.\",\n        \"Comparison of multiple algorithms helps identify the best model for this task\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"PCA for dimensionality reduction\",\n            \"Scaling features\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Performance comparison visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been constructed based on the given instruction, accurately reflecting the user's intention to build a model, their level of expertise, and the requirements for the Mohs Hardness Dataset. The area of focus, task, and application domain are specified. Performance metric (MedAE) and train-test-validation split are included. The knowledge and service aspects have been deduced from the instruction. However, specific feature engineering steps, such as PCA, and fine-tuning details are not provided as part of the JSON since the instruction doesn't specify those details. The model comparison is outlined but with placeholder 'comparison' in the demonstration. The confidence score is high given the clear instructions and mapping to the JSON schema.\",\n        \"score\": 0.92\n    }\n}"}
{"task": "question answering", "instruction": "As a project manager for our next-gen multilingual QA system, you are mandated to develop a robust solution using the advanced mBERT and XLM-R architectures. The task involves conquering the MLQA dataset, with a stringent requirement for proficiency in five languages (English, Spanish, Mandarin, and two additional lesser-known dialects). Ensure exceptional cross-lingual accuracy, even in scarce data situations, while maintaining a focus on model efficiency and computational footprint.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"multilingual QA system\",\n        \"description\": \"Develop a robust multilingual QA system using mBERT and XLM-R architectures, targeting the MLQA dataset, with a focus on proficiency in five languages (English, Spanish, Mandarin, and two additional lesser-known dialects). The solution should exhibit exceptional cross-lingual accuracy, handle scarce data, and prioritize model efficiency and computational footprint.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"cross-lingual accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model efficiency\",\n                \"value\": null,\n                \"unit\": \"% reduction in FLOPs\"\n            },\n            {\n                \"name\": \"computational footprint\",\n                \"value\": null,\n                \"unit\": \"GPU memory consumption\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MLQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"English\", \"Spanish\", \"Mandarin\", \"two lesser-known dialects\"],\n                \"data_distribution\": \"scarcity\"\n            },\n            \"description\": \"A multilingual question answering dataset for the development of the QA system.\",\n            \"preprocessing\": [\n                \"language translation (if needed for lesser-known dialects)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": [\"mBERT\", \"XLM-R\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"transformers\"]\n            },\n            \"description\": \"A robust multilingual QA model combining mBERT and XLM-R for the MLQA dataset, with an emphasis on efficiency and performance in low-resource scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"mBERT and XLM-R are advanced for multilingual tasks.\",\n        \"The model should handle cross-lingual data efficiently and effectively\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"cross-language question answering demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction, including the need for a multilingual QA system, the specified architectures, and the requirement for handling low-resource languages. However, it leaves room for flexibility with unspecified values for performance metrics, efficiency, and computational footprint. The performance metrics are set to null, and the rationale explains why. A 'target_device' and 'deployment_endpoint' are not explicitly mentioned but could be added based on the project manager's needs.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As a project manager for a machine learning-driven security analytics platform, you are tasked with developing an advanced real-time monitoring system. The system must not only detect and alert on significant fluctuations in website traffic, including extreme spikes and sudden drops, but also analyze patterns to distinguish normal seasonal variations from anomalies indicating possible bot attacks, DDoS, or user behavior changes. Provide a detailed technical roadmap and a prototype model with at least 95% accuracy within a 60-day timeframe.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"cybersecurity\",\n        \"description\": \"Develop an advanced real-time monitoring system for a machine learning-driven security analytics platform, focusing on detecting significant fluctuations in website traffic, differentiating between normal seasonal variations and anomalies related to bot attacks, DDoS, or user behavior changes, with a target accuracy of at least 95%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"real-time\",\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Website Traffic Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"traffic fluctuations\"],\n            \"specification\": {\n                \"data_length\": \"ongoing 60-day period\",\n                \"granularity\": \"hourly\",\n                \"features\": [\"volume\", \"traffic types\", \"seasonality\"]\n            },\n            \"description\": \"A time-series dataset capturing website traffic with hourly resolution, containing variables for volume, traffic type, and seasonal patterns.\",\n            \"preprocessing\": [\"resampling\", \"feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [\"pattern recognition\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Real-time Anomaly Detection Model\",\n            \"family\": \"time-series forecasting or anomaly detection algorithms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep learning\",\n                \"model_family_details\": \"such as LSTM or Autoencoder\"\n            },\n            \"description\": \"A neural network-based model designed to perform real-time anomaly detection and classification for website traffic data, detecting significant fluctuations and distinguishing between normal patterns and security threats.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Latest techniques in time-series anomaly detection and feature extraction for website traffic analysis\",\n        \"Adversarial defense strategies for bot attacks and DDoS prevention\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time monitoring system\",\n        \"deployment_endpoint\": \"platform's backend\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Kubernetes for scalability\"],\n        \"demonstration\": {\n            \"method\": \"interactive dashboard displaying real-time alerts and anomaly explanations\",\n            \"frequency\": \"hourly\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is structured according to the JSON schema, taking into account the high-level requirements of a machine learning-driven security analytics project. The user's expertise is assumed to be high, as they are a project manager with security analytics as the focus. The area of expertise is time-series analysis with a real-time focus, and the problem description is detailed, including the need for high accuracy. Complexity metrics are defined, such as real-time inference. The dataset is described as a time-series capturing traffic data with required preprocessing steps. The model, while not specific, is likely to involve advanced deep learning methods for accuracy. Known strategies for the problem domain have been mentioned. The service section details required functionalities like deployment to a backend and a demonstrative interface.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "link prediction", "instruction": "Construct a highly accurate and differentially-sensitive Attention-based Graph Neural Network (AGNN) model for link prediction in Tencent Weibo, considering temporal dynamics and user-specific influence factors, to discern subtle nuances in real-time social interactions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"social networks\",\n        \"description\": \"Construct a highly accurate and differentially-sensitive Attention-based Graph Neural Network (AGNN) model for link prediction in Tencent Weibo, taking temporal dynamics and user-specific influence factors into account to detect subtle nuances in real-time social interactions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"differential sensitivity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tencent Weibo\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"link prediction\"],\n            \"specification\": {\n                \"temporal_dimensions\": \"considered\",\n                \"user_influence_factors\": \"incorporated\"\n            },\n            \"description\": \"A social graph dataset from Tencent Weibo with temporal dynamics and personalized influence factors.\",\n            \"preprocessing\": [\n                \"temporal feature extraction\",\n                \"user influence feature engineering\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Attention-based Graph Neural Network (AGNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"details not provided\",\n                \"temporal_component\": \"considered\",\n                \"user_attention_module\": \"incorporated\"\n            },\n            \"description\": \"A state-of-the-art model designed for link prediction in Tencent Weibo, integrating attention mechanisms, temporal dynamics, and user-specific influences.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time and nuanced social interactions require consideration of temporal information and personalized influence factors.\",\n        \"High accuracy is crucial, along with the ability to discern subtle differences in link prediction.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal\",\n            \"user-specific\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"explanatory model predictions and sensitivity analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a complex model, high expertise level, and the specific problem in social network analysis. The area and downstream task are well-defined. Performance metrics like accuracy and differential sensitivity are mentioned, but their target values are not provided. The dataset includes Tencent Weibo with temporal dynamics and user-specific factors, along with preprocessing steps. The model is accurately described as an Attention-based GNN tailored for the task. However, missing details such as specific architecture and temporal component description may reduce confidence slightly.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager, you've been tasked with enhancing the existing financial news classifier for enhanced risk detection. Utilize the Reuters-21578 dataset, but challenge the ML team to devise a novel, hybrid fastText model that not only maintains efficiency but outperforms current benchmarks by 3% in precision. Target an accuracy of 90% specifically for delicate market sectors like insider trading and critical earnings disclosures. In addition, demand a multi-layered error analysis protocol to guarantee a pristine 1% false positive rate across all classification categories, ensuring regulatory compliance and user trust.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Enhance the existing financial news classifier for risk detection, using the Reuters-21578 dataset. The ML team is required to develop a novel hybrid fastText model with improved performance, specifically targeting an accuracy of 90% for sectors like insider trading and critical earnings disclosures. The model should outperform current benchmarks by 3% in precision. Additionally, a multi-layered error analysis protocol with a false positive rate target of 1% across all categories is needed for regulatory compliance and user trust.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.90\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 1.03\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"financial news categories\"],\n            \"specification\": null,\n            \"description\": \"A financial news dataset for text classification, focusing on risk detection, specifically for sectors like insider trading and critical earnings disclosures.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"hybrid fastText\",\n            \"family\": \"fastText\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": \"high\",\n                \"benchmark_comparison\": \"outperforms by 3%\"\n            },\n            \"description\": \"A novel hybrid fastText model optimized for financial news classification, designed to achieve improved precision and accuracy, particularly in delicate market sectors.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hybrid models can combine strengths for better performance.\",\n        \"Multi-layered error analysis is crucial for accurate and compliant financial classification.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A protocol for achieving a 1% false positive rate across all categories\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been parsed based on the given instruction. It covers the user's intent, expertise, problem statement, performance and complexity metrics, dataset specifics, and requirements for the novel model. The areas of enhancement, dataset name, and the need for high precision and accuracy for specific sectors have been incorporated. The model family and target metrics reflect the challenge of outperforming current benchmarks and ensuring regulatory compliance. The confidence score is high due to the clear mapping of the instruction to the JSON structure.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "node classification", "instruction": "Utilize the Mouse Brain Connectivity dataset to design a high-performance Graph Wavelet Neural Network (GWNN) model, targeting node classification with exceptional accuracy, exceeding 97%. Mandate concurrent optimization for precision, recall, and robustness against multi-scale spatial variations and outliers. Furthermore, incorporate a cutting-edge, twofold feature fusion mechanism for increased discrimination ability, ensuring interpretability in the complex network topology.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Design a high-performance Graph Wavelet Neural Network (GWNN) model using the Mouse Brain Connectivity dataset, targeting node classification with exceptional accuracy, expected to be above 97%. The model should optimize for precision, recall, and robustness against multi-scale spatial variations and outliers. A two-fold feature fusion mechanism for increased discrimination ability is required, along with interpretability in the complex network topology.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Connectivity\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": \"unsupplied\",\n                \"node_attribute_size\": \"unsupplied\",\n                \"edge_attribute_size\": \"unsupplied\",\n                \"complexity\": \"high\"\n            },\n            \"description\": \"A mouse brain connectivity dataset for node classification, containing complex network topology and multi-scale spatial variations.\",\n            \"preprocessing\": [\"feature normalization\", \"data cleaning\"],\n            \"augmentation\": [\"random graph sampling\"],\n            \"visualization\": [\"degree distribution\", \"community detection\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Wavelet Neural Network (GWNN)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"high-performance\",\n                \"feature_fusion\": \"twofold\",\n                \"interpretability\": true\n            },\n            \"description\": \"A GWNN model designed for node classification in mouse brain connectivity data, aiming for high accuracy, precision, recall, and robustness against multi-scale variations and outliers with interpretable feature fusion.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Feature fusion in a Graph Wavelet domain may enhance discrimination.\",\n        \"Interpretability is crucial for analyzing complex network topology.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"node embedding\", \"edge feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient GPU support\"],\n        \"demonstration\": \"model's decision-making on key nodes\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the essential details provided in the instruction, with high confidence. The area, task, and dataset specifics are accurately captured. The user's intent to build a model and high expertise level are assumed. Performance metrics and complexity are set as placeholders for specific values to be determined. However, the dataset size and some preprocessing steps are left unquantified as they were not provided.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "For the \"Quantum-Enabled Hypersonic Flight Dynamics\" initiative, develop a highly-secure fault detection system using a novel fusion of Quantum-enhanced eML-RRCF (Quantum-eML-RRCF) on encrypted, high-resolution telemetry gathered from supersonic aircraft. Mandate real-time analysis with sub-millisecond latency, rigorous feature extraction from complex aerospace waveforms, ensemble learning techniques leveraging quantum computing, and stringent performance validation with mission success rates exceeding 99.99%, ensuring minimal compromise of flight safety.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"aerospace engineering\",\n        \"description\": \"For the 'Quantum-Enabled Hypersonic Flight Dynamics' initiative, develop a highly-secure fault detection system using Quantum-enhanced eML-RRCF (Quantum-eML-RRCF) on encrypted, high-resolution telemetry from supersonic aircraft. The system must support real-time analysis with sub-millisecond latency, efficient feature extraction from complex aerospace waveforms, and incorporate ensemble learning techniques utilizing quantum computing. Performance must meet mission success rates of at least 99.99% to ensure flight safety.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mission success rate\",\n                \"value\": 0.9999\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 0.001\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency (response time)\",\n                \"value\": 0.001,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Quantum-Enabled Hypersonic Flight Telemetry\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"encryption\": true,\n                \"resolution\": \"high\"\n            },\n            \"description\": \"Encrypted, high-resolution telemetry data gathered from supersonic aircraft for flight dynamics analysis.\",\n            \"preprocessing\": [\n                \"feature extraction\",\n                \"quantum data decryption\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-eML-RRCF\",\n            \"family\": \"ensemble learning with quantum-enhanced eML\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"quantum_computing\": true,\n                \"ensemble_technique\": \"eML-RRCF\"\n            },\n            \"description\": \"A quantum computing-driven fault detection system using ensemble learning technique eML-RRCF on encrypted aerospace telemetry for real-time and highly secure analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Quantum-enhanced machine learning and secure encryption play a crucial role in the safety-critical nature of this project.\",\n        \"Sub-millisecond latency is essential for real-time decision making in flight dynamics\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"quantum-enhanced feature extraction\"\n        ],\n        \"target_device\": \"cloud or dedicated quantum processing unit\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Quantum-enabled data processing\"],\n        \"demonstration\": \"real-time fault detection simulation for mission scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction, where the user's intent to build a model, their high level of expertise, and the specific application domain (aerospace engineering) are captured. The problem description, performance metrics, and complexity metrics are derived directly from the task, with the key details such as the novel Quantum-eML-RRCF, real-time analysis, and stringent performance requirements. The only assumptions made are that the telemetry data is high-resolution and must be encrypted, and that the system might leverage quantum computing infrastructure for processing and ensemble learning.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "For the Coauthorship Science initiative, design an advanced node classification model that combines a novel pruning technique with a dynamic, time-evolving spectralHGNN. This model should not only leverage citation networks but also integrate domain-specific feature extraction and adaptability. Achieve superior accuracy in differentiating authors across a diverse set of sub-disciplines, maintaining precision at the subfield level for enhanced author attribution.", "resposne": "{\n  \"user\": {\n    \"intent\": \"build\",\n    \"expertise\": \"high\"\n  },\n  \"problem\": {\n    \"area\": \"graph machine learning\",\n    \"downstream_task\": \"node classification\",\n    \"application_domain\": \"scholarly research\",\n    \"description\": \"Design an advanced node classification model for the Coauthorship Science initiative, using a novel pruning technique with a dynamic, time-evolving spectralHGNN. The model should incorporate citation networks, domain-specific feature extraction, and adaptability for accurate author distinction across diverse sub-disciplines, prioritizing precision at the subfield level.\",\n    \"performance_metrics\": [\n      {\n        \"name\": \"accuracy\",\n        \"value\": null\n      }\n    ],\n    \"complexity_metrics\": []\n  },\n  \"dataset\": [\n    {\n      \"name\": \"Coauthorship Science dataset\",\n      \"modality\": [\"graph\"],\n      \"target_variables\": [\"author\"],\n      \"specification\": {\n        \"citation_network_structure\": {\n          \"dynamic\": true\n        },\n        \"domain_features\": {\n          \"custom\": true\n        }\n      },\n      \"description\": \"A graph dataset with citation networks and domain-specific features for author classification in the Coauthorship Science initiative.\",\n      \"preprocessing\": [\"novel pruning technique\", \"dynamic feature extraction\"],\n      \"augmentation\": [],\n      \"visualization\": [],\n      \"source\": \"user-link\"\n    }\n  ],\n  \"model\": [\n    {\n      \"name\": \"Advanced Pruning spectralHGNN\",\n      \"family\": \"novel pruning + spectralHGNN\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"adaptability\": {\n          \"dynamic\": true\n        },\n        \"feature_engineering\": {\n          \"domain_specific\": true\n        }\n      },\n      \"description\": \"A state-of-the-art node classification model using a dynamic, time-evolving spectralHGNN with a novel pruning technique, specifically designed for the Coauthorship Science dataset.\"\n    }\n  ],\n  \"knowledge\": [\n    \"Incorporating domain-specific features and adaptability is crucial for the Coauthorship Science initiative.\",\n    \"Advanced pruning techniques can enhance model performance in citation networks.\"\n  ],\n  \"service\": {\n    \"feature_engineering\": [],\n    \"target_device\": \"\",\n    \"deployment_endpoint\": \"\",\n    \"inference_engine\": [],\n    \"demonstration\": \"focus on subfield-level precision\"\n  },\n  \"confidence\": {\n    \"rationale\": \"The JSON response captures the essence of the Coauthorship Science model requirement with details such as novel pruning, dynamic spectralHGNN, domain-specific feature extraction, and subfield precision. However, specific accuracy targets are not provided, so they are left as null. The confidence score is based on the clarity of the task but承认s the lack of some details.\",\n    \"score\": 0.9\n  }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for a high-performance artificial intelligence research team, your mandate is to develop an innovative spatio-temporal forecasting model tailored for the ETTm2 dataset from the cutting-edge smart city sensor network. The data, scattered across three separate and encrypted partitions (archived, live, and confidential), contains intricate readings from 7 diverse environmental sensors, each with unique timestamps (INPUT_SEQ_LEN=144, reflecting hourly data for a year). The algorithm design must accommodate variable input lengths and handle missing data. The task is to architect a convolutional neural network (CNN) architecture, recurrent neural network (RNN) fusion, and attention mechanisms, optimizing for both root mean squared logarithmic error (RMSLE) and mean absolute percentage error (MAPE), ensuring robustness against seasonality and non-linear patterns in the data. The model's performance must surpass industry benchmarks by at least 15% in forecasting 144-step ahead predictions (PRED_SEQ_LEN=144, PRED_DIM=7), with a particular focus on real-time latency and resource efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart city\",\n        \"description\": \"Develop an innovative spatio-temporal forecasting model for the ETTm2 dataset from a smart city sensor network. The model should utilize a CNN, RNN fusion, and attention mechanisms, accommodating variable input lengths, handling missing data, and addressing seasonality and non-linear patterns. Target performance metrics are RMSLE and MAPE, with a focus on beating industry benchmarks by at least 15% for 144-step ahead predictions (PRED_SEQ_LEN=144, PRED_DIM=7). The model should prioritize real-time latency and resource efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSLE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": \"real-time\",\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": null,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"input_size\": {\n                    \"INPUT_SEQ_LEN\": 144\n                },\n                \"data_distribution\": {\n                    \"partitions\": [\"archived\", \"live\", \"confidential\"]\n                },\n                \"sensor_types\": \"7 diverse environmental sensors\",\n                \"timestamp_type\": \"hourly\",\n                \"time_series_length\": \"1 year\"\n            },\n            \"description\": \"A smart city sensor network dataset with hourly data from 7 sensors across three encrypted partitions.\",\n            \"preprocessing\": [\n                \"data cleaning, handling missing values\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"convolutional neural network (CNN) + recurrent neural network (RNN) fusion + attention mechanisms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"handles_variable_lengths\": true,\n                    \"deals_with_missing_data\": true\n                },\n                \"target_dimensions\": {\n                    \"PRED_SEQ_LEN\": 144,\n                    \"PRED_DIM\": 7\n                },\n                \"optimization_factors\": [\"seasonality\", \"non-linear patterns\"]\n            },\n            \"description\": \"A spatio-temporal forecasting model tailored for the ETTm2 dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Targeting industry benchmark improvement by 15% for forecasting predictions\",\n        \"Focus on real-time latency and resource efficiency\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"description of how to demonstrate forecasting accuracy and performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is parsed based on the schema, reflecting the user's intent to build a model, high expertise level, and the time-series analysis problem. The description accurately captures the task of creating a CNN-RNN fusion model with attention, handling variable lengths, and addressing specific data challenges. Performance metrics are left unspecified, as exact values would require model training. The dataset description is based on the given details about ETTm2 and the encryption nature. The confidence score is high due to the clear mapping of the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As the project manager, mandate the team to assess not only the viability but also the scalability and transfer learning potential of state-of-the-art deep learning models in handling ultra-high-resolution Sentinel-2 data. Emphasize on optimizing algorithms for complex tasks like differentiating intricate land features (e.g., oases, phytoplankton blooms, and architecturally diverse urban zones) with minimal computational footprint, while maintaining an accuracy of at least 95% within a strict 1-hour SLA for real-time, high-precision mapping in dynamic environments. Additionally, explore possible fusion with other data sources for enhanced accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"remote sensing\",\n        \"description\": \"Evaluate the viability, scalability, and transfer learning potential of state-of-the-art deep learning models for analyzing ultra-high-resolution Sentinel-2 data. Focus on tasks to differentiate complex land features like oases, phytoplankton blooms, and diverse urban zones with minimal computational footprint and maintaining accuracy of at least 95%. Ensure real-time, high-precision mapping within a 1-hour SLA for dynamic environments. Consider fusion with other data sources for improved accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"computational footprint\",\n                \"value\": null,\n                \"unit\": \"GPU FLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2 Ultra-High-Resolution Data\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": { \"width\": \"ultra-high\", \"height\": \"ultra-high\" },\n                \"bands\": [\"R\", \"G\", \"B\", \"NIR\", \"SWIR1\", \"SWIR2\"]\n            },\n            \"description\": \"High-resolution satellite imagery from Sentinel-2, focusing on land features for complex tasks.\",\n            \"preprocessing\": [\n                \"data normalization\",\n                \"image resizing\"\n            ],\n            \"augmentation\": [\"geospatial transformations\", \"lighting adjustments\"],\n            \"visualization\": [\n                \"false-color composites\",\n                \"example image segmentation maps\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization\": \"efficient algorithms for complex tasks\",\n                \"transfer_learning\": \"potentially enabled\"\n            },\n            \"description\": \"Deep learning models designed for high-resolution Sentinel-2 image analysis, targeting accuracy and computational efficiency in differentiating intricate land features.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The need to handle complex land features and dynamic environments calls for advanced deep learning techniques.\",\n        \"1-hour SLA requires efficient and real-time processing\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"geospatial feature extraction\",\n            \"multi-source data fusion\"\n        ],\n        \"target_device\": \"cloud-based or edge computing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated backends\"],\n        \"demonstration\": \"live, real-time predictions and accuracy assessments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the schema, with the user's intent as 'build', high expertise, and task centered on evaluating state-of-the-art models for Sentinel-2 data analysis. It includes specific metrics like accuracy, SLA, and computational footprint. Other details like preprocessing, augmentation, and the need for fusion with other data sources are also incorporated. However, actual model specifications and performance targets are subject to be defined by the team based on comprehensive evaluation.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "text classification", "instruction": "As a project manager for the AI research division, demand the design of a highly sophisticated sentiment analysis system using the extensive IMDB corpus. Mandate the team to apply advanced text preprocessing techniques, extract multi-level features, and fine-tune a state-of-the-art MAML-Transformer. Strive for a consistent 95%+ Macro-F1, ensuring interpretability with not only AUC-ROC and PR curves but also SHAP plots and explainability metrics. Emphasize on overcoming class imbalance with SMOTE and iteratively optimize hyperparameters via Bayesian search to achieve optimal, real-time performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"media analysis\",\n        \"description\": \"Design a highly sophisticated sentiment analysis system using the IMDB corpus. Apply advanced text preprocessing, extract multi-level features, and fine-tune a MAML-Transformer. Aim for a Macro-F1 score of 95% or higher, along with interpretability through AUC-ROC, PR curves, SHAP plots, and explainability metrics. Address class imbalance with SMOTE and optimize hyperparameters using Bayesian search for real-time performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Macro-F1\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"IMDB corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"specific corpus\": \"IMDB\"\n            },\n            \"description\": \"A large and diverse text corpus for sentiment analysis.\",\n            \"preprocessing\": [\n                \"Advanced text preprocessing techniques\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"SHAP plots\",\n                \"AUC-ROC curves\",\n                \"Precision-Recall curves\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"MAML-Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced_features\": [\"multi-level\"],\n                \"fine-tuning\": \"true\",\n                \"pre-trained_model\": \"MAML-Transformer\"\n            },\n            \"description\": \"A state-of-the-art transformer model with Meta-Learning (MAML) capabilities for sentiment analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced text preprocessing, multi-level feature extraction, and fine-tuning for optimal performance\",\n        \"Using SMOTE for class imbalance mitigation\",\n        \"Bayesian hyperparameter optimization for real-time performance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"advanced text preprocessing techniques\",\n            \"feature extraction for multi-level analysis\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Bayesian search\"\n        ],\n        \"demonstration\": \"Interpretability with SHAP plots and model explainability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has accurately captured the key elements from the instruction. The user's intent to build a sentiment analysis system, the team's requirements for advanced preprocessing, MAML-Transformer fine-tuning, and the performance metrics including interpretability are all present. However, the specific model name and pre-defined hyperparameter values were not specified, which can be assumed as part of the implementation or left open for the team's discretion. The confidence score is set to 0.95 reflecting a high confidence that the provided information is relevant and based on the given instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As the project manager for the \"Galactic Quantum Query Accelerator\" (GQQA) project, guide the cross-domain team to develop a specialized BERTinception model, utilizing Hugging Face's tailored RoBERTaX Enhanced for Astrophysics 2.0. The model must undergo precision-focused training on arXiv's ultra-specific High-Energy Astrophysics and Cosmology Q&A dataset, targeting an impeccable F1 score of 99% with sub-millisecond response times (<1ms), ensuring seamless and efficient querying for cutting-edge celestial studies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Develop a specialized BERTinception model, using Hugging Face's RoBERTaX Enhanced for Astrophysics 2.0, for the Galactic Quantum Query Accelerator (GQQA) project. The model should be precision-focused and trained on arXiv's High-Energy Astrophysics and Cosmology Q&A dataset, with a target F1 score of 99% and sub-millisecond response times (<1ms).\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time (inference time)\",\n                \"value\": 1,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv High-Energy Astrophysics and Cosmology Q&A\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"ultra-specific\",\n                \"domain\": \"High-Energy Astrophysics and Cosmology\"\n            },\n            \"description\": \"A precision-focused dataset for training the BERTinception model on Q&A tasks related to astrophysics and cosmology.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERTinception (RoBERTaX Enhanced for Astrophysics 2.0)\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERTinception\",\n                \"pretrained_model\": \"RoBERTaX\",\n                \"enhanced_version\": \"2.0\",\n                \"training_algorithm\": \"precision-focused\"\n            },\n            \"description\": \"A specialized QA model for the Galactic Quantum Query Accelerator project, targeting High-Energy Astrophysics and Cosmology domain.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Precision-oriented training and ultra-specific dataset are crucial for achieving high performance in celestial studies.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom query service\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"seamless and efficient querying for advanced celestial studies\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key aspects of the user's instruction, including the task (model development), dataset (High-Energy Astrophysics and Cosmology Q&A), performance metric (F1 score), and the technical specifications (BERTinception, RoBERTaX Enhanced, and response time). The model architecture, expert level, and application domain are also derived directly from the instruction. However, it assumes that the dataset link, preprocessing steps, and deployment details are to be provided by the user.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Elaborate on the in-depth analysis of the Enron Email dataset by designing a novel RGCN architecture with a three-tiered node classification system, specifically focusing on the interplay between HLFX (Hierarchical Latent Feature Extraction) and PCG (Progressive Community Graph Condensation). The task demands identifying subnetworks of elite communicators, accounting for intricate temporal patterns, logarithmic volume fluctuations, and ultra-high throughput processing of data exceeding 1,000,000 nodes. Implement a hybrid model combining LIME for model explainability and incremental learning to handle dynamic data growth, ensuring state-of-the-art performance and efficiency in real-world, large-scale enterprise scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"enterprise\",\n        \"description\": \"Design a novel RGCN architecture with a three-tiered node classification system, focusing on the interplay between HLFX (Hierarchical Latent Feature Extraction) and PCG (Progressive Community Graph Condensation) for the Enron Email dataset. The objective is to identify subnetworks of elite communicators, account for temporal patterns, volume fluctuations, and handle ultra-high throughput processing of data over 1,000,000 nodes. Implement a hybrid model combining LIME for model explainability and incremental learning for dynamic data growth in large-scale enterprise scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"state-of-the-art performance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email Dataset\",\n            \"modality\": [\"text\", \"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"nodes\": {\n                    \"count\": 1000000,\n                    \"attributes\": [\n                        \"text\",\n                        \"timestamp\"\n                    ]\n                },\n                \"temporal_pattern\": \"logarithmic volume fluctuations\"\n            },\n            \"description\": \"Enron email dataset with intricate temporal patterns and ultra-high node volume.\",\n            \"preprocessing\": [\"temporal feature extraction\", \"data anonymization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"network topology analysis\", \"temporal pattern visualization\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Novel RGCN Architecture\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"three-tiered node classification using HLFX and PCG\",\n                \"explainability\": \"LIME integration\",\n                \"incremental_learning\": true\n            },\n            \"description\": \"A hybrid RGCN model designed for elite communicator subnetwork analysis in the Enron email dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilizing HLFX for hierarchical feature extraction and PCG for progressive community identification is crucial for the Enron dataset.\",\n        \"Accounting for logarithmic volume fluctuations and large-scale data requires efficient processing techniques.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"LIME for model explainability\",\n            \"Incremental learning for dynamic data growth\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom enterprise-grade server\",\n        \"inference_engine\": [\"efficient graph processing libraries\"],\n        \"demonstration\": \"A demonstration showcasing the model's performance on identifying elite communicators\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the user's intent to build a machine learning model, the high level of expertise, and the specific problem domain. The Enron Email dataset and the novel RGCN architecture, along with their requirements, are clearly specified. The performance metrics, complexity metrics, and other service-related requirements are derived from the task description. However, since the state-of-the-art performance and efficiency targets are not provided in the instruction, they are left as null values.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "multimodal forgery detection", "instruction": "For a cutting-edge machine learning project, propose the implementation of a novel fusion architecture that combines transformer models with contrastive learning for real-time detection and localization of ultra-high-definition forged videos in the challenging 'Celeb-DF-HDR' dataset, which encompasses extreme lighting conditions and complex video manipulations. Detail the necessary preprocessing techniques, optimization strategies, and propose a novel metric to measure performance based on forgery type-specific precision and temporal coherence, ensuring state-of-the-art accuracy with a minimum False Positive Rate below 1%.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"cybersecurity\",\n        \"description\": \"Propose a novel fusion architecture combining transformer models with contrastive learning for real-time detection and localization of forged ultra-high-definition videos in the Celeb-DF-HDR dataset, considering extreme lighting conditions and complex manipulations. The performance must be measured by forgery type-specific precision and temporal coherence with a False Positive Rate below 1%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"forgery type-specific precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"temporal coherence\",\n                \"value\": null\n            },\n            {\n                \"name\": \"False Positive Rate\",\n                \"value\": 0.01\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF-HDR\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"resolution\": \"ultra-high-definition\",\n                \"extreme_conditions\": [\"extreme lighting\", \"complex manipulations\"],\n                \"temporal_complexity\": \"real-time\"\n            },\n            \"description\": \"Challenging multimodal dataset with ultra-high-definition forged videos under extreme lighting conditions and complex manipulations.\",\n            \"preprocessing\": [\n                \"denoising techniques\",\n                \"alignment for temporal coherence\",\n                \"adaptation to transformer input format\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-Contrastive Fusion Architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"novel fusion technique\",\n                \"optimization\": \"state-of-the-art optimization strategies\",\n                \"real-time_performance\": true\n            },\n            \"description\": \"A fusion model that combines transformer models and contrastive learning for forgery detection and localization in high-definition videos\"\n        }\n    ],\n    \"knowledge\": [\n        \"The Celeb-DF-HDR dataset's complexity necessitates a sophisticated fusion model like the Transformer-Contrastive learning architecture.\",\n        \"Optimization strategies must consider the real-time constraints and performance in varying lighting conditions.\",\n        \"A custom performance metric is needed to account for forgery type-specific precision and temporal coherence.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"extracting relevant multimodal features\"],\n        \"target_device\": \"cloud or GPU-rich environment\",\n        \"deployment_endpoint\": \"potentially a secure or specialized forgery detection service\",\n        \"inference_engine\": [\"real-time inference using optimized model\"],\n        \"demonstration\": \"live demos with forged vs genuine videos\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the proposed machine learning project, with a focus on the fusion architecture, dataset characteristics, and performance metrics specified in the instruction. The user's intent to build a novel model and their high level of expertise are reflected. However, the specific model architecture, preprocessing steps, and performance metric values would need to be filled in with more detailed research and experimentation.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "node classification", "instruction": "Reformulate the Enron Email dataset analysis for node classification in a high-dimensional, temporal context, incorporating both RGCN and Deepwalk algorithms. Enhance accuracy by integrating attention mechanisms in Latent Feature Embeddings, fine-tuning for hierarchical multi-scale graph pooling, and ensuring robustness against class imbalance. Mandate not only employee communication pattern differentiation but also the detection of sub-network trends, with a focus on interpretability, while maintaining near real-time performance even in large graphs with at least 50,000 nodes and diverse message frequency distributions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"business analysis\",\n        \"description\": \"Reformulate the Enron Email dataset analysis for node classification in a high-dimensional, temporal context using RGCN and Deepwalk. Incorporate attention mechanisms in Latent Feature Embeddings, fine-tune for hierarchical multi-scale graph pooling, and handle class imbalance for improved accuracy. The focus is on detecting sub-network trends, interpretability, and maintaining near real-time performance in large graphs with at least 50,000 nodes and diverse message frequency distributions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"near real-time\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"unit\": \"MB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email Dataset\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"node_count\": {\n                    \"min\": 50000,\n                    \"max\": 50000\n                },\n                \"message_frequency_distribution\": \"diverse\"\n            },\n            \"description\": \"A high-dimensional, temporal, and large-scale dataset for node classification, encompassing the Enron email network with diverse communication patterns.\",\n            \"preprocessing\": [\"Deepwalk for node representation\", \"RGCN for graph learning\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"exploring sub-network trends\",\n                \"interpreting attention mechanisms\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RGCN with Attention-Integrated Latent Feature Embeddings\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"graph_pooling\": \"hierarchical multi-scale\",\n                \"attention_mechanism\": \"incorporated\",\n                \"class_imbalance_handling\": true\n            },\n            \"description\": \"A graph-based machine learning model using RGCN and Deepwalk, with attention mechanisms and adaptations for class imbalance and hierarchical graph pooling for improved node classification on the Enron Email dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The Enron dataset presents unique challenges with high dimensionality and diverse message frequency\",\n        \"Attention mechanisms enhance node classification performance by focusing on relevant features\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"hierarchical feature extraction\", \"temporal feature processing\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"visual explanations for detected sub-network trends\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's instruction, with a focus on the specified dataset, model, and requirements such as high-dimensional and temporal analysis, RGCN, Deepwalk, attention mechanisms, interpretability, class imbalance, and large-scale graph handling. However, performance metrics like accuracy and specific time frame are not provided, leaving them to be inferred or set to null as in the provided examples.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a high-demand project manager, your objective is to develop a cutting-edge text classification model for Textual Entailment, requiring not only state-of-the-art performance in identifying neutral (0), contradiction (1), and entailment (2) but also adaptability to intricate dialects, sarcasm, and domain-specific language. Mandate includes sequential training with attention mechanism, transfer learning from multiple domain-specific corpora, and rigorous evaluation using explainable AI techniques. Incorporate real-time performance tracking, model explainability metrics, and periodic re-calibration to maximize efficiency and foster continuous innovation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"NLP and AI\",\n        \"description\": \"Develop a cutting-edge text classification model for Textual Entailment with state-of-the-art performance, targeting neutral (0), contradiction (1), and entailment (2). The model should be adaptable to intricate dialects, sarcasm, and domain-specific language. Requirements include sequential training with attention mechanism, transfer learning from multiple domain-specific corpora, and rigorous evaluation using explainable AI techniques. Real-time performance tracking, model explainability metrics, and periodic re-calibration for continuous improvement are mandatory.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score (weighted)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explanation fidelity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time inference speed\",\n                \"value\": null,\n                \"unit\": \"ms per example\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"neutral\", \"contradiction\", \"entailment\"],\n            \"specification\": {\n                \"sequential_format\": true,\n                \"domain_specific Corpora\": true\n            },\n            \"description\": \"Textual data for Textual Entailment task, including domain-specific and dialect-inclusive instances.\",\n            \"preprocessing\": [\"tokenizer\", \"sentence segmentation\", \"domain adaptation\"],\n            \"augmentation\": [\"sarcasm injection\", \"dialectal variation\"],\n            \"visualization\": [\"confusion matrix\", \"attention heatmaps\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"state-of-the-art Textual Entailment Model\",\n            \"family\": \"Transformer with Attention Mechanism\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_details\": [\"attention mechanism\", \"sequential training\"],\n                \"transfer_learning_corpora\": [\"domain-specific corpora\"],\n                \"model explanation Techniques\": [\"explainable AI\"]\n            },\n            \"description\": \"A neural network model designed for Textual Entailment with multi-domain adaptability, attention-based sequential training, and transfer learning capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating explainable AI for model interpretations and transparency is critical.\",\n        \"Sequential training and domain-specific corpora transfer are key for complex language understanding.\",\n        \"Real-time performance monitoring and periodic re-calibration ensure ongoing optimization.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"sequence encoding\", \"feature extraction\"],\n        \"target_device\": \"cloud for large data processing and explainability\",\n        \"deployment_endpoint\": \"model serving API\",\n        \"inference_engine\": [\"TensorFlow serving\", \"model explainability tools\"],\n        \"demonstration\": \"interactive web-based interface for model predictions and explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given user's objective for a text classification project with specific requirements. High-level details, such as the model's adaptation to complex language and explainability, are covered. However, missing specific performance metric targets and hardware requirements lead to null values. The confidence score is high based on the clear instruction but with a note on the need for more specific details.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "As the creative director of a cutting-edge AI startup in the nanotechnology domain, design and implement a hybrid fusion model that combines Hugging Face's ALBERT and Graph Neural Networks (GNN) to develop a domain-specific knowledge base for materials science. This system must demonstrate unparalleled performance, achieving an exact match accuracy of 99% on the specialized NANO-QA dataset, and guarantee sub-50 milliseconds response times for materials engineers while operating on resource-constrained edge devices. In this project, delve into innovative multi-modal learning, reinforcement learning-based pruning, and federated learning strategies to push the envelope of accuracy while maintaining real-world efficiency, with the aim to outperform MIT's best-reported benchmark by 5%. Additionally, consider privacy-preserving methods to comply with the European Union's General Data Protection Regulation (GDPR) guidelines.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"nanotechnology\",\n        \"description\": \"Design and implement a hybrid fusion model using Hugging Face's ALBERT and Graph Neural Networks for a materials science domain-specific knowledge base, targeting exact match accuracy of 99% on the NANO-QA dataset and sub-50ms response times on edge devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"exact match accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NANO-QA\",\n            \"modality\": [\"text\", \"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": \"application-specific\",\n                \"modalities\": [\"structured data from materials science literature\", \"chemical compound graphs\"]\n            },\n            \"description\": \"A specialized QA dataset for materials science, containing both text and graph data.\",\n            \"preprocessing\": [\"multi-modal data preprocessing\", \"node feature extraction from GNN\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Fusion Model (ALBERT + GNN)\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"model architecture\": \"ALBERT with GNN-based fusion component\",\n                \"federated learning\": {\n                    \"strategies\": [\"reinforcement learning pruning\", \"federated learning\"],\n                    \"performance improvement\": \"5% over MIT benchmark\"\n                }\n            },\n            \"description\": \"A cutting-edge model that combines ALBERT's language understanding with GNN's graph representation for materials science.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Innovative multi-modal learning for enhanced knowledge base performance\",\n        \"Reinforcement learning-based pruning for efficient model size\",\n        \"Federated learning to preserve privacy while maintaining accuracy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"federated feature extraction\", \"preprocessing for edge devices\"],\n        \"target_device\": \"edge device\",\n        \"deployment_endpoint\": \"privacy-preserving\",\n        \"inference_engine\": [\"GPU-optimized\", \"real-time\"],\n        \"demonstration\": \"Europe-friendly compliance with GDPR guidelines\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the provided instruction, focusing on the user's intent to build a model, their high expertise level, and the specific problem requirements in materials science and performance targets. The model architecture, performance metrics, and AI techniques like multi-modal learning, reinforcement learning, and federated learning are all incorporated. The dataset details, preprocessing, and privacy aspects are also represented. The confidence score is high due to the direct mapping of the instruction to the JSON schema.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a seasoned project manager for a trailblazing tech conglomerate, devise a cutting-edge translation app that bridges the Quechua-Spanish linguistic divide with exceptional versatility. Utilize the limited JW300 dataset, but demand the integration of an innovative CNN-Transformer fusion, ensuring cross-platform adaptability and real-time adaptation to evolving dialects. Mandate a stringent accuracy standard of 95% for colloquial expressions, aiming for a BLEU score of 45+ even in resource-constrained environments. Furthermore, optimize the model's energy efficiency for sustainable IoT deployments in remote regions, taking into account battery life and environmental footprint.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"technology\",\n        \"description\": \"Design a cutting-edge translation app that specifically bridges the Quechua-Spanish language gap, incorporating a CNN-Transformer fusion for cross-platform adaptability and real-time dialect evolution handling. The app must achieve a minimum colloquial expression accuracy of 95% and a BLEU score of 45+ in resource-constrained environments. Optimization for energy efficiency is crucial for sustainable IoT deployments in remote regions, considering battery life and environmental impact.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy (colloquial expressions)\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 45\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null, \n                \"unit\": \"optimized for IoT environments\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"JW300\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"size\": \"limited\",\n                \"language pair\": \"Quechua-Spanish\"\n            },\n            \"description\": \"A limited dataset for the task of machine translation, particularly focusing on the Quechua-Spanish language pair.\",\n            \"preprocessing\": [\"CNN-Transformer fusion integration\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"CNN-Transformer Fusion Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural machine translation\",\n            \"specification\": {\n                \"architecture\": \"CNN-Transformer fusion\",\n                \"platform adaptability\": \"cross-platform\",\n                \"real-time dialect adaptation\": \"yes\"\n            },\n            \"description\": \"A state-of-the-art machine translation model combining CNN and Transformer architectures, designed for high accuracy in Quechua-Spanish translation and optimized for IoT scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Efficient energy consumption critical for remote IoT deployments\",\n        \"Integration of a CNN-Transformer fusion is important for translation versatility\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"IoT devices in remote regions\",\n        \"deployment_endpoint\": \"undefined\",\n        \"inference_engine\": [\"optimized for sustainable IoT environments\"],\n        \"demonstration\": \"Real-time, dialect-adaptive translation in resource-constrained environments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response has been parsed based on the given instruction. The areas of natural language processing and machine translation are identified, along with the specific requirement of the Quechua-Spanish language pair and the CNN-Transformer fusion. The stringent accuracy and BLEU score are included as performance metrics, with a mention of energy efficiency optimization. The user's expertise level is assumed to be high due to their role as a seasoned project manager in a tech conglomerate. However, since some specific details like energy efficiency metrics and exact model optimization methods were not explicitly mentioned, they are left as null, but marked as needing optimization for IoT environments.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Create a highly adaptable RGCN architecture specifically tailored for the extensive Enron Email dataset. Integrate homomorphic encryption for multi-level confidentiality, ensuring cross-communication among encrypted job roles in a nested taxonomy. The model must undergo iterative design optimization, targeting precision above 99% for executive, engineering, and support roles, while employing hierarchical clustering and sophisticated feature extraction. Mandate real-time, low-latency (<5ms) analysis using_tensorpipe_ with fault tolerance up to N+2 in a resilient, microservices-based Kubernetes cluster. Implement auto-scaling and load balancing algorithms that can handle exabytes of growing data with sublinear performance degradation, ensuring continuous and efficient operation for expanding datasets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"business intelligence\",\n        \"description\": \"Create a highly adaptable RGCN architecture for the Enron Email dataset with homomorphic encryption for multi-level confidentiality. The model must optimize for precision above 99% in executive, engineering, and support roles, incorporate hierarchical clustering, advanced feature extraction, and provide real-time analysis with low latency (<5ms) via TensorPipe. It should be resilient with fault tolerance up to N+2 in a Kubernetes microservices cluster, supporting auto-scaling and load balancing for massive, growing datasets with sublinear performance degradation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99,\n                \"role_types\": [\"executive\", \"engineering\", \"support\"]\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 5,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"fault tolerance\",\n                \"value\": \"N+2\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email Dataset\",\n            \"modality\": [\"text\", \"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"exabytes\",\n                \"taxonomy\": \"nested\"\n            },\n            \"description\": \"A large-scale email dataset reflecting the Enron company's communication network, with a hierarchical taxonomy.\",\n            \"preprocessing\": [\"homomorphic encryption\", \"graph creation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Adaptable RGCN\",\n            \"family\": \"Relational Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization_steps\": \"iterative\",\n                \"feature_extraction_method\": \"sophisticated\"\n            },\n            \"description\": \"A custom RGCN designed specifically for the Enron email dataset, integrating homomorphic encryption and hierarchical clustering for confidentiality and performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Homomorphic encryption is critical for maintaining confidentiality in a nested taxonomy.\",\n        \"Real-time analysis with low-latency and fault tolerance is essential for business intelligence.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"hierarchical clustering\", \"encoding for encrypted data\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"TensorPipe-based\",\n        \"inference_engines\": [\"TensorPipe\"],\n        \"demonstration\": \"Real-time, scalable, and resilient analysis for Enron dataset\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key aspects of the user's instruction, including the model type, dataset requirements, performance metrics, and operational aspects such as deployment and clustering. The only missing information may be the exact technical details for feature extraction and homomorphic encryption methods used, as well as the exact load balancing and auto-scaling algorithms. These can be challenging to deduced from the instruction but are vital for a comprehensive response.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for a pioneering AI-driven research team, your new objective is to conceive a highly advanced and intricate machine learning system. This architecture should seamlessly integrate a groundbreaking hybrid ConvLSTM-Transformer model, specifically designed for pinpoint accuracy in forecasting hourly urban climate patterns over a demanding 7-day horizon (192-hour stretch). The UrbanWeather v2.1 dataset, now enriched with 25 intricate meteorological variables, necessitates an expansion of INPUT_SEQ_LEN to 112 for enhanced data analysis. You'll be optimizing the model's performance using two innovative evaluation metrics: MSLE+ with tailored penalties for exceptional weather events, and a custom MAPE-P variant that focuses on percentile error bands across diverse weather conditions.\n\nTo ensure top-notch generalization and adaptability across various microclimates, devise an advanced early stopping strategy incorporating auto-adaptive validation frequency, and incorporate nested rolling window cross-validation. To maintain the model's relevancy in a dynamic environment, outline a real-time adaptation plan that harnesses transfer learning and constant monitoring of external climate influencers, considering real-world data streams and seasonal fluctuations. Your blueprint should also address potential data drift and incorporate seamless integration with existing infrastructure for rapid deployment and updates.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate research\",\n        \"description\": \"Conceive an advanced machine learning system with a hybrid ConvLSTM-Transformer model for hourly urban climate pattern forecasting over a 7-day horizon. The system should incorporate the UrbanWeather v2.1 dataset with an expanded INPUT_SEQ_LEN, optimize performance using MSLE+ and custom MAPE-P, include advanced early stopping with auto-adaptive validation, nested rolling window cross-validation, and real-time adaptation with transfer learning and monitoring external influencers.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSLE+\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE-P\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UrbanWeather v2.1\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly climate patterns\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 112,\n                \"meteorological_variables\": 25\n            },\n            \"description\": \"Enriched dataset for time-series forecasting with 25 meteorological variables and extended INPUT_SEQ_LEN for better analysis.\",\n            \"preprocessing\": [\"expanding INPUT_SEQ_LEN\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid ConvLSTM-Transformer\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"ConvLSTM-Transformer\",\n                \"forecast_horizon\": 192,\n                \"data_analysis_window\": 112\n            },\n            \"description\": \"A cutting-edge model architecture for forecasting hourly urban climate patterns over a 7-day period.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integration of ConvLSTM and Transformer, MSLE+ with event penalties, and MAPE-P for diverse weather conditions\",\n        \"Advanced early stopping strategy with auto-adaptive validation frequency\",\n        \"Nested rolling window cross-validation for better generalization\",\n        \"Real-time adaptation through transfer learning and monitoring external influencers\",\n        \"Data drift management and infrastructure integration for rapid deployment and updates\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"early stopping strategy\",\n            \"auto-adaptive validation frequency\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"plan\": \"real-time monitoring and updates based on external data streams\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the instruction, specifying the high-level objectives and requirements for a complex machine learning project. However, specific metric values are not provided, leaving them as null, as the instruction does not specify these details. The confidence score takes into account the complexity and specificity of the project while acknowledging the need for further information.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager, you're tasked with a complex, multi-phased machine learning project for next-generation time-series forecasting. The ILI dataset, now expanded with external dataset integration and irregular data handling, consists of a massive, tiered training set (85%) split into intricate validation (12%) and testing (13%) subsets. Each sequence contains historical observations (INPUT_SEQ_LEN=48, featuring 15-dimensional time-sensitive indicators) to analyze. Your team must design a highly-advanced, scalable architecture integrating LSTMs, Prophet, and TBATS models, along with implementing novel feature selection methods.\n\nTo boost predictive prowess, perform a comprehensive pipeline that involves deseasonalization, trend analysis, and holiday effects extraction using domain expertise. Employ Bayesian optimization instead of GridSearchCV to fine-tune hyperparameters, focusing on minimizing both root mean squared error (RMSE) and mean absolute percentage error (MAPE) for a robust evaluation. Provide detailed interpretability reports, showcasing the contribution of each component and the explainability of the forecasting process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"business forecasting\",\n        \"description\": \"A complex, multi-phased machine learning project for next-generation time-series forecasting using the ILI dataset with expanded external data integration and handling of irregular data. The dataset is divided into a 85% training, 12% validation, and 13% testing subsets. The project involves designing a scalable architecture integrating LSTMs, Prophet, and TBATS models, with novel feature selection methods, deseasonalization, trend analysis, and holiday effects extraction using domain expertise. Bayesian optimization for hyperparameter tuning is a must, targeting RMSE and MAPE minimization with interpretability reports.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 48,\n                \"dimensions\": {\n                    \"time-sensitive indicators\": 15\n                }\n            },\n            \"description\": \"A large, tiered dataset with external integration, designed for time-series forecasting. It contains irregular data with a 85% training, 12% validation, and 13% testing split.\",\n            \"preprocessing\": [\"deseasonalization\", \"trend analysis\", \"holiday effects extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"advanced architecture (LSTMs, Prophet, TBATS)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"models\": [\"LSTM\", \"Prophet\", \"TBATS\"],\n                \"feature_selection\": \"novel methods\"\n            },\n            \"description\": \"A scalable architecture that combines LSTMs, Prophet, and TBATS models with novel feature selection techniques for enhanced time-series forecasting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Bayesian optimization for hyperparameter tuning and domain-specific feature analysis is key.\",\n        \"Interpretability reports should demonstrate each component's contribution and the forecasting process's explainability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"deseasonalization\", \"trend analysis\", \"holiday effects extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"requirement for interpretability reports\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the complex nature of the project and the specific requirements, such as the inclusion of various models, feature selection methods, and optimization techniques. The metric values are not provided as numerical targets but expected to be minimized, given the high-performance focus. Missing elements like exact model names and specific feature selection methods are assumed to be left for the project team to determine based on the task description. Overall, the confidence score is high due to the clear mapping of the instruction to the schema.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "node classification", "instruction": "As a project manager for the \"Multi-Year High School Dynamic Contact Network Analysis,\" mandate the machine learning team to develop a highly sophisticated TGN architecture. This should involve the integration of advanced spatio-temporal attention mechanisms, able to handle diverse node features, dynamic linkages that change over time, and nested temporal structures. The model must excel in distinguishing significant figures, forecasting their evolving influence, and uncovering intricate social patterns throughout a minimum of five consecutive academic periods. Emphasize the necessity for data imputation techniques and parallel processing to ensure the system's resilience to data gaps and optimal performance with massive data sets, all while maintaining interpretability and runtime efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"social network analysis\",\n        \"description\": \"Develop a highly sophisticated TGN architecture for the 'Multi-Year High School Dynamic Contact Network Analysis' project. The model should integrate advanced spatio-temporal attention mechanisms, handle diverse node features, dynamic linkages, and nested temporal structures. The main objectives are to distinguish significant individuals, forecast their influence over time, and discover intricate social patterns across at least five academic periods. Ensure the model uses data imputation techniques and parallel processing for resilience to data gaps, optimal performance with large datasets, and maintains interpretability and runtime efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy for significant figure identification\",\n                \"value\": null\n            },\n            {\n                \"name\": \"forecasting precision for evolving influence\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score for uncovering social patterns\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"runtime efficiency\",\n                \"value\": null,\n                \"unit\": \"seconds per inference\"\n            },\n            {\n                \"name\": \"data imputation effectiveness\",\n                \"value\": null,\n                \"unit\": \"imputation accuracy\"\n            },\n            {\n                \"name\": \"parallel processing capacity\",\n                \"value\": null,\n                \"unit\": \"data points processed per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multi-Year High School Dynamic Contact Network Data\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"node_features\": {\n                    \"type\": \"object\",\n                    \"description\": \"Diverse node attributes\"\n                },\n                \"link_changes\": {\n                    \"type\": \"object\",\n                    \"description\": \"Dynamic link structures over time\"\n                },\n                \"temporalgranularity\": {\n                    \"type\": \"string\",\n                    \"description\": \"Minimum time period of the data (e.g., academic periods)\"\n                }\n            },\n            \"description\": \"Contains data on high school contact networks, including diverse node features and changing linkages over time, for multiple years.\",\n            \"preprocessing\": [\"data imputation techniques\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TGN Architecture\",\n            \"family\": \"Temporal Graph Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention_mechanisms\": {\n                    \"type\": \"object\",\n                    \"description\": \"Advanced spatio-temporal attention mechanisms details\"\n                },\n                \"node_feature_integration\": {\n                    \"type\": \"string\",\n                    \"description\": \"Approach for handling diverse node features\"\n                },\n                \"temporal_structure_handling\": {\n                    \"type\": \"string\",\n                    \"description\": \"Method for managing nested temporal structures\"\n                }\n            },\n            \"description\": \"A highly sophisticated TGN with spatio-temporal attention, handling diverse node features, dynamic linkages, and nested temporal structures.\"\n        }\n    ],\n    \"confidence\": {\n        \"rationale\": \"The response reflects the user's intent to build a high-level machine learning model based on the 'Multi-Year High School Dynamic Contact Network Analysis' project. The problem details have been translated into the JSON schema, including the requirements for advanced spatio-temporal attention mechanisms and multi-year analysis. Performance and complexity metrics are included with placeholder null values since specific targets are not provided. The user's high expertise level is assumed. The dataset details, like the need for data imputation and source, are based on the instruction. Confidence is high due to the clear instruction and mapping to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "Craft a high-performance, mobile-adaptable QA system using TinyBERT, tailored for rapid news comprehension. Demand an impressive 95% F1-score on the NewsQA benchmark, while striving for real-time responsiveness (<150 ms), cross-platform consistency, and efficient energy usage under low-resource constraints for diverse devices in the market. In addition, outline a detailed architecture diagram and provide a snippet of code demonstrating hardware-aware model optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"news and media\",\n        \"description\": \"Craft a high-performance, mobile-adaptable QA system using TinyBERT for rapid news comprehension. Target an F1-score of 95% on the NewsQA benchmark, ensuring real-time responsiveness (less than 150 ms), cross-platform consistency, and energy efficiency under low-resource constraints for diverse devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95,\n                \"unit\": \"%\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Response Time\",\n                \"value\": 0.15,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Energy Efficiency\",\n                \"value\": null,\n                \"unit\": \"Under low-resource constraints\"\n            },\n            {\n                \"name\": \"Hardware-awareness\",\n                \"value\": \"Efficient\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NewsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"benchmark_size\": null,\n                \"content_type\": \"news articles\"\n            },\n            \"description\": \"A dataset for evaluating QA systems on rapid news comprehension.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT\",\n            \"family\": \"Bert-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"mobile-friendly\",\n                \"optimization\": \"Hardware-aware\"\n            },\n            \"description\": \"A lightweight BERT variant specifically designed for faster inference on mobile devices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Mobile-adaptability requires optimizing for smaller hardware and lower computational power.\",\n        \"Low-resource constraints mean the model should be efficient with limited data and computation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Hardware-aware pruning\",\n            \"Quantization\"\n        ],\n        \"target_device\": \"Mobile\",\n        \"deployment_endpoint\": \"cross-platform\",\n        \"inference_engine\": [\n            \"On-device inference\",\n            \"Serverless\"\n        ],\n        \"demonstration\": \"A snippet of code with model optimization for diverse devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent (to build a high-performance QA system), their expertise level, and the problem requirements. The area (NLP), task (QA), and application domain (news and media) are derived from the instruction. Performance metrics (F1-score and response time) are specified, and the need for hardware-awareness and low-resource efficiency is emphasized. The TinyBERT model name, family, and specifications are included, as well as the necessary preprocessing, hardware-aware model optimization, and deployment targets. A note about the architecture and a reference to a snippet of code indicates that the response considers these aspects in detail.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Design a distributed and adaptive Graph Neural Network architecture specifically tailored for the Dynamic Knowledge Graph (DKG) in the OpenKE framework, using the Wikidata5m dataset as the basis. The model must exhibit exceptionalfew-shot learning capabilities, achieving an F1 score of above 95% for unseen entity combinations. Implement a custom online learning strategy that can handle continuous data updates with minimal performance degradation, and rigorously evaluate the model's efficiency usingleave-one-subgraph-out cross-validation. Furthermore, optimize resource allocation for the model on a cluster of heterogeneous nodes, ensuring parallel scalability and energy efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graph\",\n        \"description\": \"Design a distributed and adaptive Graph Neural Network for the Dynamic Knowledge Graph (DKG) using OpenKE framework with Wikidata5m dataset. Focus on few-shot learning (specifically, exceptional few-shot), target F1 score of >95% for unseen entity combinations. Implement a custom online learning strategy, conduct leave-one-subgraph-out cross-validation for evaluation, and optimize resource allocation for parallel scalability and energy efficiency on a cluster.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 Score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Resource Allocation Efficiency\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"Parallel Scalability\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"Energy Efficiency\",\n                \"value\": null,\n                \"unit\": \"Joules per inference\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wikidata5m\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dynamic\": true,\n                \"heterogeneous\": true\n            },\n            \"description\": \"Dynamic Knowledge Graph dataset sourced from Wikidata, used for designing the GNN model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Graph Neural Network (custom)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Distributed and Adaptive\",\n                \"few_shot_learning\": \"Exceptional few-shot\"\n            },\n            \"description\": \"A Graph Neural Network designed for dynamic knowledge graphs in the OpenKE framework, with emphasis on few-shot learning and online learning strategy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Few-shot learning is critical for handling unseen entity combinations.\",\n        \"Online learning to accommodate continuous data updates\",\n        \"Leave-one-subgraph-out cross-validation for rigorous evaluation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cluster\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Distributed\"],\n        \"demonstration\": \"Energy-efficient and scalable model deployment plan\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is generated based on the given instruction, capturing the user's intent, high expertise level, and the specifics of the graph machine learning project. The problem area, task, and dataset characteristics are detailed, along with performance and complexity metrics. The custom online learning strategy, resource optimization, and evaluation method are included. However, the exact specifications for resource allocation and performance metrics' target values are not provided as they would typically be project-specific or dependent on further exploration.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager, commission the ML development team to design a scalable, sector-focused financial sentiment analysis platform, utilizing BERT and CNN for Reuters-21578 news classification. Mandate a 95% top-tier accuracy, with a fine-grained threshold for aviation, tech, and energy sectors. Integrate an advanced noise filtering mechanism that not only differentiates factual breaking news from speculative rumors but also differentiates real-time events with precision within a stringent latency constraint of 1 millisecond. Require a multi-step evaluation plan comparing performance, not only to industry benchmarks, but also incorporating a sensitivity analysis for varying market conditions. Conduct periodic audits and propose continuous improvement strategies to maintain a leading edge in the competitive landscape.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"financial services\",\n        \"description\": \"Design a scalable financial sentiment analysis platform, targeting Reuters-21578 news with BERT and CNN, achieving 95% top-tier accuracy in aviation, tech, and energy sectors. The platform must differentiate factual news from rumors and real-time events with 1 millisecond latency. Include a multi-step evaluation plan versus industry benchmarks, with sensitivity to market conditions, and propose continuous improvement strategies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"top-tier accuracy\",\n                \"value\": 0.95,\n                \"sub_metrics\": {\n                    \"aviation\": null,\n                    \"tech\": null,\n                    \"energy\": null\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 1,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"news category\"],\n            \"specification\": null,\n            \"description\": \"A financial news dataset for sentiment analysis with a focus on BERT and CNN.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"BERT and CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"BERT layers\": null,\n                    \"CNN layers\": null,\n                    \"tail model architecture\": null\n                },\n                \"FLOPs\": null,\n                \"training speed\": null,\n                \"inference time\": null\n            },\n            \"description\": \"A sentiment analysis model combining BERT and CNN for financial news classification\"\n        }\n    ],\n    \"knowledge\": [\n        \"Key sectors: aviation, tech, and energy require fine-grained accuracy\",\n        \"Noise filtering mechanism for factual vs. speculative news and real-time events\",\n        \"1 ms latency constraint for real-time analysis\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": {\n            \"use cases\": [\"live market analysis\", \"periodic benchmark comparisons\", \"dynamic model updates\"]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the provided schema. The user's intent to build is clear, as well as their high level of expertise. The problem description and sub-metrics for performance have been translated from the instruction. The advanced metrics (fine-grained sector accuracy, latency) and platform requirements (evaluation plan and continuous improvement) are included. However, exact model architecture specifications and preprocessing steps are left undefined as they would depend on the team's choices for BERT and CNN implementation, which was not provided.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design an advanced RNN-Transformer hybrid neural model, specifically tailored for translating colloquial English to Russian idioms and slang using the extensive OPUS-Xtra corpus. Aim for a state-of-the-art BLEU score of 40, yet necessitate an impressive efficiency of 200 milliseconds per sentence, guaranteeing near-instantaneous and context-aware translations that heighten user experience in impromptu chat conversations without hindering interactivity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Design an advanced RNN-Transformer hybrid neural model tailored for translating colloquial English to Russian idioms and slang, using the OPUS-Xtra corpus. The model should achieve a state-of-the-art BLEU score of 40 and have an efficiency of 200 milliseconds per sentence.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 40\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time per sentence\",\n                \"value\": 0.2,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS-Xtra\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text (Russian)\"],\n            \"specification\": {\n                \"language_pair\": [\"English (colloquial) - Russian\"],\n                \"size\": \"Extensive corpus\",\n                \"nature\": \"Multilingual\"\n            },\n            \"description\": \"A large-scale corpus for training a colloquial English to Russian translation model.\",\n            \"preprocessing\": [\"tokenizer, lowercasing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-Transformer Hybrid\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"specifics\": \"Tailored for colloquial to idiomatic slang translation\",\n                \"efficiency\": \"State-of-the-art, near-instantaneous\"\n            },\n            \"description\": \"A deep learning model combining RNN and Transformer architectures for high-quality and fast colloquial English to Russian translations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider incorporating techniques for handling colloquial language and idiomatic expressions for effective translation.\",\n        \"Optimize for low-latency performance while maintaining BLEU score accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-awareness\"],\n        \"target_device\": \"cloud or high-performance server\",\n        \"deployment_endpoint\": \"potentially a translation API service\",\n        \"inference_engine\": [\"optimized libraries like TensorFlow or PyTorch\"],\n        \"demonstration\": \"Interactive chat with real-time translations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's request to build a machine learning model for translating colloquial English to Russian idioms and slang. The user's expertise, specific requirements for the model, target corpus, and performance metrics are accurately captured. However, the BLEU score and inference time are specific targets provided by the user. The missing context-awareness and optimization techniques used for achieving efficiency are placeholders, as the exact implementation details are not specified in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a seasoned AI project lead, your team is tasked with creating a state-of-the-art text sentiment analysis system using the diverse 20 Newsgroups dataset. Choose a model that exhibits high accuracy, interpretable feature interactions, and realtime adaptability. Moreover, devise an innovative, hardware-efficient parallelization technique, taking into account the scalability challenge and the impact on energy consumption for large-scale deployment in edge devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"N/A (implied to be AI or ML-driven sentiment analysis)\",\n        \"description\": \"Create a state-of-the-art text sentiment analysis system using the 20 Newsgroups dataset. The model should exhibit high accuracy, interpretable feature interactions, and real-time adaptability. Additionally, devise an innovative parallelization technique for hardware efficiency with scalability and energy consumption considerations for edge devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"realtime adaptability\",\n                \"value\": null,\n                \"unit\": \"time (e.g., seconds, milliseconds)\"\n            },\n            {\n                \"name\": \"energy consumption\",\n                \"value\": null,\n                \"unit\": \"energy units (e.g., joules)\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\"],\n            \"specification\": null,\n            \"description\": \"A diverse text dataset for sentiment analysis, containing multiple newsgroups categories.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"interpretable, state-of-the-art model (e.g., transformer-based with explainable features)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": {\n                    \"feature_interactions\": \"explainable\",\n                    \"model_interpretability_techniques\": [\"attention mechanisms\"]\n                }\n            },\n            \"description\": \"A model designed for text classification with high accuracy and interpretable feature interactions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The 20 Newsgroups dataset presents a challenge due to its diversity, necessitating a model adaptable to varying contexts.\",\n        \"Efficient parallelization for hardware-constrained edge devices calls for a low-resource, scalable solution.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"parallel processing\"\n        ],\n        \"demonstration\": \"real-time adaptability and energy consumption results for edge devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction, capturing the build intent, high expertise, and NLP task. The dataset and model requirements are derived from the text. Performance metrics (accuracy) and complexity (real-time adaptability, energy consumption) are specified. However, specific metric values are not provided since they depend on the model choice. The source of dataset is mentioned as 'direct search' given the prominence of the dataset. The requested parallelization technique for edge devices is noted. A confidence score is set to 0.95 based on the clear and specific request in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project director for a cross-border AI collaboration focused on indigenous languages, your task is to design a state-of-the-art polyglot AI translator adept at deciphering rare dialects such as Maasai Swahili. Harness the immense and nuanced Masakhane corpus, implementing an innovative fusion of cutting-edge Transformer-XL and Dual LSTM architectures with laser-focused domain adaptation for African language experts. The objective is not just to achieve a breathtaking 30% enhancement in specialized BLEU scores, but to elevate low-resource language comprehension by 15% while maintaining unparalleled fluency and cultural authenticity. Conduct a rigorous, double-blind benchmark comparison with rival models, including a detailed error diagnostics study and a discourse on the ethical nuances in preserving endangered linguistic diversity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-border AI, indigenous languages\",\n        \"description\": \"Design a state-of-the-art polyglot AI translator specifically for Maasai Swahili, focusing on deciphering rare dialects. Use the Masakhane corpus, blend Transformer-XL and Dual LSTM architectures, and incorporate domain adaptation for African language experts. Aim for a 30% improvement in specialized BLEU scores and a 15% increase in low-resource language comprehension. Maintain fluency and cultural authenticity, and conduct a rigorous benchmark comparison and error diagnostics study, considering ethical aspects of preserving endangered linguistic diversity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"specialized BLEU score\",\n                \"value\": 1.30\n            },\n            {\n                \"name\": \"low-resource language comprehension\",\n                \"value\": 1.15\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Masakhane corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"immense\",\n                \"narrow-focus\": \"Maasai Swahili dialects\"\n            },\n            \"description\": \"A comprehensive corpus designed for African indigenous language research, including rare Maasai Swahili dialects.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL & Dual LSTM fusion\",\n            \"family\": \"neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_fusion\": [\"Transformer-XL\", \"Dual LSTM\"],\n                \"domain_adaptation\": \"African language domain experts\",\n                \"innovations\": \"laser-focused and preserve cultural authenticity\"\n            },\n            \"description\": \"A cutting-edge translation model utilizing both Transformer-XL and Dual LSTM for robust Maasai Swahili translation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Emphasis on rare dialects and domain adaptation\",\n        \"Ethical considerations for endangered languages\",\n        \"Masakhane corpus' contribution to preserving linguistic diversity\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain adaptation techniques\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"double-blind benchmark comparison and error diagnostics study\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project, reflecting the design objectives, the use of the Masakhane corpus, and the technical aspects of the AI translator. The performance metrics, expert level of the user, and the need for benchmarking and ethical considerations are all included. However, specific FLOPs or training time are not detailed as they are not explicitly mentioned in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager, you are tasked with overseeing the creation of a groundbreaking Textual Inference System that pushes the boundaries of state-of-the-art technology. Your team must engineer a hybrid model architecture, integrating cutting-edge Transfer Learning, a novel auto-encoder for efficient information extraction, and a real-time context-aware reasoning module. The system must achieve an average AUC-ROC score of 0.98 or higher across five distinct, domain-specific datasets, including low-resource languages, while maintaining low-latency response times. In addition to optimizing for accuracy, demand a 50% reduction in model size for efficient deployment on IoT devices with limited computational resources. Emphasize the development of model interpretability techniques, including SHAP values and integrated model inspection interfaces, and rigorously implement continuous bias monitoring and countermeasures to ensure fairness and transparency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"state-of-the-art technology, specifically Textual Inference\",\n        \"description\": \"Create a groundbreaking Textual Inference System with a hybrid model architecture integrating Transfer Learning, a novel auto-encoder, and a real-time context-aware reasoning module. Target AUC-ROC score of 0.98 or higher across 5 diverse datasets, including low-resource languages. Ensure low-latency, 50% model size reduction for IoT deployment, and include model interpretability techniques (SHAP values, model inspection) along with continuous bias monitoring and fairness practices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average AUC-ROC score\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model size reduction\",\n                \"value\": 0.5,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"inference time latency\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"various (5 domain-specific datasets)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"labels for text classification\"],\n            \"specification\": {\n                \"low-resource languages\": true\n            },\n            \"description\": \"Five unique datasets, focusing on textual inference tasks with emphasis on low-resource languages.\",\n            \"preprocessing\": [\"Transfer Learning, auto-encoder for info extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Textual Inference System\",\n            \"family\": \"Transfer Learning, Auto-encoder, Context-Aware Reasoning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": {\n                    \"SHAP values\": true,\n                    \"model inspection interfaces\": true\n                },\n                \"size optimization\": \"50% smaller for IoT deployment\",\n                \"bias monitoring\": true\n            },\n            \"description\": \"A novel architecture combining advanced techniques for NLP tasks.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"Transfer Learning\", \"Auto-encoder\"],\n        \"target_device\": \"IoT devices\",\n        \"deployment_endpoint\": \"mobile or cloud deployment\",\n        \"inference_engine\": [\"Real-time processing\"],\n        \"demonstration\": \"model performance and fairness demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the project requirements as given, reflecting a high-level task for building a Textual Inference System with specific performance metrics, dataset requirements, and architectural details. It takes into account the need for interpretability, bias monitoring, and target deployment. However, exact latency expectations and some preprocessing steps are not specified but are expected to be addressed during the project execution.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager for a pioneering ML team, your mandate is to design a cutting-edge text classification model on the BoolQ dataset. Mandate not only binary decision-making (0-entailment, 1-not entailment) but also devise a sophisticated mechanism to assess model resilience against contextually variant words. The new metric should be integrated, ensuring both F1 score and a comprehensive assessment of robustness, demanding multi-dimensional performance benchmarks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Design a state-of-the-art text classification model specifically for the BoolQ dataset, targeting binary decisions (0-entailment, 1-not entailment) and incorporating a sophisticated mechanism to assess model resilience against contextually variant words. The response should include integrated F1 score and a comprehensive assessment of robustness with multi-dimensional performance benchmarks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness assessment\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"entailment\"],\n            \"specification\": {\n                \"task_type\": \"binary classification\",\n                \"contextual_variants\": true\n            },\n            \"description\": \"A dataset for text classification with a focus on assessing resilience to context-dependent word meanings.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"attention mechanisms\": true,\n                    \"context-awareness\": true\n                }\n            },\n            \"description\": \"A cutting-edge text classification model designed to handle binary decision-making on the BoolQ dataset and incorporate resilience assessment against contextually variant words.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Innovative approaches to handling context in natural language understanding is crucial for the BoolQ dataset.\",\n        \"Multi-dimensional performance benchmarks, especially in F1 score and robustness, are key requirements.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstrate resilience analysis through contrasting model outputs with contextual variants\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the main aspects of the user's instruction for designing a sophisticated text classification model. The problem area, downstream task, and application domain are identified. The requirements for performance metrics (F1 score and robustness assessment) and dataset properties (BoolQ dataset with contextual variants) are correctly included. The model type and family are assumed based on the requirement for a cutting-edge solution, and additional knowledge points to support problem-solving are provided. However, specifics regarding preprocessing steps and a detailed model architecture are not explicitly mentioned, so they may be incomplete.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "question answering", "instruction": "As a seasoned project manager, your mandate is to design a groundbreaking edtech platform that combines gamification and advanced natural language processing, using the specialized KidsQA dataset. Collaborate with a multidisciplinary team to develop a simplified BERT adaptation optimized for educational purposes, while maintaining a pixel-perfect UI/UX. The platform must facilitate seamless, visually stimulating interactions, promoting cognitive growth in young learners through progressive, contextually-rich responses that adhere to Montessori principles.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education, edtech\",\n        \"description\": \"Design a groundbreaking edtech platform combining gamification and advanced natural language processing, specifically using the KidsQA dataset. The project involves optimizing a simplified BERT adaptation for educational purposes and ensuring a pixel-perfect UI/UX. The platform aims to facilitate seamless, visually engaging interactions that adhere to Montessori principles, promoting cognitive growth in young learners.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"relevance to educational content\",\n                \"value\": null\n            },\n            {\n                \"name\": \"UI/UX satisfaction\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"resource usage (CPU, GPU)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data size\": null,\n                \"question types\": \"educational\",\n                \"answer types\": \"contextually-rich\",\n                \"Montessori compliance\": true\n            },\n            \"description\": \"A specialized dataset for educational question answering, designed for young learners with a Montessori focus.\",\n            \"preprocessing\": [\n                \"BERT adaptation\",\n                \"curation for educational content\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Simplified BERT for EdTech\",\n            \"family\": \"BERT adaptation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"BERT simplified for educational context\",\n                \"optimizer\": null,\n                \"task-specific layers\": true\n            },\n            \"description\": \"A BERT-based model tailored for simplified question answering and educational use, designed to adapt to the KidsQA dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"gamification integration\": \"to enhance engagement and learning experience\"\n        },\n        {\n            \"Montessori principles\":\n                \"promotes self-directed learning, sequential exploration, and adaptability\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware question generation\",\n            \"adaptive difficulty levels\"\n        ],\n        \"target_device\": \"mobile, cloud\",\n        \"deployment_endpoint\": \"Montessori-based edtech platform\",\n        \"inference_engine\": [\n            \"GPU-optimized for faster response times\"\n        ],\n        \"demonstration\": \"interactive demonstrations showcasing adaptability and cognitive growth\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a project, high level of expertise, and the problem area (NLP). The project description, performance metrics, and complexity metrics are based on the details provided in the instruction. The reference to KidsQA dataset, simplified BERT adaptation, UI/UX, Montessori principles, and gamification are all incorporated. However, specific metric targets (like accuracy, UI/UX satisfaction) are left as null since they would be filled based on further details from the project requirements. The confidence score could be 0.97 given the clear mapping of the instruction to the JSON schema, but it's left at the default 0.95 for a slightly lower confidence due to the high-level nature of the request.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "Revise the project to design an enterprise-grade QA system for intricate queries, employing the diverse WebText+CuratedTIFU dataset. Mandate a state-of-the-art BERT-based architecture (enhanced with M6-Large for extended context understanding), along with an embedding layer for entity disambiguation. Ensure the system's real-time performance, with an optimization for at least 99.9% server response time SLA, while handling concurrent users in the millions, without sacrificing accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"enterprise\",\n        \"description\": \"Revise the project to design an enterprise-grade QA system capable of handling intricate queries using the WebText+CuratedTIFU dataset. The system should employ a state-of-the-art BERT-based architecture with M6-Large for enhanced context understanding and include an embedding layer for entity disambiguation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"server response time\",\n                \"value\": {\n                    \"unit\": \"%\",\n                    \"value\": 99.9\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": true,\n                \"unit\": null\n            },\n            {\n                \"name\": \"concurrent user handling\",\n                \"value\": {\n                    \"limit\": \"millions\",\n                    \"handling\": true\n                },\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WebText+CuratedTIFU\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"train\": {\n                        \"ratio\": null,\n                        \"data_points\": null\n                    },\n                    \"valid\": {\n                        \"ratio\": null,\n                        \"data_points\": null\n                    },\n                    \"test\": {\n                        \"ratio\": null,\n                        \"data_points\": null\n                    }\n                },\n                \"diversity\": {\n                    \"text\": \"intrinsic text variety\"\n                }\n            },\n            \"description\": \"A large-scale dataset combining WebText and CuratedTIFU for training an enterprise-grade QA system.\",\n            \"preprocessing\": [\n                \"BERT data format conversion\"\n            ],\n            \"augmentation\": [\n                \"none\"\n            ],\n            \"visualization\": [\n                \"example snippet analysis\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-based with M6-Large\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT + M6-Large\",\n                \"context Understanding\": \"enhanced\",\n                \"entity disambiguation\": true\n            },\n            \"description\": \"A state-of-the-art QA system that leverages BERT and M6-Large for improved handling of complex queries.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating WebText+CuratedTIFU dataset for extensive knowledge and various query types\",\n        \"Entity disambiguation technique for improved query resolution\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"real-time performance tuning\"\n        ],\n        \"target_device\": \"cloud or server\",\n        \"deployment_endpoint\": {\n            \"type\": \"API service\",\n            \"requirements\": \"high concurrency, low latency\"\n        },\n        \"inference_engine\": [\n            \"optimized for server-side processing\"\n        ],\n        \"demonstration\": {\n            \"showcase\": \"的例子 for intricate queries and real-time performance\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a new QA system, specifies high expertise, and details the project's requirements: BERT-based architecture, M6-Large, entity disambiguation, real-time performance, and concurrent user handling. The problem area is set to natural language processing for QA, and the dataset and model are derived directly from the instruction. Performance metrics, such as server response time and accuracy, are included with reasonable assumptions based on the SLA provided. However, specific performance values for accuracy are not provided, as they are usually determined during model training or testing. The confidence score is set to 0.97 due to the clarity of the project's focus.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "The project manager is sought to lead a specialized deep learning team in developing a highly sophisticated, iterative deep belief network (DBN) based anomaly detection algorithm for time-series data in the industrial IoT domain. The team must achieve a precision benchmark of at least 85% on the R&D-specific subset of 150 Hexagon Manufacturing Intelligence/Urban Big Data Center datasets, focusing on models optimized for resource-constrained devices with sub-millisecond latency. Incorporate real-time performance monitoring, rigorous A/B testing with LSTM and autoencoder architectures, and present a comprehensive report showcasing the algorithm's edge in efficiency and accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"industrial IoT\",\n        \"description\": \"Lead a deep learning team to develop a sophisticated, iterative DBN-based anomaly detection algorithm for time-series data, targeting resource-constrained devices with a sub-millisecond latency. Aim for a precision benchmark of at least 85% on the R&D subset of 150 Hexagon Manufacturing Intelligence/Urban Big Data Center datasets, incorporating real-time performance monitoring and rigorous A/B testing with LSTM and autoencoder architectures.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"sub-milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hexagon Manufacturing Intelligence/Urban Big Data Center\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"R&D subset of 150 datasets\"\n            },\n            \"description\": \"Time-series data with a focus on industrial IoT scenarios\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Iterative Deep Belief Network (DBN) with Anomaly Detection\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"DBN\",\n                \"optimized for\": \"resource-constrained devices\"\n            },\n            \"description\": \"A sophisticated DBN model for anomaly detection on time-series data for industrial IoT, with real-time performance and sub-millisecond latency.\"\n        },\n        {\n            \"name\": \"LSTM and Autoencoder\",\n            \"type\": \"neural networks\",\n            \"description\": \"Baseline architectures for A/B testing\"\n        }\n    ],\n    \"knowledge\": [\n        \"Performance monitoring and A/B testing with LSTM and autoencoders for model comparison.\",\n        \"Targeted at resource-constrained devices with emphasis on efficiency and accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A comprehensive report showcasing algorithm efficiency and accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the project requirements for the development of a DBN-based anomaly detection system for industrial IoT, including performance metrics and constraints. The user's intent to build, high expertise, and the specified domain are clear. Details about the dataset, model architectures, and A/B testing are also accurately represented. However, the specific preprocessing steps, real-time performance monitoring techniques, and the demonstration format are not detailed as they would be more context-dependent.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Forge a high-performance, distributed time series forecasting framework for our renewable energy app's volatile user engagement (PV) metrics. Utilize a hybrid ensemble of specialized neural networks, like DeepAR for short-term and ETS for long-term forecasts. Enhance feature extraction by isolating weekly, monthly, and quarterly trends, while applying L1 and L2 regularization to optimize interpretability. Create a novel evaluation metric, \"Energy User Correlation Coefficient\" (E4C), and prepare a comprehensive, comparative study with at least five distinct baseline algorithms, quantifying the impact on user retention and satisfaction.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"renewable energy\",\n        \"description\": \"Develop a high-performance, distributed time series forecasting framework for volatile user engagement metrics in a renewable energy app. Utilize a hybrid ensemble of DeepAR for short-term and ETS for long-term forecasts. Focus on feature extraction with weekly, monthly, and quarterly trends. Incorporate L1 and L2 regularization for interpretability. Introduce a new evaluation metric, Energy User Correlation Coefficient (E4C), and compare with five baseline algorithms to assess impact on user retention and satisfaction.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Energy User Correlation Coefficient (E4C)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PV Metrics\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"volatile user engagement\"],\n            \"specification\": {\n                \"unit\": \"unknown\",\n                \"granularity\": \"unknown\"\n            },\n            \"description\": \"Volatile user engagement data for a renewable energy app\",\n            \"preprocessing\": [\n                \"feature extraction (weekly, monthly, quarterly trends)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"data patterns analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Ensemble (DeepAR + ETS)\",\n            \"family\": \"neural networks and statistical models\",\n            \"type\": [\n                \"neural networks\",\n                \"statistical models\"\n            ],\n            \"specification\": {\n                \"ensemble_details\": \"short-term: DeepAR, long-term: ETS\",\n                \"regularization\": \"L1 and L2\"\n            },\n            \"description\": \"A high-performance framework using a hybrid ensemble for time series forecasting of renewable energy user engagement\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hybrid ensembles improve forecasting accuracy\",\n        \"L1 and L2 regularization for interpretability\",\n        \"Innovative E4C metric for unique evaluation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"L1 and L2 regularization\",\n            \"Weekly, monthly, and quarterly trend isolation\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"user retention and satisfaction comparison study\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the requested framework, including the user's intent to build, the specified performance metric (E4C), and the detailed requirements for the time series forecasting task, datasets, and model. It assumes the user wants to perform custom preprocessing and model selection based on their specifications. However, specific details like the E4C metric value or some baseline algorithms are not provided as the instruction does not specify them. This might lead to a lower confidence score.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "As a project manager, craft a GAT-based disambiguation system for ScholarNet, incorporating both temporal and semantic edge attributes into a unique GATv2. Mandate a stringent requirement of F1-score surpassing 95% in a high-velocity scholarly citation environment. Additionally, design a multifaceted ablation study comparing at least five edge weighting strategies and diverse attention mechanisms, ensuring model scalability and generating a deep interpretability report showcasing key insights.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"scholarly research\",\n        \"description\": \"Craft a GATv2-based disambiguation system for ScholarNet, incorporating both temporal and semantic edge attributes. The system should have an F1-score target of over 95% in a high-velocity scholarly citation environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ScholarNet data\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [\n                \"citation edges\"\n            ],\n            \"specification\": {\n                \"edge_types\": [\"temporal\", \"semantic\"],\n                \"temporal_coverage\": null,\n                \"size\": null\n            },\n            \"description\": \"Dataset containing scholarly citations with temporal and semantic attributes for disambiguation analysis.\",\n            \"preprocessing\": [\n                \"edge attribute extraction\",\n                \"temporal and semantic feature engineering\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"graph structure analysis\",\n                \"attention mechanism comparison\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GATv2 Disambiguation System\",\n            \"family\": \"Graph Attention Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"edge_weighting_strategies\": {\n                    \"GAT\": null,\n                    \"Edge Attention\": null,\n                    \"Node-Dependent\": null,\n                    \"Edge Type-Specific\": null,\n                    \"Hierarchical\": null\n                },\n                \"attention_mechanisms\": [\n                    \"Multi-Head Attention\",\n                    \"Self-Attention\",\n                    \"Channel Attention\",\n                    \"Positional Attention\"\n                ],\n                \"scalability\": \"assessed\"\n            },\n            \"description\": \"A GATv2 model specifically designed for disambiguation using temporal and semantic edge attributes in the ScholarNet environment with high F1-score requirement.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Temporal and semantic edge attributes in ScholarNet call for a GATv2 model with advanced edge weighting strategies.\",\n        \"Ablation study is crucial for understanding the impact of different edge weighting strategies and attention mechanisms.\",\n        \"Deep interpretability is important to identify key factors contributing to the high-accuracy F1-score.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal-saliency mapping\",\n            \"semantic feature aggregation\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"ScholarNet Disambiguation API\",\n        \"inference_engine\": [\n            \"TensorFlow Serving\",\n            \"Apache Spark MLlib\"\n        ],\n        \"demonstration\": {\n            \"focus\": [\n                \"real-time performance\",\n                \"accuracy trade-offs\"\n            ]\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the core elements of the given project manager's instruction, including the model, datasets, performance objectives, and the need for interpretability and ablation study. Given the requirements, the user's high expertise level and the need for disambiguation in a high-velocity environment, the response is considered accurate. The confidence score is based on the clarity of the instruction and the successful mapping to the JSON structure. However, specific metric values or a detailed implementation plan are not provided, which might affect the confidence slightly.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the \"Satellite Earth Vision Explorer\" initiative, your mandate is to drive a progressive project that integrates cutting-edge Sentinel-2 data with a specialized fusion of advanced models: Hierarchical Progressive CNNs, Fusion-U-Nets, and Enhanced DeepLabV3+ for land use discrimination. Your team must meticulously assess these models based on their precision in distinguishing intricate land classes - high-resolution waterways, ancient mixed forests, and rapidly evolving urban skylines. Emphasize on energy efficiency during inference, multi-scale precision, and strategic leveraging of pre-trained models on limited computational resources. The challenge lies in optimizing real-time performance, minimizing latency, and achieving a minimum 95% accuracy within a constrained training period of 14 days. Prepare a comprehensive report highlighting the trade-offs in speed and accuracy for each model, taking into account the dynamic environmental factors that may impact their performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"remote sensing and earth observation\",\n        \"description\": \"Integrate Sentinel-2 data with Hierarchical Progressive CNNs, Fusion-U-Nets, and Enhanced DeepLabV3+ for land use discrimination, focusing on precision for high-resolution waterways, ancient mixed forests, and urban skylines. The project aims for energy-efficient inference, multi-scale precision, and optimization for real-time performance and low latency, targeting at least 95% accuracy within a 14-day training period.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"energy efficiency (inference)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"multi-scale precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"frames per second\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"computational resources\",\n                \"value\": \"limited\",\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2 Data\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": \"variable (depends on the models)\",\n                \"bands\": \"RGBNIR (Sentinel-2 spectral bands)\",\n                \"spatial_res\": \"variable (取决于模型融合)\"\n            },\n            \"description\": \"High-resolution Sentinel-2 satellite imagery for land use discrimination analysis.\",\n            \"preprocessing\": [\n                \"Sentinel-2 data preprocessing (calibration, atmospheric correction)\"\n            ],\n            \"augmentation\": [\n                \"Image augmentation techniques for environmental variability\"\n            ],\n            \"visualization\": [\n                \"Visual analysis of model performance on diverse land classes\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Progressive CNNs\",\n            \"family\": \"Advanced CNNs\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"hierarchical and progressive\",\n                \"pre-trained\": false,\n                \"transfer learning\": true\n            },\n            \"description\": \"A CNN architecture optimized for land use discrimination with emphasis on energy efficiency and speed.\"\n        },\n        {\n            \"name\": \"Fusion-U-Nets\",\n            \"family\": \"Multi-Model Fusion\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"ensemble method\": \"fusion\",\n                \"pre-trained\": true,\n                \"resource-aware\": true\n            },\n            \"description\": \"An ensemble model combining U-Net architectures with emphasis on multi-scale precision.\"\n        },\n        {\n            \"name\": \"Enhanced DeepLabV3+\",\n            \"family\": \"Semantic Segmentation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"DeepLabV3+ with enhancements\",\n                \"pre-trained\": true,\n                \"fine-tuning\": true\n            },\n            \"description\": \"A state-of-the-art segmentation model tailored for land use discrimination, considering energy efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Environmental factors can affect model performance (e.g., cloud cover, seasonal variations)\",\n        \"Strategic pre-trained model selection and transfer learning for limited computational resources\",\n        \"Trade-offs between speed, accuracy, and energy efficiency during real-time inference\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Fine-tuning and model pruning for resource optimization\"\n        ],\n        \"target_device\": \"mobile or edge computing for real-time performance\",\n        \"deployment_endpoint\": \"cloud-based or on-premise\",\n        \"inference_engine\": [\n            \"TensorFlow Lite\",\n            \"OpenVINO\"\n        ],\n        \"demonstration\": \"Create a live demonstrative map showcasing the model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the user's intent to build a machine learning project, integrating multiple models for land use discrimination with a focus on the specified metrics and constraints. It captures the expert level expertise required. The details about the dataset, models, and specific requirements such as energy efficiency, multi-scale precision, and limited computational resources are derived from the task description. The confidence score is high because the instruction is clear and provides a clear direction for the project.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for a premium sentiment analysis firm, you must now enhance a domain-specific BERT model on the BoolQ dataset. In addition to achieving a benchmark precision of 95%, the project must handle imbalanced data, multi-step reasoning, and demonstrate interpretability by integrating explainable AI techniques. Utilize a custom evaluation suite with precision recall-F1 score and AUC-ROC for a holistic assessment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"sentiment analysis\",\n        \"description\": \"Enhance a domain-specific BERT model for sentiment analysis on the BoolQ dataset. The project must achieve a benchmark precision of 95%, handle imbalanced data, involve multi-step reasoning, and demonstrate interpretability through explainable AI techniques. A custom evaluation suite is required, including precision, recall, F1 score, and AUC-ROC.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\"],\n            \"specification\": {\n                \"imbalance\": \"yes\",\n                \"multi_step_reasoning\": \"yes\"\n            },\n            \"description\": \"A domain-specific dataset for sentiment analysis with a focus on multi-step reasoning and imbalanced data.\",\n            \"preprocessing\": [\"handle imbalanced data\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT (enhanced)\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": \"yes\"\n            },\n            \"description\": \"An enhanced BERT model designed for sentiment analysis on the BoolQ dataset, with interpretability features integrated.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"multi-step reasoning handling\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"require interpretable explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured to capture the details provided in the instruction. The user's intent, high level of expertise, and the problem domain are derived from the context. The project requirements, such as precision, AUC-ROC, and interpretability, are included in the performance and model specifications. The dataset properties, preprocessing steps, and source are noted, as well as the demonstration expectations. However, due to missing numerical values for recall and F1 score, the confidence is slightly lower.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for the elite AI research team, you're now faced with a multifaceted challenge to engineer a groundbreaking deep learning time series forecasting model. With the extensive Real-World Traffic Analytics dataset at hand, consisting of numerous complex patterns organized into intricate train, validation, and test partitions (each having unique temporal dynamics), your team must design an advanced architecture that:\n\n1. Processes historical data of 96 sequential observations with 862 distinct features (INPUT_SEQ_LEN=96, INPUT_DIM=862) in a high-dimensional context.\n2. Integrates not only a deep LSTM network but also hybrid models to predict the subsequent 96-step sequence, ensuring robustness against various seasonality shifts and trend fluctuations (PRED_SEQ_LEN=96, PRED_DIM=862).\n3. Embarks on implementing state-of-the-art evaluation methods, like weighted error metrics, to account for imbalances in data distribution, while simultaneously incorporating anomaly detection for early warning systems.\n4. Advances the art of forecasting by experimenting with rolling window predictions and adaptively fine-tuning models, demonstrating adaptability in dynamic environments.\n5. Develops a rigorous comparative analysis of competing models, detailing their performance, interpretability, and computational efficiency.\n6. Concocts innovative approaches using transfer learning and neural architecture search to push the boundaries of forecasting accuracy and efficiency.\n\nYour report should showcase a comprehensive methodology, detailed results, and a blueprint for future enhancement strategies, emphasizing the model's exceptional performance and sophistication.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"transportation and urban planning\",\n        \"description\": \"Engineer a groundbreaking deep learning time series forecasting model using the Real-World Traffic Analytics dataset with focus on complex patterns, high-dimensional input, and advanced evaluation methods.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"weighted error metrics\",\n                \"value\": null\n            },\n            {\n                \"name\": \"anomaly detection performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"megabytes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Real-World Traffic Analytics\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 862,\n                \"train_size\": null,\n                \"validation_size\": null,\n                \"test_size\": null,\n                \"temporal_dynamics\": \"complex and unique\"\n            },\n            \"description\": \"A high-dimensional time series dataset with intricate train, validation, and test partitions for advanced time series forecasting.\",\n            \"preprocessing\": [\n                \"normalization for high-dimensional data\",\n                \"handling seasonality and trend shifts\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep LSTM + Hybrid Model\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"LSTM with attention and hybrid models\",\n                \"sequence_length\": {\"input\": 96, \"output\": 96},\n                \"input_dim\": 862,\n                \"seasonality_handling\": \"yes\"\n            },\n            \"description\": \"A sophisticated model that combines deep LSTMs with hybrid architectures for robust forecasting and adaptability\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilizes rolling window predictions and adaptive fine-tuning\",\n        \"Employing transfer learning and NAS for model optimization\",\n        \"Emphasis on interpretability and computational efficiency\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"multi-step forecasting\",\n            \"anomaly detection algorithm integration\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"Rolling predictions and performance comparison visuals\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects a clear build request for a complex time series forecasting model. The user's high expertise level and intention to build the model are evident. The project's challenge and requirements for model design, performance, and evaluation are incorporated into the JSON schema. However, specific performance targets and computational efficiency measures are missing from the metrics, as they would typically be determined through experimentation or user-defined thresholds.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project manager, you are tasked with overseeing an advanced deep learning endeavor in the energy sector, focusing on enhancing time-series forecasting. The Electricity dataset has been significantly enhanced with domain knowledge and external factors, necessitating a more intricate evaluation of the AI's adaptability to address anomalies, seasonality fluctuations, and non-linear trends. The data, now stratified into train (65%), validation (17.5%), and test sets (17.5%), consists of extensive historical inputs (INPUT_SEQ_LEN=192, INPUT_DIM=512) representing a fusion of complex meteorological patterns, operational dynamics, and real-time indicators. The aim is to design a state-of-the-art model capable of forecasting the subsequent 192-step output (PRED_SEQ_LEN=192, PRED_DIM=512) with precision, demonstrated by improved F1-score, RMSE, and MAPE. Additionally, require the model to be resilient against outliers, and demand a high level of interpretability and transparency in the model's decision-making process, especially in light of the increased complexity and real-world applicability. Remember to incorporate explainable AI techniques to facilitate human comprehension.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy sector\",\n        \"description\": \"Oversee an advanced deep learning project in the energy sector to enhance time-series forecasting for the Electricity dataset. The dataset is enhanced with domain knowledge and external factors, targeting adaptability to anomalies, seasonality fluctuations, and non-linear trends. Evaluation metrics focus on F1-score, RMSE, and MAPE. The model should be resilient to outliers and require high interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 192,\n                \"INPUT_DIM\": 512,\n                \"PRED_SEQ_LEN\": 192,\n                \"PRED_DIM\": 512,\n                \"data_split\": [\"train\", \"validation\", \"test\", \"complex_meteorological_patterns\", \"operational_dynamics\", \"real_time_indicators\"]\n            },\n            \"description\": \"Enhanced dataset with complex meteorological patterns, operational dynamics, and real-time indicators for electricity forecasting in the energy sector.\",\n            \"preprocessing\": [\"domain knowledge enhancement\", \"stratification\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art time-series forecasting model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"resilience_to_outliers\": true\n            },\n            \"description\": \"A deep learning model designed to forecast time series with enhanced adaptability, interpretability, and robustness to anomalies.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporation of explainable AI techniques for interpretability, focusing on the model's adaptability, anomaly detection, and real-world implications.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature fusion, anomaly detection techniques\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstration of interpretability through visualizations or model-agnostic explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been crafted based on the user's instruction with the provided requirements. The user's expertise level is assumed high, and the problem area and specific tasks are clearly defined. Performance metrics are mentioned but left unspecified for optimization, and explainable AI is addressed as a crucial aspect. However, some model-specific details like the exact architecture or feature fusion methods are not provided, as they would depend on the project's starting point.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a state-of-the-art Quantum-enhanced Graph Autoencoder (Q-GAE) using the ultra-high resolution Metabolic Brain Network dataset, which integrates molecular interactions and dynamic activity patterns. The primary objective is to discern between highly specialized and functionally diverse brain microregions with sub-millisecond temporal resolution, while minimizing false positives and preserving the delicate balance of information flow. To accomplish this, the model must integrate quantum entanglement principles and demonstrate improved performance compared to traditional ML algorithms in limited computational resources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Develop a state-of-the-art Quantum-enhanced Graph Autoencoder (Q-GAE) using the ultra-high resolution Metabolic Brain Network dataset. The model aims to classify highly specialized and functionally diverse brain microregions with sub-millisecond temporal resolution, while reducing false positives and maintaining information flow balance. It should incorporate quantum entanglement principles and demonstrate superior performance compared to traditional ML algorithms in resource-constrained environments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"information flow balance preservation\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Metabolic Brain Network\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [\"node labels for microregions\"],\n            \"specification\": {\n                \"resolution\": \"ultra-high\",\n                \"temporal_res\": \"sub-millisecond\"\n            },\n            \"description\": \"An integrated dataset of molecular interactions and dynamic activity patterns with high-resolution temporal data.\",\n            \"preprocessing\": [\n                \"quantum data normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"network topology and time-series analysis\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-enhanced Graph Autoencoder (Q-GAE)\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"quantum_principles\": \"incorporating entanglement\",\n                \"computational_resources\": \"limited\"\n            },\n            \"description\": \"A graph learning model using quantum principles for improved node classification in brain microregions under resource constraints.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating quantum entanglement enhances model performance in complex brain networks.\",\n        \"Resource efficiency is crucial for brain data analysis.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"quantum-based inference\"\n        ],\n        \"demonstration\": \"comparison with traditional ML algorithms and information flow visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is generated based on the user's instruction, reflecting high expertise level, problem area (graph machine learning in neurosciences), the Q-GAE model, and the specific Metabolic Brain Network dataset. Performance metrics, complexity metrics, preprocessing, and model expectations are derived from the task. However, since the instruction doesn't provide specific values for the performance metrics, they are set to null. The confidence score is high given the clear task and the matching JSON schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the sophisticated Marine Ecosystem AI project, challenge the ML team with a complex and versatile assignment: Design an advanced hybrid of PSPNet++ and environmental fusion, specifically tailored for CoralNet's underwater image analysis. The model must demonstrate exceptional performance in differentiating intricate coral formations and diverse marine species amidst dynamic turbidity shifts, rapid lighting conditions, and thermal variations. Demand a substantial and well-supported lift in Dice Coefficient, with rigorous benchmarking using nested cross-validation strategies and comprehensive feature sensitivity analysis. Additionally, outline specific requirements for handling class imbalance and optimizing computational efficiency for real-time deployments on resource-constrained devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"oceanography\",\n        \"description\": \"Design an advanced hybrid model combining PSPNet++ and environmental fusion for CoralNet's underwater image analysis. The model should excel in differentiating coral formations and marine species under dynamic turbidity shifts, lighting conditions, and thermal variations. Aim for a significant improvement in Dice Coefficient, with rigorous benchmarking using nested cross-validation and feature sensitivity analysis. Include handling class imbalance and optimizing computational efficiency for real-time deployment on resource-constrained devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Dice Coefficient\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs or inference time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CoralNet's underwater images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"dynamic_conditions\": [\n                    \"turbidity shifts\",\n                    \"rapid lighting conditions\",\n                    \"thermal variations\"\n                ],\n                \"imbalance\": true\n            },\n            \"description\": \"Underwater images with varying environmental conditions for coral and marine species segmentation.\",\n            \"preprocessing\": [\"environmental fusion\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Hybrid Model (PSPNet++ and Environmental Fusion)\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy improvement_target\": null,\n                \"benchmarking_method\": \"nested cross-validation\",\n                \"feature_analysis\": \"sensitivity\"\n            },\n            \"description\": \"A custom-designed deep learning model for underwater image segmentation using PSPNet++ and environmental data fusion.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consideration of environmental factors and class imbalance in model design is crucial.\",\n        \"Efficient algorithms and hardware-aware training are essential for real-time deployment on resource-constrained devices.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"environmental fusion\", \"class imbalance handling\"],\n        \"target_device\": \"resource-constrained devices\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time deployment and performance demonstration under varying conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been parsed into a JSON response that captures the user's intent to build a model, their level of expertise, and the specifics of the project (area, task, application domain, and requirements). Performance metrics, complexity metrics, and specific model requirements are clearly defined. However, missing values in the performance (Dice Coefficient value) and computational efficiency (exact value or optimization goal) metrics indicate some uncertainty in the response due to the lack of specific numbers in the original instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, require the ML team to design a smartphone-compatible plant species classifier using 'Plant Seedlings Dataset'. The AI system must not only achieve over 95% accuracy under varying lighting, but also incorporate real-time semantic segmentation and dynamic transfer learning. Additionally, mandate a constraint for energy-efficient deployment, ensuring the model's inference speed surpasses 200ms even on low-end devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"agriculture or environmental science\",\n        \"description\": \"Design a smartphone-compatible plant species classifier using the 'Plant Seedlings Dataset'. The AI system must have an accuracy of over 95% under varying lighting conditions, incorporate real-time semantic segmentation, and ensure energy-efficient deployment with an inference speed of at least 200ms on low-end devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.2,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Seedlings Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\"train\": \"X\", \"validation\": \"X\", \"test\": \"X\"},\n                \"variety\": \"Plant Seedlings\",\n                \"lighting_conditions\": \"varied\"\n            },\n            \"preprocessing\": [\"image normalization, resizing\"],\n            \"augmentation\": [\"image rotation, flipping, lighting adjustments\"],\n            \"visualization\": [\"data distribution plot\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (CNN) with real-time semantic segmentation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"dynamically adaptable for transfer learning\",\n                \"energy-efficient design\": \"yes\"\n            },\n            \"description\": \"A smartphone-compatible AI system designed for plant species classification with semantic segmentation and optimized for energy consumption.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time data preprocessing, dynamic model adaptation\"],\n        \"target_device\": \"smartphones, low-end devices\",\n        \"deployment_endpoint\": \"optimized for mobile deployment\",\n        \"inference_engine\": [\"lightweight, optimized for real-time inference\"],\n        \"demonstration\": \"deployment on a representative low-end smartphone\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the project manager's requirements, including the plant species classification task, the dataset, model specifications, and the constraints. The accuracy target of 95% and the inference speed constraint are explicitly included. However, since the actual dataset size and specific models for transfer learning are not provided, they can be assumed based on the request for high accuracy and energy efficiency. Confidence is high but not 1.0 due to the missing details.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "Examine the 'AtmosphericCO2Levels' dataset with advanced, real-time fusion of multiple sensors data for CO2, temperature, and precipitation. Implement an ensemble of state-of-the-art MVA algorithms, considering seasonality and trends, to detect subtle anomalies in a high-dimensional setting. The challenge lies in creating a robust model that isolates anthropogenic impacts while maintaining interpretability, all while optimizing for minimal false positives in monitoring the global carbon footprint.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"climate science\",\n        \"description\": \"Analyze the 'AtmosphericCO2Levels' dataset with real-time fusion of CO2, temperature, and precipitation data from multiple sensors. Implement an ensemble of state-of-the-art MVA algorithms, considering seasonality and trends, to detect subtle anomalies related to anthropogenic impacts. The model must be interpretable and minimize false positives in monitoring global carbon footprint.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": \"low\"\n            },\n            {\n                \"name\": \"interpretability score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory efficiency\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"inference speed\",\n                \"value\": \"real-time\",\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AtmosphericCO2Levels\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"sensor_count\": null,\n                \"time_frequency\": null,\n                \"data_dimensions\": null\n            },\n            \"description\": \"A high-dimensional time-series dataset containing real-time fusion of CO2, temperature, and precipitation data from multiple sensors for detecting anomalies.\",\n            \"preprocessing\": [\"seasonality adjustment\", \"trend analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble of MVA algorithms\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"model_components\": [\"state-of-the-art algorithms\"],\n                \"feature_combination\": \"seasonality and trends\"\n            },\n            \"description\": \"An ensemble of advanced machine learning algorithms designed for time-series anomaly detection in a high-dimensional context, targeting anthropogenic impacts with interpretability and minimal false positives.\"\n        }\n    ],\n    \"knowledge\": [\n        \"In this task, the focus is on the 'AtmosphericCO2Levels' dataset and real-time fusion of sensor data.\",\n        \"The challenge is on detecting subtle anomalies with interpretability, seasonality, and trends are key considerations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time fusion\", \"seasonality-adjusted features\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretable models\"],\n        \"demonstration\": \"Model explainability and results visualization for anomalies\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the provided instruction, mapping the user's intent to build a model, high expertise level, and specific details about the data and task (real-time fusion, seasonality, anomaly detection). Performance metrics like F1 score, false positive rate, and interpretability are included based on the requirements. However, specific details like the algorithm components or exact data dimensions are not provided in the response as they are not detailed in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "For a data science project, you must develop a robust regression model using the Media Campaign Cost Dataset. This dataset, with its extensive numerical features, is divided into train, validation, and test sets. Your objective is to minimize the RMSLE by optimizing hyperparameters and implementing feature engineering. Additionally, ensure the model's explainability, adhering to interpretability standards, while achieving at least 90% accuracy on the validation set.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"Develop a robust regression model for a data science project using the Media Campaign Cost Dataset. The model must minimize RMSLE by optimizing hyperparameters, implement feature engineering, and adhere to interpretability standards, targeting at least 90% accuracy on the validation set.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared logarithmic error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Media Campaign Cost Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"target_variable\"],\n            \"specification\": {\n                \"split\": {\n                    \"train\": \"train set\",\n                    \"validation\": \"validation set\",\n                    \"test\": \"test set\"\n                },\n                \"numerical_features\": \"extensive\"\n            },\n            \"description\": \"A dataset containing numerical features for media campaign cost prediction, divided into train, validation, and test sets.\",\n            \"preprocessing\": [\n                \"feature scaling\",\n                \"feature selection\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization\": \"Hyperparameter tuning\",\n                \"interpretability\": \"Adheres to interpretability standards\"\n            },\n            \"description\": \"A robust regression model built using neural networks to minimize RMSLE and achieve at least 90% accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Achieving high accuracy with interpretability is a priority\",\n        \"Robustness against overfitting through feature engineering\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature scaling\",\n            \"feature transformations\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explainable predictions for key features\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essential elements from the user's instruction, including the dataset, model requirements, and performance metrics. The dataset modality, target variable, and source are inferred based on the context. The model's type and specific techniques (e.g., feature engineering) are chosen from the specified objectives. The missing values in the model specification (e.g., optimization algorithms) can be inferred based on user's high expertise level. However, it's assumed that the model name and family are left blank for the user to fill in. The confidence score is high due to the clear mapping of the requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "For this advanced real estate analytics project, you are required to develop a predictive model that forecasts house sale prices using the provided dataset. Perform a comprehensive exploratory data analysis, handle missing values, normalize numeric features, create interaction and polynomial features, and implement multiple regression models. Evaluate and compare their performance using cross-validation, and report the root mean squared error (RMSE) after applying log-transformation to both predicted and actual sale prices, focusing on the evaluation dataset.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"real estate analytics\",\n        \"description\": \"Develop a predictive model for forecasting house sale prices using a provided dataset. Conduct an exploratory data analysis, handle missing values, normalize numeric features, create interaction and polynomial features, and apply multiple regression models. Evaluation should include cross-validation, and the performance should be reported using the root mean squared error (RMSE) after applying log-transformation to both predicted and actual sale prices, focusing on the evaluation dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"sale_price\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"length\": null,\n                \"missing_values\": null\n            },\n            \"description\": \"A dataset for predicting house sale prices in an advanced real estate analytics project.\",\n            \"preprocessing\": [\n                \"exploratory data analysis\",\n                \"handling missing values\",\n                \"normalization of numeric features\",\n                \"interaction and polynomial feature creation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"multiple regression\",\n            \"type\": \"classical machine learning\",\n            \"specification\": {\n                \"model_types\": [\"linear\", \"logistic\", \"polynomial\", \"interaction\"]\n            },\n            \"description\": \"A predictive model for house sale price forecasts, involving various regression techniques.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"log-transformation of sale prices\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"metrics\": \"RMSE with log-transformed data\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response parses the instruction accurately, capturing the task of building a predictive model for house sale prices, data analysis, preprocessing steps, and the evaluation metric (RMSE after log-transformation). The expertise level, problem area, and application domain are derived from the context, and the missing details are marked as null, indicating the need for more specific information from the user.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for the AI-driven retail solution, you need to develop a stealthy system that not only detects potential shoplifting using advanced machine learning algorithms but also differentiates between genuine security threats and customer courtesies. Implement real-time analysis with minimal false positives, ensuring customer privacy and accommodating for dynamic store layouts, while integrating with existing POS systems without disruptions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"Develop a stealthy AI-driven retail system to detect potential shoplifting in real-time with low false positives. The system should maintain customer privacy, accommodate dynamic store layouts, and seamlessly integrate with existing POS systems.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": \"minimal\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"real-time\"\n            },\n            {\n                \"name\": \"resource usage\",\n                \"value\": null,\n                \"unit\": \"GPU/CPU consumption\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dynamic\": true,\n                \"store_layout_variations\": true\n            },\n            \"description\": \"Video and image data with varying store layouts for training a real-time shoplifting detection model.\",\n            \"preprocessing\": [\n                \"privacy-preserving techniques\"\n            ],\n            \"augmentation\": [\"dynamic scene adaptation\"],\n            \"visualization\": [\"model performance analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"advanced machine learning algorithms (e.g., YOLOv5, RetinaNet)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"false_positive_ratio\": \"extremely low\",\n                \"customer隐私保护措施\": \"incorporated\"\n            },\n            \"description\": \"A machine learning model designed for real-time shoplifting detection while minimizing false positives and respecting customer privacy.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"POS data integration\"],\n        \"target_device\": \"cloud or edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"customer privacy visualization, false positive reduction statistics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the requirements in the given instruction. User's intent to build a system is clear, with high expertise level. The area (computer vision) and task (object detection) are inferred from the shoplifting detection context. Performance metrics like accuracy and minimal false positives were captured. Complexity, like real-time inference and resource consumption, are also mentioned. However, specific datasets and model details are not detailed as they are not clearly stated in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a specialized project manager for the Rare Manuscript Digital Archival Initiative, your team is required to explore the cutting-edge application of Optical Character Recognition (OCR) coupled with Deep Learning algorithms for fine-grained image segmentation. The task involves differentiating and extracting intricate, period-specific illustrations from an extensive collection of 17th-century parchment texts with distinct calligraphy, while preserving historical context and layout integrity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"digital archiving\",\n        \"description\": \"Explore the use of Optical Character Recognition (OCR) combined with Deep Learning algorithms for fine-grained image segmentation in the Rare Manuscript Digital Archival Initiative. The goal is to differentiate and extract intricate, 17th-century parchment illustrations with distinct calligraphy while preserving historical context and layout integrity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"17th-century Manuscript Collection\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"total_images\": null,\n                    \"train_ratio\": null,\n                    \"validation_ratio\": null,\n                    \"test_ratio\": null\n                },\n                \"dimensions\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"image_types\": [\"parchment\", \"calligraphy\"]\n            },\n            \"description\": \"A dataset consisting of 17th-century parchment texts with distinct calligraphy, for OCR and image segmentation tasks.\",\n            \"preprocessing\": [\n                \"image cleaning\",\n                \"normalization\"\n            ],\n            \"augmentation\": [\n                \"limited padding\",\n                \"horizontal flipping\"\n            ],\n            \"visualization\": [\n                \"image segmentation masks\",\n                \"histograms of extracted features\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"OCR + Deep Learning Model\",\n            \"family\": \"OCR and Convolutional Neural Networks (CNNs)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"fine-tuning a pre-trained OCR model with custom CNN for segmentation\",\n                \"number_of_layers\": null,\n                \"training_parameters\": {\n                    \"epochs\": null,\n                    \"learning_rate\": null\n                }\n            },\n            \"description\": \"A deep learning model integrating OCR and image segmentation techniques for detailed extraction of 17th-century manuscript illustrations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"OCR algorithms for古老手写体识别\",\n        \"17th-century art and calligraphy analysis\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"historical context preservation techniques\",\n            \"layout-aware processing\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"custom web platform for archiving\",\n        \"inference_engine\": [\n            \"GPU accelerated inference\"\n        ],\n        \"demonstration\": \"Interactive platform showcasing segmentation results and preservation of layout\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response was constructed based on the given instruction, reflecting a machine learning project focused on OCR and deep learning for image segmentation. The user's expertise and the intent to build the model are included. The problem domain, dataset specifics, and the combination of OCR and Deep Learning for image segmentation are described. Performance metrics are not specified, but the values are set to null to prompt for user-defined values. Complexity metrics are also left open-ended for optimization. The knowledge and service requirements align with the task description.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a specialized machine learning project manager, you are tasked with optimizing high-performance concrete mix design for a cutting-edge infrastructure project using the compressive strength dataset from \"Bridge Material Performance Database 2.0\". The data set contains intricate, physicochemical properties and sensor measurements. Your team must develop a state-of-the-art regression model, utilizing advanced feature engineering techniques, to minimize the prediction error. Perform 10-fold cross-validation and ensure an RMSE below 5 MPa for the test set, aiming to meet strict industry standards.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"civil engineering\",\n        \"description\": \"Optimize high-performance concrete mix design for a cutting-edge infrastructure project using the compressive strength dataset from 'Bridge Material Performance Database 2.0'. The project requires state-of-the-art regression modeling with advanced feature engineering, with an RMSE target of below 5 MPa in a 10-fold cross-validation setup, adhering to strict industry standards.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared error\",\n                \"value\": 5.0\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Bridge Material Performance Database 2.0\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"compressive strength\"],\n            \"specification\": {\n                \"contains_intensive_features\": true,\n                \"contains_sensor_data\": true\n            },\n            \"description\": \"A dataset with intricate physicochemical properties and sensor measurements for concrete mix design optimization.\",\n            \"preprocessing\": [\"advanced feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art regression\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"advanced_fe\": true,\n                \"cross_validation\": \"10-fold\"\n            },\n            \"description\": \"A modern regression model designed to minimize prediction error in high-performance concrete mix design.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced feature engineering is crucial for achieving superior performance in the concrete mix design.\",\n        \"RMSE below 5 MPa is necessary to meet strict industry standards.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"10-fold cross-validation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Performance visualization or case studies\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project's requirements, including the need to build a model, the user's high expertise, the data type, and the performance metric. However, it does not explicitly state that the user has the dataset link or source, which would need to be confirmed or provided by the user.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the AI development team, you are tasked with enhancing the airline customer sentiment analysis. Utilizing the extensive British Airways reviews dataset, develop a hierarchical text classification model that not only predicts ratings on a scale of 1 to 10 but also differentiates among five star segments (1-3, 4-6, 7-8, 9-10). Aim for high precision and optimize the model with grid search, tuning hyperparameters for best RMSE performance on the test set. Document the entire pipeline, including data preprocessing, feature extraction, and model evaluation in a comprehensive report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"customer service\",\n        \"description\": \"Enhance airline customer sentiment analysis using the British Airways reviews dataset. Develop a hierarchical model that predicts ratings on a scale of 1 to 10 and differentiates 5 star segments (1-3, 4-6, 7-8, 9-10). Optimize for high precision and use grid search to tune hyperparameters for best RMSE performance on the test set.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"root mean squared error (RMSE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"British Airways Reviews Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"rating\", \"star_segment\"],\n            \"specification\": null,\n            \"description\": \"A comprehensive dataset of airline reviews from British Airways, requiring hierarchical classification for different star rating segments and a scale of 1 to 10.\",\n            \"preprocessing\": [\n                \"Data cleaning and normalization\",\n                \"Tokenization\",\n                \"Stopword removal\",\n                \"Lemmatization or stemming\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Text Classification Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"training_algorithm\": \"grid search\",\n                \"hyperparameters\": [\n                    {\n                        \"name\": \"learning rate\",\n                        \"values\": [0.0001, 0.001, 0.01]\n                    },\n                    {\n                        \"name\": \"hidden layers\",\n                        \"values\": [2, 3, 4]\n                    },\n                    {\n                        \"name\": \"dropout rate\",\n                        \"values\": [0.1, 0.2, 0.3]\n                    }\n                ]\n            },\n            \"description\": \"A deep learning model that employs a hierarchical architecture for sentiment analysis on text data, with a focus on optimizing precision and RMSE through grid search.\"\n        }\n    ],\n    \"knowledge\": [\n        \"High precision is crucial to ensure accurate sentiment differentiation.\",\n        \"Hierarchical model design for multi-level classification\",\n        \"Grid search and hyperparameter tuning for optimal RMSE performance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"TF-IDF or word embeddings for feature extraction\",\n            \"Hierarchical label encoding\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A detailed report documenting the preprocessing, feature extraction, and evaluation process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the project manager's task, specifying the user's intent as building a model, their expertise, and the problem details (area, task, performance metrics). Preprocessing steps and a hierarchical model design are derived from the instruction. However, specific metrics values are left to be determined through model testing. The source of the dataset is assumed to be a link provided by the user since 'user-link' is mentioned, and the pipeline is outlined but not fully detailed.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As a project manager for a data integrity audit, you need to guide the machine learning team in analyzing a three-year town dataset with hourly power consumption, revealing subtle outliers due to equipment failures, extreme weather events, or potential fraud. Request a step-by-step approach using advanced time-series decomposition techniques and anomaly detection models, accounting for seasonality, trends, and autocorrelation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"business analytics\",\n        \"description\": \"As a project manager for a data integrity audit, the goal is to analyze a three-year town dataset with hourly power consumption data, identifying outliers due to equipment failures, extreme weather events, or potential fraud. The approach should involve step-by-step decomposition techniques and advanced anomaly detection models, accounting for seasonality, trends, and autocorrelation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Three-Year Town Power Consumption Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"sample_rate\": 1,\n                \"time_period\": \"hourly\",\n                \"data_points\": \"three years\",\n                \"features\": \"power consumption\"\n            },\n            \"description\": \"A three-year hourly power consumption dataset with potential outliers due to equipment failures, extreme weather, and fraud patterns.\",\n            \"preprocessing\": [\"Decomposition into trend, seasonality, and residuals\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Anomaly detection visualization after decomposition and modeling\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Time-series decomposition models (e.g., STL, SARIMA, Exponential Smoothing)\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"Models for handling seasonality, trend, and autocorrelation in hourly power consumption data.\"\n        },\n        {\n            \"name\": \"\",\n            \"family\": \"Anomaly Detection Algorithms (e.g., Isolation Forest, Local Outlier Factor, Autoencoder-based)\",\n            \"type\": \"ensemble models\",\n            \"specification\": null,\n            \"description\": \"Advanced models to identify anomalies in the deconstructed and decomposed data\"\n        }\n    ],\n    \"knowledge\": [\n        \"Account for seasonality, trends, and autocorrelation in detecting outliers\",\n        \"Handle potential cascading effects in equipment failures and extreme weather events\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Handling missing data, normalization, and feature engineering for anomaly detection\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Real-time or batch processing, depending on system requirements\"],\n        \"demonstration\": \"Present a report highlighting anomalies and plausible explanations for each detected event\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been structured based on the provided instructions, with a clear focus on building a project plan for anomaly detection in a time-series dataset. The user's high expertise level is reflected, and the problem description, areas, downstream tasks, and performance/complexity metrics align closely with the requirements. However, specific models and performance metrics like accuracy, precision, and recall are yet to be provided, as they would typically be based on the team's preference or prior experience.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for an AI-driven cat image classification system, develop a specialized deep learning architecture that can not only identify species but also distinguish between subspecies, breed variations, and environmental factors affecting the appearance of cats. Provide a detailed workflow, including transfer learning techniques, required input preprocessing, and evaluation metrics to ensure high accuracy in a time-limited project.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"pet industry or wildlife conservation\",\n        \"description\": \"Develop a specialized deep learning architecture for an AI-driven cat image classification system, targeting species identification, subspecies differentiation, breed variations, and environmental factor recognition.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"species classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"subspecies classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"breed classification accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cat Image Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\", \"subspecies\", \"breed\", \"environmental_factors\"],\n            \"specification\": {\n                \"total_images\": null,\n                \"image_size\": null,\n                \"label_distribution\": null\n            },\n            \"description\": \"A diverse dataset containing cat images with varying species, subspecies, breeds, and environmental conditions.\",\n            \"preprocessing\": [\n                \"Image resizing\",\n                \"Normalization\",\n                \"Data augmentation\"\n            ],\n            \"augmentation\": [\n                \"Random cropping\",\n                \"Color jitter\",\n                \"Gaussian blur\"\n            ],\n            \"visualization\": [\n                \"Image distribution analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Cat Image Classification Architecture\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Specialized deep learning architecture\",\n                \"transfer_learning\": true,\n                \"pre-trained_model\": \"VGG16, ResNet, or other suitable baseline\"\n            },\n            \"description\": \"A customized deep neural network designed for fine-grained cat image classification, incorporating transfer learning and potentially multi-task learning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning from a generic base model to accelerate training and improve accuracy.\",\n        \"Applying multi-task learning to leverage shared representations among different classification tasks.\",\n        \"Data preprocessing is crucial for high-resolution and diverse images.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature extraction from raw images\",\n            \"Feature fusion for improved performance\"\n        ],\n        \"target_device\": \"cloud-based GPU servers for faster training\",\n        \"deployment_endpoint\": \"Web-based user interface for API access and model serving\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Keras inference\"],\n        \"demonstration\": \"Demo showcasing various scenarios and real-time prediction\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the details provided in the instruction, including the specific task of developing a deep learning architecture, the target performance metrics, and the areas of transfer learning, multi-task learning, and preprocessing. However, the exact performance metrics and feature specifications are not provided, leaving space for customization based on the project's requirements and dataset characteristics. The target device is assumed to be cloud-based due to the complexity of the tasks and available compute resources.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a highly specialized node classification model using the NELL dataset, employing a state-of-the-art Hypergraph Neural Network with nested layers. Ensure the model can handle complex, sparse connectivity and noise resilience, targeting an accuracy of at least 90%. Perform an exhaustive hyperparameter optimization study, documenting not only the optimal settings but also their impact on model performance across multiple graph metrics and real-world edge variations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic/research\",\n        \"description\": \"Develop a highly specialized node classification model using the NELL dataset with a state-of-the-art Hypergraph Neural Network featuring nested layers. The model must handle complex, sparse connectivity and be noise-resistant, targeting an accuracy of at least 90%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"large\",\n                \"sparse_connectivity\": true,\n                \"edge_variations\": \"real-world\"\n            },\n            \"description\": \"A complex and sparse graph dataset for node classification tasks with real-world edge variations.\",\n            \"preprocessing\": [\"sparse data handling\", \"edge normalization\"],\n            \"augmentation\": [\"node attribute augmentation\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hypergraph Neural Network with nested layers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"nested_layers\": true,\n                \"noise_reduction_techniques\": [\"dropout\", \"regularization\"]\n            },\n            \"description\": \"An advanced model tailored to handle complex, sparse connectivity and noise in the NELL dataset for node classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art Hypergraph Neural Networks with nested layers are expected to offer better performance and handling of complex structures.\",\n        \"Hyperparameter optimization and its impact on performance must be thoroughly studied and documented.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection for sparse data\",\n            \"categorical feature encoding\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"include detailed model performance across different edge variations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response follows the schema, with the user's intent to build a model and high expertise level clearly indicated. The problem details, such as the target accuracy and requirement for complex graph handling, are captured. The dataset and model specifications are tailored based on the instruction. The hyperparameter optimization and its impact on performance are emphasized, which is a significant part of the instruction. However, some details like edge variation types in the preprocessing phase are assumed based on the phrase 'real-world edge variations'.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "As a multi-disciplinary project manager, request a detailed analysis from the AI team: Investigate and compare various datasets like Hubble Deep Field, Galaxy Zoo, and SDSS, discussing their suitability for space image classification. Evaluate the challenges in preprocessing, feature extraction (CNN models), and potential ensemble methods to maximize accuracy. Provide a realistic accuracy benchmark and a timeline for each step.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Investigate and compare the suitability of Hubble Deep Field, Galaxy Zoo, and SDSS datasets for space image classification. Discuss challenges in preprocessing, feature extraction using CNN models, and suggest ensemble methods to optimize accuracy.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"preprocessing time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"feature extraction time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"ensemble method efficiency\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hubble Deep Field\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"preprocessing_steps\": null\n            },\n            \"description\": \"A space image dataset suitable for classification\",\n            \"preprocessing\": [\"Remove noise\", \"registration\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"Galaxy Zoo\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"preprocessing_steps\": null\n            },\n            \"description\": \"A community-driven dataset for astronomical image classification\",\n            \"preprocessing\": [\"Annotation-based cleaning\", \"standardization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"SDSS\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"preprocessing_steps\": null\n            },\n            \"description\": \"Sloan Digital Sky Survey for space image classification tasks\",\n            \"preprocessing\": [\"image cropping\", \"cosmic ray removal\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"CNN (various architectures)\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"VGG\", \"ResNet\", \"Inception\", \"Transfer Learning\"],\n                \"performance_tradeoffs\": [\"efficiency\", \"accuracy\"]\n            },\n            \"description\": \"A detailed exploration of CNN models' performance on each dataset\"\n        }\n    ],\n    \"knowledge\": [\n        \"Challenges in space image data can include cosmic ray removal, irregular backgrounds, and overlapping objects.\",\n        \"Suitability of datasets can vary depending on object distribution, resolution, and annotation quality.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Multi-scale feature extraction\",\n            \"Scale-invariant feature transforms\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorRT\", \"OpenVINO\"],\n        \"demonstration\": \"A detailed report with accuracy benchmark tables and timelines\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the consult-build intent with the high expertise level, focusing on the space image classification problem. It reflects the need for a detailed analysis of datasets, preprocessing, feature extraction, ensemble methods, and realistic accuracy benchmarks. The time-sensitive nature is captured in the complexity metrics, and the data modalities and sources are based on the datasets mentioned. The lack of specific performance metrics (set to null) indicates that these are to be derived or negotiated with the AI team.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a machine learning project manager, your team must now design a transformer-based multilingual system capable of translating not only English and Japanese but also Spanish and German. The Common Crawl dataset must be leveraged, and the model must excel in local dialects, boasting a precision of at least 98% for informal colloquial expressions across all four languages.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Design a transformer-based multilingual system that can translate between English, Japanese, Spanish, and German. The model should be specialized in local dialects, targeting precision of at least 98% for informal colloquial expressions across all four languages using the Common Crawl dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"English\", \"Japanese\", \"Spanish\", \"German\"],\n                \"dialect_coverage\": \"local colloquial expressions\"\n            },\n            \"description\": \"A multilingual corpus from Common Crawl, capturing informal language for training the model.\",\n            \"preprocessing\": [\"language detection\", \"text normalization\"],\n            \"augmentation\": [\"back-translation\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based Multilingual System\",\n            \"family\": \"Transformer (specifically Multilingual BERT or similar)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"language support\": [\"English\", \"Japanese\", \"Spanish\", \"German\"],\n                \"dialect understanding\": true\n            },\n            \"description\": \"A state-of-the-art multilingual neural machine translation model, specifically tailored for handling local dialects.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transformer models are effective in handling multiple languages due to their encoder-decoder architecture.\",\n        \"Leveraging Common Crawl for training helps in capturing real-world colloquial expressions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"lexical matching for dialects\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"User-friendly interface for multilingual translations with custom dialect support\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's project management intent and the problem requirements. The areas, tasks, and metrics are clearly defined, with the problem description and performance metric values based on the instruction. The Common Crawl dataset is suitable, and specific preprocessing and augmentation techniques are mentioned. The model type and family are in line with the task, and additional knowledge points related to transformers and dialects are included. The service requirements, such as deployment and inference, are also outlined. The confidence score is high given the clarity of the mapping to the schema.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the AI-driven text classification team, your goal is to enhance the state-of-the-art model using the novel DAIGT dataset. The challenge requires not only binary classification (0 for human-originated, 1 for AI-generated) but also distinguishing subtle nuances in machine-generated content. Prioritize multi-label accuracy, model interpretability, and robustness across various text domains. Additionally, incorporate error analysis and perform extensive A/B testing on real-world data to optimize performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI-driven text classification\",\n        \"description\": \"Enhance the state-of-the-art model for binary classification (human-originated vs AI-generated) and subtle nuances in machine-generated content using the novel DAIGT dataset. Prioritize multi-label accuracy, model interpretability, and robustness across various text domains. Include error analysis and extensive A/B testing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"multi-label accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"binary classification\", \"subtle nuances\"],\n            \"specification\": {\n                \"data_distribution\": \"not specified\",\n                \"text Domains\": \"various\"\n            },\n            \"description\": \"A novel dataset for text classification tasks with a focus on human-AI distinction and multi-label nuances.\",\n            \"preprocessing\": [\"novel dataset preprocessing steps\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high priority\",\n                \"robustness\": \"across various domains\"\n            },\n            \"description\": \"An advanced neural network model for text classification, designed to address binary classification and nuanced differentiation of AI-generated content\"\n        }\n    ],\n    \"knowledge\": [\n        \"DAIGT dataset emphasizes multi-label classification and capturing subtle nuances\",\n        \"Model interpretability is a key requirement\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"error analysis\", \"A/B testing\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-world data\"],\n        \"demonstration\": \"Optimized performance through A/B testing\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the provided instruction, with a focus on the user's intent to enhance a model, their expertise level, and the specific requirements for the text classification task, dataset, and model. The performance metrics are not specified, as they depend on the model's performance. The priority of multi-label accuracy, model interpretability, and robustness is clearly mentioned. The dataset is assumed to be user-uploaded, and preprocessing steps are suggested. The A/B testing and error analysis in the service section are derived from the request for real-world performance optimization.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "text classification", "instruction": "As a project manager for the advanced text classification project, you've been assigned the 20 Newsgroups dataset, consisting of diverse threads. The challenge now includes comparing and evaluating five advanced models (BERT, SVM, LSTM, FastText, and GNN) based on their performance, interpretability, and efficiency. Estimate the training time for each model, considering GPU utilization and fine-tuning nuances, while adhering to a tight deadline.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Compare and evaluate five advanced models (BERT, SVM, LSTM, FastText, and GNN) on the 20 Newsgroups dataset for text classification. The focus is on performance, interpretability, and efficiency, while considering training time with GPU utilization and fine-tuning nuances under a tight deadline.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"efficiency (F1 or runtime)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time with GPU utilization\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"category\"],\n            \"specification\": null,\n            \"description\": \"A diverse dataset containing threads from multiple newsgroups for text classification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"State-of-the-art transformer-based model for text classification.\"\n        },\n        {\n            \"name\": \"SVM\",\n            \"family\": \"Support Vector Machines\",\n            \"type\": \"classical machine learning\",\n            \"specification\": null,\n            \"description\": \"Kernel-based machine learning model for text classification.\"\n        },\n        {\n            \"name\": \"LSTM\",\n            \"family\": \"Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"Long Short-Term Memory for sequence-based text classification.\"\n        },\n        {\n            \"name\": \"FastText\",\n            \"family\": \"Word Embeddings\",\n            \"type\": \"statistical models\",\n            \"specification\": null,\n            \"description\": \"Efficient and distributed model for text classification using word embeddings.\"\n        },\n        {\n            \"name\": \"GNN\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"Graph-based model designed for capturing structural information in text data.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"model fine-tuning with GPU optimization\",\n            \"comparison across models\"\n        ],\n        \"target_device\": \"GPU\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"description\": \"Performance comparison graphs and interpretability analysis for each model\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's task, reflecting the need to build a machine learning project that evaluates multiple models on a given dataset. The areas of interest (performance, interpretability, and efficiency) are reflected in the performance and complexity metrics. However, specific performance targets or training time estimates are not provided as they typically depend on model complexity, dataset size, and hardware configurations. The confidence score is high as the core details are captured, but may need to be adjusted based on further context or specific project requirements.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As the machine learning development team, your task is to design an advanced image-categorization model that, in addition to sorting customer photos based on their content into 'testimonials', 'follow-up required', and 'gallery-ready', must also analyze the contextual context of the photo, such as product interactions, user engagement, and background relevance. Aim for at least 95% accuracy while maintaining real-time processing capabilities to keep up with the high volume of incoming submissions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"e-commerce or marketing\",\n        \"description\": \"Design an advanced image categorization model for customer photos, categorizing them as 'testimonials', 'follow-up required', and 'gallery-ready'. The model should also analyze context, including product interactions, user engagement, and background relevance, while aiming for at least 95% accuracy and maintaining real-time processing capabilities.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"real-time processing\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": \"optimized for real-time processing\",\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Custom Image Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"content\", \"contextual_context\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": \"variable\",\n                    \"height\": \"variable\"\n                },\n                \"label_distribution\": {\n                    \"testimonials\": null,\n                    \"follow-up_required\": null,\n                    \"gallery-ready\": null\n                },\n                \"contextual_classes\": {\n                    \"product_interaction\": \"\",\n                    \"user_engagement\": \"\",\n                    \"background_relevance\": \"\"\n                }\n            },\n            \"description\": \"A large volume of customer photos with varying content and contextual context for training the advanced image classification model.\",\n            \"preprocessing\": [\"image resizing\", \"color normalization\"],\n            \"augmentation\": [\"random cropping\", \"color jittering\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Convolutional Neural Networks (CNN) with context-aware layers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"EfficientNet, ResNet, or a customized variant\",\n                \"optimizer\": \"optimized for real-time\",\n                \"contextual_analysis\": true\n            },\n            \"description\": \"A state-of-the-art image categorization model designed specifically for high accuracy and real-time performance, incorporating context analysis for complex photo understanding.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Deep learning techniques, such as transfer learning or progressive fine-tuning, can help in achieving real-time performance.\",\n        \"Utilizing pre-trained models and customizing them for the specific task is crucial.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or edge computing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"CUDA-optimized inference libraries\"],\n        \"demonstration\": \"Interactive demos showcasing contextual context analysis and model performance.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the user's requirement for an advanced image classification model that caters to both standard and contextual context. The specified accuracy, real-time processing, and context analysis align with the 'high' expertise level provided by the user. The dataset and model sections cover the required details, and a high confidence score is given since the instruction provides clear directions. However, the specific distribution of labels within the dataset and the exact optimization methods are not given; these should be inferred or provided by the user.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager needs to oversee a mobile app-integrated fashion item segmentation system using a custom-built EfficientNet-based FCN. The tool must handle complex, overlapping clothing in the DeepFashion2 dataset, targeting 75%+ accuracy, and must adhere to stringent real-time latency constraints of less than 200ms for seamless user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"mobile technology\",\n        \"description\": \"Oversee a mobile app-integrated fashion item segmentation system using a custom-built EfficientNet-based FCN. The system should handle complex, overlapping clothing in the DeepFashion2 dataset and aim for an accuracy of 75% or higher, while maintaining a real-time latency of less than 200ms.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.75\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 200,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepFashion2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"complexity\": \"complex\",\n                \"overlap\": \"true\"\n            },\n            \"description\": \"A challenging dataset for fashion item segmentation with complex, overlapping clothing instances.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom EfficientNet-based FCN\",\n            \"family\": \"EfficientNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"FCN\",\n                \"latency_constraint\": true\n            },\n            \"description\": \"A custom-developed neural network model using EfficientNet as a foundation for fashion item segmentation in real-time, with emphasis on accuracy and low latency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The dataset's complexity requires advanced handling of overlapping clothing instances.\",\n        \"The real-time latency constraint demands efficient model architecture and inference optimizations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"seamless user experience\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the project manager's requirements. The intent to build a model and the high level of expertise are captured. The area is computer vision, task is image segmentation, and the DeepFashion2 dataset is a direct match. The accuracy and latency constraints are clearly specified. However, specific details on how the EfficientNet and FCN are integrated are not provided, which may be addressed by further inquiry or refinement based on the project manager's level of expertise.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "Develop a sophisticated recommendation system using the 'Trivago Hotel Recommendation' dataset, which necessitates not only the creation of a Graph Neural Network model but also the incorporation of advanced spatio-temporal feature extraction. Emphasize on optimizing real-time performance and personalization, ensuring seamless in-app suggestions are provided with minimal latency while accounting for regional trends and user history.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Develop a sophisticated recommendation system using the 'Trivago Hotel Recommendation' dataset, involving a Graph Neural Network model that incorporates advanced spatio-temporal feature extraction. Focus on optimizing real-time performance and personalization for in-app suggestions with minimal latency, considering regional trends and user history.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"personalization accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Trivago Hotel Recommendation\",\n            \"modality\": [\n                \"graph\",\n                \"time series\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": null,\n                \"time_series_length\": null\n            },\n            \"description\": \"A dataset for recommendation system incorporating spatio-temporal data with hotel data for both graph neural network and sequential recommendation\",\n            \"preprocessing\": [\n                \"spatio-temporal feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"user behavior analysis\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Graph Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"incorporating spatio-temporal features\",\n                \"optimization focus\": \"real-time performance and personalization\"\n            },\n            \"description\": \"A GNN model designed for sequential recommendation in a hotel recommendation scenario, with emphasis on real-time execution and personalized recommendations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The Trivago dataset contains spatial and temporal data, which demands the use of GNNs and advanced spatio-temporal feature extraction.\",\n        \"Spatio-temporal trends and user history play a significant role in the recommendation strategy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"real-time feature update\",\n            \"personalization algorithms\"\n        ],\n        \"target_device\": \"mobile and cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-optimized engines\",\n            \"lightweight models for mobile devices\"\n        ],\n        \"demonstration\": \"context-aware recommendations during in-app interactions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the instruction, including the user's build intent, the need for a Graph Neural Network in a recommendation system, and the importance of spatio-temporal features. Performance metrics such as real-time performance and personalization are mentioned, but exact values aren't provided as they'd typically be derived from experimentation. The model, dataset, and service requirements are derived from the task description. The confidence score is lower due to the lack of specific metrics and personalization algorithms.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "image classification", "instruction": "As a seasoned project manager for the dermatology AI initiative, your goal is to develop a highly efficient and state-of-the-art skin cancer classification system using EfficientNet. The HAM10000 dataset requires meticulous preprocessing, considering variations in lighting and image quality. The model must achieve a robust accuracy of at least 98%, with a stringent focus on reducing false negatives by 95% to minimize diagnostic errors in early-stage cancers. Moreover, the model must comply with Explainable AI principles, delivering interpretable explanations to facilitate expert review by dermatologists, ensuring trust and clinical validity in every prediction.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"healthcare, dermatology\",\n        \"description\": \"Develop a highly efficient and state-of-the-art skin cancer classification system using EfficientNet on the HAM10000 dataset. The model must achieve at least 98% accuracy, with a focus on reducing false negatives by 95% to minimize diagnostic errors in early-stage cancers. It should adhere to Explainable AI principles for interpretability by dermatologists.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"false negative reduction\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HAM10000\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data preprocessing\": \"accounting for variations in lighting and image quality\",\n                \"dimension\": {\n                    \"image size\": \"to match EfficientNet requirements\"\n                }\n            },\n            \"description\": \"A skin cancer dataset requiring meticulous preprocessing for accurate predictions.\",\n            \"preprocessing\": [\n                \"lighting normalization\",\n                \"image quality enhancement\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"compliant with Explainable AI principles\"\n            },\n            \"description\": \"A state-of-the-art image classification model, optimized for skin cancer detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model must be robust against variations in lighting and image quality due to the HAM10000 dataset characteristics.\",\n        \"Explainable AI is crucial for expert dermatologists' review, enhancing trust in predictions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \" Explainable AI components for model interpretability\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Facilitation of expert dermatologist review with model explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key elements of the project, reflecting the user's intent, their expertise level, the area and task, dataset requirements, the EfficientNet model choice, and the emphasis on explainability. Metrics like accuracy and false negative reduction are clearly specified, and dataset preprocessing steps are outlined. The lack of specific feature engineering details may lead to a lower confidence score, but the overall structure is valid.\",\n        \"score\": 0.90\n    }\n}"}
{"task": "machine translation", "instruction": "As a cutting-edge startup looking to develop a real-time, multilingual platform for translating legal contracts between Korean and English, the machine learning team is tasked with designing an adaptive, context-aware model. The system should not only handle informal web novels but also dense, technical legal jargon with high accuracy. Users demand minimal latency, maintaining the nuances of the source text. Additionally, the platform must incorporate a continuous learning feature to adapt to legal updates and ensure ever-improving translations. Please outline the necessary data preprocessing, model selection, and infrastructure considerations for this specialized project.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"legal\",\n        \"description\": \"Develop a real-time, multilingual translation platform for Korean and English legal contracts, capable of handling informal web novels and dense legal jargon with high accuracy, minimal latency, and continuous learning for adapting to legal updates.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"minimal\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"optimized\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"adaptability\",\n                \"value\": \"continuous\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Legal Contract Corpus (Korean-English)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"language_types\": [\"Korean\", \"English\"],\n                \"data_types\": [\"legal\", \"web novels\"],\n                \"size\": \"sufficient for domain-specific training\"\n            },\n            \"description\": \"A large corpus containing diverse legal and informal texts, reflecting the wide range of content to be translated.\",\n            \"preprocessing\": [\n                \"tokenization\",\n                \"lemmatization\",\n                \"stemming\",\n                \"removal of stop words\",\n                \"custom legal term extraction\"\n            ],\n            \"augmentation\": [\n                \"contextual back-translation\",\n                \"domain-specific noise injection\"\n            ],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Adaptive Neural Machine Translation Model\",\n            \"family\": \"Transformer with Attention Mechanisms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"context-aware, seq2seq\",\n                \"encoder-decoder design\": \"multi-head attention, positional encoding\",\n                \"adaptation strategy\": \"incremental learning\"\n            },\n            \"description\": \"A transformer-based model customized for legal translation, incorporating contextual awareness and continuous learning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Legal jargon and context sensitivity need specialized NLP techniques\",\n        \"Transformer models are suitable for multilingual translation tasks\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"legal term embeddings\",\n            \"sentence structure analysis\"\n        ],\n        \"target_device\": \"real-time, low-latency cloud infrastructure\",\n        \"deployment_endpoint\": \"highly scalable, dedicated translation API\",\n        \"inference_engine\": [\"GPU-optimized serving framework\"],\n        \"demonstration\": \"include examples of adaptation to new legal texts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is parsed based on the given instruction, capturing the user's intent to build a model, high expertise level, and the problem's nature in natural language processing. It includes a description of the legal translation task, performance metrics (accuracy and latency), and complexity metrics. The dataset details are relevant, considering the types of texts and the need for preprocessing, augmentation, and legal term handling. The model selection and requirements, including continuous learning, are justified. The infrastructure considerations emphasize the real-time and low-latency requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for the advanced inventory management system, request a state-of-the-art machine learning model, specifically a customized Faster R-CNN architecture, trained on the 'Extended SKU-110K' dataset with enhanced data augmentation. The model should excel in fine-grained product recognition on diverse shelves, ensuring high precision (minimum 95%) to minimize discrepancies. Additionally, mandate a comprehensive performance analysis, comparing precision and recall metrics for at least 15 distinct product categories, accompanied by a user-friendly interactive dashboard for live monitoring and trend visualization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"inventory management\",\n        \"description\": \"Request a state-of-the-art machine learning model, specifically a customized Faster R-CNN architecture, trained on the 'Extended SKU-110K' dataset. The model should excel in fine-grained product recognition on diverse shelves, targeting high precision (minimum 95%) to minimize discrepancies in inventory.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extended SKU-110K\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"augmentation\": \"enhanced\"\n            },\n            \"description\": \"A dataset for fine-grained product recognition, specifically designed for inventory management, with a focus on the Extended SKU-110K dataset.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"faster_rcnn_augmentation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Faster R-CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"customized\"\n            },\n            \"description\": \"A state-of-the-art machine learning model for object detection, with a focus on fine-grained product recognition and high precision in diverse shelf environments.\"\n        }\n    ],\n    \"knowledge\": [\n        \"A Faster R-CNN architecture is suitable for fine-grained product recognition.\",\n        \"High precision is crucial for inventory management, especially with a minimum target of 95%.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"dashboard\": {\n                \"features\": \"live monitoring and trend visualization\",\n                \"category_count\": 15\n            }\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been tailored to the provided instruction, covering the user's intent for building a model, the requested dataset, and the high-level requirements such as precision and diverse shelf recognition. The model type is specified (Faster R-CNN), the customization level is mentioned, and the dataset is described as the 'Extended SKU-110K' for fine-grained recognition. The performance analysis is outlined, with a focus on precision and recall for 15 categories. The user's high expertise level is also indicated, and a user-friendly interactive dashboard is mentioned. However, specific performance metric values, preprocessing details, and some additional engineering tasks are not provided, as they are not explicitly mentioned in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for the \"Dog Breed Identifier App\" project, your team has been tasked with utilizing the Stanford Dogs Dataset to develop an efficient and accurate real-time application. In light of rapidly evolving technology, explore the trade-offs between employing a time-efficient, entry-level algorithm (like linear or k-Nearest Neighbors) and a more sophisticated deep learning model (CNN) that might offer higher accuracy but demands more computational resources. Conduct a thorough evaluation of computational speed, model performance, and potential for on-device image recognition, ensuring the app's user experience meets strict latency requirements of less than 500 milliseconds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"mobile or IoT devices (app-based)\",\n        \"description\": \"Develop a Dog Breed Identifier App using the Stanford Dogs Dataset. Evaluate trade-offs between a time-efficient algorithm (linear or k-Nearest Neighbors) and a more accurate deep learning model (CNN) considering computational speed, model performance, and on-device image recognition with a latency constraint of less than 500 milliseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational speed\",\n                \"value\": {\"comparison\": \"time-efficient vs deep learning\", \"unit\": \"milliseconds\"}\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": {\"threshold\": 500, \"unit\": \"milliseconds\"},\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"computational resources\",\n                \"value\": {\"time-efficient\": \"low\", \"CNN\": \"high\"}\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Dogs Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\"],\n            \"specification\": null,\n            \"description\": \"A dataset for real-time image classification of dog breeds.\",\n            \"preprocessing\": [\"image resizing\", \"normalization\"],\n            \"augmentation\": [\"data augmentation techniques for on-device training\"],\n            \"visualization\": [\"processing time vs accuracy curves\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": [\"Linear Model\", \"k-Nearest Neighbors\", \"CNN\"],\n            \"family\": [\"Linear Regression\", \"k-NN\", \"Convolutional Neural Networks\"],\n            \"type\": [\"classical machine learning\", \"traditional ML\", \"neural networks\"],\n            \"specification\": {\n                \"linear\": {\n                    \"parameters\": \"minimal\",\n                    \"inference_speed\": \"high\"\n                },\n                \"kNN\": {\n                    \"distance_metric\": \"euclidean\",\n                    \"neighbors\": \"optimized for speed\"\n                },\n                \"CNN\": {\n                    \"model_depth\": \"optimizable for performance\",\n                    \"flops\": \"high\"\n                }\n            },\n            \"description\": \"Exploration of various model architectures and their trade-offs.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile or IoT devices\",\n        \"deployment_endpoint\": \"unknown, likely cloud-based\",\n        \"inference_engine\": [\"real-time, on-device\", \"cloud-based\"],\n        \"demonstration\": \"Focus on fast and accurate image recognition in real-time.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the project's goal, user intent, expert level, and problem domain. The trade-offs, performance metrics, and complexity constraints are appropriately modeled. However, specific model accuracy values and optimal algorithm choices are not provided, as they are context-dependent. The app's latency requirements are clearly defined, and the dataset and preprocessing steps are reasonable based on the task.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for a high-profile knowledge management platform, you're tasked with developing a cutting-edge Q&A system. This must utilize the extensive TriviaQA dataset and combine it with next-generation transformers, like the recently advanced GPT-4, to address diverse user queries - from basic facts to intricate multi-step reasoning. Your design should scale to handle substantial user traffic, prioritize efficiency, targeting sub-second response times even under heavy load, while maintaining a seamless user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"knowledge management\",\n        \"description\": \"Develop a cutting-edge Q&A system for a high-profile platform, leveraging the TriviaQA dataset and GPT-4 transformers, supporting diverse queries from basic facts to multi-step reasoning. The system must scale to high traffic, prioritize efficiency with sub-second response times, and maintain a seamless user experience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": \"sub-second\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"efficiency\",\n                \"value\": null,\n                \"unit\": \"response time\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high\",\n                \"unit\": \"user traffic\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TriviaQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_size\": \"extensive\",\n                \"question_types\": [\"basic facts\", \"multi-step reasoning\"]\n            },\n            \"description\": \"A comprehensive dataset for question answering, including diverse and challenging questions.\",\n            \"preprocessing\": [\"GPT-4 integration\", \"tokenizer\"], \n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"GPT-4\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"transformer\",\n                \"memory_consumption\": \"optimized for scalability\",\n                \"training_data\": \"TriviaQA dataset\"\n            },\n            \"description\": \"A state-of-the-art transformer model, specifically GPT-4, adapted for a high-performance Q&A system.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Leveraging GPT-4's advanced capabilities for handling complex queries.\",\n        \"Sub-second response times are crucial for scalability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual indexing\", \"query optimization\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"high-traffic hosting service\",\n        \"inference_engine\": [\"optimized for parallel processing\"],\n        \"demonstration\": \"real-time, zero-latency demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction was parsed into a JSON response based on the given schema. The intent to build a model is clear, and the system's requirements are described for a high-profile knowledge management platform. The chosen area and downstream task, as well as the application domain, are appropriate. Performance metrics (response time) and complexity metrics (efficiency and scalability) are included. The use of the TriviaQA dataset and GPT-4 are justified. The schema follows closely, and I have inferred some steps, such as preprocessing, given the missing explicit details.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "As a project manager for the advanced AI node classification project on the CORA dataset, I require a state-of-the-art Graph Convolutional Network (GCN) model with attention mechanisms. The model must exhibit a minimum accuracy of 88% and stability against both underfitting and overfitting. Additionally, optimize for efficiency, aiming for at least 95% parallelism during training and impose a constraint of not exceeding 1.5 hours training time on a high-end, scalable GPU system. Please include detailed documentation on model interpretability and hyperparameter tuning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"research and development\",\n        \"description\": \"As a project manager, I require a state-of-the-art Graph Convolutional Network (GCN) model with attention mechanisms for the advanced AI node classification on the CORA dataset. The model must have a minimum accuracy of 88%, be stable against underfitting and overfitting, and prioritize efficiency with at least 95% parallelism during training. Additionally, please include detailed documentation on model interpretability and hyperparameter tuning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.88\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"parallelism during training\",\n                \"value\": 0.95,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"training time\",\n                \"value\": 1.5,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CORA\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"nodes\": null,\n                    \"edges\": null\n                },\n                \"dimensions\": null,\n                \"length\": null\n            },\n            \"description\": \"A dataset for advanced AI node classification tasks, specifically in graph machine learning.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art GCN with Attention Mechanisms\",\n            \"family\": \"Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": null,\n                \"FLOPs\": null,\n                \"train_speed\": \"\",\n                \"inference_time\": null,\n                \"interpretability\": \"documented\",\n                \"hyperparameter_tuning\": \"documented\"\n            },\n            \"description\": \"A specialized GCN model enhanced with attention mechanisms designed for the specific requirements of the advanced AI node classification project on the CORA dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-end, scalable GPU system\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Model interpretability and hyperparameter tuning documentation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the provided instruction, detailing the user's intention (build), expertise, requirements for the GCN model, performance and complexity metrics, and additional details like model type, dataset properties, and service-related specifications. However, specific hyperparameter tuning values or exact training details were not provided in the instruction, which is reflected in the partially null specifications. Model interpretability is mentioned to be 'documented', but without further details as they are not specified in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "For the DBLP citation network in the field of Graph Neural Networks, develop a state-of-the-art GraphSAGE model with EdgeConv layers tailored to low-latency edge feature aggregation. The task is to fine-tune the model for paper classification, targeting a highly competitive F1 score of 0.90, and ensuring scalability on large-scale datasets with limited hardware resources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic\",\n        \"description\": \"Develop a state-of-the-art GraphSAGE model with EdgeConv layers for paper classification on the DBLP citation network in Graph Neural Networks. The model should be optimized for low-latency edge feature aggregation and target a highly competitive F1 score of 0.90, while ensuring scalability on large-scale datasets with limited hardware resources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.90\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DBLP Citation Network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"nodes\": {\"count\": null, \"dimension\": null},\n                \"edges\": {\"count\": null, \"dimension\": null}\n            },\n            \"description\": \"A large-scale citation network dataset within the Graph Neural Networks domain.\",\n            \"preprocessing\": [\n                \"edge feature aggregation with low-latency\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GraphSAGE with EdgeConv layers\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"GraphSAGE with EdgeConv\",\n                \"latency optimization\": \"true\"\n            },\n            \"description\": \"A high-performance GraphSAGE model designed for paper classification in the DBLP citation network, optimized for low-latency edge feature aggregation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art GraphSAGE models should be used for this task.\",\n        \"Low-latency edge feature aggregation is crucial for scalability on limited hardware.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"fine-tuning for paper classification\"],\n        \"target_device\": \"limited hardware resources (low-end GPU, cloud environment with resource constraints)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for edge feature aggregation, possibly leveraging libraries like DGL or PyTorch Geometric\"\n        ],\n        \"demonstration\": \"showcase scalability and competitive F1 score on large-scale datasets\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a model, expert level, and the requirements specific to the DBLP citation network. The problem area, task, target F1 score, and scalability demands are detailed in the 'problem' section. The dataset information, pre-processing steps, and model's specification are well represented. The confidence is high given the clear instruction, however, the exact model parameters and hardware resource consumption have not been explicitly specified and could be filling in.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, your team is tasked with developing a sophisticated deep learning model capable of multi-class image recognition, not only differentiating between cats and dogs but also accurately classifying various breeds within each species with high accuracy, while ensuring real-time performance and compatibility with mobile devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"AI-driven applications\",\n        \"description\": \"Develop a sophisticated deep learning model for multi-class image recognition, focusing on distinguishing between cats and dogs with their respective breeds. The model should achieve high accuracy and operate in real-time for mobile device compatibility.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"breed classification accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"category\", \"breed\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"number_of_classes\": {\n                    \"categories\": 2,\n                    \"breeds\": null\n                }\n            },\n            \"description\": \"A dataset containing images for multi-class recognition, including both cats and dogs with specific breeds.\",\n            \"preprocessing\": [\"image resizing\", \"data augmentation\"],\n            \"augmentation\": [\"random cropping\", \"flipping\"],\n            \"visualization\": [\"dataset statistics\", \"class distribution\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art mobile-friendly architecture like MobileNet, ShuffleNet, or EfficientNet\",\n                \"depth\": null,\n                \"width_multiplier\": null,\n                \"input_shape\": null\n            },\n            \"description\": \"A deep learning model designed for real-time multi-class image recognition, optimized for mobile devices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Mobile device compatibility requires careful architecture selection and model optimization.\",\n        \"Multi-classification with high accuracy might involve using transfer learning or ensemble methods.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Lite\", \"Core ML\"],\n        \"demonstration\": \"Real-time inference on a sample of test images.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the project requirements, including the high-level intent, problem domain, and the focus on a deep learning model for image recognition. The description of the task and metrics are well-formatted based on the schema, with the understanding that specific metrics and model architecture details are left for project execution. The suggested dataset includes preprocessing and augmentation techniques, along with the expected model architecture. However, the exact performance targets (accuracy and breed classification accuracy) are left unspecified as they require detailed development results, and the model name and specific optimization details are open to engineer's choice.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager is requested to develop a cutting-edge, age-restricted Q&A platform, specifically targeting pre-primary and early elementary aged children. The system should utilize a domain-adapted BERT model, tailored for Elementary Minds corpus, optimizing for lucid and simplified responses. The user interface must incorporate intuitive animations and gamified elements, ensuring an immersive and engaging experience for the young learners while leveraging the KidsQA dataset for training.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education, specifically pre-primary and early elementary\",\n        \"description\": \"Develop a cutting-edge Q&A platform for children aged 2 to 7, leveraging the Elementary Minds corpus and a domain-adapted BERT model optimized for lucid and simplified responses. The user interface must incorporate intuitive animations and gamification.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response clarity\",\n                \"value\": {\n                    \"type\": \"relative\",\n                    \"target\": \"high\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": {\n                    \"type\": \"low\"\n                },\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\"],\n            \"specification\": null,\n            \"description\": \"A dataset specifically designed for training the Q&A system on simplified questions and answers for young learners.\",\n            \"preprocessing\": [\"domain adaptation using Elementary Minds corpus\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-based\",\n            \"family\": \"transformers (BERT variant)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptation\": \"Elementary Minds corpus\"\n            },\n            \"description\": \"A domain-adapted BERT model optimized for answering questions from a young child's perspective.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilizing domain-adaptation techniques on BERT is crucial for understanding a simplified language and addressing the specific needs of young learners.\",\n        \"The focus on lucid responses reduces complexity and improves comprehension for young children.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"intuitive animations\", \"gamified elements\"],\n        \"target_device\": \"suitable for PC or mobile, considering the age group\",\n        \"deployment_endpoint\": \"potentially a web or mobile app platform\",\n        \"inference_engine\": [\"optimized for latency-sensitive applications\"],\n        \"demonstration\": \"designed to be engaging and immersive\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has been appropriately mapped to the given instruction, capturing the core aspects of a machine learning project, such as model type, dataset, and user interface requirements. The 'build' intent and 'high' expertise level are clear. Performance metrics have been set with the focus on accuracy and response clarity for young learners. The requirement for animations, gamification, and latency in inference has been captured in the service section. However, specific performance targets or requirements for memory footprint and inference speed have not been provided, so these are set to null or unspecified for now.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the upcoming Cityscapes dataset project, we need a real-time U-Net model with enhanced efficiency. It must handle 30 intricate urban scene classes, targeting 95% precision and maintaining at least 30 FPS on a budget GPU while ensuring compatibility with edge devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban computing, autonomous driving\",\n        \"description\": \"Build a real-time U-Net model for the Cityscapes dataset with enhanced efficiency. The model should handle 30 intricate urban scene classes, aim for 95% precision, maintain at least 30 FPS on a budget GPU, and be compatible with edge devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 30,\n                \"unit\": \"frames per second (FPS)\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": \"compatible with budget GPU\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_classes\": 30,\n                \"scene_types\": \"intricate urban scenes\"\n            },\n            \"description\": \"A dataset focusing on real-world urban scenes with a large number of fine-grained classes.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"U-Net\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"enhanced efficiency\",\n                \"target_device\": \"budget GPU and edge devices\"\n            },\n            \"description\": \"A real-time, efficient U-Net model designed specifically for the Cityscapes dataset with advanced model optimizations for performance and compatibility.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cityscapes poses a high demand for a model that can handle diverse urban scene classes and run in real-time on resource-constrained devices.\",\n        \"Precision of 95% on a budget GPU and compatibility with edge devices are key performance targets.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"budget GPU and edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for edge deployment\"],\n        \"demonstration\": \"Real-time performance demonstration with target precision and FPS\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is accurately parsed according to the schema, reflecting the user's intent to build a model, their high level of expertise, and the specific requirements for the Cityscapes project. Performance metrics are set to the provided targets, complexity metrics are relevant to the hardware constraints, and the dataset information, model family, and type are taken from the instruction. The demonstration and inference engine are specified based on the requirements for real-time and edge device compatibility.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "As a project manager for a machine learning team, your objective is to investigate the feasibility of employing transformer models, specifically designed for advanced text analysis, to develop a sophisticated video forgery detection system. The Celeb-DF dataset must be analyzed with at least 95% accuracy, ensuring the model outperforms baseline algorithms and demonstrates discernible improvements in distinguishing genuine from manipulated videos.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"media forensics\",\n        \"description\": \"Investigate the feasibility of employing transformer models for a video forgery detection system, targeting advanced text analysis, with a focus on achieving at least 95% accuracy and outperforming baseline algorithms on the Celeb-DF dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\"image\", \"audio\", \"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"length\": \"video sequence\",\n                \"dimension\": {\n                    \"audio\": \"samples\",\n                    \"video\": \"frames\"\n                }\n            },\n            \"description\": \"Multimodal dataset for detecting video forgery, containing genuine and manipulated videos of celebrities.\",\n            \"preprocessing\": [\n                \"alignment\",\n                \"feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"example forgery detection instances\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-based Video Forgery Detection\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"specifically designed for text analysis in multimodal context\"\n            },\n            \"description\": \"A transformer model optimized for advanced text analysis within a video forgery detection system\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transformer models can potentially excel in extracting meaningful features from multimodal data for forgery detection.\",\n        \"Outperforming baselines requires careful model tuning and understanding of the Celeb-DF dataset's unique characteristics.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"fusion of audio, text, and video features\"\n        ],\n        \"target_device\": \"high-performance computing environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-based\"],\n        \"demonstration\": \"live demo showcasing improved video forgery detection accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the provided instruction, with high-level user intent for model building, domain expertise, and project management focus. The multimodal nature of the problem, transformer model's focus on text analysis, and the performance metric of 95% accuracy have been clearly defined. The Celeb-DF dataset's modality, target variables (unknown), and the need for preprocessing and feature engineering to align and extract relevant features from different modalities are included. The model's type, specification, and domain for the problem are well-defined. The baseline comparison and the importance of model accuracy in outperforming them are highlighted. However, specific model architecture details and detailed preprocessing steps are not provided as they were not specified in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for a machine learning development team, you're tasked with enhancing the predictive prowess of a regression model for the Mohs Hardness Dataset. The dataset consists of intricate numerical features, organized into non-uniformly distributed train, validation, and test subsets. The objective is to minimize the median absolute error (MedAE) while optimizing model performance. Please devise a multi-step strategy, incorporating feature engineering, hyperparameter tuning, and ensemble techniques, to achieve state-of-the-art results.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"material science\",\n        \"description\": \"Enhance the predictive power of a regression model for the Mohs Hardness Dataset, featuring intricate numerical features with non-uniform distribution. Target is to minimize median absolute error (MedAE) and optimize model performance using a multi-step strategy including feature engineering, hyperparameter tuning, and ensemble techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"median absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mohs Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"distribution\": \"non-uniform\",\n                \"subsets\": [\"train\", \"validation\", \"test\"]\n            },\n            \"description\": \"A dataset with intricate numerical features distributed unevenly across train, validation, and test subsets.\",\n            \"preprocessing\": [\"handling non-uniform data distribution\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"strategy\": [\"feature engineering\", \"hyperparameter tuning\", \"ensemble techniques\"]\n            },\n            \"description\": \"An ensemble model designed to optimize performance and minimize MedAE for the Mohs Hardness Dataset with advanced preprocessing techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Non-uniform distribution of data requires specific handling during preprocessing.\",\n        \"Ensemble models can help in enhancing predictive prowess.\",\n        \"Hyperparameter tuning is crucial for minimizing error metrics.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"advanced feature extraction\", \"handling skewed data\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"CPU-optimized\", \"GPU-accelerated\"],\n        \"demonstration\": \"Visualize improvement with respect to baseline performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is a valid representation of the provided instruction, considering the project management context, the task of enhancing a regression model, and the stated objectives. The user intent is clearly expressed as 'build,' and the high expertise level is assumed given the project management role. The target task is tabular regression, and the dataset is correctly described as a 'Mohs Hardness Dataset' with numerical features and non-uniform distribution. Performance metric (MedAE) is not provided with a value, and the strategy for optimization is outlined. The confidence score is set to a high value of 0.97 due to the clarity of the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "You've been tasked with developing a robust time-series forecasting solution for a comprehensive real-world influenza-like illness (ILI) dataset. The data is divided into train, validation, and test sets, each containing sequences of past observations with a specific history (INPUT_SEQ_LEN=36 and INPUT_DIM=7). Your objective is to design and fine-tune an advanced deep learning model, utilizing techniques like autoencoders or recurrent neural networks, to generate accurate forecasts of the next 24-time steps (PRED_SEQ_LEN=24, PRED_DIM=7). Emphasize on not only minimizing mean squared error (MSE) but also improving mean absolute error (MAE) and capturing seasonality patterns. Additionally, present a detailed analysis of model performance and potential interpretability for stakeholders.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Develop a robust time-series forecasting solution for an ILI dataset with a focus on train, validation, and test sets. The model should be designed using advanced deep learning techniques such as autoencoders or recurrent neural networks, targeting 24-step ahead predictions with INPUT_SEQ_LEN=36 and INPUT_DIM=7. The objective is to minimize MSE, improve MAE, and capture seasonality patterns. Model interpretability for stakeholders is also a key requirement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 36,\n                \"INPUT_DIM\": 7\n            },\n            \"description\": \"A comprehensive real-world dataset on influenza-like illness with train, validation, and test splits, each with a specified history length and feature dimensions.\",\n            \"preprocessing\": [\"sequence normalization\", \"feature scaling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning (autoencoders or RNNs)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"e.g., LSTM with multi-layer architecture\",\n                \"sequence length\": \"PRED_SEQ_LEN=24\",\n                \"target dimensions\": \"PRED_DIM=7\",\n                \"loss function\": \"combined MSE and MAE\"\n            },\n            \"description\": \"A deep learning model designed for time-series forecasting with a focus on capturing seasonality and minimizing prediction errors.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilizing advanced techniques like autoencoders or RNNs is crucial for robust forecasting.\",\n        \"Interpretability for stakeholders is a significant aspect of the project.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"custom seasonal component extraction\",\n            \"feature engineering for capturing seasonality patterns\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Model performance analysis report and interpretability insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a high-complexity solution, with a focus on forecasting and model interpretability. The problem area, task, dataset specifics, and performance metrics are clearly defined. However, specific details on the chosen model architecture and its performance are yet to be determined, as they would depend on the user's preference or experimentation.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "multimodal emotion recognition", "instruction": "As a project manager for a high-accuracy emotion recognition system, you need to evaluate the latest IEMOCAP dataset and compare multiple deep learning architectures (e.g., VGG, LSTM, Transformer-based, and ensemble models) considering cross-cultural sensitivity, real-time performance, and interpretability. Provide a comprehensive report outlining their pros and drawbacks for a time-sensitive deployment in diverse environments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal emotion recognition\",\n        \"application_domain\": \"human-computer interaction, AI ethics\",\n        \"description\": \"Evaluate the latest IEMOCAP dataset for a high-accuracy emotion recognition system, comparing VGG, LSTM, Transformer-based, and ensemble models with a focus on cross-cultural sensitivity, real-time performance, and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cross-cultural sensitivity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"IEMOCAP\",\n            \"modality\": [\"text\", \"audio\", \"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"updated_date\": \"latest\",\n                \"number_of_clips\": null,\n                \"languages\": [\"cross-cultural\"],\n                \"cultural_diversity\": true\n            },\n            \"description\": \"The latest release of the IEMOCAP dataset for emotion recognition, including diverse modalities and cultural context.\",\n            \"preprocessing\": [\n                \"cross-cultural adaptation\",\n                \"emotion annotation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"VGG\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"VGG16 or VGG19\",\n                \"performance on cross-cultural data\": null,\n                \"real-time inference\": null,\n                \"interpretability techniques\": \"\"\n            },\n            \"description\": \"A deep CNN architecture evaluating performance on IEMOCAP, considering cross-cultural sensitivity.\"\n        },\n        {\n            \"name\": \"LSTM\",\n            \"family\": \"Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"sequence modeling\": true,\n                \"cross-cultural adaptation\": null,\n                \"real-time inference speed\": null,\n                \"interpretable components\": []\n            },\n            \"description\": \"An LSTM model examining real-time performance on IEMOCAP, while considering interpretability.\"\n        },\n        {\n            \"name\": \"Transformer-based\",\n            \"family\": \"Transformer Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention mechanism\": true,\n                \"cross-cultural analysis\": null,\n                \"real-time deployment\": null,\n                \"interpretability tools\": \"\"\n            },\n            \"description\": \"A Transformer model evaluating cross-cultural sensitivity and interpretability for IEMOCAP.\"\n        },\n        {\n            \"name\": \"Ensemble models\",\n            \"family\": \"Combination of models\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"diverse architectures\": [\"CNN, LSTM, Transformer\"],\n                \"cross-cultural robustness\": null,\n                \"real-time trade-offs\": null,\n                \"interpretability improvement\": null\n            },\n            \"description\": \"An ensemble combining multiple models for a comprehensive evaluation on IEMOCAP.\"\n        }\n    ],\n    \"knowledge\": [\n        \"cross-cultural adaptation techniques\",\n        \"real-time vs. accuracy trade-offs\",\n        \"evaluating interpretability in deep learning models\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"adaptation to real-time constraints\",\n            \"cross-cultural feature extraction\"\n        ],\n        \"target_device\": \"high-performance computing (HPC) or edge devices\",\n        \"deployment_endpoint\": \"customizable API or cloud service\",\n        \"inference_engine\": [\n            \"TensorFlow, PyTorch, or specific inference libraries\"\n        ],\n        \"demonstration\": \"explainable outputs, performance profiles, and case studies\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the user's intent to consult and build a model, considering multiple architectures. The expertise level is high, indicating the need for in-depth analysis. The problem description, dataset, and model specifications are derived from the given instruction. Performance metrics and complexity constraints, such as cross-cultural sensitivity and real-time performance, have been included. However, specific performance values are not provided as they would require further research or data analysis.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "In light of the competitive real estate market analysis, the project manager is tasked with utilizing the 'Housing Market Dynamics' dataset that spans multiple regions and economic cycles. The development team must architect a Bayesian Ridge Regression model, leveraging its adaptive shrinkage and multi-regime capabilities. The objective is not only to reach a predictive accuracy of 5% or better, but also to deliver probabilistic forecasts with at least 95% confidence intervals, ensuring accurate risk assessments for each property prediction in a time-sensitive market environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"real estate\",\n        \"description\": \"The project involves using the 'Housing Market Dynamics' dataset for a real estate market analysis. A Bayesian Ridge Regression model is required, with a focus on adaptive shrinkage and multi-regime capabilities to achieve predictive accuracy of at least 5% and deliver probabilistic forecasts with 95% confidence intervals for property predictions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"predictive accuracy\",\n                \"value\": 0.05\n            },\n            {\n                \"name\": \"confidence intervals\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Housing Market Dynamics\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"property-related features\"],\n            \"specification\": null,\n            \"description\": \"A dataset covering multiple regions and economic cycles, suitable for real estate market analysis\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Bayesian Ridge Regression\",\n            \"family\": \"neural networks\",\n            \"type\": \"statistical models\",\n            \"specification\": {\n                \"shrinkage\": \"adaptive\",\n                \"multi-regime capability\": true\n            },\n            \"description\": \"A Bayesian model for regression with adaptive shrinkage, designed to analyze the Housing Market Dynamics dataset and provide accurate property predictions.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"probabilistic forecasts with confidence intervals\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a model, with a focus on high-level expertise. The problem domain (real estate) and dataset properties are derived from the housing market analysis context. The required model, Bayesian Ridge Regression, is correctly identified, and its specific features and goal (accuracy and confidence intervals) are clearly stated. The missing information, such as preprocessing or target variables, can be inferred based on typical real estate data preprocessing steps and the nature of the dataset. The confidence score is high due to the clear requirements and matching response.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "For our upcoming AI-driven website analytics project, we seek a sophisticated anomaly detection system. The system must not only monitor real-time traffic variations but also perform multi-level analysis: categorize fluctuations by source, user behavior, and frequency, and provide historical context and potential threat indicators. Expect detailed reports with actionable insights for immediate action.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"web analytics\",\n        \"description\": \"Develop a sophisticated anomaly detection system for real-time website analytics, which categorizes fluctuations by source, user behavior, and frequency. The system should provide historical context and potential threat indicators with actionable insights.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"response time for detailed reports\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Website Traffic Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"Unstructured time-series data capturing real-time website traffic variations for anomaly detection analysis.\",\n            \"preprocessing\": [\n                \"data normalization\",\n                \"feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"time series plots\",\n                \"multi-level analysis visualizations\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Sophisticated Anomaly Detection System\",\n            \"family\": \"time-series anomaly detection algorithms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"deep learning-based\",\n                \"complexity\": \"high\",\n                \"model training speed\": null,\n                \"specific algorithms\": [\"autoencoders\", \"LSTM\"]\n            },\n            \"description\": \"A deep learning model designed for real-time website analytics, capable of multi-level analysis and detection of anomalies.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The system must handle real-time and historical data effectively.\",\n        \"Categorization based on source, user behavior, and frequency is crucial.\",\n        \"Actionable insights and immediate threat detection are requirements.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"behavioral segmentation\",\n            \"contextual modeling\"\n        ],\n        \"target_device\": \"cloud infrastructure\",\n        \"deployment_endpoint\": \"API for real-time monitoring and reports\",\n        \"inference_engine\": [\"custom built or open-source solutions\"],\n        \"demonstration\": \"interactive dashboards for real-time monitoring\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is generated based on the given instruction, with high-level intent to build a sophisticated anomaly detection system. The problem area is specified as time-series analysis, and the expertise level as high. Performance metrics are left unspecified to accommodate various detection strategies. Dataset is described as time-series traffic data with pre-processing steps, and the model type is inferred as neural networks-based, tailored for anomaly detection in the given context. Additional complexities such as inference time, memory size, and response time are included as required. Missing values in the metrics indicate the need for specification based on the model's chosen methodology.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project calls for fine-grained semantic segmentation of DOTA-500, leveraging a Tiny YOLOv4 backbone tailored for urban object extraction. The goal is to precisely segment 500 classes of objects, optimizing for both intricate vehicle models (e.g., electric vs. gasoline cars) and complex infrastructure, with a stringent requirement of real-time performance (under 100ms) on 4K images while maintaining exceptional F1 score.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"Fine-grained semantic segmentation using Tiny YOLOv4 backbone for DOTA-500 dataset, focusing on urban object extraction, with a specific focus on differentiating between intricate vehicle types like electric and gasoline cars, and complex infrastructure. The project requires real-time performance on 4K images with an F1 score constraint of under 100ms.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"average inference time\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": 100\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance (inference time)\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": 100\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DOTA-500\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": {\n                    \"height\": \"4K\",\n                    \"width\": \"4K\"\n                },\n                \"classes\": 500\n            },\n            \"description\": \"A dataset with 500 classes for fine-grained semantic segmentation, focusing on urban objects and vehicle differentiation.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Tiny YOLOv4\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"backbone\": \"Tiny YOLOv4\",\n                \"target_objects\": 500,\n                \"specific_object_classes\": [\"electric cars\", \"gasoline cars\"]\n            },\n            \"description\": \"A Tiny YOLOv4-based model tailored for fine-grained semantic segmentation in the urban context, optimized for real-time performance and complex infrastructure differentiation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Tiny YOLOv4's architecture is crucial for handling a large number of classes and real-time constraints.\",\n        \"Advanced feature extraction for vehicle differentiation, e.g., based on type-specific characteristics.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"GPU optimized for real-time performance\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized C++ or CUDA implementation\"],\n        \"demonstration\": \"4K video demonstration showcasing real-time segmentation with a focus on differentiating vehicles and infrastructure\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the intent to build a model, the user's high expertise level, the area of computer vision, and the specific task of semantic segmentation. It details the requirements on object classes, real-time performance, and complex infrastructure. Performance metrics are set to null, indicating they need to be provided based on model performance. The dataset and model properties are derived from the given instruction, while uncertainties like the F1 score and demonstration are left to be filled based on the model evaluation.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As the project lead for a high-performance ML team, your mandate involves enhancing the predictive accuracy of a regression model on the diverse Mohs Hardness Dataset. The complex data comprises non-linear, skewed features distributed unevenly among train, validation, and test sets. Your challenge is to design an advanced, iterative approach:\n\n1. Conduct an in-depth feature exploration, identifying hidden correlations and interactions.\n2. Implement a staged hyperparameter optimization strategy using Bayesian tuning with cross-validation.\n3. Experiment with multiple feature scaling techniques and dimensionality reduction methods.\n4. Integrate and fine-tune an ensemble of regression models (e.g., gradient boosting, neural networks, and random forests).\n5. Evaluate model performance with a focus on not just MedAE but also area under the ROC curve and time efficiency.\n6. Compare and analyze the impact of model complexity against overfitting risk.\n7. Prepare a detailed report outlining the rationale, chosen techniques, and the resulting improvements in model performance.\n\nRemember to document intermediate results and justify each step to showcase your team's expertise in driving innovation in the field.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"material science\",\n        \"description\": \"Enhance the predictive accuracy of a regression model on the Mohs Hardness Dataset with complex, non-linear, and skewed features distributed among train, validation, and test sets. The project involves an iterative approach with detailed exploration, hyperparameter optimization, feature scaling, ensemble modeling, and performance evaluation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"median absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"area under the ROC curve\",\n                \"value\": null\n            },\n            {\n                \"name\": \"time efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mohs Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"hardness\"],\n            \"specification\": null,\n            \"description\": \"A diverse dataset with non-linear, skewed features and uneven distribution across train, validation, and test sets, specific to material science applications.\",\n            \"preprocessing\": [\n                \"feature exploration\",\n                \"non-linear feature transformations\",\n                \"skewness correction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature interaction visualizations\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": [\"ensemble models\", \"neural networks\", \"gradient boosting\"],\n            \"type\": \"neural networks, ensemble models\",\n            \"specification\": null,\n            \"description\": \"An advanced regression model ensemble, focusing on gradient boosting, neural networks, and random forests, designed for enhancing performance on the complex Mohs Hardness Dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Non-linear feature exploration and interactions are critical in uncovering hidden patterns in the data.\",\n        \"Bayesian tuning with cross-validation aids in efficient hyperparameter optimization.\",\n        \"Combining multiple regression models helps reduce overfitting and improve overall performance.\",\n        \"Careful feature scaling and dimensionality reduction techniques enhance model performance and efficiency.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\",\n            \"feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Step-by-step model improvements and results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a model, their high level of expertise, and the problem domain. The problem description, metrics, and dataset characteristics reflect the given instruction. However, specific techniques and target values for performance metrics have not been included for optimization and evaluation purposes, they are marked as null. Intermediate steps and reasoning are provided in the 'knowledge' and 'service' sections, showcasing the iterative approach and the team's expertise.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "For the specialized DBLP citation network in Quantum Physics, engineer a highly efficient GraphSAGE variant with Hierarchical EdgeConv layers designed for real-time processing in material science. The challenge is to optimize a model that differentiates between subfields with extreme precision, aiming for an outstanding F1 score of 0.95, while demonstrating exceptional resourcefulness in dealing with massive datasets on minimal GPU-equipped workstations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"material science\",\n        \"description\": \"Engineer a GraphSAGE variant with Hierarchical EdgeConv layers for real-time processing in material science, specifically for the specialized DBLP citation network in Quantum Physics. The goal is to achieve high precision, targeting an F1 score of 0.95, and demonstrate efficiency on large datasets on limited GPU-equipped workstations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resourcefulness (on GPU workstations)\",\n                \"value\": \"optimized\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"milliseconds\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory size\",\n                \"unit\": \"MB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DBLP Citation Network (Quantum Physics)\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"nodes\": null,\n                    \"edges\": null\n                },\n                \"density\": null\n            },\n            \"description\": \"A specialized graph dataset for quantum physics, containing the DBLP citation network.\",\n            \"preprocessing\": [\"filtering for material science relevant papers\", \"node embedding extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical GraphSAGE with EdgeConv layers\",\n            \"family\": \"GraphSAGE\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Hierarchical EdgeConv\",\n                \"real-time processing\": true\n            },\n            \"description\": \"A GraphSAGE variant tailored for real-time node classification in material science with hierarchical EdgeConv layers, designed to differentiate subfields in Quantum Physics with high precision.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hierarchical EdgeConv layers enhance feature extraction for complex graphs.\",\n        \"Real-time processing is critical for resource-constrained workstations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection for material science subfields\"],\n        \"target_device\": \"minimal GPU-equipped workstations\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for GPU\"],\n        \"demonstration\": \"demonstrate performance on a sample dataset and showcasing the impact of resource efficiency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent (expert-level model engineering), the task in graph machine learning domain (node classification), and the specific goal (F1 score of 0.95). The dataset and model details are derived from the problem description, and the complexity and resource constraints are directly addressed. The service section accounts for the GPU-equipped workstations and the challenge of dealing with large datasets. Missing specifics like exact model performance metrics and dataset size were left undefined as per the provided information.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "node classification", "instruction": "Design a highly sophisticated node classification system utilizing the NELL dataset, employing a custom-built Hypergraph Neural Network with nested, self-attentive layers. Mandate adaptability to dynamic, real-time edge alterations, noise tolerance, and a requirement to achieve at least 95% precision with varying graph structures. Implement a rigorous multi-objective hyperparameter optimization, documenting the optimal configurations' influence on performance, including edge density, centrality measures, and robustness under synthetic noise injections, with clear visualizations and a comparative analysis of competing algorithms.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic/research\",\n        \"description\": \"Design a highly sophisticated node classification system using the NELL dataset with a custom-built Hypergraph Neural Network that includes nested, self-attentive layers. The model should be adaptable to real-time edge changes, noise-resistant, and must achieve at least 95% precision across varying graph structures. A multi-objective hyperparameter optimization is required, detailing the impact on performance, such as edge density, centrality measures, and robustness under synthetic noise. Clear visualizations and a comparative analysis of competing algorithms are mandatory.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"edge density optimization\",\n                \"value\": null, \n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"centrality measures\",\n                \"value\": null,\n                \"unit\": \"dimensionless\"\n            },\n            {\n                \"name\": \"robustness under noise\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"edge_density\": null,\n                \"node_features\": null,\n                \"edge_features\": null,\n                \"synthetic_noise_rate\": null\n            },\n            \"description\": \"A large and dynamic dataset for node classification with complex graph structures and a focus on real-time edge alterations.\",\n            \"preprocessing\": [\n                \"real-time edge updates\",\n                \"node feature engineering\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"performance comparisons\",\n                \"hyperparameter tuning results\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Hypergraph Neural Network\",\n            \"family\": \"custom-built\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"nested, self-attentive layers\",\n                \"node_embedding_size\": null,\n                \"edge_embedding_size\": null,\n                \"layer_depth\": null,\n                \"optimizer\": null\n            },\n            \"description\": \"A sophisticated model designed for NELL's node classification task, utilizing a custom Hypergraph Neural Network with self-attention mechanisms.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptability to real-time edge changes is key\",\n        \"Hyperparameter optimization requires multi-objective analysis\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"extracting node centrality\", \"custom edge feature engineering\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom prediction API\",\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"live edge updates and performance demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a sophisticated model, their high level of expertise, and the problem's specific requirements such as the NELL dataset and the custom Hypergraph Neural Network. Performance metrics, such as precision and optimization criteria are clearly specified. However, the absence of exact numerical values for performance measures indicates some ambiguity or lack of specific details in the original instruction. To fully flesh out these, further clarification might be needed.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "node classification", "instruction": "Design an advanced, fine-grained RGCN model for the Enron Email dataset analysis, employing a unique three-phase node classification strategy: a nested HLFX module for hierarchical feature extraction, a PCG module that integrates temporal dynamics via multi-resolution edge convolutions, and a logarithmic volume-sensitive subgraph builder. The challenge lies in detecting specialized subcommunities of key communicators, accounting for complex temporal patterns, subtle volume fluctuations, and exceptional scalability to process data with over a million nodes in real-time. Implement a hybrid method that couples LIME for explainability with a dynamic reservoir learning mechanism, optimizing for top-notch performance and resource efficiency amidst constantly evolving, large-scale corporate email networks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"business analytics\",\n        \"description\": \"Design an advanced RGCN model for Enron Email dataset analysis with a three-phase strategy: nested HLFX for hierarchical feature extraction, PCG with multi-resolution edge convolutions for temporal dynamics, and a logarithmic volume-sensitive subgraph builder. The goal is to detect specialized subcommunities of key communicators while accounting for temporal patterns, volume fluctuations, and scalability for large-scale, real-time processing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory_footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"1 million nodes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_nodes\": null,\n                \"node_types\": null,\n                \"edge_types\": null,\n                \"temporal_aspect\": true\n            },\n            \"description\": \"Large-scale email communication dataset with over a million nodes representing key communicators, multi-resolution edges, and temporal information.\",\n            \"preprocessing\": [\n                \"graph cleaning\",\n                \"node embedding\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"community detection\",\n                \"temporal analysis\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced RGCN\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"RGCN with HLFX, PCG, and logarithmic subgraph builder\",\n                \"components\": [\n                    {\n                        \"name\": \"HLFX\",\n                        \"description\": \"Hierarchical feature extraction module\"\n                    },\n                    {\n                        \"name\": \"PCG\",\n                        \"description\": \"Temporal dynamics via multi-resolution edge convolutions\"\n                    },\n                    {\n                        \"name\": \"Logarithmic Subgraph Builder\",\n                        \"description\": \"Volume-sensitive subgraph construction\"\n                    }\n                ]\n            },\n            \"description\": \"A custom-designed RGCN model for fine-grained node classification in large-scale corporate email networks, leveraging explainability with LIME and dynamic reservoir learning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The three-phase node classification approach addresses complex network characteristics.\",\n        \"Model must handle million-node scalability while maintaining real-time processing.\",\n        \"LIME for interpretability and reservoir learning for dynamic learning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"LIME explanation\",\n            \"Dynamic reservoir learning\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"real-time processing\"\n        ],\n        \"demonstration\": \"Explained predictions for key subcommunities\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response closely reflects the given instruction, considering the user's intent, advanced model design, and the specific dataset details. The performance and complexity metrics are left unspecified for now, as they would require further analysis or input from the user. The hybrid explainability approach with LIME and dynamic reservoir learning is represented. However, the target deployment endpoint is left blank, and a low-level detail like specific performance values would need additional clarification from the user.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "node classification", "instruction": "For the Coauthorship Science project, develop an innovative node classification model that combines a hybrid pruning strategy with a temporal-spectralHGNN adaptation. Mandate the model to seamlessly integrate multiple citation networks, domain-specific feature extraction via deep learning, and a real-time learning mechanism. The objective is to achieve state-of-the-art accuracy in author recognition across interdisciplinary sub-disciplines, ensuring precision down to subfield levels while maintaining adaptability and scalability over time.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"science, academia\",\n        \"description\": \"Develop an innovative node classification model for the Coauthorship Science project. The model combines a hybrid pruning strategy with a temporal-spectral HGNN adaptation, integrating multiple citation networks and domain-specific feature extraction through deep learning, and incorporates a real-time learning mechanism.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null,\n                \"target\": \"state-of-the-art\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"adaptability\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coauthorship Science\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"network_type\": \"citation networks\",\n                \"subfield_coverage\": \"interdisciplinary\"\n            },\n            \"description\": \"A dataset that encompasses multiple citation networks from diverse sub-disciplines for author recognition tasks.\",\n            \"preprocessing\": [\"domain-specific feature extraction\", \"deep learning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hybrid Pruning Temporal-Spectral HGNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"pruning_strategy\": \"hybrid\",\n                \"temporal_spectral_adaptation\": true,\n                \"real_time_learning\": true\n            },\n            \"description\": \"A cutting-edge node classification model designed for coauthorship networks, leveraging deep learning and real-time adaptation for precision in interdisciplinary subfields.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hybrid pruning strategy helps in model efficiency and accuracy.\",\n        \"Temporal-spectralHGNN enhances handling dynamic networks.\",\n        \"Domain-specific feature extraction through deep learning captures subtle patterns.\",\n        \"Real-time learning ensures adaptability and scalability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific feature extraction\", \"real-time\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"state-of-the-art accuracy and precision at subfield levels\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's instructions, with a focus on building an advanced model for the Coauthorship Science project. It covers the desired areas of expertise, hybrid pruning, temporal-spectral HGNN adaptation, and deep learning, while incorporating the performance metrics and real-time learning mechanisms. However, the user did not provide specific target accuracy values for the metrics, so they are set to null. It assumes the user is aware of the state-of-the-art and aims to match or exceed it.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a seasoned project manager for the elite data science division, you've been tasked with a multifaceted assignment involving advanced machine learning. Utilizing the complex Mohs Hardness Dataset with numerous, interdependent variables, your team must execute a rigorous 80-10-10 stratified train-test-validation split, ensuring no data leakage. Your mission is to design a cutting-edge regression model with a keen focus on reducing the mean absolute percentage error (MAPAE) for enhanced precision. Mandate the exploration of feature interactions, optimization through hyperparameter tuning using Bayesian methods, and compare not just three but five state-of-the-art regression algorithms, accounting for their interpretability and scalability. Prepare a detailed report showcasing each model's performance and discussing their potential real-world implications. Remember to address potential biases and outliers in the data preprocessing stage.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"material science\",\n        \"description\": \"Design a cutting-edge regression model for the Mohs Hardness Dataset, focusing on reducing mean absolute percentage error (MAPAE). Perform an 80-10-10 stratified train-test-validation split, consider feature interactions, optimize hyperparameters through Bayesian methods, and compare five state-of-the-art algorithms with emphasis on interpretability and scalability. Address potential biases and outliers during data preprocessing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mohs Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"hardness\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"dimensions\": null,\n                \"length\": null\n            },\n            \"description\": \"A dataset with numerous interdependent variables for regression analysis in material science.\",\n            \"preprocessing\": [\n                \"bias correction\",\n                \"outlier detection and treatment\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"regression algorithms (e.g., Random Forest, XGBoost, Lasso, Ridge, Bayesian Ridge, etc.)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"hyperparameters\": {\n                    \"Bayesian optimization\": true\n                },\n                \"interpretability\": null,\n                \"scalability\": null\n            },\n            \"description\": \"A set of five advanced regression algorithms to be compared in terms of performance and characteristics.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interpretablity and scalability are crucial for practical implementation in material science applications.\",\n        \"Bayesian optimization is recommended for hyperparameter tuning.\",\n        \"Attention to data preprocessing, including bias and outliers, is paramount.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature interactions exploration\",\n            \"feature selection\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"report\": \"Performance comparison and implications of each model will be documented in a detailed report.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the instruction, assuming a high-expertise user. It covers the task of building a regression model, the dataset details, the requirement for feature interactions, optimization through Bayesian tuning, and the need for multiple algorithms comparison. However, specifics regarding the dataset size and feature dimensions are not provided as they are not present in the given instruction. The mean absolute percentage error (MAPAE) value is set to null, indicating it should be determined through model evaluation.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project lead for the AI translation elite squad, your mission is to design an advanced transformer model utilizing the diverse WMT19 multilingual commentary data. Prioritize cross-lingual prowess and pursue precision through fine-tuning on parallel corpora. Implement a rigorous iterative process, demanding a minimum BLEU score of 45, and don't forget to track METEOR, TER, and ROUGE-L for a holistic performance benchmarking. Additionally, ensure strict adherence to energy-efficient training while maintaining a deadline of six-month project completion.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"As the project lead for the AI translation elite squad, design an advanced transformer model using WMT19 multilingual commentary data. Prioritize cross-lingual performance and precision through fine-tuning on parallel corpora, with a focus on achieving a minimum BLEU score of 45. Track METEOR, TER, and ROUGE-L for comprehensive performance metrics, while ensuring energy-efficient training and a six-month project completion deadline.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU\",\n                \"value\": 45\n            },\n            {\n                \"name\": \"METEOR\",\n                \"value\": null\n            },\n            {\n                \"name\": \"TER\",\n                \"value\": null\n            },\n            {\n                \"name\": \"ROUGE-L\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 Multilingual Commentary\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"diversity\": \"multilingual\",\n                \"data_format\": \"parallel corpora\",\n                \"source_languages\": []\n            },\n            \"description\": \"A multilingual dataset for advanced transformer model development, focusing on commentary data from the WMT19 translation task.\",\n            \"preprocessing\": [\"Fine-tuning on parallel corpora\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Transformer\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"focus\": \"cross-lingual\",\n                \"efficiency\": \"energy-efficient\"\n            },\n            \"description\": \"A highly advanced transformer model designed for machine translation, targeting the WMT19 multilingual commentary data and optimized for precision.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Emphasis on cross-lingual prowess and fine-tuning on parallel corpora for improved translation performance.\",\n        \"Minimum performance benchmark (BLEU score) set to 45.\",\n        \"Additional metrics for holistic performance: METEOR, TER, and ROUGE-L.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Track and present results during the six-month project.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the details from the instruction, including the user's intent, the advanced transformer model design, the WMT19 dataset, and the performance metrics. The high expertise level is indicated based on the project lead role. Energy-efficient training is included in the model specification, and the specific model name is left open to be chosen according to project requirements. The project's six-month deadline is reflected in the 'demonstration' requirement under service. The confidence score is not explicitly provided, but is expected to be high due to the clear and detailed instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a skilled project manager, your mandate includes not only developing a cutting-edge time-series classification model for the 'Hierarchically Enhanced Real-World Handwriting Dynamics Dataset', incorporating advanced feature fusion (discrete and continuous) with a length of 300 input sequences (INPUT_SEQ_LEN=300) and enriched dimensions (INPUT_DIM=20). The model must address not only precision but also temporal dependencies, handle extreme class variances using stratified nested cross-validation, and strive for at least 97% accuracy. Emphasize model explainability through SHAP values, optimize with a focus on resource efficiency, and present a comprehensive report detailing the interpretable deep learning techniques and fine-tuning methods used.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"data analysis\",\n        \"description\": \"Develop a cutting-edge time-series classification model for the 'Hierarchically Enhanced Real-World Handwriting Dynamics Dataset'. The model should incorporate advanced feature fusion (discrete and continuous) with an input sequence length of 300 (INPUT_SEQ_LEN=300) and enriched dimensions (INPUT_DIM=20). It must handle temporal dependencies, extreme class variances using stratified nested cross-validation, aim for at least 97% accuracy, and focus on model explainability through SHAP values.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hierarchically Enhanced Real-World Handwriting Dynamics Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 300,\n                \"INPUT_DIM\": 20\n            },\n            \"description\": \"A time-series dataset for handwriting dynamics with discrete and continuous features.\",\n            \"preprocessing\": [\"feature fusion\", \"sequence normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time-series plots\", \"feature importance plots\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"interpretable deep learning models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"temporal dependencies\": true,\n                \"extreme class handling\": \"stratified nested cross-validation\"\n            },\n            \"description\": \"A deep learning model designed for time-series classification with a focus on feature fusion, temporal understanding, and high accuracy, while maintaining explainability through SHAP values.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced feature fusion enhances model performance.\",\n        \"Stratified nested cross-validation helps address class imbalances and validation.\",\n        \"SHAP values ensure interpretability and model understanding.\",\n        \"Optimizing for resource efficiency during model development.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"sequence alignment\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"model hosting platform\",\n        \"inference_engine\": [\"CPU-optimized\", \"GPU-accelerated\"],\n        \"demonstration\": \"comprehensive report with visual explanations and fine-tuning methods\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a cutting-edge model with a focus on the given requirements. The problem area, tasks, and performance metrics are derived from the given instructions. The dataset specifications, feature preprocessing, and model optimization details are deduced from the context. Missing information might include specific model architecture, but the 'interpretable deep learning models' family serves as a placeholder. The confidence score is high, considering the clear instructions and extensive specifications.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "tabular regression", "instruction": "As a top-tier project manager for an elite AI research team, you've been handed a complex challenge to enhance the state-of-the-art in solving a specialized multi-faceted regression problem with the \"Lithium-Ion Battery Performance\" dataset. This extensive dataset, now expanded to 100 unique chemical characteristics, contains intricate interdependencies and non-uniform distribution across the extended train (70%), validation (15%), and test sets (15%). Your assignment is not only to design a highly performant model that reduces the mean absolute percentage error (MAPE) for accurate prediction of electrode material capacities, but also to engineer a model with algorithmic transparency. Additionally, you must conduct a comprehensive, explainable AI study to map feature importance using novel interpretability techniques and present a compelling visualization of the insights derived.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"materials science\",\n        \"description\": \"Enhance state-of-the-art in solving a specialized multi-faceted regression problem with the 'Lithium-Ion Battery Performance' dataset. Target is to reduce MAPE for electrode material capacities prediction, while maintaining algorithmic transparency. Include a comprehensive interpretability study with novel techniques and an explanatory visualization of feature importance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute percentage error (MAPE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"algorithmic transparency\",\n                \"value\": null,\n                \"unit\": \"none\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Lithium-Ion Battery Performance\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"electrode material capacities\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": 70,\n                    \"validation\": 15,\n                    \"test\": 15\n                },\n                \"features\": 100,\n                \"interdependency\": \"intricate\",\n                \"distribution\": \"non-uniform\"\n            },\n            \"description\": \"An expanded dataset with 100 chemical characteristics, presenting complex interdependencies and a multi-faceted train, validation, and test split.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"highly transparent regression model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transparency\": \"explainable AI techniques\",\n                \"feature_importance_method\": \"novel\"\n            },\n            \"description\": \"A state-of-the-art model for predicting electrode material capacities, with an emphasis on algorithmic transparency and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The dataset's complexity requires a deep understanding of intricate interdependencies and non-uniform distribution.\",\n        \"Algorithmic transparency is critical for extensive material science research.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"explanatory visualization of feature importance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a high-performing model with a high level of expertise. The problem description reflects the task's specifics, including the data characteristics and the performance metric (MAPE). The complexity metric for algorithmic transparency is included, although the specific value is not provided as it would typically require domain-specific knowledge or experimentation. The dataset, model, and service sections are populated with details based on the provided instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project lead for a high-tech startup, challenge the ML development team to develop an AI-driven neural translation system that not only translates between intricate languages like archaic Japanese, Mandarin, and diverse Swahili dialects, but also requires constant self-learning to adapt and distinguish between subtle lexical variations. The team must design a groundbreaking fusion of Transformer-XL and Dual Encoder architectures, which must demonstrate exceptional efficiency in translating on cutting-edge wearable devices with minimal battery capacity. Ensure that the system not only accurately conveys cultural subtleties and colloquial expressions but also includes a sophisticated inductive learning component to preserve linguistic authenticity. Moreover, devise an intelligent energy optimization module utilizing a reinforcement learning algorithm, which can predict and dynamically adjust the system's power consumption based on real-time user interactions, aiming for an extended, interruption-free experience in resource-constrained ecosystems.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"high-tech startup\",\n        \"description\": \"Challenge the ML development team to create an AI-driven neural translation system that translates between intricate languages like archaic Japanese, Mandarin, and diverse Swahili dialects, and has self-learning capabilities for lexical variations. The system must use Transformer-XL and Dual Encoder architectures, be efficient on cutting-edge wearable devices with minimal battery, preserve cultural subtleties and colloquial expressions, and include an inductive learning component. Additionally, require an energy optimization module using reinforcement learning for predictive power consumption based on real-time interactions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cultural preservation score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"reinforcement learning performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"energy consumption\",\n                \"value\": null,\n                \"unit\": \"percentage of battery\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"languages\": [\"archaic Japanese\", \"Mandarin\", \"Swahili dialects\"],\n                \"lexical_variations\": true\n            },\n            \"description\": \"Large multilingual corpus with nuanced texts for training and evaluation\",\n            \"preprocessing\": [\"lexical diversity\", \"cultural adaptation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL and Dual Encoder fusion\",\n            \"family\": \"neural networks\",\n            \"type\": \"Transformer\",\n            \"specification\": {\n                \"architecture\": [\"Transformer-XL\", \"Dual Encoder\"],\n                \"resource_efficiency\": \"wearable devices, minimal battery capacity\"\n            },\n            \"description\": \"AI-driven neural translation system using innovative architecture\"\n        }\n    ],\n    \"knowledge\": [\n        \"Archaeological and colloquial texts, diverse dialect variations\",\n        \"Inductive learning for language authenticity\",\n        \"Reinforcement learning for energy optimization\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"linguistic authenticity\", \"energy efficiency\"],\n        \"target_device\": \"cutting-edge wearable devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time interaction-driven\"],\n        \"demonstration\": \"preserves linguistic authenticity and extends battery life in resource-constrained environments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response captures the key aspects of the instruction, including the translation system's requirements, the target languages, and the architectural choices. It includes the specific challenge of self-learning, inductive learning, and energy optimization. Performance metrics for accuracy, cultural preservation, and reinforcement learning are included, though not specified values. The model's efficiency on wearable devices with minimal battery capacity is addressed. The missing values in performance metrics and system-specifics indicate the need for additional details that are not explicitly provided.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series classification", "instruction": "Incorporate a multifaceted challenge for the high-performing ML team, where you're tasked with designing an avant-garde, real-time time series classifier for an exclusive pharmaceutical R&D competition. The classifier must address the intricate Heartbeat dataset with complex non-linear temporal patterns and varying sampling rates within a narrow band (50-200 samples, INPUT_SEQ_LEN), featuring 750 sophisticated features derived from advanced time and frequency domain analysis (INPUT_DIM=750). Emphasize on devising an ensemble model with SMOTE's innovative over-sampling strategy, targeting a class balance of 15%, while ensuring a minimum 10% improvement in minority class representation. Implement intricate nested cross-validation with stratified 15-folds, focusing on not just AUC-ROC, accuracy, and F1 score, but alsoMatthews correlation coefficient and precision-recall tradeoff. Prioritize ultra-stability, speed, and model transparency, while crafting tailored LIME explainability reports for each class (label categories: normal, arrhythmia), and create a drift-resistant mechanism to consistently excel by at least 15% above state-of-the-art benchmarks across various unseen and evolving datasets. Demonstrate resilience to performance degradation under high-dimensional scaling and real-world data noise, maintaining explainability as a cornerstone of your groundbreaking solution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"pharmaceutical research\",\n        \"description\": \"Design a real-time, avant-garde time series classifier for an exclusive pharmaceutical R&D competition. Target the Heartbeat dataset with complex non-linear temporal patterns and varying sampling rates (50-200 samples, INPUT_SEQ_LEN) and 750 advanced features from time and frequency domain analysis. Use SMOTE over-sampling to balance the class distribution (15% minority class), seeking a 10% improvement. Implement nested cross-validation with stratified 15-folds, optimizing for AUC-ROC, accuracy, F1 score, Matthews correlation coefficient, and precision-recall. Prioritize stability, speed, and transparency, including LIME explanations for each class (normal, arrhythmia), and create a drift-resistant mechanism for consistent performance (15% above state-of-the-art) across evolving datasets. Ensure resilience to performance degradation under high-dimensional scaling and real-world noise, maintaining explainability as a core aspect.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Matthews correlation coefficient\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision-recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Heartbeat\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"class labels: normal, arrhythmia\"],\n            \"specification\": {\n                \"sampling_rate\": \"50-200 samples\",\n                \"input_dim\": 750\n            },\n            \"description\": \"A complex time series dataset with non-linear temporal patterns and varying sampling rates.\",\n            \"preprocessing\": [\n                \"advanced time and frequency domain feature extraction\",\n                \"SMOTE over-sampling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_methods\": [\"SMOTE oversampled models\"],\n                \"class_balance\": 0.15,\n                \"minority_class_improvement\": 0.1,\n                \"nested_cross-validation\": true,\n                \"folds\": 15\n            },\n            \"description\": \"Avant-garde time series classifier with ensemble of models, including SMOTE for minority class balance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Emphasis on high performance stability, speed, transparency, and explainability with LIME reports.\",\n        \"Designing for drift resistance, outperforming state-of-the-art by at least 15%\",\n        \"Robustness to high-dimensional scaling and real-world noise\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"tailored for input sequence and complexity\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"ultra-stable, fast\"],\n        \"demonstration\": \"Include LIME explainability reports for each class and performance demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction, adapting it to the JSON schema. Specific model details, performance targets, and design choices are extracted. However, since the actual performance metrics and model names are not provided in the instruction, they are set to null for now. The confidence score can be adjusted according to the clarity of the input, with a high score of 0.99 for a well-defined and specific task.\",\n        \"score\": 0.99\n    }\n}\n"}
{"task": "node classification", "instruction": "For the \"Teenage Inter-School Communication Gateway\" (TICG) project, design a distributed TGN tailored for educational institutions, employing tiered attention mechanisms specific to academic interactions. Implement a resilient system with real-time anomaly detection to manage sudden data spikes from extracurricular events. Optimize resource allocation for diverse hardware environments, ensuring compatibility with varying node degrees in the contact network. Rigorously validate the model with a stringent accuracy benchmark of 95% within 10 seconds, considering dynamic student demographics, periodic subject correlations, and maintaining an F1-score above 0.95 throughout the network evolution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"education\",\n        \"description\": \"Design a distributed TGN for the Teenage Inter-School Communication Gateway (TICG) project, specifically tailored for educational institutions with tiered attention mechanisms for academic interactions. Implement a resilient system with real-time anomaly detection for managing sudden data spikes from extracurricular events. Optimize resource allocation for diverse hardware environments, ensuring compatibility with varying node degrees in the contact network.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": 10,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95,\n                \"unit\": \"\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource utilization\",\n                \"value\": \"optimized\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TICG Education Dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dynamic_student_demographics\": true,\n                \"periodic_subject_correlations\": true\n            },\n            \"description\": \"A graph dataset representing student interactions within educational institutions, capturing real-time events and network evolution.\",\n            \"preprocessing\": [\"handling variable node degrees\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Distributed TGN for TICG\",\n            \"family\": \"transformer-based Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention_tiers\": \"academic-oriented\",\n                \"anomaly_detection\": true\n            },\n            \"description\": \"A distributed GNN designed for academic interaction analysis with tiered attention mechanisms and real-time anomaly detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Implementing tiered attention handles the varying relevance of academic interactions.\",\n        \"Resilience is critical to manage data spikes from extracurricular activities.\",\n        \"Optimized resource allocation ensures compatibility with diverse hardware environments.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"dynamic node feature updating\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\", \"streaming\"],\n        \"demonstration\": \"showcase resilience during data spikes and network evolution\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the TICG project requirements, focusing on graph machine learning with node classification, distributed GNN, and performance metrics. The expert level, hardware compatibility, and data preprocessing steps are taken into account. However, the exact target device and deployment endpoint are left unspecified as they were not part of the instruction. The confidence score is high due to the clear mapping.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Redesign the Enron Email dataset analysis for node classification, incorporating RGCN with Hierarchical Attention and Temporal Graph Convolution. Enhance the model to identify not only communication patterns but also detect subgroups within teams, ensuring interpretability, scalability, and real-time performance, even for graphs with 100,000 nodes, while accommodating for varying message frequency and time-dependent trends.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Redesign the Enron Email dataset analysis, integrating RGCN (Relational Graph Convolutional Networks) with Hierarchical Attention and Temporal Graph Convolution. The goal is to identify communication patterns and detect subgroups within teams, prioritize interpretability, scalability, and real-time performance for graphs up to 100,000 nodes, while handling varying message frequency and temporal trends.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": {\n                    \"unit\": \"real-time performance\",\n                    \"threshold\": \"100,000 nodes\"\n                }\n            },\n            {\n                \"name\": \"real-time performance\",\n                \"value\": \"on graphs with varying message frequency and time-dependent trends\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds per graph\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB per graph\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"nodes\": {\n                    \"max_nodes\": 100000,\n                    \"node_distribution\": \"variable\"\n                },\n                \"edges\": {\n                    \"max_edges\": null,\n                    \"edge_distribution\": \"variable\"\n                }\n            },\n            \"description\": \"Dataset containing email communication data from Enron for node classification with temporal and structural dependencies.\",\n            \"preprocessing\": [\"temporal feature extraction\", \"node embedding\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"interpretable visualization of communication subgroups\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RGCN with Hierarchical Attention and Temporal GCN\",\n            \"family\": \"relational graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A scalable and interpretable model using RGCN, hierarchical attention, and temporal convolution for node classification on dynamic graphs with a focus on team analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"RGCN's ability to handle node classification on graphs\",\n        \"Hierarchical attention for subgroup detection\",\n        \"Temporal convolution for time-dependent trends\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"node embeddings\", \"temporal feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time classification of nodes and subgroup analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects a clear intent to build a more complex model based on the provided Enron Email dataset, specifying key components like RGCN, hierarchical attention, and temporal GCN. It addresses the requirement for subgroup detection, interpretability, scalability, and real-time performance. Specific metric targets are not provided, leaving them as null, but the context and performance metrics are well-defined. The source of the dataset is user-upload, and the preprocessing steps are mentioned. Model-related details like interpretability, scalability, and feature engineering are highlighted.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "The project now mandates a highly secure, multi-lingual financial news classification system using the Reuters-21578 archive. The team must optimize an ensemble of fastText and BERT models, targeting at least 90% F1 score, while discerning nuanced market trends with <1% false positive rate for key industry segments, and strictly adhering to real-time processing constraints. Additionally, all models must comply with GDPR regulations for data privacy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Develop a highly secure, multi-lingual financial news classification system using the Reuters-21578 archive. The system must optimize an ensemble of fastText and BERT models targeting at least 90% F1 score. It should discern subtle market trends with a <1% false positive rate for key industry segments and meet real-time processing constraints. The system must adhere to GDPR for data privacy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": null,\n                \"unit\": \"Yes/No\"\n            },\n            {\n                \"name\": \"false positive rate (key industry segments)\",\n                \"value\": 0.01,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"classification_label\"],\n            \"specification\": {\n                \"archive_size\": \"\",\n                \"languages\": \"multi-lingual\",\n                \"industry_segments\": []\n            },\n            \"description\": \"A large financial news corpus for text classification, supporting multiple languages and relevant industry segments.\",\n            \"preprocessing\": [\" GDPR compliant data preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ensemble\",\n            \"family\": [\"fastText\", \"BERT\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_complexity\": null,\n                \"accuracy_tradeoff\": {\n                    \"fastText\": null,\n                    \"BERT\": null\n                }\n            },\n            \"description\": \"An ensemble of fastText and BERT models optimized for financial news classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time processing and GDPR compliance require efficient data pipelines and encryption.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\" GDPR compliant\", \"Industry segment-specific tuning\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"real-time financial news classification service\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \" showcasing nuanced market trend detection and GDPR compliance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the requirements specified in the instruction, including the high-level user intent, problem domain, model specifications, performance targets, and data privacy concerns. The real-time processing constraint is reflected in the service section, and the F1 score target is captured as a performance metric. The dataset's multi-lingual nature and industry segment information are represented. However, some details like precise model complexity and preprocessing steps are not provided due to the nature of the instruction. GDPR compliance is assumed to involve data privacy and secure pipelines, but no specific implementation details are given.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "object detection", "instruction": "As the project lead, your challenge is to upgrade the machine learning smart city parking management system by integrating M2Det into an ultra-efficient, resilient architecture. The system must handle 'PKLot' data,not only monitoring current parking occupancy but also forecasting future space patterns. Emphasize on deploying a model with state-of-the-art adversarial defense mechanisms, ensuring robustness against varying imaging conditions such as adverse lighting, torrential rain, and partial obstructions. In addition to this, design a real-time analytics pipeline with ultra-low latency (<100ms),aiming for a 95% accuracy rate in detecting anomalies, while maintaining a strict QoS standard. Prepare a detailed phased implementation roadmap, a high-level system design diagram highlighting scalability, and a rigorous evaluation strategy incorporating A/B tests, performance metrics, and sensitivity analysis of different model variants.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city parking management\",\n        \"description\": \"Upgrade the machine learning smart city parking management system by integrating M2Det into an ultra-efficient and resilient architecture. The system should handle PKLot data and forecast future parking space patterns, with state-of-the-art adversarial defense mechanisms, and robustness against varying imaging conditions. Aim for real-time analytics with low latency (<100ms) and a 95% accuracy rate for anomaly detection, maintaining strict QoS. Require a detailed implementation roadmap, high-level system design, and a comprehensive evaluation strategy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 100,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PKLot data\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dimensions\": {\n                    \"height\": [0, \"unclear\"],\n                    \"width\": [0, \"unclear\"]\n                },\n                \"data_types\": [\"image\", \"time series\"],\n                \"variable_types\": [\"sensor data\", \"imagery\"]\n            },\n            \"description\": \"Data from parking lots with varying imaging conditions\",\n            \"preprocessing\": [\"adversarial noise removal\", \"imaging condition normalization\"],\n            \"augmentation\": [\"lighting variations\", \"weather simulations\"],\n            \"visualization\": [\"imaging condition heatmaps\", \"anomaly detection visualizations\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M2Det with adversarial defense\",\n            \"family\": \"M2Det architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy under adversarial attacks\": \"state-of-the-art\",\n                \"robustness against imaging conditions\": {\n                    \"lighting\": \"varied\",\n                    \"weather\": [\"rain\", \"partial obstruction\"]\n                },\n                \"latency\": {\n                    \"training\": \"<unclear>\",\n                    \"inference\": \"<unclear>\"\n                }\n            },\n            \"description\": \"M2Det enhanced for parking occupancy monitoring and forecasting, with focus on adversarial defense and varying imaging conditions.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time anomaly detection pipeline\"],\n        \"target_device\": \"ultra-efficient architecture\",\n        \"deployment_endpoint\": \"smart city parking management system\",\n        \"inference_engine\": [\"real-time, low-latency\"],\n        \"demonstration\": {\n            \"phases\": [\"implementation roadmap\", \"A/B testing\", \"model performance tuning\"],\n            \"metrics\": [\"accuracy\", \"latency\", \"sensitivity analysis\"],\n            \"evaluation_strategy\": \"A/B tests and performance analysis\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the key aspects of the given instruction, such as the integration of M2Det, adversarial defense, performance requirements, and evaluation strategy. The user's expertise and the project's area are derived from the context. However, some details like the exact model specifications (latency during training and inference), specific feature engineering techniques, and the exact phases of the implementation roadmap are not provided as they are not detailed in the instruction. The confidence score is set to 0.9, acknowledging that some information might be incomplete but sufficient to capture the essence.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a highly sophisticated, deep-learning-based time series forecasting framework for our mobile app's user engagement, combining an advanced auto-encoder architecture with dynamic state-space modeling. Mandate the model to not only accommodate concept drift, but also decipher complex temporal patterns, address seasonality with high granularity, and implement rigorous Bayesian regularization for unparalleled performance and adaptability in predicting user activity for the next 12 quarters.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"mobile app, user engagement\",\n        \"description\": \"Develop a sophisticated deep-learning-based time series forecasting framework, combining an advanced auto-encoder architecture with dynamic state-space modeling. The model must handle concept drift, decipher complex temporal patterns, address high-granularity seasonality, and employ Bayesian regularization for accurate predictions of user activity for the next 12 quarters.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"completeness\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness to concept drift\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"megabytes\"\n            },\n            {\n                \"name\": \"computational complexity\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mobile App User Engagement Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"user activity\"],\n            \"specification\": {\n                \"length\": null,\n                \"frequency\": null,\n                \"granularity\": \"high\"\n            },\n            \"description\": \"Contains historical user engagement data for our mobile app, suitable for deep learning time series forecasting.\",\n            \"preprocessing\": [\"cleaning\", \"normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"weekly trends\", \"seasonality analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Auto-encoder with Dynamic State-Space Model\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"advanced auto-encoder\",\n                \"dynamic_state_space\": true,\n                \"regularization\": \"Bayesian\"\n            },\n            \"description\": \"A deep learning model specifically designed for mobile app user engagement time series forecasting, leveraging auto-encoders and dynamic state-space modeling.\"\n        }\n    ],\n    \"knowledge\": [\n        \"To handle concept drift, the model needs to incorporate online learning techniques.\",\n        \"Adaptability for seasonality is crucial for accurate 12-quarter predictions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"window-based feature extraction\",\n            \"seasonality decomposition\"\n        ],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"mobile app backend\",\n        \"inference_engine\": [\"real-time inference\"],\n        \"demonstration\": \"visualizations and performance comparisons\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intention, with a high level of expertise. It specifies the problem as time series analysis with advanced requirements. The dataset and model are in line with the task. Performance metrics and complexity considerations are included, and a clear rationale is provided for the confidence score. However, specific performance targets for error metrics and model complexity are not provided, as they might depend on the dataset and expert input.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, devise an advanced image recognition system for the AI team, requiring not just straightforward implementation but a multi-faceted approach. Your responsibilities now encompass:\n\n1. Gather and preprocess a massive, varied 'Plant Diversity Dataset' (500 species with numerous sub-variants and environmental conditions) for in-depth analysis of inter-species differentiation and adaptation.\n2. Design a hybrid deep learning architecture, integrating cutting-edge techniques for fine-grained classification, considering the subtle nuances of taxonomy and environmental influence.\n3. Leverage transfer learning with extreme precision (97%+), targeting resource-constrained mobile devices, yet maintaining real-time responsiveness (<50ms per image) through efficient model pruning.\n4. Implement rigorous iterative testing using nested cross-validation, considering deployment scenarios and optimizing for scalability, with performance metrics for each iteration.\n5. Create a scalable, containerized deployment strategy, minimizing computation while retaining high accuracy (98%) on low-end devices, ensuring compatibility with diverse mobile platforms.\n6. Document the comprehensive workflow, including model interpretability, error analysis, and edge-case handling, with a detailed plan for continuous improvement, adhering to industry best practices and regulatory requirements. \n\nIncorporate key milestones, key performance indicators, and a roadmap for phased enhancements throughout the project lifespan.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"ecology and conservation\",\n        \"description\": \"Design an advanced image recognition system for plant species classification, considering inter-species differentiation, adaptation, and resource-constrained mobile deployment with real-time performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": \"97%+\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": \"<50ms per image\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory size for mobile\",\n                \"unit\": \"MB\",\n                \"value\": \"optimized\"\n            },\n            {\n                \"name\": \"inference efficiency\",\n                \"unit\": \"real-time\",\n                \"value\": \"yes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Diversity Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\", \"sub-variants\", \"environmental conditions\"],\n            \"specification\": {\n                \"number of species\": 500,\n                \"variety of sub-variants\": \"large\",\n                \"environmental conditions\": \"diverse\"\n            },\n            \"description\": \"A massive, varied dataset for detailed analysis of plant species differentiation and adaptation.\",\n            \"preprocessing\": [\"image resizing\", \"data augmentation\"],\n            \"augmentation\": [\"random cropping\", \"color jittering\"],\n            \"visualization\": [\"data distribution plots\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Deep Learning Architecture\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"fine-grained classification\",\n                \"transfer learning approach\": \"precise\",\n                \"pruning for mobile devices\": \"yes\"\n            },\n            \"description\": \"Integrating state-of-the-art techniques for high accuracy with resource efficiency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"image enhancement\", \"feature extraction\"],\n        \"target_device\": \"mobile devices\",\n        \"deployment_endpoint\": \"containerized\",\n        \"inference_engine\": [\"efficient inference algorithms\"],\n        \"demonstration\": \"deployment and real-time performance demonstration\"\n    },\n    \"key_milestones\": [\n        {\n            \"phase\": \"preprocessing\",\n            \"description\": \"Completion of data collection and preprocessing\"\n        },\n        {\n            \"phase\": \"model design\",\n            \"description\": \"Design and implementation of the hybrid deep learning model\"\n        },\n        {\n            \"phase\": \"training\",\n            \"description\": \"Nested cross-validation for iterative testing and optimization\"\n        },\n        {\n            \"phase\": \"deployment\",\n            \"description\": \"Containerized deployment and accuracy testing on low-end devices\"\n        },\n        {\n            \"phase\": \"maintenance and enhancements\",\n            \"description\": \"Continuous improvement and updates to address edge cases and regulatory compliance\"\n        }\n    ],\n    \"KPIs\": [\n        \"Accuracy (98%)\",\n        \"Inference time (50ms)\",\n        \"Model size (optimized for mobile)\",\n        \"Scalability\"\n    ],\n    \"roadmap\": {\n        \"month 1-3\": {\n            \"focus\": \"Data collection and preprocessing\"\n        },\n        \"month 4-6\": {\n            \"focus\": \"Model design and implementation\"\n        },\n        \"month 7-9\": {\n            \"focus\": \"Training and optimization\"\n        },\n        \"month 10-12\": {\n            \"focus\": \"Deployment and continuous improvement\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the project manager's requirements for an advanced image recognition system, including tasks, dataset details, performance indicators, and deployment considerations. Key milestones, KPIs, and a roadmap are included. Since the instruction mentions '98%' accuracy and '97%+' transfer learning precision without specific values, some values are placeholders or estimated. The confidence score is high due to the structure and clarity of the provided information.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "visual question answering", "instruction": "As the project lead for a prestigious middle school's cutting-edge exhibition, design an advanced VQA system using the latest M6 transformer and TextVQA v2.0 dataset. Mandate a modular architecture that combines advanced fine-grained cross-modal fusion, context-dependent multi-head attention, and a novel fusion of bottom-up and top-down visual cues. Alongside this, prepare a meticulously detailed technical report that encompasses a robust benchmarking framework, intricate algorithm derivation with pseudocode, a comparison of competitor models through ablation studies, and a demonstration of the system's incremental performance improvements over time. Ensure the report highlights interpretability and novelty to impress industry experts and educators alike.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"education, AI research\",\n        \"description\": \"Design an advanced VQA system using the latest M6 transformer and TextVQA v2.0 dataset for a middle school's exhibition. The system should have a modular architecture with cross-modal fusion, context-dependent multi-head attention, and a combination of bottom-up and top-down visual cues.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"model robustness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TextVQA v2.0\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": \"variable\",\n                \"text_vocab_size\": \"variable\",\n                \"annotated_questions\": \"variable\"\n            },\n            \"description\": \"A dataset for advanced visual question answering using the latest M6 transformer and focused on fine-grained cross-modal fusion and contextual understanding.\",\n            \"preprocessing\": [\n                \"Transforming images\",\n                \"Tokenizing text\"\n            ],\n            \"augmentation\": [\n                \"Random cropping\",\n                \"Color jittering\"\n            ],\n            \"visualization\": [\n                \"Attention maps for interpretation\",\n                \"Performance trend over epochs\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced VQA system\",\n            \"family\": \"Transformer (M6)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"modular, cross-modal fusion, multi-head attention, bottom-up/top-down cues\",\n                \"number_of_transformer_layers\": null,\n                \"attention_head_count\": null,\n                \"parameters\": null\n            },\n            \"description\": \"A state-of-the-art VQA system utilizing M6 transformer with advanced features for middle school exhibition.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Emphasis on interpretability and novelty needed to impress industry experts\",\n        \"Modular architecture for flexibility and scalability\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Advanced fusion techniques\", \"Algorithm optimization\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorRT\", \"ONNX\"],\n        \"demonstration\": \"Performance progress visualization and live QA scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been crafted based on the given instruction, capturing the user's intent to build a model, their high level of expertise, and the specified problem area and task. Performance metrics like accuracy and model complexity are listed as null, as these require benchmarking and specifics not provided in the instruction. The dataset, model, and additional requirements from the report are included as per the details given in the instruction. The knowledge section highlights the key aspects that need to impress industry experts, and the service component includes tasks related to the preparation of the technical report.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop an innovative, edge-computing neural architecture that seamlessly integrates a dual-attention mechanism RNN-Transformer fusion for instant translation of ancient Sanskrit to contemporary Mandarin dialects in real-time. The model should surpass a challenging MINDEL score of 72 while guaranteeing sub-50 milliseconds response time for archaic scripts, adapting to regional slang, and capturing the essence of philosophical discourse without sacrificing cultural subtleties. Additionally, implement a context-aware personalization feature that learns and responds in the user's unique vernacular style, all within a secure, privacy-preserving framework.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics and translation\",\n        \"description\": \"Develop an innovative, edge-computing neural architecture that integrates a dual-attention mechanism RNN-Transformer fusion for real-time translation of ancient Sanskrit to contemporary Mandarin, targeting a MINDEL score of 72 and sub-50ms response time. The model must adapt to regional slang and preserve cultural subtleties in philosophical discourse, and include a context-aware personalization feature with a privacy-preserving framework.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MINDEL score\",\n                \"value\": 72\n            },\n            {\n                \"name\": \"Response time\",\n                \"value\": 50\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\",\n                \"optimized_for\": \"real-time\"\n            },\n            {\n                \"name\": \"Memory size\",\n                \"value\": null,\n                \"unit\": \"MB\",\n                \"optimized_for\": \"edge-computing\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"languages\": [\"Sanskrit\", \"Mandarin\"],\n                \"ancient Script\": true,\n                \"regions\": [\"contemporary\"],\n                \"genre\": \"philosophical\"\n            },\n            \"description\": \"Ancient Sanskrit and modern Mandarin text corpus for training, validation, and testing.\",\n            \"preprocessing\": [\"preserving archaic script\", \"region-specific slang normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dual-Attention RNN-Transformer Fusion\",\n            \"family\": \"Neural Architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention_types\": [\"RNN\", \"Transformer\"],\n                \"personalization\": true,\n                \"privacy_features\": [\"encryption\", \"anonymization\"]\n            },\n            \"description\": \"A cutting-edge model combining RNN and Transformer for real-time ancient to modern language translation, with personalization and privacy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Ancient Sanskrit and Mandarin require translation involving different writing systems and cultural nuances.\",\n        \"Context-aware personalization is essential to maintain cultural and dialectic subtleties.\",\n        \"Privacy-preserving measures are crucial due to the sensitive nature of philosophical discourse.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual adaptation\", \"custom vocabulary embedding\"],\n        \"target_device\": \"edge computing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"Translation with personalized dialect and privacy protection\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent (building a neural architecture), expertise, and details about the problem (NLP for ancient-to-modern translation with specific performance and complexity requirements). The dataset, model, and performance metrics are derived from the specified characteristics of the model, while the requirements for personalization and privacy are incorporated. Missing information such as exact model specifications or dataset name would need to be obtained from the user or inferred based on general practices for such tasks.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the AI team, your mandate is to design and deploy a state-of-the-art text classification system using the BoolQ dataset. In addition to optimizing for precision (90%+ F1-score), demand a multi-faceted model that: \n\n1. Facilitates feature-level interpretability through advanced SHAP (SHapley Additive exPlanations) analysis.\n2. Prioritizes model efficiency by fine-tuning a BERT-base architecture within a strict budget of 5 epochs.\n3. Elucidates complex decision-making through salient reasoning examples for each predicted class.\n4. Conduct rigorous evaluation of model robustness with adversarial and cross-lingual testing.\n5. Prepare a detailed technical report outlining the trade-offs and iterative improvements for stakeholders.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Design and deploy a state-of-the-art text classification system using the BoolQ dataset, targeting precision (90%+ F1-score) with a multi-faceted approach.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"epochs\",\n                \"value\": 5,\n                \"unit\": \"epochs\"\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": 0.8,\n                \"unit\": \"BERT-base efficiency\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"class labels\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": \"unknown\",\n                    \"validation\": \"unknown\",\n                    \"test\": \"unknown\"\n                },\n                \"dimension\": \"unknown\"\n            },\n            \"description\": \"A text classification dataset for the state-of-the-art system.\",\n            \"preprocessing\": [\"BERT fine-tuning\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"SHAP analysis\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Text Classification System\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT-base\",\n                \"epochs\": 5,\n                \"interpretability\": \"SHAP analysis\",\n                \"efficiency\": {\n                    \"train_speed\": \"optimized for 5 epochs\"\n                }\n            },\n            \"description\": \"A multi-faceted text classification model utilizing BERT-base, with emphasis on interpretability, efficiency, and salient reasoning examples.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced SHAP analysis for feature-level interpretability.\",\n        \"BERT fine-tuning within 5 epochs for model efficiency.\",\n        \"Salient reasoning examples for class understanding.\",\n        \"Adversarial and cross-lingual testing for robustness evaluation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"BERT fine-tuning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"unknown\",\n        \"inference_engine\": [\"unknown (BERT-compatible)\"],\n        \"demonstration\": \"Salient reasoning examples for each predicted class\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a model with high-level requirements mentioned in the instruction. It covers the aspects of the text classification system, data requirements, model specification, and evaluation criteria. Some details are left as 'unknown' because they aren't specified in the instruction, such as exact dataset size and details about inference engine. The overall confidence is high due to the clear mapping of the given requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a leading urban planner, Task the machine learning team with the intricate design and implementation of a state-of-the-art SegNet++ model, specifically tailored for the demanding Vaihingen dataset. This dataset consists of high-resolution images revealing a vast array of vegetation types, capturing nuanced variations in species, vitality, and eco-adaptation to minute microclimate conditions. The model must exhibit exceptional resilience in low-light, dawn, and twilight conditions, while incorporating an intelligent real-time adaptive system that factors in seasonal changes. The ultimate goal is to produce a model that not only enhances predictive urban forestry strategies but also provides actionable insights for sustainable city planning, considering the dynamic interplay between environment and vegetation growth.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"Design and implement a state-of-the-art SegNet++ model for the Vaihingen dataset, focusing on vegetation segmentation in high-resolution images with detailed variations in species, vitality, and eco-adaptation. The model must be resilient in low-light, dawn, and twilight conditions, and incorporate an adaptive system that accounts for seasonal changes, ultimately providing insights for sustainable urban forestry strategies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean intersection over union (IoU)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"adaptability (seasonal changes)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vegetation classes\"],\n            \"specification\": {\n                \"resolution\": null,\n                \"number_of_images\": null,\n                \"image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                }\n            },\n            \"description\": \"High-resolution images capturing a variety of vegetation types with microclimate considerations.\",\n            \"preprocessing\": [\n                \"resizing\",\n                \"light normalization\"\n            ],\n            \"augmentation\": [\n                \"low-light, dawn, and twilight simulations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SegNet++\",\n            \"family\": \"deep learning segmentation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"SegNet++\",\n                \"optimizer\": null,\n                \"loss_function\": null,\n                \"resilience_features\": [\n                    \"low-light, dawn, and twilight adaptations\"\n                ],\n                \"adaptive_system\": \"seasonal change incorporation\"\n            },\n            \"description\": \"A state-of-the-art model designed for advanced vegetation segmentation in the Vaihingen dataset with targeted enhancements for urban forestry and sustainable city planning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consideration of dynamic interplay between environment and vegetation growth for actionable insights\",\n        \"Resilience against challenging lighting conditions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"PC or cloud\",\n        \"deployment_endpoint\": \"AI platform or cloud-based deployment\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"Real-time adaptive and seasonal results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is tailored to the provided instruction, capturing the user's intent to build a tailored SegNet++ model, the expert's high level, and the specific problem and requirements of the Vaihingen dataset. Performance metrics such as accuracy and inference time are mentioned but not specified, reflecting the need for further input for exact values. Similarly, the complexity metrics and dataset details are based on the nature of the image segmentation task and the challenges mentioned.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, craft an advanced QA model using TinyBERT, fused with device-adaptive architecture, for mobile news filtering. Demand a NewsQA dataset benchmark exceeding 95% accuracy, sub-150 ms response times, and flawless cross-platform compatibility on devices with minimal processing power, while ensuring energy efficiency and GDPR compliance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"news and media\",\n        \"description\": \"Craft an advanced QA model using TinyBERT fused with a device-adaptive architecture, targeting mobile news filtering. The model should achieve a benchmark accuracy of over 95% on the NewsQA dataset, with sub-150 ms response times, cross-platform compatibility on low-resource devices, and energy efficiency while adhering to GDPR guidelines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": 150,\n                \"unit\": \"ms\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": 1.0,\n                \"unit\": \"efficiency score\"\n            },\n            {\n                \"name\": \"cross-platform compatibility\",\n                \"value\": 1.0\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NewsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"answer\"],\n            \"specification\": {\n                \"size\": \"sufficient for benchmark\",\n                \"features\": {\n                    \"language\": \"English\",\n                    \"domain\": \"news\"\n                }\n            },\n            \"description\": \"A dataset for News question answering with a benchmark accuracy goal.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT with Device-Adaptive Architecture\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"device-adaptive\",\n                \"model_size\": \"TinyBERT\",\n                \"environment\": \"mobile-friendly\"\n            },\n            \"description\": \"A state-of-the-art QA model built on TinyBERT, optimized for mobile devices with low processing power.\"\n        }\n    ],\n    \"knowledge\": [\n        \"TinyBERT's efficiency and effectiveness on NLP tasks make it suitable for resource-constrained devices.\",\n        \"Device-adaptive architecture ensures optimal performance on diverse mobile platforms.\",\n        \"NewsQA's high accuracy goal pushes for a model that can handle complex news questions well.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"GDPR compliant\",\n        \"inference_engine\": [\"energy-efficient\"],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key aspects of the project requirements, including the user's intent to build, the high-level expertise, and the problem's focus on NLP for QA. The specified task and dataset align with the description, performance metrics are set, complexity metrics are addressed, and the TinyBERT and device-adaptive details are included. However, some aspects like preprocessing, exact device requirements, and GDPR compliance details are not detailed, which might be added depending on further specifications or guidelines.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "node classification", "instruction": "Construct an advanced machine learning model, specifically leveraging Graphormer architecture and IBA-discovery algorithm, for differentiating between five unique subtypes of neuronal ensembles in the Canine Hippocampus. Utilize the exclusive Feline Cerebral Cortex Connectivity Atlas with spatiotemporal constraints, ensuring the model's robustness. Implement a real-time adaptable framework that visualizes edge weights and cell-specific contributions with high resolution, while maintaining a precision above 95% and a recall of at least 90% in incremental learning scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Create an advanced machine learning model using Graphormer architecture and IBA-discovery algorithm to differentiate between five neuronal subtypes in Canine Hippocampus. The model should leverage the Feline Cerebral Cortex Connectivity Atlas with spatiotemporal constraints and ensure robustness. The model must implement a real-time adaptable framework for visualizing edge weights and cell-specific contributions with high resolution, maintaining precision above 95% and recall of at least 90% in incremental learning scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.90\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Feline Cerebral Cortex Connectivity Atlas\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"temporal_constraints\": {\n                    \"unit\": \"days\",\n                    \"range\": [0, \"variable\"]\n                },\n                \"spatial_resolutions\": {\n                    \"unit\": \"micrometers\",\n                    \"value\": 1\n                }\n            },\n            \"description\": \"Dataset for differentiating neuronal ensembles in Canine Hippocampus, based on the Feline Cerebral Cortex Connectivity Atlas with spatiotemporal constraints.\",\n            \"preprocessing\": [\"spatiotemporal filtering\", \"IBA-discovery\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"edge weight visualization\",\n                \"cell-specific contributions\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graphormer with IBA-discovery\",\n            \"family\": \"neural networks\",\n            \"type\": \"graph neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"Graphormer\",\n                \"adaptability\": \"real-time\",\n                \"framework\": \"incremental learning\",\n                \"embedding_dimensions\": [128, 256],\n                \"number_of_layers\": 5\n            },\n            \"description\": \"A Graphormer model enhanced with IBA-discovery algorithm for neuronal subtype differentiation in Canine Hippocampus, utilizing the Feline Cerebral Cortex Connectivity Atlas.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"adaptation for real-time visualization\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"incremental learning-friendly\"],\n        \"demonstration\": \"high-resolution, real-time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response closely captures the given instruction, detailing the Graphormer and IBA-discovery based model, the datasets, and the performance and complexity requirements. The user's intent to build a model and high expertise are reflected. The spatiotemporal constraints, precision, recall, and incremental learning scenario are clearly incorporated. However, the confidence score might be less than 1.0 due to the lack of information about the exact graph construction steps and possible limitations in achieving high precision and recall targets.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a seasoned project manager leading a top-notch machine learning team, your mandate is to design a cutting-edge time series classifier for a rival pharmaceutical R&D project. The challenge demands tackling the complex, heterogeneous Heartbeat dataset, which now presents an added layer of complexity with non-uniform sampling rates varying dynamically (INPUT_SEQ_LEN). Each data point encompasses an expansive feature set of 750 attributes derived from both time and frequency domains (INPUT_DIM), requiring multidimensional feature extraction and analysis.\n\nEmphasize the use of advanced SMOTE techniques to address severe class imbalance within the imbalanced dataset. Implement a robust ensemble modeling strategy, such as stacking multiple deep learning architectures, to ensure competitive performance. Engage in an intricate nested cross-validation process with 5 folds, maintaining a focus on stratified sampling to preserve class distribution.\n\nIncorporate not only precision, recall, and AUC-ROC metrics, but also demanding a 10% improvement in F1 score compared to state-of-the-art. Ensure the model is robust to concept drift, able to adapt to changing real-world conditions. Strive for real-time inference speed without compromising performance, and prioritize interpretability by generating detailed, actionable visualizations for each class prediction (labels: healthy and anomalous). The project's success will be measured by outperforming established benchmarks by a substantial margin and delivering a comprehensive, user-friendly interpretability report to substantiate the model's decision-making process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"pharmaceutical research & development\",\n        \"description\": \"Design a cutting-edge time series classifier for a rival pharmaceutical R&D project using the Heartbeat dataset, addressing class imbalance with advanced SMOTE, ensemble modeling with stacking and deep learning, and ensuring real-time inference with interpretability. Challenge includes improving F1 score by 10% compared to state-of-the-art, handling concept drift, and providing actionable visualizations for healthy and anomalous predictions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"milliseconds\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Heartbeat\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"healthy\", \"anomalous\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": {\n                    \"value\": null,\n                    \"description\": \"Non-uniform sampling rate varying dynamically.\"\n                },\n                \"INPUT_DIM\": {\n                    \"value\": 750,\n                    \"description\": \"Extensive feature set from time and frequency domains\"\n                }\n            },\n            \"description\": \"A complex, heterogeneous dataset with non-uniform sampling rates and 750 attribute-rich time and frequency domain features.\",\n            \"preprocessing\": [\"SMOTE for class imbalance\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"detailed class predictions\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble, deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"stacking\": {\n                        \"models\": [\"deep learning models\"]\n                    },\n                    \"interpretablility\": true\n                }\n            },\n            \"description\": \"Advanced ensemble model employing stacking with deep learning architectures, designed for time-series classification, emphasizing interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"SMOTE for class imbalance handling\",\n        \"Robust to concept drift and adaptable to real-world changes\",\n        \"F1 score improvement of 10% compared to state-of-the-art\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"multidimensional feature extraction\"],\n        \"target_device\": \"real-time\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"interpretable class predictions for visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response follows the JSON schema and structures the information based on the provided instruction. Performance metrics and complexity requirements are clearly defined. The use of advanced SMOTE, ensemble modeling, and interpretability are incorporated. However, specific metric values, such as F1 score improvement or inference time, are left to be determined as the instruction does not provide numerical targets for these.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a highly sophisticated, distributed and modular time series forecasting framework for the intricate in-app user activity (DAU) in our exclusive mobile gaming application. Combine and enhance three advanced deep learning models - Hierarchical Attention LSTM, Facebook's Prophet with dynamic seasonality adjustments, and ARIMA with domain-specific external factors. Implement a multi-step data preprocessing pipeline that integrates tensor decomposition with TBATS and wavelet analysis for enhanced feature extraction. Introduce adaptive regularization techniques, including L1, L2, and a tailored dropout strategy to address sparse data and minimize overfitting in niche gaming contexts. Design a novel evaluation metric, \"Weighted and Time-dependent Horizon Mean Absolute Percentage Error\" (WHMAPE), which differentiates error weights based on both prediction horizon and temporal patterns. Conduct an extensive benchmarking exercise comparing the performance of your proposed model against previous state-of-the-art methods using a comprehensive, game genre-diverse historical database. Prepare a comprehensive report, including interactive visualizations and interpretative insights, to demonstrate the model's superiority in predicting long-term and short-term user trends.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"mobile gaming\",\n        \"description\": \"Develop a highly sophisticated, distributed, and modular time series forecasting framework for in-app user activity (DAU) in a mobile gaming app. The framework should combine and enhance Hierarchical Attention LSTM, Facebook's Prophet with dynamic seasonality, and ARIMA with external domain-specific factors. The preprocessing should involve tensor decomposition, TBATS, and wavelet analysis for feature extraction. Adaptive regularization techniques, such as L1, L2, and dropout, should be employed to handle sparse data and minimize overfitting. A new metric, 'Weighted and Time-dependent Horizon Mean Absolute Percentage Error' (WHMAPE), should be defined for evaluation, and the model's superiority should be demonstrated through benchmarking with state-of-the-art methods on a diverse historical database. The response should include interactive visualizations and interpretative insights.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"WHMAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": 9.0,\n                \"unit\": \"1000 users\"\n            },\n            {\n                \"name\": \"inference_speed\",\n                \"value\": null,\n                \"unit\": \"seconds per prediction\"\n            },\n            {\n                \"name\": \"memory_usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"In-App User Activity (DAU)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"DAU\"],\n            \"specification\": {\n                \"sample_length\": {\n                    \"training\": 365,\n                    \"validation\": 60,\n                    \"testing\": 60\n                },\n                \"sparsity_rate\": 0.5\n            },\n            \"description\": \"Historical in-app user activity data with diverse gaming genres.\",\n            \"preprocessing\": [\n                \"tensor decomposition\",\n                \"TBATS decomposition\",\n                \"wavelet analysis\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Attention LSTM\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"Prophet with dynamic seasonality\",\n            \"family\": \"Facebook's time series forecasting\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"ARIMA with domain-specific external factors\",\n            \"family\": \"statistical models\",\n            \"type\": \"statistical models\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integration of tensor decomposition for better feature extraction.\",\n        \"Adaptive regularization for sparse data and overfitting.\",\n        \"WHMAPE: a unique evaluation metric.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"tensor decomposition\", \"wavelet analysis\"],\n        \"target_device\": \"mobile, cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"interactive visualizations, insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure is well-suited for the provided instruction, capturing the user's intent, problem specifics, and requirements for advanced deep learning models, data preprocessing, regularization, evaluation, benchmarking, and report generation. However, exact model performance values, inference speed, and memory usage were left unspecified due to their custom nature, and they would typically be determined through model development and testing.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "For an expert in the domain of specialized aliphatic and aromatic hydrocarbons, design a cutting-edge QA platform tailored to the niche Salsbury Oil Refinery Extension of TriviaQA. Implement a highly-optimized BERT fusion model enhanced with domain-specific knowledge extracted from in-depth technical data on refinery processes and arcane drilling techniques. The system should deliver instantaneous (<5 ms) and scalable responses under heavy load (100,000 users), maintaining 99.99% uptime amidst fluctuations in extreme weather conditions with bespoke fault-tolerance mechanisms.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"specialized hydrocarbons\",\n        \"description\": \"Design a cutting-edge QA platform for the niche Salsbury Oil Refinery Extension of TriviaQA, focusing on aliphatic and aromatic hydrocarbons. Implement a BERT fusion model enhanced with domain-specific knowledge from refinery processes and drilling techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 5,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high\",\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"uptime\",\n                \"value\": 0.9999,\n                \"unit\": \"\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": \"5\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"load_capacity\",\n                \"value\": 100000,\n                \"unit\": \"users\"\n            },\n            {\n                \"name\": \"fault_tolerance\",\n                \"value\": \"bespoke\",\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"robustness\",\n                \"value\": \"yes\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Salsbury Oil Refinery Extension of TriviaQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain_keywords\": [\"aliphatic hydrocarbons\", \"aromatic hydrocarbons\", \"refinery processes\", \"drilling techniques\"],\n                \"source\": \"technical data on refinery processes and arcane drilling techniques\"\n            },\n            \"description\": \"A QA dataset specific to aliphatic and aromatic hydrocarbons within the Salsbury Oil Refinery context.\",\n            \"preprocessing\": [\"BERT fine-tuning\", \"domain knowledge integration\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT Fusion Model with Domain-Specific Knowledge\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"type\": \"BERT-based QA model\",\n                \"enhancements\": [\"domain-specific knowledge injection\", \"fusion mechanism\"],\n                \"fine_tuning_details\": \"\"\n            },\n            \"description\": \"A BERT model tailored to the Salsbury Oil Refinery domain with fast inference and domain-expert knowledge.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Insights from refining processes and drilling techniques in aliphatic and aromatic hydrocarbons.\",\n        \"Criticality of low-latency and robustness under extreme weather conditions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"optimized for high load\",\n        \"inference_engine\": [\"high-performance, optimized BERT\"],\n        \"demonstration\": \"Customized response times and scalability demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the requirements of the given instruction, including the task of building a QA platform, the specific BERT model enhancement, and the performance, complexity, and service requirements. The user's high expertise level and domain-specific knowledge are assumed. Some details like fine-tuning details for the BERT model are not specified as the instruction didn't delve into that level of detail. However, a reasonable level of confidence is expressed based on the provided requirements.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "question answering", "instruction": "As the project lead for an elite AI research team, our mandate is to revolutionize a highly specialized BERT-empowered question-answering system using the intricate SQuAD 2.0 Enhanced dataset. Our goal is to attain unprecedented performance with a stringent F1 score of 97% in the advanced interpretability setting, whilst maintaining an ultra-competitive latency of 1.3 seconds or less for users interacting with the next-generation, ultra-fast Q-BERT model. Explore innovative multitasking training algorithms, balance precision and recall through dynamic fine-tuning, and delve into neural architecture search for model efficiency, all while pushing boundaries and outperforming rival benchmark models by at least 5%. Don't forget to document your methodology for reproducibility and future improvements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"elite AI research\",\n        \"description\": \"Revolutionize a BERT-empowered question-answering system using the SQuAD 2.0 Enhanced dataset. Aim for F1 score of 97% in the advanced interpretability setting, with a latency of 1.3 seconds or less for the ultra-fast Q-BERT model. Investigate multitasking algorithms, dynamic fine-tuning for precision and recall balance, and neural architecture search for efficiency. Outperform benchmarks by at least 5% and ensure methodology is document for reproducibility and future improvements.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference latency\",\n                \"value\": 1.3,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0 Enhanced\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"interpretability\": \"advanced\",\n                \"enhancements\": \"BERT empowered\"\n            },\n            \"description\": \"Intricate and specialized question-answering dataset for enhancing the BERT model performance.\",\n            \"preprocessing\": [\"BERT tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Q-BERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_type\": \"sequence-to-sequence\",\n                \"neural_architecture_search\": \"conducted\",\n                \"model_efficiency\": null\n            },\n            \"description\": \"Next-generation ultra-fast BERT-based QA model with advanced interpretability features.\"\n        }\n    ],\n    \"knowledge\": [\n        \"BERT fine-tuning for multitasking, dynamic precision and recall\",\n        \"Neural architecture search for model efficiency\",\n        \"Ultra-fast Q-BERT targeting 1.3-second latency\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"dynamic fine-tuning\", \"neural architecture search\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"high-performance server\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"Interactive Q&A system showcase\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is generated based on the detailed instruction provided, which specifies a high-end AI research project's requirements. It covers the BERT-based question-answering system, SQuAD 2.0 dataset usage, performance targets, and the exploration of advanced techniques like fine-tuning, neural architecture search, and benchmark outperformance. The specific model fine-tuning and efficiency details are left to some extent as they aren't specified in the provided instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for the advanced data science team, you've been tasked with a complex regression project. Using the extensive Wild Blueberry Yield Dataset, preprocess and analyze not only the train, validation, and test sets independently but also ensure the data normalization process accounts for nonlinear relationships among features. Your mandate is to design a cutting-edge model that not only forecasts yield with remarkable precision (targeting an MAE below 5%) but also demonstrates superior performance compared to current industry standards, considering seasonality and regional variations, while adhering to strict interpretability requirements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Design a complex regression project using the Wild Blueberry Yield Dataset. The project involves preprocessing and analyzing train, validation, and test sets, with a focus on data normalization for nonlinear relationships and accounting for seasonality and regional variations. The goal is to achieve high precision (MAE below 5%) and outperform industry standards while maintaining interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wild Blueberry Yield Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"yield\"],\n            \"specification\": {\n                \"nonlinear_relationships\": true,\n                \"seasonality\": true,\n                \"regional_variations\": true,\n                \"data_split\": [\"train\", \"validation\", \"test\"]\n            },\n            \"description\": \"An extensive dataset containing blueberry yield data, with a focus on capturing nonlinear relationships, seasonality, and regional variations.\",\n            \"preprocessing\": [\"data normalization for nonlinear relationships\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretable\": true\n            },\n            \"description\": \"A cutting-edge regression model designed to predict blueberry yield with high precision, accounting for various factors and meeting interpretability requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The project requires advanced data preprocessing and analysis techniques due to the complexity of the dataset.\",\n        \"Interpretability is crucial as per the project mandate.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"accounting for seasonality and regional variations\",\n            \"interpretable model development\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Focus on the explanation of model decisions and predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been crafted based on the instruction, considering the user's intent to build a model, high level of expertise, and the detailed requirements for the complex regression project. The area, downstream task, and performance metrics are defined explicitly. The dataset is properly described with a normalization emphasis and source, and the model type is inferred as neural networks to handle complexity. The knowledge section reflects the project's key requirements, while the service section outlines the steps towards fulfilling these needs. However, specific model details,如architecture or feature engineering steps, are not provided due to the lack of information in the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "As a cutting-edge project manager for an innovative AI text categorization project, develop a specialized strategy that integrates tailored deep learning (using convolutional neural networks), support vector machines (utilizing radial basis function kernels), and gradient-boosted random forests on the diverse 20 Newsgroups dataset with 10,000 unique documents. Strive for utmost interpretability by implementing SHAP feature importance and LIME explanations, optimizing hyperparameters through Bayesian optimization for optimal performance. Rigorously assess scalability and efficiency across a hybrid infrastructure of specialized GPUs (NVIDIA V100) for parallel acceleration, TPUv3 for tensor-core computation, and energy-efficient CPUs (Intel i9-11900K) for hybrid environments. Conduct iterative benchmarking while accounting for load balancing, real-time model refinement through federated learning, and maintaining compatibility with emerging hardware standards. Ensure secure deployment within a privacy-preserving framework, respecting data sovereignty, and adhere to the latest EU AI Act guidelines within a 6-month project timeline.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI text categorization project\",\n        \"description\": \"A project aims to categorize text using a combination of deep learning with convolutional neural networks, support vector machines with radial basis function kernels, and gradient-boosted random forests. Emphasis on interpretability with SHAP and LIME, hyperparameter optimization through Bayesian optimization, scalability across hybrid infrastructure involving GPUs (NVIDIA V100), TPUs (TPUv3), and energy-efficient CPUs (Intel i9-11900K), as well as federated learning, and compliance with privacy, EU AI Act, and data sovereignty guidelines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean average precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"throughput/latency\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"MJ/operation\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"categories\"],\n            \"specification\": {\n                \"number_of_documents\": 10000,\n                \"unique_documents\": 10000,\n                \"average_document_length\": null\n            },\n            \"description\": \"A dataset containing 20 newsgroups, diverse in content, for text classification.\",\n            \"preprocessing\": [\"lemmatization\", \"stop-word removal\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature distribution\",\n                \"class distribution\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Learning (CNN)\",\n            \"family\": \"convolutional neural networks\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"Support Vector Machines (RBF)\",\n            \"family\": \"support vector machines\",\n            \"type\": \"classical machine learning\"\n        },\n        {\n            \"name\": \"Gradient-Boosted Random Forest\",\n            \"family\": \"ensemble models\",\n            \"type\": \"ensemble models\"\n        }\n    ],\n    \"knowledge\": [\n        \"SHAP for feature importance\",\n        \"LIME for local explanations\",\n        \"Bayesian optimization for hyperparameter tuning\",\n        \"Federated learning for real-time model refinement\",\n        \"Privacy-preserving deployment\",\n        \"Data sovereignty compliance\",\n        \"EU AI Act guidelines\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"data preprocessing\", \"feature extraction\"],\n        \"target_device\": [\"NVIDIA V100 GPUs\", \"TPUv3\", \"Intel i9-11900K CPUs\"],\n        \"deployment_endpoint\": \"Hybrid infrastructure\",\n        \"inference_engine\": [\"GPU accelerated\", \"TPU tensor-core\", \"CPU compatible\"],\n        \"demonstration\": \"Iterative benchmarking, load balancing, and model updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's instruction, including the project's objectives, algorithms, and compliance requirements. The provided dataset information and model details are deduced from the instruction. However, specific performance metrics like accuracy values are not given in the instruction and left null, as they typically require model training results. The confidence score is high given the comprehensive structure that aligns with the JSON schema.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "Act as a seasoned project manager for a cutting-edge AI research team, tasked with designing a state-of-the-art text classification model. Utilize the BoolQ dataset to not only predict entailment (0/1) but also devise a sophisticated metric for robustness against linguistic nuances. Demand a comprehensive evaluation, including precision, recall, and F1-score, to guarantee a multifaceted understanding of model performance across various linguistic contexts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"Design a state-of-the-art text classification model for predicting entailment (0/1) using the BoolQ dataset. The model should have a robustness metric against linguistic nuances. Comprehensive evaluation with precision, recall, and F1-score is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"entailment\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"length\": null\n            },\n            \"description\": \"A dataset for text classification, focusing on entailment prediction with linguistic nuances.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"not specified\"\n            },\n            \"description\": \"A cutting-edge text classification model for entailment prediction, targeting robustness against linguistic nuances.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the provided instruction, capturing the task to build a text classification model using the BoolQ dataset, and the requirements for robustness against linguistic nuances and comprehensive evaluation. However, exact model specifications (e.g., architecture) and performance metric values are not included as they require more details from the instruction. The confidence score is high, as most information is derived from the given instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, you must oversee the creation of an advanced educational Q&A platform for elementary schools, incorporating the innovative Kidscaptivating dataset and a lightweight, curriculum-focused BERT adaptation. Emphasize a seamless, visually stimulating design that caters to various learning styles, promotes cognitive growth, and guarantees age-appropriate, contextually rich responses that encourage step-by-step problem-solving through interactive gameplay.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Design an advanced Q&A platform for elementary schools, integrating the Kidscaptivating dataset and a lightweight, curriculum-focused BERT adaptation. The platform must have a seamless, visually stimulating design, support diverse learning styles, foster cognitive growth, and provide contextually rich, step-by-step problem-solving through interactive gameplay.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Kidscaptivating dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"large-scale, tailored for educational context\",\n                \"content\": \"age-appropriate, contextual questions and answers\"\n            },\n            \"description\": \"A dataset specifically designed for an educational Q&A platform, catering to young learners.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [\"content-based, age-appropriate visualizations\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Lightweight, Curriculum-focused BERT Adaptation\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"complexity\": \"efficient for lightweight implementation on educational devices\",\n                \"educational_focused\": true\n            },\n            \"description\": \"A BERT adaptation optimized for educational purposes, ensuring fast and appropriate question understanding.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Kidscaptivating dataset should emphasize educational relevance and context for young learners.\",\n        \"BERT adaptation must be lightweight and adapted for a step-by-step problem-solving approach.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"cognitive development optimization\",\n            \"learning style adaptability\"\n        ],\n        \"target_device\": \"education-friendly, resource-constrained devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"user-friendly, interactive interface\"],\n        \"demonstration\": \"step-by-step, interactive question-answering sessions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key elements from the instruction, including the project's nature (Q&A platform), dataset ( Kidscaptivating), BERT adaptation, and requirements for design and functionality. Performance metrics are left unspecified as they would be context-dependent, and the platform's performance on cognitive growth and problem-solving is built into the description. The user's high level of expertise and the focus on educational technology are reflected. However, specific numbers or detailed specifications, such as the size of BERT adaptation, are not provided without further guidance.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "For the High School Adapting Social Dynamics Research Initiative, devise an advanced, explainable TGAT architecture with novel self-organizing memory capacity, incorporating real-time data fusion from diverse sources and multi-level temporal resolution. The model should not only accurately discern key actor influences but also detect transient power dynamics shifts and hierarchical clusters, maintaining efficiency across large-scale, high-frequency network fluctuations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"social sciences\",\n        \"description\": \"Design an advanced, explainable TGAT architecture for the High School Adapting Social Dynamics Research Initiative. The model must have novel self-organizing memory capacity, support real-time data fusion from diverse sources, handle multi-level temporal resolution, and accurately discern actor influences, detect transient power dynamics shifts, and identify hierarchical clusters. Efficiency should be maintained across large-scale, high-frequency network changes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explainability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"transient detection\",\n                \"value\": null\n            },\n            {\n                \"name\": \"hierarchical clustering\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory_size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High School Social Dynamics Data\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"actor_influences\", \"power_dynamics\", \"hierarchical_clusters\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"dimensions\": null,\n                    \"samples\": null\n                },\n                \"edges\": {\n                    \"types\": null,\n                    \"dimensions\": null\n                },\n                \"temporal_resolutions\": null\n            },\n            \"description\": \"Data source for the High School Adapting Social Dynamics initiative, containing real-time, multi-source, multi-level data for graph analysis.\",\n            \"preprocessing\": [\n                \"real-time data fusion\",\n                \"diverse source integration\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TGAT Architecture\",\n            \"family\": \"Transformers for Graphs\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novel_features\": \"self-organizing memory\",\n                \"explainability Techniques\": null,\n                \"real-time fusion\": true\n            },\n            \"description\": \"An advanced explainable TGAT model with self-organizing memory for real-time fusion on dynamic high school social networks.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Challenges in large-scale, high-frequency network adaptation\",\n        \"Importance of explainability and interpretability in social dynamics research\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware node embeddings\",\n            \"temporal edge embeddings\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"inference framework for real-time predictions\"],\n        \"demonstration\": \"Visualization of power dynamics and influence over time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction is parsed based on the given schema, reflecting high-level aspects of a machine learning project. The user's intent to build a model and high expertise are clear. The problem area is graph machine learning, focusing on node classification. Details like the model's advanced features and the requirements for the dataset are deduced from the instruction. However, specific metric targets, efficiency measures, and preprocessing techniques are not specified, hence they are marked as null or yet to be optimized.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager, mandate the team to develop a state-of-the-art text classification model for a complex, multi-domain Textual Entailment task. In addition to optimizing for precision and recall with a target F1-score of 0.95, they must design a model that adapts dynamically to context shifts, incorporates explainable AI techniques, and meets strict latency constraints for real-time analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"cross-industry (multi-domain)\",\n        \"description\": \"Mandate the team to develop a state-of-the-art text classification model for a complex Textual Entailment task, with a target F1-score of 0.95. The model must also be dynamic to context shifts, incorporate explainable AI techniques, and meet strict latency constraints for real-time analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": \"<insert desired latency value>\",\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multi-domain Textual Entailment Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"classifications\"],\n            \"specification\": {\n                \"size\": \"<specify dataset size>\",\n                \"modalities\": \"multi-domain\",\n                \"contextual_shifts\": true\n            },\n            \"description\": \"A large and diverse text dataset for entailment analysis, containing multiple domains to address context shifts.\",\n            \"preprocessing\": [\n                \"domain adaptation techniques\",\n                \"contextual embeddings\"\n            ],\n            \"augmentation\": [\"contextual noise injection\"],\n            \"visualization\": [\"model interpretability analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-Art Textual Entailment Model\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"<insert model architecture details>\",\n                \"explanation_techniques\": [\"LIME\", \"SHAP\"]\n            },\n            \"description\": \"A neural network-based model designed for Textual Entailment with emphasis on high precision, recall, and dynamic adaptation to context changes.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Multi-domain tasks require models robust to context shifts.\",\n        \"Incorporating explainable AI techniques is crucial for interpretability in complex NLP tasks.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"dynamic feature extraction\",\n            \"context-aware feature selection\"\n        ],\n        \"target_device\": \"<specify target device>\",\n        \"deployment_endpoint\": \"<deployment service or platform>\",\n        \"inference_engine\": [\"real-time inference optimized for latency\"],\n        \"demonstration\": \"Include detailed step-by-step example of how to perform real-time analysis with the model.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the schema and derived directly from the user's instructions. It captures the essence of the project manager's mandate: developing a high-performing, context-sensitive, explainable, and latency-constrained text classification model. However, specific technical details like the target latency value, model architecture, or dataset size, are missing and would need to be provided based on the team's expertise or requirements analysis.\",\n        \"score\": 0.92\n    }\n}"}
{"task": "node classification", "instruction": "As a project manager, you're tasked with overseeing the design and implementation of a cutting-edge machine learning framework. The development team must engineer a Hierarchical Attention Graph Wavelet Convolutional Neural Network (HAGW-CNN) model that surpasses current benchmarks for the comprehensive analysis of Mouse Brain Connectivity and Electrophysiology data. The model should excel in differentiating between intricate neuron subtypes with extreme precision, while simultaneously deciphering complex functional connectivity dynamics. It's crucial to integrate interpretability methods, enabling users to understand decision-making patterns, and ensure the model's scalability for handling massive datasets measured in tens of millions of nodes, necessitating efficient resource allocation and design optimizations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"biomedical research\",\n        \"description\": \"Design and implement a Hierarchical Attention Graph Wavelet Convolutional Neural Network (HAGW-CNN) for comprehensive analysis of Mouse Brain Connectivity and Electrophysiology data. The model should outperform current benchmarks in differentiating neuron subtypes with high precision and interpreting complex functional connectivity dynamics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy for neuron subtype differentiation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision for functional connectivity dynamics\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Connectivity and Electrophysiology\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [\"neuron subtypes\", \"functional connectivity dynamics\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"count\": null,\n                    \"scale\": \"tens of millions\"\n                },\n                \"time_series_length\": null\n            },\n            \"description\": \"A large-scale dataset for mouse brain analysis containing graph and time-series data.\",\n            \"preprocessing\": [\"feature normalization\", \"denoising\"],\n            \"augmentation\": [\"wavelet transforms\"],\n            \"visualization\": [\"connectivity heatmaps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HAGW-CNN\",\n            \"family\": \"Hierarchical Attention Graph Wavelet Convolution\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"not specified\",\n                \"interpretability methods\": \"integrated\",\n                \"resource allocation strategy\": \"optimized\"\n            },\n            \"description\": \"A state-of-the-art model for analyzing mouse brain connectivity and electrophysiology data, emphasizing high precision on neuron subtyping and functional connectivity deciphering.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cutting-edge model and interpretability are priorities\",\n        \"Scalability with large datasets\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"wavelet decomposition\", \"graph convolution\"],\n        \"target_device\": null,\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for resource efficiency\"],\n        \"demonstration\": \"Visual explanations of decision-making patterns and network analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the user's requirement to build a high-end machine learning project, integrating a complex model for the specified task and considering interpretability and scalability. Specific performance metrics are left unspecified due to the lack of detailed benchmark numbers. The user's high expertise level is considered. Confidence is high, but some metrics and exact model architecture specifics are uncertain.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "text classification", "instruction": "As a demanding project director, outline a sophisticated, step-by-step strategy for the AI team to address this challenging text classification problem on the diverse DAIGT dataset. Demmand not only a sequential process of feature engineering, sophisticated style dissection, and iterative cross-validation, but also strive for groundbreaking precision, explainable AI, and continuous refinement through benchmarking against rival models.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"academic or industrial with a focus on AI research\",\n        \"description\": \"Outline a sophisticated strategy for addressing the text classification problem on the diverse DAIGT dataset. The strategy should involve feature engineering, advanced style dissection, iterative cross-validation, high precision, explainable AI, and continuous improvement by benchmarking against competing models.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"class labels\"],\n            \"specification\": {\n                \"diversity\": \"high\",\n                \"data formats\": [\"structured\", \"unstructured\"]\n            },\n            \"description\": \"A diverse text classification dataset that challenges the AI team to handle various styles and content.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"tokenization\",\n                \"lowercasing\",\n                \"stemming\"\n            ],\n            \"augmentation\": [\n                \"synonym replacement\",\n                \"text spinning\",\n                \"sentence permutation\"\n            ],\n            \"visualization\": [\n                \"feature importance analysis\",\n                \"confusion matrix\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"Transformer\", \"BERT\"],\n                \"explainability techniques\": [\"LIME\", \"SHAP\"]\n            },\n            \"description\": \"A deep learning model, potentially utilizing BERT, for text classification with a focus on explainability and precision.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The DAIGT dataset's diversity demands a nuanced and adaptable approach to feature engineering.\",\n        \"Employing advanced style dissection techniques is crucial for improved performance.\",\n        \"Iterative cross-validation helps refine the model and prevent overfitting.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"word embeddings\",\n            \"TF-IDF\",\n            \"custom engineered features\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-optimized inference\"\n        ],\n        \"demonstration\": \"model performance and interpretability illustrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the instruction, considering the user's intent to build a strategy for addressing the text classification problem on the DAIGT dataset. The schema was adapted to include high-level details like dataset description, preprocessing steps, and the model's precision and explainability goals. The user's expertise level is assumed to be high given the demanding project director role. However, specific model architectures, performance metrics, and continuous improvement benchmarks are not provided as they would typically be determined during the project's execution.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a visionary celestial expedition project director, your team is tasked with designing an advanced image recognition system for a myriad of cosmic entities. Mandate the retrieval of the 'GalaxyZoo2' dataset from multiple, secure, and peer-reviewed archives. Rigorously benchmark not only VGG16, ResNet50, and InceptionV3 CNNs, but also explore novel models like EfficientNet and DenseNet. Implement and fine-tune custom data augmentation strategies to handle challenging scenarios, such as low-light images and morphological variations. Furthermore, propose a multi-step approach to address class imbalance using techniques like SMOTE or re-sampling, and develop a hybrid noise reduction algorithm that combines convolutional and statistical methods. Ensure a detailed analysis of trade-offs for each model, ultimately selecting the highest precision model that balances computational efficiency and accuracy amidst these complexities.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Design an advanced image recognition system for various cosmic entities, focusing on retrieving the GalaxyZoo2 dataset from secure archives and benchmarking multiple models (VGG16, ResNet50, InceptionV3, EfficientNet, DenseNet). Implement custom data augmentation, address class imbalance with SMOTE or re-sampling, and develop a hybrid noise reduction algorithm for low-light images and morphological variations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"GalaxyZoo2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source\": \"multiple, secure, and peer-reviewed archives\"\n            },\n            \"description\": \"A dataset for advanced image recognition in the cosmos, containing a wide variety of cosmic entities.\",\n            \"preprocessing\": [\n                \"data retrieval from secure archives\"\n            ],\n            \"augmentation\": [\n                \"custom data augmentation strategies for low-light images and morphological variations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": [\"VGG16\", \"ResNet50\", \"InceptionV3\", \"EfficientNet\", \"DenseNet\"],\n            \"family\": [\"Convolutional Neural Networks\", \"Pre-trained\"],\n            \"type\": [\"neural networks\"],\n            \"specification\": {\n                \"benchmarking\": true\n            },\n            \"description\": \"A collection of well-known CNNs for image classification\"\n        }\n    ],\n    \"knowledge\": [\n        \"Explore novel models like EfficientNet and DenseNet for improved performance.\",\n        \"Custom data augmentation and handling of class imbalance are crucial to handle challenging scenarios.\",\n        \"A hybrid noise reduction algorithm combining convolutional and statistical methods for enhanced image recognition.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A multi-step process involving model evaluation, fine-tuning, and selection.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is a reflection of the provided instruction, focusing on building an advanced image recognition system. High level of expertise is assumed. Metrics are set to null as exact values are not provided, and a combination of VGG16, ResNet50, InceptionV3, EfficientNet, and DenseNet is covered. The model requirements, dataset, and challenges are clearly described. However, some complexities like computational trade-offs between models are not detailed as they would require specific benchmarking results.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, your deep learning team is tasked with optimizing the ImageClassifier-SLD for Stanford Cars Dataset v2.0 with a cutting-edge adaptation of Inception-v4+, called 'InceptionX-4+.+' This enhanced model must exhibit exceptional resilience to challenging conditions, including complex occlusions, nuanced color shifts, and diverse weather effects, targeting a minimum mAP of 98%. Each class must demonstrate a superior benchmark of 95% precision and recall, ensuring robustness not only in laboratory settings but also in real-world, high-precision applications. In addition to delivering the model, prepare a comprehensive, interactive presentation showcasing advanced augmentation techniques, their impact on performance, and a meticulous comparison with the previous top-tier models, emphasizing incremental improvements and potential setbacks. Please also include a detailed evaluation report and code demonstrating the implementation of the novel architecture and augmentation strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"autonomous vehicles / robotics\",\n        \"description\": \"Optimize ImageClassifier-SLD for Stanford Cars Dataset v2.0 with a custom InceptionX-4+ model. The model should exhibit high resilience to challenging conditions and achieve a minimum mAP of 98% and precision/recall of 95% per class for real-world applications.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean average precision (mAP)\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Dataset v2.0\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"description\": \"Stanford Cars dataset with diverse car categories and challenging conditions\"\n            },\n            \"preprocessing\": [\"Advanced augmentation techniques (e.g., occlusions, color shifts, weather effects)\"],\n            \"augmentation\": [\"InceptionX-4+'s custom adaptations\"],\n            \"visualization\": [\"Model performance with augmentation techniques and model comparison\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ImageClassifier-SLD + InceptionX-4+\",\n            \"family\": \"Inception\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explanation\": \"Enhanced Inception-v4+ for challenging conditions\"\n            },\n            \"description\": \"A deep learning model for image classification, focusing on robustness in diverse real-world scenarios\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Resilience to complex occlusions, color shifts, and weather conditions is key.\",\n            \"impact\": \"Aids in model robustness\"\n        },\n        {\n            \"comparison\": \"Incremental improvements over previous top-tier models\"\n        },\n        {\n            \"potential_setbacks\": \"Possible trade-offs between performance and computational complexity\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Custom for the InceptionX-4+' model\"],\n        \"demonstration\": \"Interactive presentation showcasing model performance, augmentation techniques, and comparisons\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent (to build a model), high expertise, the problem area (computer vision), and a detailed problem description. It includes required model performance metrics, dataset characteristics, and custom augmentation strategies. The confidence score is high due to the clear instruction and the appropriate representation of the project requirements.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "For a groundbreaking project, you're tasked with creating a quantum-enhanced transformer model designed specifically for translating unconventional Korean poetry into Elizabethan English, preserving the intricate rhythmic patterns and subtle cultural references in Jangjo and pansori narratives. Your corpus consists of a unique 50,000-episodic anthology with minimal digital footprint. In light of this, detail the specialized NLP techniques, quantum computing integration, and model fine-tuning strategies that will guarantee efficient, real-time translation with artistic authenticity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"artificial intelligence, literature translation\",\n        \"description\": \"Create a quantum-enhanced transformer model for translating unconventional Korean poetry (Jangjo, pansori) to Elizabethan English, preserving rhythmic patterns and cultural references. The corpus consists of a 50,000-episode anthology with minimal digital footprint.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"preservation of rhythmic patterns\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cultural reference preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time translation\",\n                \"value\": {\n                    \"metric_type\": \"throughput\",\n                    \"value\": \">=25 sentences per second\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"50,000-Episode Korean Poetry Anthology\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"size\": 50000,\n                \"language_polarity\": \"unconventional\",\n                \"data_distribution\": \"limited\",\n                \"rhythmic_patterns\": \"Yes\",\n                \"cultural_references\": \"Yes\"\n            },\n            \"description\": \"A unique, episodic anthology of Jangjo and pansori poetry with minimal digital footprint\",\n            \"preprocessing\": [\"quantum data encoding\", \"noise reduction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-Enhanced Transformer\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"quantum_integrations\": [\"quantum attention mechanism\", \"quantum-enhanced tokenization\"],\n                \"fine-tuning_strategies\": [\"transfer learning\", \"custom-domain adaptation\"]\n            },\n            \"description\": \"A model specialized for translating Korean poetry while preserving intricate rhythms and cultural nuances using quantum computing principles.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"quantum-aware feature extraction\"],\n        \"target_device\": \"Quantum Processing Unit (QPUs)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time translator API\"],\n        \"demonstration\": \"Live poetry translation with preserved patterns and cultural context\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the requirements of the given task, detailing a quantum-enhanced transformer model for Korean poetry translation with specific performance metrics (though values are yet to be determined) and integrating quantum computing for real-time efficiency. The high-level description of the project's challenges and desired outcomes, as well as the details on data preprocessing and model fine-tuning, are captured. The confidence score is set to high based on the clear and specific task, but actual model performance metrics will need to be filled in.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, require the machine learning team to design a smartphone-ready plant species classifier that utilizes the 'Plant Seedlings Dataset,' integrating advanced image recognition in low-light scenarios. mandate a real-time AI architecture with a predicted accuracy of at least 98%, and specify a strict constraint of 150ms response time for optimal user experience. Additionally, incorporate transfer learning for efficient model deployment and validated results on multiple mobile platforms.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"mobile and IoT\",\n        \"description\": \"Design a smartphone-ready plant species classifier using the 'Plant Seedlings Dataset', focusing on image recognition in low-light conditions. The architecture must be real-time with a predicted accuracy of at least 98% and have a strict response time constraint of 150ms for optimal user experience. Transfer learning is mandatory for efficient model deployment and validation on multiple mobile platforms.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.15,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Seedlings Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"plant species\"],\n            \"specification\": null,\n            \"description\": \"A dataset for plant species classification, particularly suitable for low-light conditions on smartphones.\",\n            \"preprocessing\": [\"low-light image enhancement\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Real-time AI Classifier\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"AI with real-time capabilities\",\n                \"transfer_learning\": \"applied\"\n            },\n            \"description\": \"A smartphone-ready classifier using transfer learning for plant species identification in low-light conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced image recognition and transfer learning are key for accurate plant classification in low-light scenarios\",\n        \"Real-time response with a 150ms constraint ensures smooth user experience\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"smartphone\",\n        \"deployment_endpoint\": \"mobile platforms (iOS, Android)\",\n        \"inference_engine\": [\"on-device, cloud\"],\n        \"demonstration\": \"Low-light plant species identification in real-time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is parsed according to the provided schema and instruction. The user's intent, problem area, and expertise levels are clearly captured. The performance metric (accuracy) and complexity metric (response time) are both included. The Plant Seedlings Dataset is represented, emphasizing its image modality and low-light suitability. The model requirements for transfer learning and real-time AI are met, as well as the expected deployment on multiple mobile platforms.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "link prediction", "instruction": "Construct a highly scalable PIN model for Yeast Protein Interaction Network, integrating temporal dynamics, with a minimum F1-score of 0.9. Ensure interpretability of topological patterns and validate predictions against gold-standard datasets with at least 5 cross-validation folds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Construct a highly scalable PIN (Protein Interaction Network) model for Yeast, incorporating temporal dynamics, targeting a minimum F1-score of 0.9. Emphasis on interpretability of topological patterns is essential. The model must be validated using at least 5 cross-validation folds against gold-standard datasets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Yeast Protein Interaction Network (with temporal data)\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [\"interactions\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": {\n                    \"nodes\": null,\n                    \"edges\": null,\n                    \"temporal_steps\": null\n                },\n                \"time_series_frequency\": \"daily\"\n            },\n            \"description\": \"A graph dataset representing Yeast protein interactions with temporal dynamics.\",\n            \"preprocessing\": [\n                \"cleaning\",\n                \"feature engineering (temporal features)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"topological patterns visualization\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Scalable PIN Model\",\n            \"family\": \"Temporal Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"PIN with interpretable topological patterns\",\n                \"parameters\": {\n                    \"interpretability Techniques\": [\"attention mechanisms\"]\n                }\n            },\n            \"description\": \"A highly scalable PIN model integrating temporal dynamics for the Yeast Protein Interaction Network, designed to achieve a minimum F1-score of 0.9.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model must account for the hierarchical nature of the yeast protein interactions.\",\n        \"Understanding of topological patterns is crucial for interpretability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"time series embeddings\", \"node centrality measures\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Distributed systems for scalability\"\n        ],\n        \"demonstration\": \"Visualizations and explanations of predicted interactions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the user's request accurately. The problem area, model requirements (high scalability and interpretability), target performance metric, and gold-standard data validation are captured. The dataset, model specifications, and feature engineering steps align with the specified task. However, specific numbers for dataset size or number of cross-validation folds are not provided, so they are left as null. The confidence score is high given the clear instruction and the mapped JSON schema.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a PM for a specialized ML team, we need to develop a state-of-the-art sequential video recommendation system using the 'Esports Tournament Chat' dataset. The model should employ advanced natural language processing and deep learning techniques to parse complex gaming conversations, considering real-time user engagement and context. Targeting low-latency, optimize for gamers' watching habits on mobile platforms, ensuring scalability and a seamless user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"gamification, esports\",\n        \"description\": \"Develop a state-of-the-art sequential video recommendation system for the 'Esports Tournament Chat' dataset. The model should employ advanced NLP and deep learning to understand complex gaming conversations, consider real-time user engagement, and context. Target for low-latency on mobile platforms, with scalability and a seamless user experience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean average precision at k (MAP@k)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"normalized discounted cumulative gain (NDCG)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall at k (R@k)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"real-time\",\n                \"unit\": \"seconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"platform compatibility\",\n                \"value\": \"mobile\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Esports Tournament Chat\",\n            \"modality\": [\"text\", \"video\"],\n            \"target_variables\": [\"video recommendations\"],\n            \"specification\": {\n                \"data_type\": \"time series\",\n                \"size\": \"large\",\n                \"contextual_variables\": [\"real-time user engagement\", \"game context\"]\n            },\n            \"description\": \"A dataset containing complex gaming conversations, real-time engagement data, and context for video recommendations.\",\n            \"preprocessing\": [\"NLP pipeline for gaming conversations\"],\n            \"augmentation\": [],\n            \"visualization\": [\"sequence modeling visualization\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Sequential Video Recommendation System\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"techniques\": [\"NLP models\", \"deep learning\"],\n                \"latency_target\": \"real-time\",\n                \"mobile_optimized\": true\n            },\n            \"description\": \"A state-of-the-art model for video recommendation with advanced NLP and deep learning techniques, targeting mobile platforms and low-latency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced NLP and deep learning will require domain adaptation and gaming lexicon understanding.\",\n        \"Low-latency on mobile platforms requires efficient inference and lightweight architecture.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual user engagement features\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"game platform API or app store\",\n        \"inference_engine\": [\"optimized for mobile devices\"],\n        \"demonstration\": \"Interactive video recommendation showcase within gaming sessions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent, expertise, and the main goal of building a video recommendation system. It includes the relevant areas of AI and ML (multimodal learning, NLP, and deep learning) based on the nature of the data and the task. Performance metrics and complexity constraints for low-latency and mobile are represented, along with a high confidence in the parsed information. However, specific model parameters are not detailed, as the instruction doesn't specify a particular model or metric values.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a specialized data science project manager for a niche media advertising firm, your team is tasked with developing a state-of-the-art regression model using the non-trivial \"Digital Marketing Mix and ROI Impact Analysis Dataset\". This contains intricate financial variables, distinct campaign types, and time-sensitive factors. The dataset has been meticulously shuffled into train, validation, and test subsets. The objective is to forecast the precise cost per unique niche audience segment, ensuring a minimal mean absolute percentage error (MAPE) on the 'Premium Advertiser Spend'. Your challenge is to design an efficient model that not only forecasts accurately but also considers real-world constraints and market dynamics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"media advertising\",\n        \"description\": \"Develop a state-of-the-art regression model using the Digital Marketing Mix and ROI Impact Analysis Dataset, with a focus on forecasting the precise cost per unique niche audience segment. The model must minimize mean absolute percentage error (MAPE) on the 'Premium Advertiser Spend', considering real-world constraints and market dynamics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": 0.01\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": 1024,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Digital Marketing Mix and ROI Impact Analysis Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"cost_per_unique_segment\"],\n            \"specification\": {\n                \"columns\": {\n                    \"financial_variables\": [\"intrinsic complexity\"],\n                    \"campaign_types\": [\"number of distinct categories\"],\n                    \"time_sensitive_factors\": [\"temporal range\"]\n                },\n                \"split\": {\n                    \"train\": {\"percentage\": 70},\n                    \"validation\": {\"percentage\": 15},\n                    \"test\": {\"percentage\": 15}\n                }\n            },\n            \"description\": \"A dataset containing intricate financial, campaign, and time-sensitive data for predicting cost per niche audience segment. It has been shuffled into train, validation, and test subsets.\",\n            \"preprocessing\": [\"feature engineering for financial variables\", \"encoding categorical campaign types\"],\n            \"augmentation\": [],\n            \"visualization\": [\"exploratory data analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art regression\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"explainable\",\n                \"optimization\": \"real-world constraints considerations\",\n                \"seasonality\": \"time-series analysis aware\"\n            },\n            \"description\": \"A sophisticated regression model designed for forecasting advertising costs, integrating financial variables, campaign types, and time-sensitive factors.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Dataset's non-trivial nature requires advanced modeling techniques.\",\n        \"Consideration of market dynamics is crucial for accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"handling multicollinearity\",\n            \"feature selection\"\n        ],\n        \"target_device\": \"cloud server\",\n        \"deployment_endpoint\": \"internal advertising platform API\",\n        \"inference_engine\": [\"high-performance batch inference\"],\n        \"demonstration\": \"live predictions with custom dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction, with the user's intent, high expertise level, and problem domain accurately incorporated. The problem description is detailed, including the financial nature of the dataset and the specific metric (MAPE) to be optimized. Complexity metrics like inference time and memory requirements are mentioned. The data preprocessing and model requirements are inferred from the real-world constraints and market dynamics. However, specific model architecture details were not provided in the instruction, leaving room for interpretation or selection by the team.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "image classification", "instruction": "As a seasoned AI project manager, I've been tasked with developing a comprehensive botanical classification system using a mix of deep learning models. The 'Plant Seedlings Dataset' with 5,000 diverse images presents challenges in real-time image recognition. Propose a multi-stage Convolutional Neural Network (CNN) architecture, incorporating transfer learning and ensemble methods, to optimize accuracy (targeting >95% on a validation set) while maintaining a balance between speed (under 1 second per image) and computational efficiency. Describe the potential trade-offs and provide a detailed experimental plan, along with expected performance improvements over current state-of-the-art models.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"botany\",\n        \"description\": \"Develop a comprehensive botanical classification system using a mix of deep learning models, specifically focusing on the 'Plant Seedlings Dataset' with 5,000 diverse images. The system should achieve high accuracy (targeting >95% on a validation set), real-time performance (under 1 second per image), and computational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"speed\",\n                \"unit\": \"seconds per image\",\n                \"value\": 1\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"unit\": \"FLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Plant Seedlings Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_count\": 5000,\n                \"image_size\": \"variable\",\n                \"diversity\": \"high\"\n            },\n            \"description\": \"A dataset for real-time botanical image recognition with diverse seedlings.\",\n            \"preprocessing\": [\"image resizing\", \"data augmentation\"],\n            \"augmentation\": [\"random cropping\", \"color jittering\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multi-Stage CNN\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"transfer learning with ensemble\",\n                \"FLOPs\": \"optimized for efficiency\"\n            },\n            \"description\": \"Proposed architecture incorporating transfer learning and ensemble methods to achieve high accuracy with speed constraints.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"trade-offs\": \"Potential trade-offs include sacrificing some accuracy for faster performance or optimizing computational efficiency at the risk of lower accuracy.\"\n        },\n        {\n            \"experimental_plan\": \"An experimental plan would involve training and fine-tuning multiple CNN models on the dataset, evaluating ensemble methods, and selecting an optimized architecture that meets the performance and speed requirements.\"\n        },\n        {\n            \"performance_improvements\": \"Expected improvements over state-of-the-art models include higher accuracy, faster inference, and better computational efficiency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"image preprocessing\",\n            \"transfer learning tuning\"\n        ],\n        \"target_device\": \"mobile, cloud\",\n        \"deployment_endpoint\": \"specific API or cloud service\",\n        \"inference_engine\": [\"efficient inference libraries\"],\n        \"demonstration\": \"Interactive plant identification dashboard or live inference demos\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the user's instruction accurately, reflecting their task as an AI project manager to develop a botanical classification system with specific requirements. The metrics, dataset details, and model family were derived from the instruction. However, due to the complexity and details mentioned, it's possible that some information may be subjective or incomplete without further context. The confidence score reflects the high-level understanding of the task.\",\n        \"score\": 0.90\n    }\n}"}
{"task": "image classification", "instruction": "In light of an increasingly diverse customer base with stringent dietary requirements, our restaurant aims to enhance our digital menu cataloguing by leveraging advanced image recognition technology. We need a system that not only detects dishes based on ingredients but also maps them to various dietary categories (vegan, gluten-free, nut-free) and integrates seamlessly with training materials. This must involve multi-level data processing and intelligent classification to ensure accuracy and user-friendliness.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"restaurants\",\n        \"description\": \"The task is to develop an image recognition system for the restaurant's digital menu catalog that detects dishes based on ingredients and maps them to dietary categories (vegan, gluten-free, nut-free). The system should integrate with training materials and require multi-level data processing and intelligent classification for high accuracy and user-friendliness.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision_dietary_categories\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall_dietary_categories\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory_usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"training_data_processing_speed\",\n                \"value\": null,\n                \"unit\": \"images per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"ingredients\", \"dietary_categories\"],\n            \"specification\": null,\n            \"description\": \"A dataset of menu images with corresponding ingredients and dietary categories for training the image recognition system.\",\n            \"preprocessing\": [\"image resizing\", \"data augmentation\"],\n            \"augmentation\": [\"crop\", \"flip\", \"color jitter\"],\n            \"visualization\": [\"dataset analysis\", \"error analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Food Recognition System\",\n            \"family\": \"Convolutional Neural Networks (CNN) with Multi-label Classification\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"number_of_layers\": null,\n                \"architecture\": \"ResNet or EfficientNet\",\n                \"accuracy_target\": null,\n                \"multi-label_compatibility\": true\n            },\n            \"description\": \"An advanced image recognition model using CNNs specifically designed for detecting dishes and mapping to dietary categories.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Multi-level processing is crucial for handling varying levels of complexity in dish recognition and ingredient-dietary mapping.\",\n        \"Integrating with training materials requires model interpretability and suitable interfaces for updating and validating the system.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific feature extraction\", \"adaptation to dietary categories\"],\n        \"target_device\": \"mobile, cloud\",\n        \"deployment_endpoint\": \"digital menu platform API\",\n        \"inference_engine\": [\"TensorFlow.js for seamless web experience\", \"on-device inference for real-time performance\"],\n        \"demonstration\": \"Interactive demo showcasing ingredient recognition and dietary suggestions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the essence of the problem (image recognition for a diverse menu), the expert-level user, and the application domain. Performance metrics are not specified but are assumed to be highly important given the user's requirement for accuracy and user-friendliness. The provided data modalities, preprocessing, and augmentation align with the nature of the task, as does the model type and its compatibility with multi-label classification. The service requirements and demonstration hint at the integration and expected use of the system.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a specialized PM for an esoteric collection of ancient parchments, your team must explore cutting-edge Optical Character Recognition (OCR) algorithms enhanced with image processing techniques to accurately segment intricate historical illustrations in dense and faded manuscripts, ensuring minimal loss of historical context.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"cultural heritage\",\n        \"description\": \"Explore cutting-edge Optical Character Recognition (OCR) algorithms enhanced with image processing techniques to accurately segment intricate historical illustrations in dense and faded manuscripts, preserving historical context.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"segmentation_iou\",\n                \"value\": null\n            },\n            {\n                \"name\": \"loss of historical context\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory_usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ancient Manuscripts\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_format\": \"parchments\",\n                \"image_density\": \"dense and faded\"\n            },\n            \"description\": \"A dataset consisting of intricate historical illustrations and faded manuscripts for OCR and image segmentation tasks.\",\n            \"preprocessing\": [\"image enhancement\", \"denoising\"],\n            \"augmentation\": [\"histogram equalization\", \"contrast stretching\"],\n            \"visualization\": [\"image segmentation masks\", \"accuracy heatmaps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"OCR + Enhanced Image Segmentation\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"OCR network\", \"image segmentation module\"],\n                \"enhancement Techniques\": [\"OCR pre-processing\", \"image processing\"]\n            },\n            \"description\": \"A deep learning model combining OCR and advanced image processing techniques for segmenting historical illustrations in manuscripts.\"\n        }\n    ],\n    \"knowledge\": [\n        \"In this project, we will need to consider OCR algorithms resistant to faded texts and resistant to occlusion in dense illustrations.\",\n        \"Preserving historical context is crucial during segmentation, requiring delicate handling of OCR and image processing parameters.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"script recognition\", \"handwriting style recognition\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"custom artifact repository\",\n        \"inference_engine\": [\"GPU accelerated\", \"CPU-friendly\"],\n        \"demonstration\": \"Interactive visualizations showcasing segmentation results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the user's intent to build a model, high expertise, and the specific problem in the cultural heritage domain. It covers computer vision and image segmentation tasks, with a focus on OCR and image processing for historical manuscripts. The performance metrics and complexity metrics are suggested based on the requirements, and dataset information and model family are based on the hints. Some details, like metric values and preprocessing augmentation techniques, are left unspecified to indicate that they should be derived from the project requirements and available tools.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager seeking a cutting-edge solution, you have a massive dataset of cat images collected from various environments and breeds. Develop a robust deep learning model with at least five different classification algorithms comparison, accounting for transfer learning and real-time performance optimization. Additionally, the model should handle subtle variations in appearance and lighting conditions. Provide a detailed evaluation report and a user-friendly front-end for easy species identification.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"pet identification\",\n        \"description\": \"Develop a deep learning model for classifying cat images, considering transfer learning and real-time performance optimization, with at least five different classification algorithms. The model should be robust to variations in appearance and lighting conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean average precision (mAP)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"optimized for real-time\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cat Image Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\"],\n            \"specification\": {\n                \"data_size\": \"massive\",\n                \"variations\": {\n                    \"environments\": \"various\",\n                    \"breeds\": \"multiple\"\n                }\n            },\n            \"description\": \"Large dataset containing cat images collected from diverse environments and breeds.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"image resizing and normalization\"\n            ],\n            \"augmentation\": [\n                \"lighting\",\n                \"rotation\",\n                \"scaling\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"classification_algorithms\": [\n                    \"ResNet\",\n                    \"Inception\",\n                    \"VGGNet\",\n                    \"MobileNet\",\n                    \"EfficientNet\"\n                ],\n                \"transfer_learning\": true,\n                \"optimization\": \"real-time\"\n            },\n            \"description\": \"A robust deep learning model for cat image classification with multiple algorithms, transfer learning, and real-time performance considerations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consideration of transfer learning is crucial for handling a massive dataset efficiently.\",\n        \"Real-time performance optimization is essential for a user-friendly experience.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\"],\n        \"target_device\": \"mobile, cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow.js\", \"Coral Edge TPU\"],\n        \"demonstration\": \"Web-based user interface for easy species identification\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's intent to build a robust model, considering the high level of expertise. The project's area is computer vision, and the downstream task is image classification. The user's requirements for multiple classification algorithms, transfer learning, real-time performance, and robustness against appearance variations are captured in the description, performance metrics, and complexity metrics. The dataset and model details are based on the provided information, with room for specific model specifications.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "For a cutting-edge e-commerce customer support chatbot, using the extensive AmazonQA dataset, I require a scalable, yet entry-level model. The prototype should exhibit decent accuracy, handle diverse queries efficiently, and possess a rapid response time, ideally integrating with our CRM system for seamless integration. Please propose a lightweight architecture suitable for this specific business scenario.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Develop a scalable and entry-level chatbot for an e-commerce customer support system using the AmazonQA dataset. The model should have decent accuracy, handle diverse queries efficiently, and have a fast response time, with seamless integration into the CRM system.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response_time\",\n                \"value\": {\n                    \"unit\": \"seconds\",\n                    \"value\": {\n                        \"min\": 1,\n                        \"max\": 2\n                    }\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory usage\",\n                \"value\": {\n                    \"unit\": \"MB\",\n                    \"value\": 100\n                },\n                \"description\": \"Low memory footprint for rapid deployment and operation\"\n            },\n            {\n                \"name\": \"inference_speed\",\n                \"value\": {\n                    \"unit\": \"queries/s\",\n                    \"value\": 20\n                },\n                \"description\": \"Fast inference to handle high volumes of queries\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AmazonQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"Extensive dataset for training a customer support chatbot, focused on e-commerce queries.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Lightweight NLP Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-based (like BERT or ALBERT)\",\n                \"model_size\": \"small\"\n            },\n            \"description\": \"A lightweight, scalable NLP model suitable for fast processing and integration with the CRM system.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Entry-level model performance may sacrifice full complexity in exchange for speed and resource efficiency.\",\n        \"Seamless integration with CRM requires well-designed APIs and training data mapping.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"API-first, integrates well with existing infrastructure\"],\n        \"demonstration\": \"Interactive demo with mock customer queries and response times\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the provided instruction, with emphasis on building a lightweight model for e-commerce customer support. User's high level of expertise, the specific dataset (AmazonQA), and the desired attributes (accuracy, efficiency, response time, and integration) are captured in the problem description and performance metrics. The model type and architecture are chosen based on typical requirements for efficient NLP. The confidence score is set to a high value (0.98) because the instruction is clear and the JSON response closely adheres to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Your mission, as a machine learning project manager, is to develop an advanced state-space model for multivariate temporal forecasting in the ETTm2 dataset, which contains intricate industrial sensor data. The dataset has been meticulously divided into training (80%), validation (10%), and testing (10%) subsets, each characterized by a non-overlapping rolling window of 96 time steps (INPUT_SEQ_LEN) with 7 distinct features (INPUT_DIM). The challenge lies in forecasting a subsequent 96-step sequence (PRED_SEQ_LEN) with equal detail, while optimizing for both mean squared error (MSE) and mean absolute error (MAE) in a highly competitive benchmark. Your team must design a customized architecture to capture long-term dependencies and transient patterns within these complex time-series patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"industrial automation\",\n        \"description\": \"Develop an advanced state-space model for multivariate temporal forecasting in the ETTm2 dataset, which contains industrial sensor data with non-overlapping rolling windows and a focus on optimizing for both mean squared error (MSE) and mean absolute error (MAE). The challenge involves forecasting a 96-step sequence following each input sequence.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 96\n            },\n            \"description\": \"A complex industrial sensor dataset with 7 features and a non-overlapping rolling window for training, validation, and testing split.\",\n            \"preprocessing\": [\n                \"Rolling window with 96 time steps\",\n                \"Window overlap management\",\n                \"7 feature handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"In-depth data exploration, including time-series patterns\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"State-Space Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"Customized architecture\": {\n                    \"long-term dependency capture\": \"LSTM or Transformer\",\n                    \"transient patterns\": \"Temporal Convolution Networks or Attention mechanisms\"\n                },\n                \"Training details\": {\n                    \"Optimization algorithm\": \"Adam or other advanced optimizer\",\n                    \"Regularization techniques\": \"e.g., dropout, weight decay\"\n                }\n            },\n            \"description\": \"A tailored neural network model for advanced time-series forecasting in the ETTm2 dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Managing non-overlapping rolling windows and handling complex industrial sensor data requires attention to data handling and feature engineering.\",\n        \"Balancing MSE and MAE as evaluation metrics requires appropriate model architecture choices and tuning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature extraction for state representation\",\n            \"Feature engineering to capture dependencies\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstration of forecasting performance with visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction, identifying the problem area as time-series analysis, and the specific dataset and model requirements. However, specific technical details about the state-space model architecture and performance optimization methods are not provided as they would require deeper domain knowledge. The confidence score accounts for this, allowing for room to elaborate if the expert's instructions are more specific.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "node classification", "instruction": "For a machine learning project, the YouTube Algebric Connectivity dataset must be employed to develop a Significance Graph Neural Network (SGNN) for node classification. The model must achieve at least 85% accuracy in distinguishing five distinct categories of content creators, while demonstrating exceptional performance in dealing with highly sparse data and optimizing for computational efficiency using pruning techniques.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"video analysis\",\n        \"description\": \"Develop a Significance Graph Neural Network (SGNN) for node classification on the YouTube Algebric Connectivity dataset. The model must achieve at least 85% accuracy in distinguishing five content creator categories, and must be efficient with pruning techniques for handling sparse data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"optimized\",\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": \"optimized with pruning techniques\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube Algebric Connectivity\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"density\": \"sparse\",\n                \"data_format\": \"connectivity matrix\"\n            },\n            \"description\": \"A sparse graph dataset for node classification, representing the YouTube creators and their connections in terms of algebric connectivity.\",\n            \"preprocessing\": [\"handling sparse data\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Significance Graph Neural Network (SGNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"distinguishing 5 categories\",\n                \"pruning_techniques\": \"employed\"\n            },\n            \"description\": \"A GNN designed for node classification on highly sparse datasets like YouTube Algebric Connectivity, utilizing pruning for efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"SGNNs excel in sparse graph data with pruning, making them suitable for the YouTube Algebric Connectivity dataset.\",\n        \"Achieving high accuracy of 85% on a classification task with multiple categories.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"demonstrate performance on test data with explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction, including the dataset, task, performance requirements, and the model. The expert level of the user, specific model architecture, and the need for pruning techniques are clearly defined. However, a detailed description of preprocessing steps for handling sparsity is missing, so the process is assumed. The confidence score is 0.97 due to the clarity of the instruction and the structured response, but the absence of specific detail on preprocessing.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "node classification", "instruction": "The project manager should request a highly optimized node classification model for the Coauthorship Science dataset, utilizing a tailored HGNN architecture with feature integration from multi-scale graph levels and in addition, incorporate author metadata for enhanced accuracy, while maintaining a constraint on model interpretability for scientific transparency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic\",\n        \"description\": \"Request a highly optimized node classification model for the Coauthorship Science dataset. The model should be based on a tailored HGNN (Hypergraph Neural Network) architecture, with feature integration from multi-scale graph levels and enhanced accuracy by incorporating author metadata. The model should also prioritize interpretability for scientific transparency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"optimized\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"parameters\"\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": \"high\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coauthorship Science\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_nodes\": null,\n                \"graph_dimensions\": {\n                    \"node_types\": [],\n                    \"edge_types\": []\n                },\n                \"metadata_features\": {\n                    \"author\": {\n                        \"number_of_fields\": null\n                    }\n                }\n            },\n            \"description\": \"A graph dataset for node classification in the field of science, with author metadata for enhanced model performance.\",\n            \"preprocessing\": [\n                \"multi-scale graph integration\",\n                \"author metadata incorporation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"tailored HGNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"HGNN with multi-scale feature integration\",\n                \"interpretability_layers\": \"include for interpretability\"\n            },\n            \"description\": \"A highly-optimized hypergraph neural network model for node classification in the Coauthorship Science dataset, designed with feature integration from multi-scale graph levels and author metadata for improved accuracy and scientific transparency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating author metadata can significantly enhance model performance in a coauthorship context.\",\n        \"Designing for model interpretability is crucial for maintaining scientific transparency.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"require interpretability-friendly visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response closely follows the schema, with the intent, task, and problem description derived from the given instruction. User expertise and domain are assumed high based on the project manager role. Performance metrics like accuracy and constraints on model complexity are included, although specific values are not provided (accuracy, model size). The dataset and model details are tailored to the Coauthorship Science dataset and the specified architecture. Some missing information can be inferred (dataset size, author metadata details), but the confidence score is reduced accordingly.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create a cutting-edge neural machine translation system utilizing the Large Scale Multilingual dataset, specifically tailored for translating complex scientific jargon from English to Russian. The model should employ the M6 architecture, incorporating memory-efficient techniques such as M6-MT for resource-constrained environments like foldable tablets with limited computational power and long battery life. Additionally, design an interactive user interface that seamlessly integrates speech-to-text and text-to-speech capabilities, ensuring real-time and accurate translation during live scientific presentations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"scientific research\",\n        \"description\": \"Create a cutting-edge neural machine translation system using the Large Scale Multilingual dataset, focusing on English to Russian translation of complex scientific jargon. The model should employ the M6 architecture and incorporate memory-efficient techniques like M6-MT for resource-constrained environments like foldable tablets with limited computational power and long battery life.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"translation accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Large Scale Multilingual\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"languages\": [\"English\", \"Russian\"],\n                \"complexity\": \"Scientific jargon\"\n            },\n            \"description\": \"A large-scale multilingual dataset with a focus on English to Russian translation, covering complex scientific terminology.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"M6\",\n                \"memory-efficient\": true,\n                \"M6-MT\": true\n            },\n            \"description\": \"An advanced neural machine translation model using the M6 architecture, optimized for resource-constrained environments.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"foldable tablets\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"speech-to-text\", \"text-to-speech\"],\n        \"demonstration\": \"real-time and accurate translation during live scientific presentations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a machine translation model, their high expertise level, and the requirements for a complex, memory-efficient system tailored for foldable tablets. The problem area, description, and the need for M6-MT are based on the instruction. The target performance metrics are not specified but can be inferred as translation accuracy. Complexity metrics such as inference time and memory consumption are included, acknowledging the resource constraints. The dataset and model specifications match the given instructions, and the service requirements for the user interface are clear. However, specific values for performance metrics and details on user interface design are left to be filled with more detailed information.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Utilize the Mouse Brain Connectivity Dataset, presenting a novel approach that combines GWNN and attention mechanisms for enhanced node labeling. The model should exhibit state-of-the-art performance in discriminating between rare neuron subclasses, while maintaining scalability and efficiency in handling massive, high-resolution connectivity graphs with intricate edge patterns reflecting microcircuit complexities.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Utilize the Mouse Brain Connectivity Dataset to develop a novel model combining GWNN (Graph Wavelet Neural Networks) and attention mechanisms for enhanced node labeling, focusing on discriminating rare neuron subclasses while ensuring scalability and efficiency for large, high-resolution connectivity graphs with complex edge patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Connectivity Dataset\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [\"neuron subclasses\"],\n            \"specification\": {\n                \"size\": null,\n                \"node_features\": null,\n                \"edge_features\": null,\n                \"graph_structure\": \"high-resolution\"\n            },\n            \"description\": \"A large, high-resolution dataset of mouse brain connectivity with complex edge patterns and intricate microcircuit details.\",\n            \"preprocessing\": [\n                \"denoise\",\n                \"feature extraction\"\n            ],\n            \"augmentation\": [\"synthetic edge generation\"],\n            \"visualization\": [\n                \"edge pattern analysis\",\n                \"neuron subclass clustering\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GWNN with Attention Mechanisms\",\n            \"family\": \"neural networks\",\n            \"type\": \"graph neural networks\",\n            \"specification\": {\n                \"architectural_details\": \"combined GWNN and attention mechanisms\",\n                \"specifically_for_node_classification\": true\n            },\n            \"description\": \"A state-of-the-art model that combines Graph Wavelet Neural Networks with attention for improved node labeling in the context of mouse brain connectivity data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Combining graph wavelets and attention can boost performance in discriminating rare neuron subclasses.\",\n        \"Scalability and efficiency are essential due to the massive, high-resolution nature of the dataset and microcircuit complexities.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"graph embedding\",\n            \"attention-based feature refinement\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"model-as-a-service platform\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"interactive neuron subclass visualization and performance comparison\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the task of building a high-performance node classification model using the Mouse Brain Connectivity Dataset. The model combines GWNN and attention mechanisms, and the user has high expertise. Metrics for performance and specific edge patterns are included. The model specifications and the dataset source are inferred from the instruction, though the exact performance metrics are not provided. The confidence score is high due to the clear user intent and the well-structured response aligning with the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, request a high-performance bidirectional LSTM model, utilizing the extensive Europarl v7 dataset for simultaneous Spanish-English translation. Emphasize the development of an optimized architecture that minimizes latency for real-time, ultra-responsive applications, targeting sub-300ms response times with continuous benchmarking and rigorous efficiency optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Request a high-performance bidirectional LSTM model for simultaneous Spanish-English translation using the Europarl v7 dataset. The model should prioritize optimized architecture for minimizing latency to achieve sub-300ms response times, suitable for real-time, ultra-responsive applications, with continuous benchmarking and efficiency optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 300.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null, \n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Europarl v7\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"language_types\": [\"Spanish\", \"English\"]\n            },\n            \"description\": \"A dataset for simultaneous Spanish-English translation, sourced from Europarl v7.\",\n            \"preprocessing\": [\"tokenization\", \"lemmatization\", \"sentence segmentation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"High-performance Bidirectional LSTM\",\n            \"family\": \"Sequence to Sequence models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"bidirectional\",\n                \"model_size\": \"optimized\",\n                \"inference_speed\": \"real-time\"\n            },\n            \"description\": \"A state-of-the-art model designed for simultaneous translation, focusing on Spanish-English, with a focus on minimizing latency for sub-300ms response times.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Efficiency is crucial for achieving ultra-responsive applications, and optimized architecture will play a vital role.\",\n        \"Real-time performance is key due to the requirement for ultra-responsiveness.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain adaptation\", \"transfer learning\"],\n        \"target_device\": \"cloud-based or edge computing\",\n        \"deployment_endpoint\": \"cloud or dedicated server\",\n        \"inference_engine\": [\"TensorFlow\", \"TensorRT\", \"Custom inference engine\"],\n        \"demonstration\": \"real-time translation demonstration with latency measurements\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the user's request to build a high-performance model for a specific task (Spanish-English translation). The area of expertise, dataset, performance metrics (emphasizing response time), and complexity metrics (specifically latency) are extracted from the instruction. The model name, family, and type are derived from the LSTM and bidirectional nature of the model. Given the focus on real-time and ultra-responsiveness, the performance metrics and additional service requirements reflect these aspects. However, exact preprocessing and model specification details are not provided in the instruction, leaving room for interpretation or consultation.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "object detection", "instruction": "In a specialized grocery retail sector, develop a state-of-the-art YOLOv5 deep learning model trained specifically on the \"GreenFoods2022\" dataset, designed for detecting and classifying perishable and organic products with high accuracy (95% AP@0.5:0.95). The model must handle images captured in low light conditions and demonstrate exceptional real-time performance, achieving sub-second latency (0.5 seconds per shelf scan) for seamless inventory updates in the \"EcoFriendlyMart\" system.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"Develop a state-of-the-art YOLOv5 model for detecting and classifying perishable and organic products in the specialized 'GreenFoods2022' dataset. The model should have an accuracy of at least 95% AP@0.5:0.95, handle low light conditions, and achieve sub-second latency (0.5 seconds per shelf scan) for efficient inventory updates in the 'EcoFriendlyMart' system.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy (AP@0.5:0.95)\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency (per shelf scan)\",\n                \"value\": 0.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"GreenFoods2022\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_dim\": \"[height, width] (adapted for YOLOv5)\",\n                \"data_distribution\": \"low light conditions\"\n            },\n            \"description\": \"A dataset specifically for detecting and classifying perishable and organic products in low light conditions.\",\n            \"preprocessing\": [\"data augmentation for low light conditions\"],\n            \"augmentation\": [\"image enhancements for low light scenarios\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5 (state-of-the-art)\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specifically designed for YOLOv5\",\n                \"parameters\": \"optimized for low light performance and real-time detection\",\n                \"FLOPs\": \"[value] (high for real-time capability)\"\n            },\n            \"description\": \"Advanced deep learning model for real-time object detection in green food products with high accuracy and low-latency performance\"\n        }\n    ],\n    \"knowledge\": [\n        \"Low light image handling and real-time performance are crucial for the EcoFriendlyMart's inventory system.\",\n        \"State-of-the-art YOLOv5 model will ensure high detection and classification accuracy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"YOLOv5 compatible data preprocessing techniques\"],\n        \"target_device\": \"edge or mobile (for real-time performance)\",\n        \"deployment_endpoint\": \"EcoFriendlyMart's inventory management system\",\n        \"inference_engine\": [\"optimized for YOLOv5 deployment on the target device\"],\n        \"demonstration\": \"Model performance on sample low light shelf scans\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been parsed into a JSON response that covers all required aspects. The user's intent to build a model, their high expertise, and the specific requirements for the model (accuracy, low light, and latency) are reflected. The dataset and model details, as well as preprocessing, augmentation, and inference engines, are included. The confidence is high, given the clear instruction and appropriate mapping to the JSON schema. However, specific model parameters and training details, such as FLOPs or optimal data preprocessing techniques, are not included due to the lack of explicit information.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "link prediction", "instruction": "For a machine learning project, design an advanced system that can analyze intricate citation patterns in the diverse Citeseer dataset by incorporating text embeddings and feature engineering, targeting high accuracy and robustness in identifying latent connections between scholarly publications.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Design an advanced system for analyzing intricate citation patterns in the Citeseer dataset using text embeddings and feature engineering. The goal is to achieve high accuracy and robustness in detecting latent connections between scholarly publications.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Citeseer\",\n            \"modality\": [\n                \"text\"\n            ],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": {\n                    \"text_embedding\": {},\n                    \"feature_engineered_features\": {}\n                },\n                \"sparse\": null\n            },\n            \"description\": \"A diverse scholarly publication dataset with citation information, suitable for text analysis and link prediction tasks.\",\n            \"preprocessing\": [\n                \"text tokenization\",\n                \"text embedding generation\",\n                \"feature engineering from text data\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"advanced citation analysis model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep learning\",\n                \"layers\": [],\n                \"learning_method\": \"supervised\"\n            },\n            \"description\": \"A neural network model that incorporates text embeddings and feature engineering techniques specifically designed for citation pattern analysis in academic literature.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Citation patterns show the interconnectedness of scholarly publications, necessitating a system that can handle complex relations in the data.\",\n        \"Incorporating text embeddings helps capture meaning and context in citation references.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"TF-IDF, word embeddings, domain-specific features\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Case studies or visualizations illustrating link prediction performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the user's instruction accurately, reflecting the task's focus on designing an advanced system for analyzing citation patterns in the Citeseer dataset with text embeddings and feature engineering. The area and downstream task are specified as natural language processing and link prediction, respectively. Expertise level is set to high, and the dataset is assumed to be accessed through a user-provided link. The model type is assumed to be neural networks, but exact specifications are not provided as the instruction does not specify a particular architecture. Performance metrics (accuracy and robustness) are not provided numeric targets, leaving room for optimization. The confidence score is high based on the clear interpretation of the task description.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for a high-precision AI development team, you are tasked with tackling a multifaceted text classification problem. The Textual Entailment dataset, enriched with domain-specific lexicons, contains thousands of pairs of texts with nuanced relationships: neutral (0), contradiction (1), and entailment (2). Your goal is to enhance the team's performance by optimizing a state-of-the-art deep learning model, ensuring robustness and interpretability. Emphasize on both macro-average F1-score and interpretability metrics, and conduct rigorous experimentation with various architectures and feature extraction techniques for a cutting-edge solution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"As a project manager, task is to enhance the team's performance on a multifaceted Textual Entailment problem using a state-of-the-art deep learning model. Focus on improving macro-average F1-score and interpretability while experimenting with different architectures and feature extraction techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"macro-average F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Textual Entailment\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"neutral\", \"contradiction\", \"entailment\"],\n            \"specification\": {\n                \"size\": \"thousands\",\n                \"domain_specific_lexicons\": true\n            },\n            \"description\": \"Enriched dataset containing pairs of texts with nuanced relationships - neutral, contradiction, and entailment.\",\n            \"preprocessing\": [\"domain-specific lexicon augmentation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art deep learning models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"robustness\": \"high precision and interpretability\",\n                \"interpretability_features\": \"to be determined\"\n            },\n            \"description\": \"A cutting-edge model for text classification, focusing on Textual Entailment task with emphasis on optimization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Multifaceted nature of the Textual Entailment dataset requires deep understanding of domain lexicons.\",\n        \"Enhancing performance and interpretability are key goals.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"exploratory analysis\", \"feature selection\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-friendly deep learning inference libraries\"],\n        \"demonstration\": \"results and model interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the task, including the goal (enhancing model performance and interpretability), the dataset (Textual Entailment with domain lexicons), the focus on F1-score and interpretability metrics, and the experimentation with various architectures and feature extraction methods. The user's intent, high expertise, and the relevance of the application domain are also covered. The dataset source is assumed to be a user-provided link due to the mention of 'enriched dataset'. Confidence is high, but target F1-score value is not specified and may need to be filled based on experimental results.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "For a large-scale, real-time translation system of Korean web novels into multiple languages, including English, you need to select a versatile and scalable machine translation model. Consider requirements such as domain adaptation, parallel corpus, transfer learning potential, and efficiency in handling vast amounts of text. Also, discuss the impact of preserving cultural nuances and the necessity for post-editing to ensure quick, fluent, and high-quality outputs.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"translation services\",\n        \"description\": \"Select a versatile and scalable machine translation model for real-time translation of Korean web novels into multiple languages (e.g., English). Consider domain adaptation, parallel corpus, transfer learning potential, and efficiency in handling large text volumes. Discuss the importance of preserving cultural nuances and the post-editing requirements for high-quality, fluent outputs in a real-time context.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"speed\",\n                \"value\": null,\n                \"description\": \"Efficiency in handling vast amounts of text in real-time\"\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null,\n                \"description\": \"Versatility and quality of translations\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"parallel corpus utilization\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Korean Web Novel Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"large\",\n                \"text_vocabulary\": null,\n                \"parallel_corpora\": null\n            },\n            \"description\": \"A massive dataset of Korean web novels for translation model training and domain adaptation\",\n            \"preprocessing\": [\"cultural adaptation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural machine translation (NMT)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer\",\n                \"domain_adaptation\": true,\n                \"transfer_learning\": true\n            },\n            \"description\": \"A scalable NMT model with strong potential for domain adaptation and transfer learning, designed for real-time translation of Korean web novels.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"cultural_preservation\": \"Model must account for cultural nuances to retain authenticity in translations.\"\n        },\n        {\n            \"post_editing\": \"Post-editing for quality control is crucial given the high volume of translation outputs.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"customization for target languages\"],\n        \"target_device\": \"real-time translation server\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"GPU-accelerated processing\"\n        ],\n        \"demonstration\": \"Demonstration of rapid and high-quality translations with minimal post-editing required\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent for model development, the high-level requirements, and the specific problem domain. Performance metrics are suggested based on the context (speed and accuracy), considering the real-time aspect of the translation system. The model choice and its features are tailored to the task, and the importance of cultural nuances and post-editing is recognized. However, specific metrics like speed, accuracy, parallel corpus usage, and memory footprint are not provided due to lack of explicit values in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "As the lead project manager for a niche luxury retail inventory management solution, we require a specialized Faster R-CNN model, fine-tuned on the exclusive 'Premium Limited Edition SKU-110K' dataset. The model should excel in recognizing and precisely counting rare artefacts and high-end garments on boutique store shelves, ensuring zero misclassification. Demand a comprehensive report showcasing the precision-recall trade-off for distinct product subclasses, including haute couture and vintage collectibles.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury retail\",\n        \"description\": \"As a project manager, we need a specialized Faster R-CNN model fine-tuned on the 'Premium Limited Edition SKU-110K' dataset, focusing on recognizing and counting rare artefacts and high-end garments with zero misclassification in boutique store inventory.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Premium Limited Edition SKU-110K\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"exclusive\",\n                \"modality_type\": \"boutique inventory\"\n            },\n            \"description\": \"A specialized dataset containing rare artefacts and high-end garments for fine-tuning an object detection model.\",\n            \"preprocessing\": [\"data cleaning\", \"resizing\"],\n            \"augmentation\": [\"horizontal flips\", \"small translations\"],\n            \"visualization\": [\"class distribution\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Faster R-CNN\",\n            \"family\": \"Region-based CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"fine-tuned\",\n                \"target_classes\": [\"haute couture\", \"vintage collectibles\"]\n            },\n            \"description\": \"A specialized Faster R-CNN model optimized for recognizing and counting rare artefacts and high-end garments in boutique inventory, with emphasis on zero misclassification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The dataset requires meticulous handling due to its exclusive nature and relevance to luxury retail.\",\n        \"The model should exhibit high precision and recall on the distinct product subclasses for accurate identification.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"custom ROI extraction\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"custom-managed platform\",\n        \"inference_engine\": [\"TensorFlow Serving\"],\n        \"demonstration\": \"A report showcasing the precision-recall trade-off for haute couture and vintage collectibles subsets.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the user's intent to build a specialized model with high requirements for precision and recall on the given dataset. Expertise is high, reflecting the project manager's role. The dataset properties, preprocessing steps, and model specifics are derived from the instruction, while the absence of explicit metric values allows for interpretation and filling based on the actual project execution.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a machine learning project manager for a cutting-edge research team, your objective is to develop a state-of-the-art time series classification model using the UWave Gesture Library dataset with advanced pre-processing techniques. The train, validation, and test splits consist of highly complex, high-dimensional sequences (INPUT_SEQ_LEN=315, INPUT_DIM=3) representing diverse real-world gestures. The model must not only demonstrate superior accuracy but also exhibit interpretability and resilience to noise. Ensure the model's generalization capability is demonstrated on unseen sequences with a target label range of {0, 1, ..., 7}, and employ innovative evaluation strategies, such as F1-score and AUC-ROC, to measure performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"research\",\n        \"description\": \"Develop a state-of-the-art time series classification model using the UWave Gesture Library dataset with advanced pre-processing techniques. The data consists of complex, high-dimensional sequences (INPUT_SEQ_LEN=315, INPUT_DIM=3) representing diverse real-world gestures. The model should exhibit superior accuracy, interpretability, and noise resilience. The model's generalization capability should be tested on unseen sequences with a target label range of {0, 1, ..., 7}. Innovative evaluation strategies, such as F1-score and AUC-ROC, are required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model interpretability\",\n                \"value\": null,\n                \"unit\": \"interpretability\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UWave Gesture Library\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 315,\n                \"INPUT_DIM\": 3\n            },\n            \"description\": \"A high-dimensional time series dataset containing real-world gesture data with complex sequences.\",\n            \"preprocessing\": [\"advanced pre-processing techniques\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art time series classification\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true\n            },\n            \"description\": \"A deep learning model tailored for time series classification on the UWave Gesture Library dataset, emphasizing interpretability and noise resilience.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced pre-processing techniques are crucial for handling complex and high-dimensional sequences.\",\n        \"Model interpretability is required for a cutting-edge research project.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"advanced pre-processing\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Model performance on unseen sequences\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction by describing the project objective as a 'build' task with a 'high' level of expertise. It captures the research area, task, and requirements such as accuracy, interpretability, and noise resilience. Performance metrics and dataset details have been included based on the given constraints. However, exact metric values and model specifications are not specified as they depend on the specific model chosen, which is left open. The confidence score is high considering the clear mapping of the instruction to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for a mobile app that demands real-time dog breed recognition with high accuracy, using the Stanford Dogs Dataset, you seek a lightweight, yet powerful deep learning model. The app must have minimal latency and be scalable for diverse breeds, while ensuring energy efficiency on mobile devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"mobile app development\",\n        \"description\": \"Develop a lightweight, yet powerful deep learning model for real-time dog breed recognition in a mobile app using the Stanford Dogs Dataset. The model must have high accuracy, low latency, be scalable across various breeds, and optimize energy efficiency on mobile devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"minimal\"\n            },\n            {\n                \"name\": \"model complexity\",\n                \"value\": \"lightweight\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\n                    \"unit\": \"ms\",\n                    \"value\": \"<200\"\n                }\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": {\n                    \"unit\": \"MB\",\n                    \"value\": \"<100\"\n                }\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": \"optimized\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Dogs Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data format\": \"JPEG\",\n                \"number of breeds\": null,\n                \"sample images\": \"diverse\",\n                \"preprocessing required\": \"for mobile app use\"\n            },\n            \"description\": \"A large-scale dataset for dog breed recognition with a focus on real-world scenarios and mobile device performance.\",\n            \"preprocessing\": [\"for mobile app use, likely resizing, normalization\"],\n            \"augmentation\": [\"to increase breed diversity in the training set\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (CNN) or MobileNet Variants\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"depthwise separable convolution\": \"for efficiency\",\n                \"number of parameters\": \"limited\"\n            },\n            \"description\": \"A lightweight deep learning model optimized for high accuracy, low latency, and mobile device energy efficiency, specifically for the task of real-time dog breed recognition.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"on-device inference\", \"server-side inference for improved accuracy\"],\n        \"demonstration\": \"real-time performance on mobile app prototype\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the user's requirements as a project manager. The intent is to build, the dataset is Stanford Dogs, the task is image classification, the user's expertise is high, and the application domain is mobile app development. Performance metrics such as accuracy and latency are specified, along with the desire for a lightweight model. Complexity metrics targeting inference time, memory usage, and energy efficiency are included. The model family is chosen considering the constraints, and some preprocessing steps, augmentation, and inference engines are mentioned.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "As the project manager for a cutting-edge machine learning team, we've been tasked with designing a state-of-the-art medical image analysis system using the Combined Radiomic-MRI dataset. The team must employ novel Graph Neural Networks (GNNs) to identify early-stage Alzheimer's disease with an unprecedented AUC of 98%. The project must showcase GNNs' ability to capture intricate spatial relationships and outperform existing deep learning architectures like SOTA transformers. Additionally, a thorough ethical assessment on data privacy and interpretability of predictions must be included, emphasizing responsible AI practices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medical\",\n        \"description\": \"Design a state-of-the-art medical image analysis system using the Combined Radiomic-MRI dataset. The system should employ Graph Neural Networks (GNNs) to identify early-stage Alzheimer's disease with an AUC of 98%, demonstrating GNNs' ability to capture spatial relationships and outperform SOTA transformers.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Combined Radiomic-MRI\",\n            \"modality\": [\"image\", \"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_format\": \"MRI+Radiomics\",\n                \"size\": {\n                    \"images\": {\n                        \"number\": \"unknown (will be required for evaluation)\",\n                        \"dimensions\": \"\"\n                    },\n                    \"graphs\": \"unknown (will be required for GNN implementation)\"\n                },\n                \"type\": \"medical images\"\n            },\n            \"description\": \"Dataset for identifying early-stage Alzheimer's disease using combined radiomic and MRI data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novelty\": \"state-of-the-art in GNNs\",\n                \"outperforms\": \"SOTA transformers\",\n                \"spatial_relationship\": \"explicitly captured\"\n            },\n            \"description\": \"A novel GNN-based system designed specifically for identifying early-stage Alzheimer's from radiomic-MRI data.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"novelty_of_gnn\": \"emphasizes the superiority of GNNs in capturing intricate spatial relationships\",\n            \"ethical_practices\": \"includes a thorough ethical assessment on data privacy and interpretability\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"responsible AI practices and performance demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a cutting-edge system, highlighting GNNs, AUC performance target, and the emphasis on responsible AI practices. However, specific dataset details are not directly provided, requiring a search for Combined Radiomic-MRI data with appropriate specifications. The model details and feature engineering steps are left open-ended due to the lack of specific deep learning techniques or feature extraction requirements.\",\n        \"score\": 0.93\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For our next-generation urban scene analysis, we need a state-of-the-art U-Net model optimized for the Cityscapes dataset. It should demonstrate exceptional segmentation prowess by identifying and categorizing 30 intricate urban elements with a strict minimum accuracy of 95%, ensuring interpretability. The model must not only outperform current benchmarks but also excel in real-time efficiency, processing high-resolution images at a consistent 50 FPS on the most advanced GPUs while maintaining latency below 5 milliseconds. Moreover, kindly incorporate a modular design for easy integration with future city planning projects and provide detailed documentation for model reproduction and fine-tuning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"Design a state-of-the-art U-Net model optimized for the Cityscapes dataset to analyze urban scenes, identifying and categorizing 30 intricate elements. The model must achieve minimum 95% accuracy, outperform benchmarks, operate in real-time (50 FPS) on advanced GPUs, and maintain latency < 5 milliseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 50,\n                \"unit\": \"FPS\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 5,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"urban elements labels\"],\n            \"specification\": null,\n            \"description\": \"A high-resolution urban scene dataset for image segmentation, with a focus on complex urban elements classification.\",\n            \"preprocessing\": [\n                \"image resizing to a fixed resolution for real-time performance\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"U-Net\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"state-of-the-art U-Net\",\n                \"number_of_classes\": 30,\n                \"real-time_performance\": true,\n                \"GPU_suitability\": \"advanced\"\n            },\n            \"description\": \"A specialized U-Net model for the Cityscapes dataset, designed for high accuracy, real-time processing, and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Modular design is essential for easy integration with future city planning projects.\",\n        \"Detailed documentation is required for model reproduction and fine-tuning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"custom image resizing for real-time performance\"],\n        \"target_device\": \"most advanced GPUs\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for high-resolution image processing\"],\n        \"demonstration\": \"Exemplify segmentation results at various 50 FPS on advanced GPUs and low latency.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's requirements, from the state-of-the-art U-Net model's design to performance expectations and real-time efficiency. The user's intention to build the model and high level of expertise are reflected. The included performance metrics and complexity constraints match the specified criteria. However, more precise details on model architecture and hyperparameters are not provided as they are not directly stated in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager seeks a real-time, energy-efficient U-Net model for accurate segmentation of indoor scenes, specifically identifying furniture with minimal computational delay, while ensuring compatibility with resource-constrained devices for widespread deployment in smart home applications.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"smart home\",\n        \"description\": \"Develop a real-time, energy-efficient U-Net model for accurate segmentation of indoor scenes, focusing on furniture identification with minimal computational delay. The model should be compatible for deployment on resource-constrained devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"f1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational_delay\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"inference energy consumption\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": null, \n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source\": \"user-upload\",\n                \"data_split\": [\"train\", \"validation\", \"test\"],\n                \"image_size\": \"variable (user specified for real-time)\",\n                \"label_type\": \"binary (for furniture segmentation)\"\n            },\n            \"description\": \"A diverse set of indoor scenes with varying furniture content for model training and evaluation\",\n            \"preprocessing\": [\"image resizing\", \"data augmentation\"],\n            \"augmentation\": [\"horizontal flip\", \"lighting adjustments\"],\n            \"visualization\": [\"confusion matrix\", \"performance heatmaps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net\",\n            \"family\": \" convolutional neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"encoder_blocks\": \"appropriate for real-time constraints\",\n                    \"decoder_blocks\": \"optimized for efficiency\"\n                },\n                \"pre-trained\": \"optional (depends on provided dataset)\"\n            },\n            \"description\": \"A real-time, energy-efficient U-Net designed specifically for furniture segmentation in indoor scenes, targeting resource-constrained devices\"\n        }\n    ],\n    \"knowledge\": [\n        \"U-Net is suitable for dense image segmentation tasks.\",\n        \"Real-time constraints and low computational delay are key requirements.\",\n        \"Compatibility with resource-constrained devices is a major focus.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"lightweight feature extraction\"],\n        \"target_device\": \"resource-constrained IoT devices\",\n        \"deployment_endpoint\": \"smart home platform\",\n        \"inference_engine\": [\"optimized for mobile or embedded systems\"],\n        \"demonstration\": \"mobile app-based real-time segmentation demo\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response follows the JSON schema, capturing the user's intent, high expertise level, the specific problem in computer vision (image segmentation), with emphasis on U-Net and efficiency for smart home applications. Performance metrics, including accuracy, F1-score, computational delay, energy efficiency, and memory footprint, are relevant to the task. The request for a user-uploaded dataset and the need for lightweight preprocessing and inference methods are addressed. However, the specific values for the performance metrics are not provided as they depend on the model training results. The confidence score is high given the clarity of the task description.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for a state-of-the-art machine learning team, you're tasked with developing a highly sophisticated regression model. The Mohs Hardness Dataset, now with an extensive 500,000 data points, presents a multidimensional challenge due to its complex interactions between features, non-linear relationships, and outliers. Utilize advanced feature engineering, ensemble techniques, and handle potential missing values. The objective is not only to minimize MedAE but also achieve interpretability, considering industry-specific constraints and complying with fairness metrics. Train, validate, and optimize the model on a high-performance cluster, ensuring real-time performance and scalability for large-scale deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"materials science\",\n        \"description\": \"Develop a highly sophisticated regression model for the Mohs Hardness Dataset with 500,000 data points. The model should handle complex interactions, non-linear relationships, outliers, and missing values through advanced feature engineering and ensemble techniques. Target is to minimize mean absolute error (MedAE) while maintaining interpretability and adhering to industry-specific fairness metrics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": 1.0,\n                \"unit\": \"Sec\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": 0.9\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mohs Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"hardness\"],\n            \"specification\": {\n                \"data_points\": 500000,\n                \"feature_dimensions\": null\n            },\n            \"description\": \"A large-scale dataset with complex interactions, non-linear relationships, and outliers, focusing on materials science.\",\n            \"preprocessing\": [\n                \"advanced feature engineering\",\n                \"handling missing values\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_techniques\": true,\n                \"interpretability_methods\": true\n            },\n            \"description\": \"A sophisticated regression model using ensemble techniques, designed for the Mohs Hardness Dataset with a focus on advanced feature engineering and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Industry-specific constraints and fairness metrics should be considered.\",\n        \"Performance on a high-performance cluster to ensure real-time and scalable operation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for high-performance cluster\"\n        ],\n        \"demonstration\": \"Industry-specific use cases and interpretability analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the given instruction, accurately capturing the build intent with high expertise, the task of developing a regression model for the complex Mohs Hardness Dataset, and the specific requirements, such as handling complex data, ensemble techniques, and fairness metrics. The performance metrics are not provided with specific target values, leaving room for optimization, and the target device and endpoint are not specified for the deployment. The confidence is high given the clarity of the requirements and the mapping to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager needs to develop a cutting-edge, resource-efficient translation application, exclusively for Quechua-Spanish translation, leveraging the underutilized JW300 corpus. Emphasize on designing an advanced LSTM architecture with intricate attention mechanisms, capable of capturing subtle linguistic variations and achieving a high performance benchmark of BLEU score above 35. Moreover, ensure the model's compactness for seamless operation on next-generation, memory-constrained mobile devices with limited processing power.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"translation services\",\n        \"description\": \"Develop a cutting-edge, resource-efficient Quechua-Spanish translation application using the JW300 corpus. The model should employ an advanced LSTM architecture with intricate attention mechanisms, targeting high performance with a BLEU score above 35. It should prioritize compactness for mobile devices with limited resources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 35.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory size\",\n                \"value\": \"optimized for mobile devices\",\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": \"minimal\",\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"JW300\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Quechua\", \"Spanish\"],\n            \"specification\": {\n                \"corpus_size\": \"underutilized\",\n                \"language_pair\": \"Quechua-Spanish\"\n            },\n            \"description\": \"Underutilized Quechua-Spanish parallel corpus for translation\",\n            \"preprocessing\": [\"language preprocessing for Quechua and Spanish\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced LSTM with Attention Mechanisms\",\n            \"family\": \"neural networks\",\n            \"type\": \"seq2seq models\",\n            \"specification\": {\n                \"model_structure\": \"LSTM\",\n                \"attention_layers\": \"intricate\"\n            },\n            \"description\": \"A compact and resource-efficient translation model targeting Quechua-Spanish translation with a focus on capturing subtle linguistic variations\"\n        }\n    ],\n    \"knowledge\": [\n        \"The JW300 corpus can provide a diverse and underutilized training source for the Quechua-Spanish language pair.\",\n        \"Advanced LSTM with attention mechanisms enhance translation quality and performance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile devices with limited resources\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for mobile devices\"],\n        \"demonstration\": \"Lightweight and seamless operation on mobile devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the project requirements, including the user's intent, expertise, and the focus on a resource-efficient Quechua-Spanish translation application. The area, task, and domain are defined, with specific metrics like BLEU score and compactness. The dataset and model information are derived from the instruction, and the knowledge points highlight the potential sources and model characteristics. The confidence score is high because the structure aligns with the schema and the requirements are clearly addressed.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Elaborate on a state-of-the-art, hybrid deep learning architecture, integrating ResNet-CNN with a hierarchical LSTM, for the comprehensive analysis of the urban traffic flow dataset. Request a multi-step forecasting model capable of predicting not just the next month's hourly traffic volumes but also anticipate trends and anomalies during exceptional events like holidays, public vacations, and weekends, with a focus on the distinct patterns during peak hours.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"urban planning or transportation\",\n        \"description\": \"Design a state-of-the-art hybrid deep learning model combining ResNet-CNN and hierarchical LSTM for comprehensive analysis of the urban traffic flow dataset. The model should be able to predict hourly traffic volumes for the next month, anticipate trends and anomalies during exceptional events like holidays, public vacations, and weekends, with a focus on distinct patterns during peak hours.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"hourly traffic volume prediction\",\n                \"value\": null\n            },\n            {\n                \"name\": \"trend forecasting accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"anomaly detection rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Urban Traffic Flow Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly_traffic\"],\n            \"specification\": {\n                \"time_series_length\": null,\n                \"number_of_features\": null\n            },\n            \"description\": \"A time-series dataset containing hourly traffic data for urban areas, including both normal and exceptional events.\",\n            \"preprocessing\": [\n                \"resampling to hourly resolution\",\n                \"handling missing data\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"data exploration for pattern identification\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNet-CNN + Hierarchical LSTM\",\n            \"family\": \"neural networks\",\n            \"type\": \"hybrid deep learning\",\n            \"specification\": {\n                \"architectural details\": \"residual connections and hierarchical LSTM structure\",\n                \"number_of_layers\": null,\n                \"parameters\": null\n            },\n            \"description\": \"A cutting-edge model combining ResNet-CNN for extracting spatial features and hierarchical LSTM for temporal analysis in the context of urban traffic flow forecasting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integration of CNN with LSTM is known to capture spatiotemporal patterns effectively.\",\n        \"Hierarchical LSTM can handle varying temporal scales.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature engineering for peak hour patterns\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time or offline\"],\n        \"demonstration\": \"visualizations of predicted traffic flow and performance metrics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent, detailing a complex machine learning project using a hybrid architecture. The required tasks and metrics are reflected in the problem area and performance metrics. However, specific architectural details and performance targets are not provided, as they are usually project-specific. The dataset and source are clearly described based on the instruction. The confidence score is medium-high since the detail level is derived from the instruction but some precise technical aspects are left to the reader's understanding or requirement.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "multimodal emotion recognition", "instruction": "As a project manager for an emotion detection project, you need to evaluate the IEMOCAP dataset, comparing various deep learning models (CNN, LSTM, and Transformer-based) while considering real-time processing constraints, accuracy, and interpretability, before finalizing the most suitable model for deployment on a smartphone application.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"multimodal emotion recognition\",\n        \"application_domain\": \"affect computing\",\n        \"description\": \"Evaluate the IEMOCAP dataset for an emotion detection project, comparing CNN, LSTM, and Transformer-based deep learning models while considering real-time processing constraints, accuracy, and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time processing constraints\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"IEMOCAP\",\n            \"modality\": [\"image\", \"audio\", \"video\"],\n            \"target_variables\": [\"emotions\"],\n            \"specification\": {\n                \"number_of_classes\": {\n                    \"image\": 0,\n                    \"audio\": 0,\n                    \"video\": 0\n                },\n                \"sampling_rate\": {\n                    \"image\": null,\n                    \"audio\": null,\n                    \"video\": null\n                }\n            },\n            \"description\": \"Dataset for multimodal emotion recognition, suitable for comparing deep learning models on real-world data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"image-based augmentation\",\n                \"audio-based augmentation\"\n            ],\n            \"visualization\": [\n                \"data exploration on various modalities\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"CNN\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"layers\": null\n                },\n                \"parameters\": {\n                    \"number_of_layers\": 0,\n                    \"FLOPs\": null\n                }\n            },\n            \"description\": \"A convolutional neural network for emotion detection\"\n        },\n        {\n            \"name\": \"LSTM\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"units\": null,\n                \"memory_consumption\": null\n            },\n            \"description\": \"A long short-term memory network for emotion recognition\"\n        },\n        {\n            \"name\": \"Transformer-based\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention_layers\": null,\n                \"sequence_length\": null\n            },\n            \"description\": \"A Transformer model for emotion detection\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"smartphone\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-accelerated\",\n            \"CPU-optimized\"\n        ],\n        \"demonstration\": \"Demonstrate emotion detection in real-time on a smartphone\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a model, their high level of expertise, and the specific problem in affect computing with a focus on deep learning models (CNN, LSTM, and Transformer). The IEMOCAP dataset and multimodal nature are correctly represented along with real-time processing and interpretability as evaluation factors. The performance metrics are set to null as they would depend on model comparison, and the complexity metrics are also specified. However, without the specific dataset description or model architecture details, the confidence score is 0.9.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager is sought to develop an advanced e-commerce product description translation system, leveraging the Taobao dataset. Mandate the team to design a state-of-the-art U-Net model tailored specifically for the retail and fashion sectors, optimizing not only for precision (BLEU score of 40+) but also for preserving the context and colloquial nuances that resonate with consumer demand. The model should be capable of real-time, scalable integration with multiple diverse e-commerce platforms, ensuring seamless internationalization without hindering user experience or sales conversion potential.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"e-commerce, retail, fashion\",\n        \"description\": \"Develop an advanced e-commerce product description translation system using the Taobao dataset. Design a U-Net model tailored for the retail and fashion sectors, optimizing for precision with a BLEU score of 40+ and preserving context and colloquial nuances for better consumer engagement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 40\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": \"required\",\n                \"unit\": \"time\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high\",\n                \"unit\": null\n            },\n            {\n                \"name\": \"integration complexity\",\n                \"value\": \"high\",\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Taobao\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A dataset specific to the retail and fashion sector for e-commerce product description translation.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net for Product Description Translation\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": {\n                    \"tailored_for_retail_fashion\": true\n                },\n                \"preserves_context_colloquial_nuances\": true\n            },\n            \"description\": \"A state-of-the-art U-Net model designed for real-time, scalable integration with diverse e-commerce platforms in the retail and fashion sectors.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model must account for context and colloquial language to cater to the retail and fashion customer base.\",\n        \"Real-time performance is crucial for seamless internationalization.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud-based, suitable for real-time inference\",\n        \"deployment_endpoint\": \"multi-platform integration service\",\n        \"inference_engine\": [\n            \"real-time inference API\"\n        ],\n        \"demonstration\": \"Illustrate seamless integration and impact on user experience and sales conversion\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the main elements of the instruction, including the user's intent, problem context (e-commerce, retail, fashion), the task (advanced translation model), the dataset (Taobao), and the target U-Net model with specific optimization criteria (BLEU score, context preservation). The missing information is left out, such as the detailed model architecture or specific feature extraction steps, as they would depend on the engineering team's choices. Confidence score is high due to the structure of the JSON schema and the clarity of the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The task calls for a real-time, high-resolution aerial imagery segmentation project using DOTA dataset, with YOLOv4 enhanced for multi-class object and infrastructure detection. Crucial requirements are: precision above 95%, recall surpassing 90%, sub-second (<500ms) inference on large-scale images, and rigorous evaluation under varying lighting and weather conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"aerospace or remote sensing\",\n        \"description\": \"The task is to develop a real-time high-resolution aerial imagery segmentation project using the DOTA dataset. The YOLOv4 model should be enhanced for multi-class object and infrastructure detection, targeting precision over 95%, recall above 90%, and sub-second inference time (less than 500ms) on large-scale images while maintaining performance under varying lighting and weather conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DOTA\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": \"high-resolution\",\n                \"image_scale\": \"large-scale\",\n                \"variety\": [\"aerial imagery\"]\n            },\n            \"description\": \"A high-resolution aerial imagery dataset for multi-class object and infrastructure detection in the context of real-time segmentation projects.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"lighting variations\",\n                \"weather condition simulation\"\n            ],\n            \"visualization\": [\"inference speed analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"YOLOv4\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"enhanced for multi-class object and infrastructure detection\",\n                \"speed\": {\n                    \"inference_time\": 0.5,\n                    \"accuracy_threshold\": \"low\",\n                    \"computational_complexity\": \"optimized\"\n                }\n            },\n            \"description\": \"An advanced YOLOv4 model adapted for real-time aerial imagery segmentation with high precision, recall, and fast inference.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"high-performance hardware (e.g., GPUs)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for real-time scenarios\"\n        ],\n        \"demonstration\": \"rigorous testing under varying lighting and weather conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the task, focusing on the YOLOv4 enhancement, performance targets, and hardware requirements. The user's high level of expertise, area of expertise, and the application domain are derived from the task description. Performance metrics and inference time constraints are set to the specified values, and the necessary preprocessing and augmentation are indicated. However, the specific YOLOv4 modifications and exact image preprocessing steps are not provided due to the lack of detail in the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a cutting-edge, ensemble deep learning framework that combines ResNet-CNN, a hierarchical LSTM, and an attention mechanism, specifically tailored for the real-time analysis of an extensive, high-resolution urban traffic flow dataset from multiple sources. Design a model capable of not only forecasting hourly traffic volumes for the next quarter, but also differentiating between long-term trends, short-term anomalies, and dynamically adjusting to predict the impact of concurrent events (e.g., festivals, weather changes), and exhibit superior performance during weekly and monthly patterns, particularly during rush hours and peak season.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"transportation planning\",\n        \"description\": \"Develop a cutting-edge ensemble deep learning framework combining ResNet-CNN, a hierarchical LSTM, and an attention mechanism for real-time analysis of a large, high-resolution urban traffic flow dataset with multiple sources. The model should forecast hourly traffic volumes for the next quarter, differentiate between long-term trends, short-term anomalies, and adapt to concurrent events like festivals and weather changes, with superior performance during weekly and monthly patterns, especially peak hours and the peak season.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error (MAE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean squared error (MSE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Urban Traffic Flow Dataset\",\n            \"modality\": [\"image\", \"time series\"],\n            \"target_variables\": [\"hourly traffic volumes\"],\n            \"specification\": {\n                \"resolution\": \"high\",\n                \"data_source\": \"multiple\",\n                \"size\": \"extensive\"\n            },\n            \"description\": \"A real-time dataset containing high-resolution urban traffic flow data from multiple sources, including high-definition images and time-series measurements.\",\n            \"preprocessing\": [\n                \"feature engineering (image and time series)\",\n                \"data normalization\"\n            ],\n            \"augmentation\": [\n                \"time-series augmentation (e.g., shifting, scaling)\"\n            ],\n            \"visualization\": [\n                \"data exploration and feature visualization\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Ensemble Deep Learning Framework\",\n            \"family\": [\"ResNet-CNN\", \"Hierarchical LSTM\", \"Attention Mechanism\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"custom for real-time traffic analysis\"\n            },\n            \"description\": \"A state-of-the-art deep learning model that combines the power of ResNet-CNN, hierarchical LSTM, and attention mechanisms to handle real-time analysis, long-term trend forecasting, short-term anomalies, and concurrent event impact prediction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced data fusion techniques\",\n        \"Adaptability to dynamic events and seasonal patterns\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"traffic data preprocessing (potentially)\",\n            \"feature extraction for ensemble\"\n        ],\n        \"target_device\": \"real-time, edge or cloud\",\n        \"deployment_endpoint\": \"smart traffic management system\",\n        \"inference_engine\": [\n            \"TensorFlow Serving\",\n            \"Real-time inference APIs\"\n        ],\n        \"demonstration\": \"Visualization of predictive accuracy and impact\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is derived from the given instruction, mapping the user's intent, requirements for the model, and dataset characteristics accurately. The expert level of expertise is assumed due to the complexity of the requested model. Performance metrics are left as null as specific values were not provided. The model's specifications are left open-ended to allow for a custom design tailored to the user's instruction. Complexity metrics, such as inference time and memory footprint, are included based on real-world expectations of an ensemble deep learning framework. However, missing parts include the specific metrics targets or exact preprocessing steps for real-time fusion of diverse data sources. The confidence score reflects a high level of confidence based on the clear instructions provided.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the next-gen urban scene analysis, we require a highly optimized U-Net architecture, specifically tailored for Cityscapes. The model must excel in segmenting 30 complex urban elements, achieving a minimum 97% accuracy with interpretability. In addition to surpassing existing benchmarks, ensure real-time efficiency on top-end GPUs, maintaining 50 FPS and sub-1 millisecond latency. Implement a scalable modular design, enabling seamless integration with future urban plans, and provide comprehensive, step-by-step guidelines and reusable code for reproducibility and customized fine-tuning, all within an interdisciplinary collaboration document.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"Develop a highly optimized U-Net architecture for next-gen urban scene analysis, specifically tailored for Cityscapes dataset. The model must segment 30 complex urban elements with a minimum accuracy of 97% and provide interpretability. Focus on real-time efficiency with 50 FPS and sub-1 millisecond latency, while maintaining scalability for future urban plans. Include comprehensive guidelines, reusable code, and a collaborative document for reproducibility and fine-tuning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time efficiency\",\n                \"value\": 50,\n                \"unit\": \"FPS\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 1,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_classes\": 30\n            },\n            \"description\": \"A dataset for urban scene analysis with complex urban elements for model training and evaluation.\",\n            \"preprocessing\": [\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\n                \"random cropping\",\n                \"flipping\"\n            ],\n            \"visualization\": [\n                \"segmentation masks\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_complexity\": \"optimized for real-time\",\n                \"number_of_parameters\": \"minimized for scalability\"\n            },\n            \"description\": \"A tailored U-Net architecture designed for efficient and highly accurate segmentation of 30 complex urban elements on Cityscapes, with a focus on interpretability and real-time performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interdisciplinary collaboration is essential for future-proofing the model with urban planning considerations.\",\n        \"Scalability and fine-tuning guidelines enhance model adaptability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain-specific feature extraction\"\n        ],\n        \"target_device\": \"top-end GPUs\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing engines\"],\n        \"demonstration\": {\n            \"guidelines\": \"step-by-step for 50 FPS performance and sub-1ms latency\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a tailored U-Net model for an urban scene analysis task. The user's expertise level, problem area, and specific requirements like accuracy, real-time performance, and interpretability are included. The Cityscapes dataset, model details, and complexity metrics are derived directly from the instruction. The missing information for the deployment endpoint and target devices will be added if specified during the collaboration process or can be deduced from the target_device context.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a cutting-edge fusion of Quantum Graph Neural Networks (QGNNs) and Non-local Neural Attention (NLNA) for the analysis of primate astrocyte network data, with the objective of detecting subpopulations responsible for functional synchronization in the human cerebellum. The algorithm should demonstrate unprecedented accuracy, even in the presence of sparse data and dynamic network topology, while optimizing hardware resource usage for real-time in vivo monitoring. Additionally, propose a novel training strategy that leverages transfer learning from simplified models for improved generalization across various brain regions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Develop a cutting-edge model combining Quantum Graph Neural Networks (QGNNs) and Non-local Neural Attention (NLNA) for analyzing primate astrocyte network data, targeting the detection of subpopulations responsible for functional synchronization in the human cerebellum. The algorithm should have high accuracy, be robust to sparse data and dynamic topology, and optimize hardware resource usage for real-time in vivo monitoring. Also, include a transfer learning strategy from simplified models for improved generalization across brain regions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource optimization\",\n                \"value\": null,\n                \"unit\": \"real-time constraints\"\n            },\n            {\n                \"name\": \"Hardware utilization\",\n                \"value\": null,\n                \"unit\": \"FLOPs or GPU usage\"\n            },\n            {\n                \"name\": \"real-time monitoring\",\n                \"value\": null,\n                \"unit\": \"in vivo monitoring time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Primate Astrocyte Network Data\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"sparse_data\": true,\n                \"dynamic_topology\": true\n            },\n            \"description\": \"Dataset containing astrocyte network data from primate brains with focus on human cerebellum, characterized by sparse data and dynamic connectivity patterns.\",\n            \"preprocessing\": [\"quantization\", \"denoising\"],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [\"network topology analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"QGNN-NLNA Fusion Model\",\n            \"family\": \"Quantum Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"uses_QGNN\": true,\n                \"uses_NLNA\": true,\n                \"focus_on_resource_optimization\": true\n            },\n            \"description\": \"A model combining QGNNs for quantum learning and NLNA for capturing long-range dependencies, designed for the challenging task of detecting synchronized subpopulations in primate astrocyte networks.\"\n        }\n    ],\n    \"knowledge\": [\n        \"QGNNs enhance learning with quantum computing principles, while NLNA improves signal extraction from complex networks.\",\n        \"Spatio-temporal transfer learning from simplified models will aid in generalizing across diverse brain regions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"transfer learning pre-processing\"],\n        \"target_device\": \"Edge computing for real-time monitoring\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource efficiency\"],\n        \"demonstration\": \"Performance evaluation on unseen in vivo data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the instruction and is formatted according to the JSON schema. The user's intent is to build a model, their level of expertise is assumed to be high due to the complexity of the task. The area is specified as graph machine learning, downstream task as node classification, and application domain as neuroscience. Performance metrics like accuracy and resource optimization are included, with their values set to null for now as they would be determined during model development. The task's specifics, such as the dataset and the challenges, are accurately reflected in the response.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a highly experienced project manager, develop a state-of-the-art neural machine translation system, leveraging the ultra-large and domain-specific Sci-XL dataset. Emphasize on preserving intricate scientific terminologies and concepts, specifically from niche disciplines in the life sciences and physics. Mandate the implementation of M6-MT+, a novel memory optimization technique, to ensure optimal performance on next-generation foldable tablets with stringent requirements like sub-1W power consumption and extended battery life. Moreover, design a versatile interface that seamlessly combines advanced speech recognition (real-time captioning) and natural-sounding text-to-speech conversion, enabling fluent and precise translations for real-life live presentations in multinational conferences, incorporating a built-in error correction algorithm for enhanced accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"science, specifically life sciences and physics\",\n        \"description\": \"Develop a state-of-the-art neural machine translation system targeting scientific domain-specific terminologies and concepts. Use the Sci-XL ultra-large dataset, focusing on precision with intricate terminology. Implement M6-MT+ for memory optimization in a next-gen foldable tablet with sub-1W power consumption and extended battery life.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"conversational flow\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"power consumption\",\n                \"value\": 1.0,\n                \"unit\": \"W\"\n            },\n            {\n                \"name\": \"battery life\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory efficiency\",\n                \"value\": null,\n                \"unit\": \"MegaBytes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sci-XL\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"total\": null,\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"domain\": \"life sciences and physics\"\n            },\n            \"description\": \"Ultra-large and domain-specific scientific dataset for machine translation, focusing on intricate terminologies.\",\n            \"preprocessing\": [\"memory optimization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6-MT+\",\n            \"family\": \"neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"M6-MT+\",\n                \"memory_management\": \"novel memory optimization technique\",\n                \"target_device\": \"next-gen foldable tablets\"\n            },\n            \"description\": \"A neural machine translation system using M6-MT+ for optimal performance on life sciences and physics domain with memory optimization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integration of advanced speech recognition and natural-sounding TTS for real-time captioning.\",\n        \"Error correction algorithm for improved translation accuracy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"memory-efficient\",\n            \"real-time speech recognition\"\n        ],\n        \"target_device\": \"foldable tablets with sub-1W power and extended battery life\",\n        \"deployment_endpoint\": \"None specified\",\n        \"inference_engine\": [\"optimized for power consumption\"],\n        \"demonstration\": {\n            \"setting\": \"multinational conferences\",\n            \"features\": \"fluent and precise translations\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a machine translation system and their high level of expertise. The problem area, dataset, and model are derived directly from the instruction. The performance metrics are not set to a specific value, as they are often determined during development. Similarly, complexity metrics are hypothetical based on the power consumption requirement. The demo and error correction algorithm are included. Missing information could include specific performance targets, but it's clear how to address them based on the requirements.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a visionary project manager, craft a multi-layered, agile methodology for the ML team to conquer the intricate text classification task on the expansive and heterogeneous DAIGT corpus. Mandate not only a progressive fusion of advanced techniques like word embeddings, sentiment analysis, and syntax parsing, but also prioritize hyperparameter optimization, model interpretability, and real-time performance tuning by consistently outperforming established competitors with live model comparison and A/B testing.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Craft a multi-layered, agile methodology for the ML team to address the complex text classification task on the extensive and heterogeneous DAIGT corpus. The methodology should include advanced techniques like word embeddings, sentiment analysis, and syntax parsing, and emphasize hyperparameter optimization, model interpretability, and real-time performance tuning, outperforming established competitors through live model comparison and A/B testing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"real-time performance\",\n                \"value\": {\n                    \"description\": \"Efficiency in processing and adapting to real-time data.\"\n                },\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"training\": null,\n                    \"validation\": null,\n                    \"testing\": null\n                },\n                \"heterogeneity\": {\n                    \"description\": \"Extensive and diverse text data\"\n                }\n            },\n            \"description\": \"A large, heterogeneous corpus for text classification tasks.\",\n            \"preprocessing\": [\"data cleaning\", \"tokenization\", \"lemmatization\"],\n            \"augmentation\": [\n                \"synthetic data generation\",\n                \"word dropout\"\n            ],\n            \"visualization\": [\n                \"context analysis\",\n                \"performance trend visualization\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multi-layered NLP Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ensemble of word embeddings, sentiment analysis, and syntax models\",\n                \"hyperparameter optimization\": true,\n                \"interpretability techniques\": true\n            },\n            \"description\": \"A model designed for the DAIGT corpus, integrating advanced NLP techniques and focused on real-time performance and hyperparameter tuning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Agile methodology will facilitate flexibility and adaptability to the corpus's complexity\",\n        \"Hyperparameter optimization is crucial for high performance\",\n        \"Model interpretability is vital for understanding and refining the model\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"incremental learning\",\n            \"transfer learning\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"ML-as-a-service platform\",\n        \"inference_engine\": [\"real-time streaming\", \"GPU-optimized\"],\n        \"demonstration\": \"live model performance demonstration with DAIGT corpus examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the schema, capturing the user's intent to build a model, high expertise, and the complex text classification task on the DAIGT corpus. The application domain is set to information retrieval, and the description highlights the emphasis on multi-layered techniques and performance metrics. Specific details like the model architecture, performance tuning methods, and dataset characteristics are included, based on the instruction's requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the elite AI development team, your task is to architect a sophisticated text classification system using the BoolQ dataset that not only meets high performance standards with a minimum F1-score of 95%, but also necessitates a multi-step approach. Key requirements include:\n\n1. Design a model with explainable features, employing sophisticated SHAP analysis to provide deep insights into feature importance.\n2. Optimize efficiency by finetuning a highly efficient BERT-base model within a strict budget of 5 epochs, while maintaining focus on computational efficiency.\n3. Implement a novel explanation mechanism that generates exemplar rationales for each predicted class, showcasing complex decision-making processes.\n4. Rigorously test model resilience through adversarial and cross-lingual evaluations, ensuring robustness across diverse scenarios.\n5. Compile a comprehensive technical report, illustrating trade-offs, incremental improvements, and benchmark comparisons, to demonstrate the model's superior performance and explainability to stakeholders across multiple industries.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI development\",\n        \"description\": \"Architect a sophisticated text classification system using the BoolQ dataset with high performance (minimum F1-score of 95%) and a multi-step approach, emphasizing explainability, efficiency, exemplar rationales, and robustness.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Training epochs\",\n                \"value\": 5,\n                \"unit\": \"epochs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"unknown\",\n                \"language\": \"English\",\n                \"multi-language support\": \"cross-lingual evaluation\"\n            },\n            \"description\": \"A dataset for text classification that necessitates a sophisticated model\",\n            \"preprocessing\": [\"SHAP analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [\"exemplar rationales\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_name\": \"BERT-base\",\n                \"efficiency\": \"optimized for 5 epochs\",\n                \"interpretable_features\": \"SHAP analysis\"\n            },\n            \"description\": \"A BERT-based model with explainable features and efficient fine-tuning for text classification\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"feature_insights\": \"SHAP analysis for model interpretability\",\n            \"efficiency_tradeoff\": \"5 epochs for computational efficiency\",\n            \"explanations\": \"novel exemplar rationales mechanism\"\n        },\n        {\n            \"robustness_tests\": \"adversarial and cross-lingual evaluations\"\n        },\n        {\n            \"report_content\": \"trade-offs, improvements, and benchmark comparisons\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"finetuning for performance\",\n            \"SHAP analysis integration\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"complex decision-making process showcase for stakeholders\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key requirements mentioned in the user's instruction, including the tasks, dataset, model, performance expectations, and additional requirements such as explainability and robustness tests. The given performance metric (F1-score) and the multi-step approach with explanations and efficiency optimizations are reflected. However, specific details like BERT version and exact dataset size are not provided, so they are assumed to be unknown. The confidence score reflects the clarity of the instruction and the completeness of the response based on the information provided.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Act as a seasoned project manager for an elite K-12 institution, tasked with developing an avant-garde Visual Question Answering (VQA) system. Utilize the cutting-edge M6 transformer and the TextVQA v2.0 dataset, incorporating a sophisticated architecture that integrates advanced deep cross-modal fusion, context-adaptive multi-head attention, and a fusion of exclusive visual perception pathways. In addition to a comprehensive technical report, prepare:\n\n1. A modular design blueprint with fine-grained explainability diagrams.\n2. A rigorous evaluation methodology that includes not only benchmarking but also exploratory performance diagnostics.\n3. The development of a step-by-step algorithm walkthrough with annotated pseudocode for transparency.\n4. An in-depth analysis comparing competitor models through advanced ablation studies, showcasing the unique advantages of your system.\n5. A historical performance trajectory table with statistical significance tests to demonstrate continuous and distinct improvements.\n6. Emphasize the educational impact by outlining the system's ability to cater to diverse learning styles and facilitate interactive, AI-assisted learning environments.\n7. Ensure the report is not only technically sound but also engaging, with visually appealing graphics and clear storytelling to captivate the attention of industry leaders and educators.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Develop an avant-garde VQA system for an elite K-12 institution using the M6 transformer and TextVQA v2.0 dataset. The system should incorporate advanced techniques like deep cross-modal fusion, context-adaptive multi-head attention, and exclusive visual perception pathways. The report should include a modular design blueprint, evaluation methodology, algorithm walkthrough, competitor analysis, performance trajectory, and educational impact.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean reciprocal rank\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"GPU memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TextVQA v2.0\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [\"question\", \"answer\"],\n            \"specification\": null,\n            \"description\": \"A state-of-the-art dataset for visual question answering tasks, suitable for the M6 transformer and advanced fusion techniques.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6 Transformer-based VQA System\",\n            \"family\": \"transformer architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"deep cross-modal fusion\": true,\n                    \"context-adaptive multi-head attention\": true,\n                    \"exclusive visual perception pathways\": true\n                },\n                \"parameters\": {\n                    \"M6 model size\": \"customized\",\n                    \"training parameters\": null\n                }\n            },\n            \"description\": \"A cutting-edge VQA system leveraging the M6 transformer and integrating advanced techniques for high accuracy and explainability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced ablation studies will highlight the system's performance against competitors.\",\n        \"System designed for diverse learning styles and interactive AI-assisted environments.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom platform or existing VQA service\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"Interactive and visually engaging user interface\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the detailed requirements from the instruction, mapping technical terms to the schema. The user's intent, expertise, problem area, and specific performance metrics (with their values set to null as they are to be determined), are accurately represented. The modular design blueprint and the evaluation methodology include broad instructions, leaving room for specific implementation details. However, specific values and detailed steps are not provided as they would require extensive research and development.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager, you're tasked with overseeing the design and implementation of a cutting-edge time series forecasting engine for our mobile application, focusing on user engagement. The team must leverage state-of-the-art deep learning techniques, specifically a fusion of a sophisticated deep auto-encoder and a multi-level, adaptive state-space model. The challenge lies in creating a model resilient to concept drift, able to uncover intricate temporal dynamics, handle high-resolution seasonal fluctuations, and incorporate rigorous Bayesian regularization for unprecedented accuracy. Additionally, ensure the model's ability to forecast user behavior for the next 36 months, maintaining adaptability and real-time adjustments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"mobile, user engagement\",\n        \"description\": \"Design and implement a cutting-edge time series forecasting engine for a mobile app, using a fusion of deep auto-encoder and multi-level, adaptive state-space model. The model must be resilient to concept drift, able to uncover intricate temporal dynamics, handle high-resolution seasonal fluctuations, incorporate Bayesian regularization, and forecast user behavior for the next 36 months with real-time adjustments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"seasonality handling\",\n                \"value\": \"high-resolution\"\n            },\n            {\n                \"name\": \"concept drift resilience\",\n                \"value\": 1.0\n            },\n            {\n                \"name\": \"forecast horizon\",\n                \"value\": 36\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"concept drift adaptation\",\n                \"value\": 1.0\n            },\n            {\n                \"name\": \"real-time adjustments\",\n                \"value\": 1.0\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"user engagement data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"user engagement metrics\"],\n            \"specification\": null,\n            \"description\": \"High-resolution data capturing user behavior over time for the mobile application.\",\n            \"preprocessing\": [\"Bayesian regularization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Auto-Encoder and Adaptive State-Space Model Fusion\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep auto-encoder and multi-level, adaptive state-space model\",\n                \"techniques\": [\"Bayesian regularization\"],\n                \"drift_handling\": \"yes\"\n            },\n            \"description\": \"A cutting-edge deep learning model that fuses a deep auto-encoder and a multi-level adaptive state-space model for accurate time series forecasting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Combination of deep auto-encoders and state-space models is state-of-the-art for time series forecasting.\",\n        \"Bayesian regularization enhances model's robustness and accuracy.\",\n        \"Concept drift resilience and adaptability are essential for user engagement forecasting in a dynamic environment.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"latent feature extraction\", \"state estimation\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"real-time or cloud-based\",\n        \"inference_engine\": [\"TensorFlow Lite for mobile, Kubernetes for serverless environments\"],\n        \"demonstration\": \"Real-time forecasting dashboard for in-app performance monitoring\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the provided instruction, reflecting the task's deep learning focus, time-series aspects, specific model combination, and performance requirements. The expert level expertise is assumed. Some details like the exact accuracy value, inference time, and feature engineering steps are left unspecified as they require more information from the user or are assumptions based on the task. The confidence score is high due to a clear understanding of the project requirements.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "object detection", "instruction": "As the project manager for an enhanced smart city parking management system, enhance the existing M2Det integration by incorporating a fault-tolerant, self-adapting framework that not only processes 'PKLot' data with enhanced real-time occupancy monitoring and predictive forecasting, but also differentiates between multiple parking types and cross-references with surrounding parking restrictions. Implement a model using the latest defense mechanisms against both adversarial attacks and environmental challenges (low-light, heavy rain, and occlusions), ensuring at least 99% accuracy under such conditions. Design an edge-to-cloud architecture with sub-millisecond latency (<1ms) for ultra-fast anomaly detection, maintaining a stringent QoS of 99.9%. Devise a phased implementation roadmap, provide detailed microservices design diagrams for horizontal scalability, and propose an extensive evaluation strategy involving A/B tests, microbenchmarking, and comprehensive sensitivity analysis of not just model variants but also diverse environmental scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city parking management\",\n        \"description\": \"Enhance the existing M2Det integration for a smart city parking management system. Integrate fault-tolerant, self-adapting framework for real-time occupancy monitoring and predictive forecasting, including differentiation between parking types and consideration of surrounding restrictions. Implement a model resilient to adversarial attacks and environmental challenges (low-light, heavy rain, occlusions) with a minimum accuracy of 99% in such conditions. Design an edge-to-cloud architecture with sub-millisecond latency for anomaly detection and maintain a 99.9% QoS. Provide a phased implementation roadmap, microservices design diagrams for horizontal scalability, and a comprehensive evaluation strategy including A/B tests, microbenchmarking, and sensitivity analysis of model variants and diverse environmental scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.001,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"QoS\",\n                \"value\": 0.999\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PKLot data\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"occupancyMonitoring\": true,\n                \"realTime\": true,\n                \"predictiveForecasting\": true\n            },\n            \"description\": \"Data for enhanced real-time occupancy monitoring and predictive forecasting of parking lots.\",\n            \"preprocessing\": [\n                {\n                    \"type\": \"data cleansing\"\n                },\n                {\n                    \"type\": \"image augmentation\"\n                }\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"M2Det with advanced defense mechanisms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"defenseAgainstAdversarial\": true,\n                \"resilienceToEnvironmentalChallenges\": true,\n                \"accuracyThreshold\": 0.99\n            },\n            \"description\": \"A deep learning model for object detection in parking scenarios with enhanced resilience and real-world conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Fault-tolerant and self-adapting framework\",\n        \"Anomaly detection with sub-millisecond latency\",\n        \"Horizontal scalability through microservices design\",\n        \"Extensive evaluation strategy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge-to-cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"low-latency\", \"real-time\"],\n        \"demonstration\": \"Phased implementation and detailed design diagrams\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is parsed according to the schema, capturing the user's intention to build a system, high expertise level, and specific requirements for object detection and performance metrics. The dataset, microservices, and evaluation strategies are derived from the provided instruction, ensuring that the response is relevant and complete. However, some specific metrics like F1 score or QoS are not explicitly mentioned in the instruction, and thus are not included. The confidence score is set to a high value given the clear and detailed nature of the user's instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project director for a high-impact ML research group, your mandate is to boost the predictive sophistication of a regression model for the advanced Pohlenz Mineral Hardness Dataset. The dataset encompasses complex, non-linearly correlated features, distributed unevenly across 10 stratified data partitions: train, early validation, development, and two distinct test sets. Your objective is to not only minimize the weighted median absolute error (WMedAE) with a focus on resilience, but also to optimize for explainability and computational efficiency. Design a multi-layered methodology that includes deep feature extraction, Bayesian hyperparameter optimization, and ensemble learning with diverse architectures, targeting a new global benchmark in the field.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"material science\",\n        \"description\": \"Boost the predictive sophistication of a regression model for the Pohlenz Mineral Hardness Dataset, which has complex, non-linearly correlated features distributed unevenly across 10 stratified data partitions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"weighted median absolute error (WMedAE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computation time efficiency\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Pohlenz Mineral Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"hardness\"],\n            \"specification\": {\n                \"number_of_features\": \"complex, non-linearly correlated\",\n                \"data_distribution\": \"10 stratified partitions\"\n            },\n            \"description\": \"Dataset for regression with focus on material hardness, featuring complex features and distributed across multiple subsets.\",\n            \"preprocessing\": [\"deep feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"multi-layered methodology\",\n            \"type\": \"ensemble learning\",\n            \"specification\": {\n                \"methods\": [\"deep feature extraction\", \"Bayesian hyperparameter optimization\"],\n                \"model_architectures\": [\"diverse\"]\n            },\n            \"description\": \"A multi-layered model using deep learning, Bayesian optimization, and ensemble techniques for improved predictive performance, resilience, explainability, and computational efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The dataset's complexity requires sophisticated feature extraction and handling of non-linear correlations.\",\n        \"Optimizing for explainability is essential in material science applications.\",\n        \"Balancing computational efficiency is crucial due to the nature of the stratified data partitions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Bayesian hyperparameter tuning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"focus on interpretability and computational time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the intent to build a regression model, the high expertise level of the user, and the details from the provided instruction. The problem area, downstream task, and dataset characteristics are extracted, as well as the specific methodologies (deep feature extraction, Bayesian optimization, and ensemble learning). The performance metric (WMedAE) and the focus on computational efficiency are included. The knowledge section reflects points highlighted in the instruction. The confidence score is high as the information is directly derived from the given instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "As a project manager for the cutting-edge AI node classification project on the expanded and diversified COLA dataset, I mandate a next-generation GCN model with integrated self-attention and hierarchical aggregation mechanisms. The model should deliver a precision of at least 90% while maintaining explainability, robustness to under- and overfitting within complex graph structures, and demonstrate a minimum F1 score of 0.9. Strive for scalability, achieving no less than 98% distributed computing efficiency during training, while limiting the duration to 1 hour on state-of-the-art, energy-efficient GPUs. Furthermore, provide extensive empirical evidence on interpretative methods and exhaustive hyperparameter tuning with at least three different search strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"industry (e.g., computer science, data science)\",\n        \"description\": \"A project manager mandates a next-generation GCN model with integrated self-attention and hierarchical aggregation mechanisms for the expanded and diversified COLA dataset. The model should achieve precision of at least 90%, F1 score of 0.9, and be explainable. It must be scalable, demonstrate 98% distributed computing efficiency, and have a training duration of 1 hour on energy-efficient GPUs. Comprehensive interpretative methods and hyperparameter tuning with multiple search strategies are required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"distributed computing efficiency\",\n                \"value\": 0.98,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"training time\",\n                \"value\": 3600,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COLA\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"expanded\": true,\n                \"diversified\": true\n            },\n            \"description\": \"An expanded and diversified graph dataset for node classification, emphasizing complex structures.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Next-Gen GCN with self-attention and hierarchical aggregation\",\n            \"family\": \"Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": true,\n                \"robustness\": \"under- and overfitting\",\n                \"complex_graph_handling\": true\n            },\n            \"description\": \"A state-of-the-art model designed for node classification with self-attention and hierarchical aggregation, targeting performance and interpretability\"\n        }\n    ],\n    \"knowledge\": [\n        \"Explainability and robustness are key requirements.\",\n        \"Hyperparameter tuning with multiple strategies needed\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"state-of-the-art, energy-efficient GPUs\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"interpretative methods and extensive empirical evidence\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction by specifying the user's intent, expertise, problem details, model requirements, dataset characteristics, and service expectations. It includes precision, F1 score, and computing efficiency targets. However, the specific distribution of the dataset, augmentation, preprocessing, and detailed explanation for empirical evidence are not covered due to the lack of specifics in the instruction.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager is tasked with developing a real-time, mobile app-integrated fashion item segmentation system utilizing an Enhanced EfficientNet-101 as the foundation for a Fully Convolutional Network (FCN). The system must demonstrate superior accuracy, achieving at least 80% on the challenging DeepFashion2 dataset with complex and overlapping garments. Furthermore, ensure the model's performance meets the strictest latency requirements of <150ms, guaranteeing a seamless and ultra-responsive user experience for high-end e-commerce platforms.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"fashion retail\",\n        \"description\": \"Develop a real-time, mobile app-integrated fashion item segmentation system using Enhanced EfficientNet-101 as the base model for a Fully Convolutional Network (FCN). The system must achieve at least 80% accuracy on the DeepFashion2 dataset with complex and overlapping garments, while meeting strict latency requirements of <150ms.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.8\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 150,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepFashion2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"height\": 512,\n                    \"width\": 512,\n                    \"depth\": 3\n                },\n                \"complexity\": {\n                    \"garments\": \"complex and overlapping\"\n                }\n            },\n            \"description\": \"A challenging fashion item segmentation dataset with complex and overlapping garments.\",\n            \"preprocessing\": [\n                \"image resizing to 512x512\",\n                \"data augmentation techniques (to handle complexity)\"\n            ],\n            \"augmentation\": [\n                \"random cropping\",\n                \"horizontal flipping\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced EfficientNet-101\",\n            \"family\": \"EfficientNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"FCN\",\n                \"architecture_params\": {\n                    \"output_stride\": \"16\"\n                },\n                \"parameters\": {\n                    \"trainable\": {\n                        \"base_model\": \"EfficientNet-101\",\n                        \"fine-tuning\": true\n                    }\n                }\n            },\n            \"description\": \"A state-of-the-art FCN using the Enhanced EfficientNet-101 for fashion item segmentation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The system requires a robust architecture like EfficientNet-101 to handle complex garments.\",\n        \"Fine-tuning the Enhanced EfficientNet-101 is crucial for achieving high accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"image resizing\",\n            \"data augmentation\"\n        ],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"not specified\",\n        \"inference_engine\": [\n            \"optimized for real-time performance\"\n        ],\n        \"demonstration\": \"mobile app demo with latency monitoring\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes the necessary components based on the provided instruction. The user's intent, high level of expertise, and the problem details (area, task, dataset, model, and performance metrics) are extracted directly from the task description. The complexity metrics are also derived from the need for low latency in the app. However, there might be a need to confirm the preprocessing steps and augmentation techniques based on the DeepFashion2 dataset specifics. The confidence score is high due to the clear mapping of the instruction to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As the seasoned AI development unit, your challenge is to develop a cutting-edge image classifier that discerns not only 'testimonials', 'follow-up needed', and 'gallery-worthy' photos but also decodes complex visual interactions, user sentiment, and scene-related significance. Strive for a precision above 97% and ensure real-time, high-throughput processing to seamlessly manage an exploding influx of customer submissions without lag, while considering diverse lighting, angle, and background complexities.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"customer service/analysis\",\n        \"description\": \"Develop a cutting-edge image classifier capable of categorizing images as 'testimonials', 'follow-up needed', 'gallery-worthy', as well as understanding visual interactions, decoding user sentiment, and recognizing scene-related significance. Target precision is above 97% and ensure real-time, high-throughput processing for high volume customer submissions without lag, accounting for diverse lighting, angle, and background complexities.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": null, \n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"high throughput\",\n                \"value\": null, \n                \"unit\": \"images/second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"customer submissions\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"testimonials\", \"follow-up needed\", \"gallery-worthy\"],\n            \"specification\": null,\n            \"description\": \"A diverse set of images with varying lighting, angles, and backgrounds for training and testing the classifier.\",\n            \"preprocessing\": [\"light normalization\", \"image augmentation\"],\n            \"augmentation\": [\"real-time variations\"],\n            \"visualization\": [\"model performance heatmaps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art image classifier\",\n                \"parameters\": \"optimized for speed and accuracy\"\n            },\n            \"description\": \"An advanced deep learning model designed to handle complex visual tasks, including advanced image classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced techniques for scene understanding and sentiment analysis are key\",\n        \"Incorporating real-time adjustments and high throughput is essential\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"semantic segmentation\"\n        ],\n        \"target_device\": \"cloud/edge\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\", \"batch processing\"],\n        \"demonstration\": \"Interactive live demo showcasing real-time classification and performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build an advanced image classifier with high precision and real-time capabilities. It also takes into account the diverse requirements, including scene understanding, user sentiment, and real-world processing constraints. The specific model architecture and performance metrics are left open-ended to allow for flexibility and selection of a suitable model. However, since the exact performance target values and some preprocessing techniques were not specified, the confidence score is set to 0.9, indicating a moderate confidence.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project director for the elite machine learning team, your mission is to develop a cutting-edge text classification model by leveraging the recently acquired DAIGT dataset. Strive for a multiclass classification system capable of differentiating between human-written, AI-generated, and a spectrum of intermediate styles with high precision. Demand not only binary labels (0-3 scale) but also ensure model transparency, domain adaptability, and resilience to noise. Mandate in-depth feature importance analysis, extensive A/B testing with diverse datasets from various industries, and continuous monitoring of model drift in live deployments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI research and development\",\n        \"description\": \"Develop a cutting-edge text classification model using the recently acquired DAIGT dataset, focusing on multi-class differentiation between human-written, AI-generated, and intermediate styles. The model must have high precision, non-binary labels (0-3 scale), emphasize model transparency, domain adaptability, and resilience to noise. Include feature importance analysis, extensive A/B testing with diverse datasets, and continuous model drift monitoring in live deployments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"classification_labels\"],\n            \"specification\": {\n                \"multiclass_labels\": {\n                    \"binary\": false,\n                    \"range\": [0, 3]\n                }\n            },\n            \"description\": \"A recent text dataset designed for multi-class classification of human-written, AI-generated, and intermediate style content.\",\n            \"preprocessing\": [\"model transparency techniques\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep learning model tailored for text classification\",\n                \"transparency\": \"incorporating explainable AI techniques\"\n            },\n            \"description\": \"A cutting-edge deep learning model for text classification, designed for multi-class and precision-oriented analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"DAIGT dataset's unique content mix may require advanced feature extraction techniques.\",\n        \"Model transparency and resilience to noise call for interpretability and robustness methods.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"explanable AI\",\n            \"domain adaptation techniques\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A/B testing results and live deployment performance metrics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given user's mission, detailing the problem area, data requirements, and desired model characteristics. The user's intent to build a model and high expertise level are included. Performance metrics like precision and recall are necessary for a classification task, but specific target values are not given. In-depth feature analysis and model monitoring are represented as part of the requirements. The dataset and model components are formulated based on the provided context.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "node classification", "instruction": "As a seasoned project manager for a cutting-edge AI team, design and implement a tailored node classification model based on the NELL dataset. Utilize a highly sophisticated Hypergraph Neural Network architecture with multi-level, interwoven layers, emphasizing scalability for dense yet sparse data with intricate relationships. Mandate the model to exhibit exceptional robustness against noise and maintain a benchmark accuracy of 95%. Conduct an in-depth, multi-stage hyperparameter optimization process, requiring detailed analysis of not only the peak performance settings but also their differential effects on a diverse range of graph theoretical measures and realistic edge variability scenarios, while maintaining reproducibility and interpretability in the findings.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"industry/advanced research\",\n        \"description\": \"Design and implement a tailored node classification model using the NELL dataset, with a Hypergraph Neural Network architecture that includes multi-level, interwoven layers for scalability in dense and sparse data with complex relationships. The model should be highly robust against noise and aim for a benchmark accuracy of 95%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"density\": \"varied\",\n                \"edge_types\": \"intricate\"\n            },\n            \"description\": \"A dataset requiring complex, multi-relational analysis due to its dense and sparse nature with intricate relationships.\",\n            \"preprocessing\": [\"node feature engineering\", \"data cleaning\"],\n            \"augmentation\": [\"dropout\", \"adversarial examples\"],\n            \"visualization\": [\"graph structure analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hypergraph Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-level, interwoven layers\",\n                \"scalability\": \"dense & sparse data, complex relationships\",\n                \"hyperparameter_details\": \"in-depth optimization for robustness against noise\"\n            },\n            \"description\": \"A state-of-the-art model designed specifically for node classification on NELL dataset, featuring a highly sophisticated architecture with a focus on scalability and noise robustness.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider the impact of diverse edge variability scenarios on hyperparameters.\",\n        \"Ensure interpretability while analyzing performance under different graph theoretical measures.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"hyperparameter tuning\", \"interpretability methods\"],\n        \"target_device\": \"high-performance computing cluster\",\n        \"deployment_endpoint\": \"private cloud-based AI platform\",\n        \"inference_engine\": [\"TensorFlow or PyTorch\"],\n        \"demonstration\": \"interactive visualizations and performance report\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's requirement for a build intent, with high expertise level. The problem statement clearly defines the task and sets specific performance expectations. The NELL dataset's complexity is reflected in the model's requirements and the preprocessing steps. The focus on multi-stage hyperparameter optimization and reproducibility is addressed in the service section. The confidence score is high due to the clear instruction and structured response.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for the elite AI research ensemble, you are tasked with architecting a revolutionary deep learning time series forecasting platform. Utilizing the extensive and diversified Real-World Traffic Insights repository, containing an intricate labyrinth of datasets exhibiting complex interdependencies, your team must execute the following multi-layered objectives:\n\n1. Employ advanced preprocessing techniques to handle the massive input dataset with 96 historical observations and 862 dynamic attributes (INPUT_SEQ_LEN=96, INPUT_DIM=862), extracting hidden patterns in high-dimensional spatio-temporal contexts.\n2. Collaborate to integrate multiple cutting-edge architectures, such as deep LSTM, transformers, and hybrid models, ensuring resilience against intricate seasonality shifts and trends (PRED_SEQ_LEN=96, PRED_DIM=862), while considering the impact of data distribution diversities.\n3. Design custom evaluation metrics, incorporating Bayesian uncertainty estimates and time-sensitive weights, to address class imbalance and enhance anomaly detection capabilities for early warning signals in real-time scenarios.\n4. Develop a progressive forecasting strategy that combines rolling window analysis, reservoir sampling, and dynamic model retraining, showcasing adaptability amidst volatile market dynamics and non-stationary data.\n5. Conduct a comprehensive comparative study of leading model ensembles, considering not just accuracy but also interpretability, model explainability, and computational requirements, with a focus on resource efficiency.\n6. Experiment with state-of-the-art transfer learning methods, including few-shot and continual learning, to optimize model performance, and explore neural architecture search algorithms for fine-tuning and automatic design of models.\n\nYour milestone report should document a robust and innovative pipeline, complete with in-depth performance benchmarks, intuitive visualization of results, and a forward-thinking roadmap for continuous improvement, emphasizing the groundbreaking achievements of your model's predictive prowess and sophistication.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"transportation, urban planning\",\n        \"description\": \"Design and develop a deep learning time series forecasting platform using the Real-World Traffic Insights repository with advanced preprocessing, diverse model architectures, custom evaluation metrics, and progressive forecasting strategy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Bayesian uncertainty\",\n                \"value\": null\n            },\n            {\n                \"name\": \"time-sensitive weights for anomaly detection\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model size (memory footprint)\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Real-World Traffic Insights repository\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 862,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 862\n            },\n            \"description\": \"A massive repository with complex interdependencies, containing high-dimensional spatio-temporal datasets for traffic insights.\",\n            \"preprocessing\": [\n                \"advanced techniques for large datasets\",\n                \"handling high-dimensional spatio-temporal data\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": [\"deep learning\", \"LSTM\", \"transformers\", \"hybrid\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"resilience against seasonality and trends\": true,\n                \"adaptability to class imbalance and anomaly detection\": true\n            },\n            \"description\": \"A robust and adaptive model incorporating deep LSTM, transformers, and hybrid architectures for time series forecasting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced preprocessing for complex, high-dimensional data\",\n        \"Custom evaluation metrics for Bayesian uncertainty and time-sensitive weights\",\n        \"Rolling window analysis and dynamic model retraining\",\n        \"Model ensemble study with emphasis on interpretability and resource efficiency\",\n        \"Transfer learning and neural architecture search\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Bayesian uncertainty integration\",\n            \"custom evaluation metrics development\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"reservoir sampling\", \"dynamic model retraining\"],\n        \"demonstration\": \"Rolling window analysis visualizations and model explainability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project requirements, translating the description, objectives, and metrics into a structured format. It highlights the need for a high-expertise level in AI and ML, and provides specific areas such as model design, preprocessing, and evaluation methods. However, specific target values for performance metrics are not provided as they would require more detailed experimental results or assumptions about the task. Complexity metrics, such as inference time and memory usage, are also left unspecified.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop an advanced, fault-tolerant time series forecasting engine for our renewable energy app's user engagement indices (microscopic PV fluctuations), leveraging a cutting-edge fusion of specialized neural networks like Temporal Fusion Transformer (TFT) for real-time and Seasonal ARIMA (SARIMA) for mid-term predictions. Improve feature analysis by segmenting trends into sub-hourly, daily, weekly, monthly, and quarterly patterns, and apply symbolic Lasso regularization for enhanced model transparency. Design a unique metric, \"Energy User Cohesion Index\" (EUCI), and conduct an exhaustive benchmarking exercise with a minimum of eight state-of-the-art baselines, examining the direct effect on user retention and satisfaction across diverse user segments, while ensuring deployment on a distributed cloud infrastructure with dynamic load balancing.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"renewable energy\",\n        \"description\": \"Develop an advanced, fault-tolerant time series forecasting engine for the renewable energy app's user engagement indices (microscopic PV fluctuations), using a fusion of Temporal Fusion Transformer (TFT) and Seasonal ARIMA (SARIMA) models. Perform feature analysis by segmenting trends into sub-hourly, daily, weekly, monthly, and quarterly patterns. Apply symbolic Lasso regularization for model transparency. Create a new metric, 'Energy User Cohesion Index' (EUCI), and benchmark against at least eight state-of-the-art baselines, evaluating its impact on user retention and satisfaction across diverse segments. Ensure deployment on a distributed cloud infrastructure with dynamic load balancing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Energy User Cohesion Index\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"User Engagement Indices (Microscopic PV Fluctuations)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"user engagement indices\"],\n            \"specification\": {\n                \"granularity\": [\"sub-hourly\", \"daily\", \"weekly\", \"monthly\", \"quarterly\"]\n            },\n            \"description\": \"Time series data capturing fluctuations in user engagement with the renewable energy app, including microscopic PV data.\",\n            \"preprocessing\": [\"feature segmentation\", \"data cleaning\", \"normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"trend visualization\", \"feature importance\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Time Series Forecasting Engine\",\n            \"family\": \"Temporal Fusion Transformer (TFT) & Seasonal ARIMA (SARIMA)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time forecasting\": true,\n                \"mid-term prediction\": true,\n                \"symbolic Lasso regularization\": true\n            },\n            \"description\": \"An AI-powered engine that combines TFT and SARIMA to forecast renewable energy app's user engagement in real-time and mid-term, with enhanced model interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Fusion of specialized models for robust forecasting\",\n        \"Symbolic Lasso for improved transparency\",\n        \"EUCI metric for evaluating user cohesion\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"segmentation\", \"transparency via symbolic Lasso\"],\n        \"target_device\": \"distributed cloud infrastructure\",\n        \"deployment_endpoint\": \"dynamic load balancing\",\n        \"inference_engine\": [\"Distributed\", \"Real-time\"],\n        \"demonstration\": \"Performance showcasing on user retention and satisfaction\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a complex forecasting model, their high expertise, and the specific requirements for the renewable energy app's use case. However, the performance metric 'Energy User Cohesion Index' needs a target value, and some details about the data preprocessing steps are assumed based on the instruction. The description and requirements for the distributed cloud deployment are also clear.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "In your role as the visionary project manager for an elite AI research lab, you've been tasked with the development of a groundbreaking emotion-ception model. This model, built upon the multi-faceted Affect Norms for English Words (ANEW) dataset, should not only accurately classify text into seven nuanced emotional categories (including joy, anger, surprise, etc.) but also propose a dynamic framework to measure the model's sensitivity to connotation shifts in sarcasm and idiomatic expressions. The new evaluation metric must include an interpretable component, combining Word Embedding Similarity Index (WESI) with contextual adaptability index, to ensure a holistic performance rating, while maintaining state-of-the-art accuracy and adaptability in real-world conversational scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"Develop a groundbreaking emotion-ception model using the Affect Norms for English Words (ANEW) dataset, focusing on accurate classification into seven emotional categories and a dynamic framework for sarcasm and idiomatic expressions detection. The evaluation metric should incorporate WESI and a contextual adaptability index for interpretable performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"state-of-the-art accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"contextual adaptability index\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"interpretable\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ANEW\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"emotional categories\"],\n            \"specification\": {\n                \"multifaceted\": true,\n                \"sarcasm and idiomatic expressions\": \"incorporated\"\n            },\n            \"description\": \"A multi-faceted dataset for text emotion analysis, with emphasis on handling sarcasm and idiomatic expressions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Emotion-ception Model\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"incorporated through WESI and contextual adaptability index\"\n            },\n            \"description\": \"A state-of-the-art NLP model for text emotion classification with emphasis on sarcasm and idiomatic detection, using the ANEW dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced understanding of ANEW's emotional annotations, sarcasm, and idiomatic patterns required.\",\n        \"WESI and contextual adaptability for interpretable performance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"contextual and interpretive model demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's request for an advanced emotion detection model with specific requirements, such as dealing with sarcasm and idiomatic expressions, and incorporating an interpretable evaluation metric. The high level of expertise is assumed for the project manager. The dataset, model, and performance metrics are derived from the given description, though values for performance metrics and interpretability are left null as they are typically determined through the model development process. The confidence score is high due to a clear understanding of the instructions.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "Design a state-of-the-art, scalable Question and Answer (QA) solution employing TinyBERT, targeting instantaneous news understanding on portable devices. Require not only a robust performance of 97% F1-score on the stringent NewsQA leaderboard but also:\n\n1. Ensure seamless cross-platform compatibility with sub-100 ms latency for ultra-responsive interaction.\n2. Demonstrate native adaptability across a wide array of devices with varying hardware configurations and resource constraints.\n3. Present a holistic architectural blueprint, showcasing modular design principles and real-time adaptability.\n4. Include a code snippet that showcases advanced hardware-aware TinyBERT optimization techniques, taking into account power efficiency and computational load balancing.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Design a state-of-the-art, scalable QA solution using TinyBERT specifically for instantaneous news understanding on portable devices. The solution must achieve a high performance of 97% F1-score on the NewsQA leaderboard and meet strict latency requirements.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 100,\n                \"unit\": \"ms\",\n                \"description\": \"Sub-100 ms latency for ultra-responsive interaction\"\n            },\n            {\n                \"name\": \"platform compatibility\",\n                \"value\": null,\n                \"description\": \"Cross-platform compatibility with varying hardware configurations\"\n            },\n            {\n                \"name\": \"adaptability\",\n                \"value\": \"native\",\n                \"description\": \"Native adaptability across a wide range of devices\"\n            },\n            {\n                \"name\": \"computational load balancing\",\n                \"value\": null,\n                \"unit\": \"%\",\n                \"description\": \"Hardware-aware optimization for power efficiency and load balancing\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NewsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"leaderboard\": \"NewsQA leaderboard\"\n            },\n            \"description\": \"A benchmark dataset for news understanding, focusing on instant news response.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT\",\n            \"family\": \"BERT-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art QA model\",\n                \"optimizer\": \"Hardware-aware optimization\",\n                \"resource_constraints\": \"Portable devices\"\n            },\n            \"description\": \"A TinyBERT instance tailored for real-time, power-efficient news answering\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art, modular design for scalability\",\n        \"Latency optimization techniques\",\n        \"Power-efficient TinyBERT adaptations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"portable devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\", \"latency-aware\"],\n        \"demonstration\": {\n            \"interaction\": \"ultra-responsive\",\n            \"customization\": \"across diverse hardware\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects a clear understanding of the user's intent to build a QA solution with TinyBERT, focusing on theNewsQA leaderboard and specific performance requirements. The user's expertise level is assumed to be high, and the complexity metrics address the latency and hardware adaptations specified. However, due to the lack of specific hardware configurations or load balancing details in the instruction, the associated metrics have null values.\",\n        \"score\": 0.93\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As an experienced project manager for a top-tier AI research team, your challenge is to design a groundbreaking spatio-temporal predictive model that specifically addresses the complex ETTm2 dataset, derived from an advanced encrypted smart city sensor network with data partitioned in three distinct layers: historical (archived), active (live), and highly secure (confidential). The multi-source data spans across seven specialized sensors, each contributing hourly data for an entire year, with variable input sequences of 144 time steps (INPUT_SEQ_LEN=144) and intricate timestamp patterns.\n\nTo meet this task, you are required to architect a hybrid architecture fusing advanced Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and cutting-edge attention mechanisms. The model must demonstrate adaptability to varying input lengths, effectively manage missing data, and incorporate intricate seasonality and non-linear pattern detection.\n\nThe optimization criteria should encompass a combination of the root mean squared logarithmic error (RMSLE) and mean absolute percentage error (MAPE), aiming for a minimum 15% improvement over industry standards in forecasting 144-step ahead predictions (PRED_SEQ_LEN=144) for all seven environmental variables (PRED_DIM=7).\n\nIn addition to accuracy, place a premium on real-time performance, ensuring minimal latency and efficient resource utilization, considering the model's deployment in resource-constrained smart city infrastructure. Provide a detailed design rationale, a rigorous evaluation plan, and outline strategies for data preprocessing and model pruning for enhanced efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart city infrastructure\",\n        \"description\": \"Design a groundbreaking spatio-temporal predictive model for the ETTm2 dataset derived from an advanced encrypted smart city sensor network. The model must address three data layers, handle variable input sequences of 144 time steps, and exhibit adaptability, manage missing data, and detect seasonality and non-linearity. Optimization goals include RMSLE and MAPE improvements of at least 15% over industry standards for 144-step ahead predictions of 7 environmental variables while maintaining real-time performance and efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared logarithmic error (RMSLE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute percentage error (MAPE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": null, \n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"resource utilization (CPU, memory)\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"input_seq_len\": 144,\n                \"time_series_depth\": 7,\n                \"data_layers\": [\"historical\", \"active\", \"highly secure\"]\n            },\n            \"description\": \"Multi-source data from seven specialized sensors, spanning an entire year with hourly data, complex timestamp patterns, and three distinct layers.\",\n            \"preprocessing\": [\"data anonymization\", \" Handling missing values\", \"Feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Analyze data patterns\", \"Temporal correlations\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hybrid CNN-RNN with Attention Mechanisms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"CNN + RNN fusion, incorporates attention mechanisms\",\n                \"input_shape\": \"[sequence_length, PRED_DIM=7, INPUT_SEQ_LEN=144]\",\n                \"adaptability\": \"variable input lengths\"\n            },\n            \"description\": \"A model for time-series forecasting with adaptive input handling, data imputation, seasonality detection, and non-linear patterns.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal feature engineering\",\n            \"inter-sensor feature correlations\"\n        ],\n        \"target_device\": \"edge computing infrastructure\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time execution\"],\n        \"demonstration\": \"Including visualizations of model performance and predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the provided instruction, with an emphasis on the high-level requirements. The model architecture is described in detail, reflecting the fusion of CNN, RNN, and attention mechanisms. The optimization criteria are set, although exact values are not specified. The pre-processing steps and model pruning strategies are noted. However, some performance and latency metrics have null values, as specific numeric targets are not provided in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a novel Graph Attention Fusion Network (GAFN) architecture, leveraging the Zebrafish Visual Cortex Connectivity dataset, to tackle the problem of fine-grained subcellular component classification with sub-90% false positive rate. The model should demonstrate state-of-the-art performance in terms of F1-score, and be adaptable to noisy environments while maintaining a low inference time. Implement a hierarchical attention mechanism and a novel explainable attention map generation technique to enhance understanding of the network's decision-making process, all while ensuring robustness against adversarial attacks on edge attributes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Develop a novel Graph Attention Fusion Network (GAFN) architecture for fine-grained subcellular component classification using the Zebrafish Visual Cortex Connectivity dataset. Target a sub-90% false positive rate, achieve state-of-the-art performance in F1-score, maintain low inference time, and implement hierarchical attention and explainable attention map generation to enhance interpretability while being robust against adversarial attacks on edge attributes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"false positive rate\",\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Zebrafish Visual Cortex Connectivity\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"Dataset specific, not mentioned\",\n                \"dimensions\": null,\n                \"connectivity_type\": \"Visual Cortex\"\n            },\n            \"description\": \"A dataset for fine-grained subcellular component classification in the Zebrafish visual cortex, containing graph data.\",\n            \"preprocessing\": [\n                \"Graph normalization\"\n            ],\n            \"augmentation\": [\n                \"Robustness against noisy environments\"\n            ],\n            \"visualization\": [\n                \"Explainable attention maps\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GAFN\",\n            \"family\": \"Graph Attention Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novel_features\": [\"Hierarchical attention mechanism\", \"Explainable attention map generation\"],\n                \"robustness\": [\"Adversarial attack resistance\"],\n                \"attention_type\": \"Edge attention\"\n            },\n            \"description\": \"A state-of-the-art Graph Attention Fusion Network for subcellular component classification with enhanced interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Focusing on fine-grained classification, high F1-score is a priority for the model.\",\n        \"Adaptability to noisy environments and low inference time are essential.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Hierarchical feature extraction\",\n            \"Exploratory data analysis\"\n        ],\n        \"target_device\": \"Edge devices or low-latency systems\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Lightweight or optimized for speed\"\n        ],\n        \"demonstration\": \"Generated attention maps to showcase decision-making process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the given instruction accurately, considering the user's intent to build a novel model, their high level of expertise, and the specifics of the problem (graph-based classification, performance metrics, and requirements). It follows the schema without any ambiguity, but certain details like specific model's F1-score, inference time, and novel attention mechanisms' implementation are not provided as they require more details from the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for an elite AI research division, you are now mandated to devise a highly sophisticated temporal fusion model for the intricate ETTm2 dataset, featuring rich, multi-source sensor data from various operational environments. The dataset is organized into nested partitions, train-gen1, train-gen2, validation-gen1, validation-gen2, and test-gen1, each with unique data characteristics. The input sequences consist of historical sensor readings, consisting of 96 timestamps with a diverse set of 15 interconnected features (INPUT_SEQ_LEN=96, INPUT_DIM=15), presenting temporal dependencies of varying frequencies. Your team must not only engineer a novel algorithm that can handle this high-dimensional complexity but also optimize it for concurrent forecasting of multiple horizons (PRED_SEQ_LEN in steps of 16, ranging from 16 to 144, PRED_DIM=15). The model's performance must surpass state-of-the-art in terms of root mean squared error (RMSE), mean absolute percentage error (MAPE), and demonstrate resilience in dealing with sudden anomalies and seasonal patterns, while adhering to stringent real-time latency constraints. Ensure the model's explainability is preserved for further scientific scrutiny and business decision-making.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"operations research or smart energy management\",\n        \"description\": \"Design a temporal fusion model for the ETTm2 dataset, featuring multi-source sensor data from different operational environments. The model must handle high-dimensional input sequences with varying temporal dependencies, concurrent forecasting for multiple horizons, and address anomalies and seasonal patterns with low RMSE and MAPE while maintaining explainability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 15,\n                \"nested partitions\": [\n                    \"train-gen1\",\n                    \"train-gen2\",\n                    \"validation-gen1\",\n                    \"validation-gen2\",\n                    \"test-gen1\"\n                ],\n                \"data characteristics\": \"rich, multi-source sensor data with varying characteristics\"\n            },\n            \"description\": \"An intricate dataset for time-series analysis with nested partitions and diverse sensor readings.\",\n            \"preprocessing\": [\"handling nested partitions and feature engineering for diverse input data\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Temporal Fusion Model or a variant\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"handles high-dimensional input, multiple horizons, and temporal dependencies\",\n                \"explainability\": \"preserved for scientific scrutiny and business decision-making\"\n            },\n            \"description\": \"A novel algorithm designed for the specific ETTm2 dataset with emphasis on multi-source fusion and real-time performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"for high-dimensional and nested data\"\n        ],\n        \"target_device\": \"cloud or edge computing for real-time constraints\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for concurrent multi-horizon forecasting\"],\n        \"demonstration\": \"showcasing resilience to anomalies and seasonal patterns\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the provided details, capturing the advanced nature of the task, dataset characteristics, and performance requirements. The user's intent, high level of expertise, and application domain are appropriately captured. Specific metrics are left unspecified as the values are usually determined through experimentation. The model's exact family and specific specifications are left open-ended due to the nature of the request for a novel algorithm. The confidence score is not 1.0 due to the lack of specific numerical targets for the performance metrics.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a forward-thinking project manager, your mandate is to lead a team in developing a next-generation time-series classification model for the 'Advanced Multimodal Handwriting Dynamics Dataset,' incorporating rare and temporal patterns. The dataset's expansion (200-step sequences, 10-dimensional data) introduces complexity, demanding not only peak accuracy of 97% under stratified time-windowed k-fold cross-validation, but also addressing class imbalance, handling missing values, and ensuring real-time inference. The model must rank among the top 5 in benchmark comparisons while maintaining transparency, interpretability, and energy-efficient computations. Your final report should detail a thorough explainability roadmap, optimization techniques, and a comparison of efficiency metrics with competitor models.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"artificial intelligence\",\n        \"description\": \"Lead a team in developing a next-generation time-series classification model for the Advanced Multimodal Handwriting Dynamics Dataset, incorporating rare and temporal patterns. The dataset expansion (200-step sequences, 10-dimensional data) requires high accuracy (97%), addressing class imbalance, handling missing values, and ensuring real-time inference with energy efficiency. The model must rank among the top 5 in benchmark comparisons and include a comprehensive explainability roadmap, optimization techniques, and a comparison of efficiency metrics with competitor models.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"stratified time-windowed k-fold cross-validation\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time inference\",\n                \"value\": null,\n                \"unit\": \"time\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"Joules/operation\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Advanced Multimodal Handwriting Dynamics Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"class\"],\n            \"specification\": {\n                \"sequence_length\": 200,\n                \"dimensionality\": 10,\n                \"imbalance_handling\": true,\n                \"missing_values\": true\n            },\n            \"description\": \"A dataset for time-series classification with rare and temporal patterns in a large 200-step, 10-dimensional setting.\",\n            \"preprocessing\": [\"data normalization\", \"handling missing values\", \"data augmentation\"],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [\"time-series patterns analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Next-generation time-series classification model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"top_k_ranking\": true,\n                \"interpretability\": true,\n                \"energy_efficiency\": null\n            },\n            \"description\": \"A model designed for advanced time-series classification, incorporating rare patterns and energy-efficient computations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Model must consider class imbalance and temporal dependencies within the dataset.\",\n        \"Incorporate real-time inference and efficient energy usage for hardware constraints.\",\n        \"Include explainability techniques for transparency and interpretability\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for interpretability\"],\n        \"target_device\": \"energy-efficient (embedded or cloud)\",\n        \"deployment_endpoint\": \"API for real-time predictions\",\n        \"inference_engine\": [\"optimized for time-series, low-latency inference\"],\n        \"demonstration\": \"Explainable model deployment and benchmark comparison\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the requirements clearly, capturing the high-level project goal, user's intent, and problem details. Specifics like accuracy, handling missing values, and real-time inference have been included. However, certain exact model specifications like details of feature extraction or the targeted benchmark comparison competitors are not provided. A confidence score of 0.95 reflects the precision, with room for ambiguity in competitor comparisons.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for an advanced deep learning ensemble, you now mandate the team to address a multifaceted optimization challenge. Utilizing the \"Lithium-Ion Battery Performance\" dataset with its intricate 50 chemical attribute variations, the data is divided into non-uniformly distributed train, validation, and test sets of varying sizes. Your objective is to design a next-generation, interpretable regression model that not only reduces the mean absolute percentage error (MAPE) for accurately forecasting electrode capacity but also employs feature permutation importance analysis, SHAP values, and partial dependence plots. In addition, propose a cross-validation strategy and demonstrate the model's performance over different hyperparameter configurations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"energy, materials science\",\n        \"description\": \"Design a next-generation interpretable regression model using the 'Lithium-Ion Battery Performance' dataset with 50 chemical attribute variations. The model should forecast electrode capacity with reduced MAPE, include feature permutation importance analysis, SHAP values, and partial dependence plots. The dataset is divided into non-uniformly distributed train, validation, and test sets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"hyperparameter tuning\",\n                \"value\": null,\n                \"unit\": \"configurations\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Lithium-Ion Battery Performance\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"electrode capacity\"],\n            \"specification\": {\n                \"attribute_dimensions\": 50,\n                \"data_distribution\": \"non-uniform\",\n                \"dataset_splitting\": [\n                    {\"size\": \"train\", \"ratio\": null},\n                    {\"size\": \"validation\", \"ratio\": null},\n                    {\"size\": \"test\", \"ratio\": null}\n                ]\n            },\n            \"description\": \"A dataset with 50 chemical attribute variations for Lithium-ion battery performance prediction.\",\n            \"preprocessing\": [\n                \"data normalization or balancing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature importance, SHAP values, partial dependence plots\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"interpretable deep learning ensemble\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true,\n                \"hyperparameter_configs\": [\n                    \"custom\"\n                ]\n            },\n            \"description\": \"A next-generation model designed for interpretable regression using deep learning ensemble techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Non-uniform dataset distribution requires careful handling in cross-validation.\",\n        \"Understanding feature importance and model behavior is critical.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature importance\", \"SHAP values generation\", \"partial dependence plot\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"approach\": \"cross-validation results and performance for various hyperparameter configurations\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the complex nature of the task, including the requirement for advanced deep learning ensemble, interpretability, and performance analysis. The dataset description and requirements are based on the details provided in the instruction. However, exact data distribution and split ratios, as well as the desired MAPE reduction, are not specified, hence set to 'null' in the JSON.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "As the project manager for the \"Quantum-Coupled Cosmological Knowledge Mining and Response Optimization\" (QCCKMRO) project, guide the team of diverse experts to design and develop a cutting-edge fusion of BERT and Ainstein's innovative QuantumBERT variant. The model should be specifically tailored for the arXiv's exclusive Cosmology, Astrophysics, and High Energy Physics Question-Answering Dataset. Your objective is to achieve an unprecedented F1 score of 99% while maintaining real-time interaction with a sub-millisecond latency constraint, to efficiently propel groundbreaking discoveries in the realm of astrophysical inquiry, considering energy consumption optimization and parallel processing constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"astronomy, cosmology, astrophysics, high energy physics\",\n        \"description\": \"Design and develop a fusion of BERT and Ainstein's QuantumBERT variant, specifically tailored for the arXiv's Cosmology, Astrophysics, and High Energy Physics Question-Answering Dataset. Aim for an F1 score of 99% with real-time interaction with sub-millisecond latency while optimizing energy consumption and leveraging parallel processing.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 Score\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 0.001,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Energy Consumption\",\n                \"value\": null,\n                \"unit\": \"unknown (requires optimization)\"\n            },\n            {\n                \"name\": \"Parallel Processing Efficiency\",\n                \"value\": null,\n                \"unit\": \"unknown\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv's Cosmology, Astrophysics, and High Energy Physics Question-Answering Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"question, answer\"],\n            \"specification\": {\n                \"genre\": \"question-answering\",\n                \"domain_relevance\": \"cosmology, astrophysics, high energy physics\",\n                \"preprocessing_done\": \"none (inferred)\"\n            },\n            \"description\": \"A specialized dataset for question answering in the astrophysical domain, sourced from arXiv.\",\n            \"preprocessing\": [\"none (assuming task-specific preprocessing)\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-Coupled Cosmological Knowledge Mining Model\",\n            \"family\": \"BERT Fusion (BERT and QuantumBERT)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT with QuantumBERT adaptations\",\n                \"target_score\": 0.99,\n                \"real_time_inference\": true,\n                \"latency_target\": \"sub-millisecond\"\n            },\n            \"description\": \"A state-of-the-art model combining BERT and QuantumBERT, optimized for high F1 score and real-time performance on arXiv's cosmology question-answering dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"BERT and QuantumBERT integration is a cutting-edge approach in NLP.\",\n        \"The need to maintain sub-millisecond latency is critical for astrophysical inquiry.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Energy-efficient algorithms\",\n            \"Parallel processing implementation\"\n        ],\n        \"target_device\": \"cloud or high-performance computing cluster\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"custom (optimized for energy and latency)\"],\n        \"demonstration\": \"Real-time response to astrophysical questions with visualizations of latency and energy consumption\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent (to build a model), their expertise level, the project's focus on question answering and dataset requirements, performance and complexity metrics, along with the specific details and requirements for the model and service. The missing parts of the model and service specifications indicate that the team might need to explore advanced models, inference engines, and possible optimizations to meet the ambitious F1 score and latency goals.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "text classification", "instruction": "As the visionary head of the elite deep learning development unit, your task is to develop a cutting-edge sentiment analysis model using the extensive and fine-grained TREC-QP dataset. The classifier should accurately discern between intricate levels of textual entailment (0: no cause-and-effect, 1: precise match, 2: implied agreement), while incorporating a robust metric that measures the model's robustness against semantic variations and rare syntactic anomalies. Mandate a meticulous, multi-step evaluation that includes harmonic mean of F1 score, precision, and recall, with a focus on the effect of cross-domain adaptation across audio-visual inputs for a robust performance benchmark.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"sentiment analysis\",\n        \"description\": \"Develop a cutting-edge sentiment analysis model using the TREC-QP dataset for fine-grained textual entailment detection (0: no cause-and-effect, 1: precise match, 2: implied agreement). The model should incorporate robustness against semantic variations and rare syntactic anomalies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"harmonic mean\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"robustness against semantic variations\",\n                \"value\": null,\n                \"unit\": null\n            },\n            {\n                \"name\": \"resistance to rare syntactic anomalies\",\n                \"value\": null,\n                \"unit\": null\n            },\n            {\n                \"name\": \"cross-domain adaptation performance\",\n                \"value\": null,\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TREC-QP\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"fine-grained\": true,\n                \"entailment_levels\": [\"0: no cause-and-effect\", \"1: precise match\", \"2: implied agreement\"]\n            },\n            \"description\": \"An extensive and fine-grained sentiment analysis dataset with a focus on textual entailment detection across audio-visual inputs.\",\n            \"preprocessing\": [\"robust preprocessing for semantic and syntactic variations\"],\n            \"augmentation\": [\"methods to handle cross-domain adaptation\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"classification_levels\": [\"no cause-and-effect\", \"precise match\", \"implied agreement\"],\n                \"domain_adaptation\": true\n            },\n            \"description\": \"A state-of-the-art sentiment analysis model designed for detecting fine-grained textual entailment with robustness against semantic and syntactic variations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Semantic variations and rare syntactic anomalies require careful consideration in model design.\",\n        \"Cross-domain adaptation for audio-visual inputs is a key factor in benchmarking model robustness.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain-specific feature extraction\",\n            \"context-aware representation learning\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Showcase model performance across different domains and modalities\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is tailored based on the provided instruction, with high level of confidence. The problem area, task, and user's expertise are derived from the context. The metrics for evaluation, such as harmonic mean, precision, recall, and cross-domain adaptation performance, are included as requested. However, specific performance targets and detailed model architecture are not provided as they are left open for the expert to decide based on user's requirements. The confidence score is high given the clear instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "Request the ML team to design a cutting-edge plant species classifier, using the 'Extensive, High-Resolution Seedling Dataset with Complex Photographic Variations and Realistic Noise,' integrating not just ResNet50 and DenseNet121, but a heterogeneous ensemble of five deep learning architectures from diverse domains. Mandate implementation of specialized nighttime imaging capabilities, optimizing for ultra-low power Google Coral Edge TPUs (10mW) with quantum efficiency enhancements. Propose a scalable, real-time mobile-first deployment strategy for Snapdragon 888-powered devices, ensuring sub-50ms latency on mid-range budget phones and resource-constrained IoT gadgets, while maintaining model interpretability for user trust in decision-making backed by local reasoning algorithms.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"ecology\",\n        \"description\": \"Design a cutting-edge plant species classifier using the 'Extensive, High-Resolution Seedling Dataset with Complex Photographic Variations and Realistic Noise.' Include ResNet50 and DenseNet121, as well as a heterogeneous ensemble of five deep learning architectures from different domains, focusing on nighttime imaging and ultra-low power optimization for Google Coral Edge TPUs (10mW) with quantum efficiency enhancements.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"sub-50ms\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"power_consumption\",\n                \"value\": 10,\n                \"unit\": \"mW\"\n            },\n            {\n                \"name\": \"quantum_efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extensive, High-Resolution Seedling Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species labels\"],\n            \"specification\": {\n                \"size\": \"high-resolution\",\n                \"variability\": \"complex\",\n                \"noise\": \"realistic\"\n            },\n            \"description\": \"A dataset for plant species classification with high-resolution images, complex variations, and realistic noise.\",\n            \"preprocessing\": [\"nighttime imaging\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Plant Species Classifier Ensemble\",\n            \"family\": \"deep learning\",\n            \"type\": [\"neural networks\"],\n            \"specification\": {\n                \"architectures\": [\"ResNet50\", \"DenseNet121\", \"other four diverse architectures\"],\n                \"nighttime_imaging\": true,\n                \"quantum_efficiency\": null\n            },\n            \"description\": \"A high-performance ensemble using diverse deep learning architectures for plant species classification.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": [\"Snapdragon 888\", \"Google Coral Edge TPUs\"],\n        \"deployment_endpoint\": \"mobile-first, scalable for IoT\",\n        \"inference_engine\": [\"Google Coral Edge ML\"],\n        \"demonstration\": \"Model interpretability with local reasoning algorithms\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the given instruction. The problem area and downstream task are inferred from the task of classifying plant species with complex data. The user's intent to build a model and high expertise level are indicated. The performance metric (accuracy) and latency constraint are mentioned. The power consumption and quantum efficiency metrics are added based on the hardware requirements. The dataset and model specifications reflect the details provided in the instruction. However, some specific metric values and model architecture details are missing, as they were not provided in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a specialized AI consultant for \"MediCognify,\" your task is to design a highly sophisticated deep learning ensemble model for a precision diagnostics division. The model must decipher intricate EHRs of patients with ultra-rare syndromes, pinpointing subtextual correlations between specific, gene-targeted therapies and their long-term health outcomes. Employ advanced progressive transfer learning techniques, generate interactive visual explanations utilizing the state-of-the-art SHAPley Additive exPlanations (SHAP-360) framework, and consistently aim for an exceptional AUC-ROC of 0.999. Conform to the most stringent healthcare data privacy regulations, such as HIPAA and GDPR, and optimize for a minimum 7% improvement in predictive accuracy over the previous iteration's model, benchmarked on a patient cohort with pre-existing conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Design a deep learning ensemble model for precision diagnostics in MediCognify's precision medicine for ultra-rare syndromes. Model must identify subtextual correlations between gene-targeted therapies and long-term health outcomes, using advanced progressive transfer learning, and generate interactive visual explanations with SHAP-360. Aim for an AUC-ROC of 0.999, adhering to HIPAA and GDPR, and achieve a 7% improvement over previous model's accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": 0.999\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"predictive accuracy improvement\",\n                \"value\": 0.07,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ultra-Rare Syndrome EHR\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"health outcomes\", \"gene-targeted therapies\"],\n            \"specification\": {\n                \"size\": \"clinical trial\",\n                \"structured\": true,\n                \"privacy_protected\": true\n            },\n            \"description\": \"Dataset containing EHRs of patients with ultra-rare syndromes and pre-existing conditions.\",\n            \"preprocessing\": [\"anonymization\", \"PII masking\"],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [\"SHAP-360 explanations\"],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Ensemble Deep Learning Model\",\n            \"family\": \"Deep Learning Ensemble\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ensemble of multiple deep learning models\",\n                \"transfer_learning\": \"progressive\",\n                \"privacy_strategies\": [\"HIPAA compliant\", \"GDPR compliant\"]\n            },\n            \"description\": \"Sophisticated ensemble model designed for deep learning and precision diagnostics with an emphasis on ultra-rare syndromes.\"\n        }\n    ],\n    \"knowledge\": [\n        \"SHAP-360 for interpretable explanations\",\n        \"Advanced transfer learning for complex EHR data\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"gene Ontology feature extraction\", \"medical concept embeddings\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"cloud-based\", \"federated learning\"],\n        \"demonstration\": \"Interactive model explanations and result visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes the user's intent to build a model, high level of expertise, and a detailed problem description that aligns with NLP and healthcare. The metrics and model requirements are clearly specified, such as AUC-ROC and accuracy improvement. The source of the dataset and the model requirements are inferred from the context. However, the specific pre-existing models and privacy measures may need additional details.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "In your capacity as a meticulous project director for an elite AI lab specializing in precision meteorology, your assignment is to create a highly specialized deep-learning architecture specifically designed for the Metropolis-X Weather System. This architecture requires a fusion of a state-of-the-art ConvLSTM-Transformer, tailored for the UrbanWeather v2.1+'s expanded dataset (25 variables, with a complex INPUT_SEQ_LEN of 1,440 hourly readings), focusing on sub-hourly predictions in densely populated megacities.\r\n\r\nThe target is a 192-hour (7.5-day) forecast horizon, optimized with a novel ExtremeClimateImpact Metric (ECIM), which imposes a dynamic penalty for severe weather events. The model must also showcase excellence in Multi-Scale Error Logarithmic Penalty (MSLE+) and a custom Mean Absolute Percentile Prediction Error (MAPPE-) adjusted for variable error bands across different climate conditions. Implement an adaptive validation process using nested rolling window cross-validation every week, ensuring seasonality adaptability in optimization.\r\n\r\nAdditionally, devise a progressive model evolution strategy that combines transfer learning from minutely updated satellite data, continuous monitoring of extrinsic factors like greenhouse gases, and frequent fine-tuning every 10 days. The objective is to maintain peak accuracy in the dynamic and rapidly changing urban climate environment, where environmental resilience is key.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"precision meteorology\",\n        \"description\": \"Create a specialized deep-learning architecture for the Metropolis-X Weather System using a ConvLSTM-Transformer. The model should handle the UrbanWeather v2.1+'s expanded dataset with 25 variables and complex INPUT_SEQ_LENGTH (1,440 hourly readings). The target is a 192-hour forecast horizon with ExtremeClimateImpact Metric (ECIM), Multi-Scale Error Logarithmic Penalty (MSLE+), and custom Mean Absolute Percentile Prediction Error (MAPPE-) for variable error bands. Implement nested rolling window cross-validation weekly, accounting for seasonality, and a progressive model evolution strategy involving transfer learning from minutely updated satellite data, greenhouse gas monitoring, and frequent fine-tuning every 10 days.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"ExtremeClimateImpact Metric (ECIM)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Multi-Scale Error Logarithmic Penalty (MSLE+)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Percentile Prediction Error (MAPPE-)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UrbanWeather v2.1+\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"input_seq_len\": 1440,\n                \"variables_count\": 25\n            },\n            \"description\": \"A dataset with 25 variables for the expanded UrbanWeather v2.1+, focused on sub-hourly predictions in dense megacities.\",\n            \"preprocessing\": [\"nested rolling window cross-validation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ConvLSTM-Transformer-based Architecture\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"target_horizon\": 192,\n                \"custom_metrics\": [\n                    \"ECIM\", \n                    \"MSLE+\", \n                    \"MAPPE-\"\n                ]\n            },\n            \"description\": \"A deep-learning architecture that combines ConvLSTM and Transformer specifically for time-series forecasting with UrbanWeather v2.1+'s enhanced dataset and complex requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptability to seasonality, environmental resilience, and dynamically changing climate conditions.\",\n        \"Transfer learning from minutely updated satellite data and greenhouse gas monitoring.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"custom multi-scale and error adjustment\"],\n        \"target_device\": \"high-performance computing cluster\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"high-performance, GPU-based\"],\n        \"demonstration\": \"seasonal and forecast accuracy demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the meteorological deep-learning project, including the specific task, architecture, dataset, and performance metrics. The high expertise level of the user and the intricate requirements of the project are reflected. The metrics' values are left unspecified as they would require numerical targets, which were not provided in the instruction. However, seasonality adaptability and progressive model evolution strategies are clearly defined. The confidence score is high due to the clarity of mapping the instruction to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for our next-generation precision agriculture project, mandate the machine learning team to design and implement a DeepLabV3+ architecture that not only meets the existing standards but also incorporates domain-specific transfer learning using the AgriSeg-Advanced dataset. The model should achieve a minimum of 95% precision and 92% recall, with a stringent real-time performance benchmark of 35 FPS, ensuring seamless operation on resource-restricted edge devices functioning optimally across varying illumination, temperature, and soil moisture conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"precision agriculture\",\n        \"description\": \"Design and implement a DeepLabV3+ architecture for precision agriculture, incorporating domain-specific transfer learning with the AgriSeg-Advanced dataset. The model must meet a performance target of at least 95% precision and 92% recall, while ensuring real-time performance with 35 FPS on resource-restricted edge devices operating under varying illumination, temperature, and soil moisture conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.92\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 35,\n                \"unit\": \"FPS\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AgriSeg-Advanced\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain-specificity\": true\n            },\n            \"description\": \"A dataset for precision agriculture that requires domain transfer learning, specifically designed for the AgriSeg-Advanced use case.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepLabV3+\",\n            \"family\": \"convolutional neural networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": true,\n                \"adaptation_to_domain\": \"precision agriculture\",\n                \"target_performance\": \"95% precision, 92% recall\"\n            },\n            \"description\": \"A DeepLabV3+ model tailored for precision agriculture with domain-specific transfer learning and tailored performance metrics.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance on resource-restricted devices\"],\n        \"demonstration\": \"seamless operation across varying conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's intent to build a machine learning model, the expected high expertise level, and the specific problem and performance requirements. The problem area (computer vision) and downstream task (image segmentation) are based on the DeepLabV3+ architecture and dataset, while the performance metrics and inference time are clearly specified. The model's CNN type and the use of transfer learning are directly derived from the instruction. However, specific details like precise preprocessing steps are missing as they are not mentioned in the provided instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for an elite AI research team working on a sophisticated wavelet-based fusion model for smart grid energy forecasting, you are tasked with addressing a complex challenge involving the UK National Grid's ElectricityDataset_2022. This dataset, rich in minute-grained temporal patterns and anomalies, necessitates the creation of distinct training (75% sequence overlap), validation (25% overlap), and test partitions with a peculiar focus on harmonizing wavelet-based features (INPUT_SEQ_LEN=96, input dimensions expanded to 500 unique sensors readings). The architecture must forecast the next 72 hours (PRED_SEQ_LEN=72) while accounting for irregular seasonal fluctuations and dynamic trends in renewable energy sources. The team should employ state-of-the-art deep learning architectures like Temporal Fusion Transformers and apply Bayesian optimization for hyperparameter tuning, comparing performance against advanced ensemble methods such as stacked generalization. Document the trade-offs, considering computational efficiency, interpretability, and the impact of seasonality adjustment techniques on error metrics like log-cosh and Symmetric Mean Absolute Percentage Error (SMAPE).", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart grid energy management\",\n        \"description\": \"Address a complex energy forecasting challenge using the UK National Grid's ElectricityDataset_2022. The model should be based on a wavelet-based fusion model, incorporate minute-grained temporal patterns and anomalies, and forecast the next 72 hours with adjustments for irregular seasonal fluctuations and dynamic renewable energy sources. Employ Temporal Fusion Transformers and Bayesian optimization for hyperparameter tuning, comparing with advanced ensemble methods like stacked generalization. Document trade-offs in computational efficiency, interpretability, and impact of seasonality adjustment techniques on log-cosh and SMAPE.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"log-cosh\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Symmetric Mean Absolute Percentage Error (SMAPE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs or hours\"\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"model complexity or explainability score\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ElectricityDataset_2022\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"input_dimensions\": 500,\n                \"sensors_readings\": \"unique from 500 sensors\"\n            },\n            \"description\": \"A dataset with minute-grained temporal patterns and anomalies from the UK National Grid's smart grid. Targeted for wavelet-based fusion with 75% training, 25% validation and test partitions, focusing on unique sensor readings and harmonized wavelet features.\",\n            \"preprocessing\": [\"sequence partitioning\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time-series patterns, anomalies\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Wavelet-based Fusion Model (with Temporal Fusion Transformers)\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Temporal Fusion Transformers\",\n                \"hyperparameter_optimization\": \"Bayesian\",\n                \"ensemble_method\": \"stacked generalization\"\n            },\n            \"description\": \"A state-of-the-art model combining wavelet features for smart grid energy forecasting, accounting for seasonal fluctuations and renewable sources.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Document the trade-offs in model performance, efficiency, and interpretability.\",\n        \"Consider the impact of seasonality adjustments on log-cosh and SMAPE.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"wavelet feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"potentially parallel processing\"],\n        \"demonstration\": \"smart grid performance visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the given instruction, capturing the research area, task, dataset specifics, and model requirements. However, exact performance values for metrics are not provided as they typically require model evaluation. Confidence score is high due to clear guidance, but missing details on specific preprocessing steps, exact ensemble methods, and computational environment.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a seasoned project manager, you must now guide your machine learning team to design an innovative regression model for the Crab Age Dataset, considering its complex 10,000-dimensional features and diverse data types. The dataset has undergone rigorous stratification into specialized zones (train, validation A, validation B, and test), each reflecting different environmental factors. Your task is to enhance performance by reducing MAE, deciphering non-linear patterns, and disentangling interaction effects. Implement an extensive feature extraction pipeline, explore cutting-edge ensemble methods, and conduct rigorous cross-validation experiments, ensuring interpretability and robustness, all while maintaining a competitive edge in age prediction amidst stringent industry benchmarks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"data science or industry\",\n        \"description\": \"Design an innovative regression model for the Crab Age Dataset with 10,000-dimensional complex features and diverse data types. Perform extensive feature extraction, explore ensemble methods, handle stratified zones, reduce MAE, and ensure interpretability and robustness against industry benchmarks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Crab Age Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"age\"],\n            \"specification\": {\n                \"dimensionality\": 10000,\n                \"data_types\": [\"complex\", \"diverse\"]\n            },\n            \"description\": \"A dataset with 10,000-dimensional features and diverse data types, stratified into train, validation A, validation B, and test zones to reflect different environmental factors.\",\n            \"preprocessing\": [\n                \"feature extraction pipeline\",\n                \"data normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble methods\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretable\": true,\n                \"robustness\": \"high\",\n                \"cross-validation\": \"rigorous\"\n            },\n            \"description\": \"A cutting-edge regression model with a focus on feature extraction, ensemble techniques, and non-linear pattern analysis, designed for the Crab Age Dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consideration of 10,000-dimensional features and diverse data types\",\n        \"Stratified zones for evaluating performance under different environmental conditions\",\n        \"Goal to reduce MAE and enhance model interpretability\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"deep feature extraction\",\n            \"dimensionality reduction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explain model decisions with feature importances\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent and expertise level based on the scenario. The problem description, task, and specific requirements are derived from the instruction, including performance metrics (MAE), and complexity constraints. The data source and preprocessing steps are extracted from the dataset characteristics. The model type and family are set according to the requirements for innovation and non-linear analysis. The missing values for inference time and memory size are placeholders for any optimized values the team would determine. The missing target_device and deployment details would be filled based on team's choice or project needs.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design a state-of-the-art translation model, leveraging the specialized ParaCrawl v6 dataset for low-resource languages. Implement a dual-attention transformer architecture with adaptive sentiment analysis, specifically tailored for code-switching in formal and informal English-Dutch exchanges. Emphasize preservation of cultural idioms and subtext, ensuring cross-cultural semantic fidelity while maintaining tone consistency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-cultural communication\",\n        \"description\": \"Design a state-of-the-art translation model using the ParaCrawl v6 dataset for low-resource languages. The model should employ a dual-attention transformer architecture with adaptive sentiment analysis, specifically tailored for code-switching in formal and informal English-Dutch exchanges. Emphasis should be on preserving cultural idioms and subtext while maintaining semantic fidelity and tone consistency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cross-cultural semantic similarity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl v6\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"low-resource\",\n                \"languages\": [\"English\", \"Dutch\"]\n            },\n            \"description\": \"A specialized dataset for low-resource languages, focusing on code-switching in English-Dutch exchanges.\",\n            \"preprocessing\": [\"adaptation for low-resource setting\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-Art Dual-Attention Transformer\",\n            \"family\": \"Transformer architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"dual-attention\",\n                \"sentiment_analysis\": \"adaptive\",\n                \"language_pair\": \"English-Dutch\",\n                \"code-switching\": true,\n                \"cultural_idioms\": true\n            },\n            \"description\": \"A tailored model for code-switching with sentiment-aware translation and preservation of cultural nuances.\"\n        }\n    ],\n    \"knowledge\": [\n        \"ParaCrawl v6's low-resource nature calls for innovative techniques to handle data scarcity.\",\n        \"Adaptive sentiment analysis is crucial for handling nuanced language in code-switching scenarios.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"focus on cultural preservation and tone consistency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a model, their high level of expertise, and the problem area (NLP). The problem statement accurately reflects the instruction, specifying the state-of-the-art model, dataset, and its features. The model's specification includes key components like dual-attention transformer, sentiment analysis, and code-switching. Performance metrics are included, but values are not specified as they would typically be optimized during development. The dataset source is set as a user link, indicating the need for external data access. The confidence score is high due to clear interpretation of the task and accurate schema mapping.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, design a cutting-edge gamified Q&A platform for primary school kids, incorporating the exclusive KidsQA+ dataset. Mandate a novel fusion of visual storytelling, adaptive storytelling with cartoon illustrations, and a stripped-down BERT model optimized for simplicity. Alongside educational content, guarantee cognitive milestone tracking, maintaining a GUES-score above 85% for age-appropriate, error-free responses, and restrict the system to process queries with real-time natural language understanding within a strict 200-millisecond latency constraint. Additionally, prioritize accessibility for special needs students with minimal audio cues.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education, children's learning\",\n        \"description\": \"Design a cutting-edge gamified Q&A platform for primary school kids using the exclusive KidsQA+ dataset. The platform should incorporate visual storytelling, adaptive storytelling with cartoon illustrations, and a simplified BERT model. Key performance metrics include maintaining a GUES-score above 85% for error-free and age-appropriate responses, real-time natural language understanding with a latency of 200 milliseconds, and accessibility for special needs students with minimal audio cues.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"GUES-score\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 0.2,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA+\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"custom\",\n                \"age-appropriate\": true,\n                \"content-type\": \"educational, gamified\"\n            },\n            \"description\": \"A dataset designed specifically for a primary school kids' Q&A platform, incorporating educational and gamified content.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Stripped-down BERT\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"simplified_architecture\": true,\n                \"model_depth\": \"reduced\",\n                \"optimizer_for\": \"real-time performance\"\n            },\n            \"description\": \"A simplified BERT model optimized for question answering with real-world constraints.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time processing, visual storytelling, and accessibility for special needs students are key features.\",\n        \"Fusion of multiple storytelling techniques and a GUES-score-based evaluation for educational effectiveness.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"low-resource\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"gaming-like, age-appropriate interactive experience\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response accurately captures the key aspects of the project as described in the instruction. The user's intent is to build a platform, and their high level of expertise is assumed. The problem area is natural language processing for Q&A, with performance and complexity metrics specified. The KidsQA+ dataset and the platform's features, like visual storytelling and latency, are represented. However, the precise model adaptation details for simplicity, audio cues for accessibility, and exact GUES-score criteria are not provided, as they are not clearly defined in the instruction.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager, lead the AI team to develop a real-time, energy-efficient text classifier for the 20 Newsgroups dataset using state-of-the-art, explainable AI. Implement privacy-preserving federated learning with rigorous security protocols, optimizing for minimal latency on resource-constrained edge devices with limited compute power.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Lead the AI team to develop a real-time, energy-efficient text classifier for the 20 Newsgroups dataset using explainable AI. Implement privacy-preserving federated learning with strict security protocols, focusing on minimizing latency for resource-constrained edge devices with limited compute power.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy consumption\",\n                \"value\": null,\n                \"unit\": \"Joules\"\n            },\n            {\n                \"name\": \"computational latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"data transfer efficiency\",\n                \"value\": null,\n                \"unit\": \"bits per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"category\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensionality\": null,\n                \"length\": null,\n                \"sampling_frequency\": null\n            },\n            \"description\": \"A real-time text classification dataset with news articles\",\n            \"preprocessing\": [\"text normalization\", \"tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art explainable AI\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"\",\n                \"explanability_method\": \"\"\n            },\n            \"description\": \"A real-time, energy-efficient text classifier for resource-constrained edge devices\"\n        }\n    ],\n    \"knowledge\": [\n        \"Using privacy-preserving federated learning with explainability techniques for transparency\",\n        \"Targeting resource-constrained edge devices with focus on energy efficiency and low latency\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"federated data preprocessing\",\n            \"interpretability techniques\"\n        ],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time\", \"efficient deployment\"],\n        \"demonstration\": \"explaining model decisions and performance on edge devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the project manager's intention to build a state-of-the-art text classifier, utilizing explainable AI and federated learning. Details on the problem's area, task, domain, and requirements are derived from the instruction. Performance and complexity metrics are not explicitly provided, but placeholders are included to show the need for optimizing accuracy, energy efficiency, and latency. The dataset specification, preprocessing steps, and model requirements are derived from the use of 20 Newsgroups, state-of-the-art AI, and resource-constrained edge devices. The request for strict security and privacy implies additional elements in the feature engineering and service requirements.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the AI development team, you've been tasked with designing a state-of-the-art text classification model that not only employs the BoolQ dataset but also pushes the boundaries of explainable AI. The model must achieve a pinnacle of precision with an F1-score of at least 0.95, while maintaining interpretability through interactive feature importance heatmaps. Moreover, the project necessitates a sophisticated BERT-base architecture optimized within a stringent resource constraint of only 3 epochs, while also considering computational efficiency and runtime efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Design a state-of-the-art text classification model using the BoolQ dataset, with a focus on explainability, achieving an F1-score of at least 0.95, and supporting interactive feature importance heatmaps. The model should be optimized using a BERT-base architecture within a resource constraint of 3 epochs, prioritizing computational and runtime efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"epochs\",\n                \"value\": 3,\n                \"unit\": \"epochs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": \"small\",\n                \"training_data_ratio\": 0.8,\n                \"test_data_ratio\": 0.2\n            },\n            \"description\": \"A text classification dataset for the state-of-the-art model development.\",\n            \"preprocessing\": [\"BERT tokenization\", \"padding\"],\n            \"augmentation\": [],\n            \"visualization\": [\"interactive feature importance heatmaps\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"BERT-base\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT\",\n                \"epochs\": 3,\n                \"resource_constraint\": \"computational and runtime efficiency\"\n            },\n            \"description\": \"A state-of-the-art BERT-based model designed for text classification, with emphasis on interpretability and optimized within a 3-epoch constraint.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Employing BERT-base for text classification while maintaining explainability\",\n        \"3-epoch resource constraint and its impact on optimization\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"BERT tokenization\",\n            \"limited preprocessing due to resource constraints\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"BERT inference-friendly implementation\"\n        ],\n        \"demonstration\": \"Interactive feature importance heatmaps\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response parses the user's instruction with focus on the main requirements: a state-of-the-art text classification model, high F1-score, and explainability. Performance metrics, dataset characteristics, and a BERT-base architecture with resource constraints are captured. Missing parts include fine-grained preprocessing steps and specific model configuration details since they are not entirely specified in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As a project manager for a high-performance industrial monitoring system, you need the development team to design and integrate a scalable anomaly detection solution. They must build and fine-tune at least three state-of-the-art lightweight transformer models, like EfficientTS, TinyTimeformer, and MiniTrans, with a focus on efficiency and interpretability. Each model must demonstrate robustness with an average accuracy of over 75% across the extensive 250 Hexagon ML/UCR benchmark datasets, while ensuring real-time performance with no more than 5% latency for seamless production-line surveillance. Additionally, document the model's explainability and provide detailed comparison metrics with competitor methods in a comprehensive report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"industrial monitoring\",\n        \"description\": \"Design and integrate a scalable, efficient, and interpretable anomaly detection solution for an industrial monitoring system. Build and fine-tune at least three state-of-the-art lightweight transformer models (EfficientTS, TinyTimeformer, MiniTrans), targeting an average accuracy of over 75% on 250 Hexagon ML/UCR benchmark datasets while maintaining real-time performance with no more than 5% latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average accuracy\",\n                \"value\": 0.75\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.05,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": [\"Hexagon ML/UCR benchmark datasets\"],\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"anomaly detection\"],\n            \"specification\": {\n                \"size\": \"extensive (250 datasets)\",\n                \"performance\": \"seamless production-line surveillance\"\n            },\n            \"description\": \"A diverse collection of time-series datasets for benchmarking anomaly detection models\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": [\"EfficientTS\", \"TinyTimeformer\", \"MiniTrans\"],\n            \"family\": \"Lightweight Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": \"high\",\n                \"interpretability\": \"required\"\n            },\n            \"description\": [\n                \"Each model optimized for performance and interpretability\",\n                \"Focus on efficiency and scalability\"\n            ]\n        }\n    ],\n    \"knowledge\": [\n        \"Focus on lightweight transformer models for real-time industrial monitoring\",\n        \"Anomaly detection must be robust and interpretable for critical decision-making\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"fine-tuning for efficiency and interpretability\"],\n        \"target_device\": \"工业环境, requires real-time performance\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for industrial deployment\"],\n        \"demonstration\": \"real-time production-line surveillance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the project manager's requirements, detailing the problem area, target models, performance and complexity metrics, dataset, and final service expectations. The request for state-of-the-art transformers, high accuracy, and real-time performance is clearly captured. The confidence score is high as the template is closely adhered to and critical information is included.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "For the \"Majestic Lexicon Odyssey\" project, our development team must delve into a highly specialized BERT model architecture, refining the RoBERTa-large variant with an emphasis on Explainable Hierarchical Question-Answering (EHQA) adaptation from a curated SQuAD 3.0 subset. Our objective is to exceed a demanding F1 score of 96% in the specialized biomedical and legal domains, while ensuring energy efficiency with a tight response time ceiling of 1 millisecond, all while optimizing for real-time performance on a ultra-modern, distributed computing infrastructure with dynamic resource allocation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": [\"biomedical\", \"legal\"],\n        \"description\": \"For the 'Majestic Lexicon Odyssey' project, we need to develop a highly specialized BERT model architecture, specifically focusing on Explainable Hierarchical Question-Answering (EHQA) adaptation from a curated SQuAD 3.0 subset for biomedical and legal domains. The target F1 score is 96%, and the model must be energy-efficient with a response time ceiling of 1 millisecond for real-time performance on a distributed, ultra-modern infrastructure with dynamic resource allocation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.96\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.001,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 3.0 (customized for EHQA in biomedical and legal domains)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"subset\": \"curated for EHQA\",\n                \"domains\": [\"biomedical\", \"legal\"]\n            },\n            \"description\": \"A curated subset of SQuAD 3.0 for EHQA adaptation with emphasis on specialized domains.\",\n            \"preprocessing\": [\n                \"Hierarchical segmentation\",\n                \"BERT pretraining\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-RoBERTa-large with Explainable Hierarchical Question-Answering (EHQA)\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"domain_specificization\": \"EHQA\",\n                \"architecture\": \"BERT-RoBERTa-large\",\n                \"resource_efficiency\": \"energy-efficient\",\n                \"runtime\": \"real-time\"\n            },\n            \"description\": \"BERT model variant optimized for hierarchical question answering in biomedical and legal domains with a target F1 score of 96%.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"BERT pretraining\", \"question-answer component fine-tuning\"],\n        \"target_device\": \"ultra-modern, distributed computing infrastructure\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"explainable results and real-time performance demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a specialized model (BERT-RoBERTa-large for EHQA), the high level of expertise, and the specific requirements for the project. The problem area is NLP, with a focus on question answering. The F1 score and response time metrics are included, and the dataset is described with relevant preprocessing and model development steps. Energy efficiency and real-time performance on a distributed infrastructure are captured. However, the exact details of the dynamic resource allocation strategy would need further clarification.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a cutting-edge sentiment-aware machine translation system, specifically tailored for preserving nuanced emotions in English-to-Dutch communication. Utilize the state-of-the-art ParaCrawl DeepLinguistic dataset, employing a complex siamese transformer-based dual-encoder model with multi-layer emotion classification, demanding exceptional accuracy in both lexical and contextual emotion transmission.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-cultural communication\",\n        \"description\": \"Develop a sentiment-aware machine translation system for English-to-Dutch, focusing on preserving nuanced emotions. Use the ParaCrawl DeepLinguistic dataset and a siamese transformer-based dual-encoder model with multi-layer emotion classification.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"lexical emotion transmission accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"contextual emotion transmission accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl DeepLinguistic\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"linguistic_depth\": \"state-of-the-art\",\n                \"emotion_labels\": null\n            },\n            \"description\": \"A high-quality dataset for English-to-Dutch translation, focusing on sentiment preservation through complex linguistic analysis.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Siamese Transformer Dual-Encoder\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"complex siamese transformer\",\n                \"emotion_classification_layers\": \"multi-layer\"\n            },\n            \"description\": \"A state-of-the-art machine translation model designed for preserving nuanced emotions specifically in English-to-Dutch communication.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Emphasis on sentiment preservation in machine translation requires fine-grained emotion classification and deep linguistic analysis.\",\n        \"Siamese transformers are known for their ability to handle similar tasks on both inputs, which suits the translation and emotion classification components.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Sentiment-preserved translation demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a high-level machine translation system, the problem's details regarding dataset, task, and performance metrics. Given the complexity of the task, the user's expertise level is assumed to be high. The model's sophistication, as described, matches the instruction. However, explicit values for performance metrics and time complexity are not provided as they would typically depend on experimentation and specific results. The missing information is not significant as it can be determined post-model development.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "visual question answering", "instruction": "As the project director, your objective is to design a highly sophisticated TextVQA platform for a prestigious educational institution. The system must demonstrate advanced level understanding by fusing image captions, recognizing subtle visual cues, and decoding intricate multimodal data. Mandate the integration of cutting-edge architectures like M6 and LXMERT, while optimizing for precision, large-scale handling, and efficient deployment on heterogeneous computing infrastructure. Additionally, devise a detailed evaluation plan involving A/B testing and continuous monitoring to ensure continuous improvement in performance over time.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Design a highly advanced TextVQA platform for an educational institution, featuring advanced understanding of images through fused image captions, recognition of subtle visual cues, and decoding complex multimodal data. Target architectures include M6 and LXMERT, while focusing on precision, scalability, and efficient deployment on diverse hardware. An evaluation plan involving A/B testing and continuous monitoring for performance improvement is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"scale efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"milliseconds\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"unit\": \"GB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Custom Educational Dataset\",\n            \"modality\": [\"text\", \"image\", \"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"imagesource\": \"homogeneous\",\n                \"caption_length\": {\n                    \"average\": null,\n                    \"min\": null,\n                    \"max\": null\n                },\n                \"video_duration\": {\n                    \"average\": null,\n                    \"min\": null,\n                    \"max\": null\n                }\n            },\n            \"description\": \"A custom dataset curated for the TextVQA platform, containing image captions, video data, and educational context.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"M6\",\n                \"embedding_dim\": null,\n                \"hidden_layers\": null\n            },\n            \"description\": \"An advanced text-to-image model built with M6 architecture.\"\n        },\n        {\n            \"name\": \"LXMERT\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"LXMERT\",\n                \"pretrained_model_size\": null,\n                \"fusion_layers\": null\n            },\n            \"description\": \"A multimodal model with LXMERT, designed for fusing text and vision inputs.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integration of M6 and LXMERT requires a thorough understanding of state-of-the-art multimodal architectures.\",\n        \"Scalability and efficiency require tailored implementation strategies for heterogeneous hardware.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"heterogeneous deployment support\", \"efficiency optimization\"],\n        \"target_device\": \"heterogeneous computing infrastructure\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"custom optimized inference libraries\"],\n        \"demonstration\": {\n            \"methodology\": \"A/B testing and continuous monitoring\",\n            \"frequency\": \"periodic\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent (to build a TextVQA platform), expertise level (high), and the problem description (with emphasis on precision, scalability, and multimodal fusion). The areas, tasks, and domains are derived from the given context. Performance metrics, complexity metrics, and the platform's evaluation plan are formulated based on the objectives provided. Specific details like model specifications, datasets, and feature engineering can be filled in more detailed research. The confidence score is high due to the clear mapping of the instruction to the JSON schema.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for an elite galactic research initiative, direct your transdisciplinary team to develop a highly specialized algorithm that distinguishes between various types of Andromeda Nebula sub-species. Instruct them to fetch the 'GalaxyZoo2-Andromeda Subset' from at least five accredited extragalactic data repositories, ensuring data authenticity through rigorous hash code verification. Scrutinize the prowess of three state-of-the-art CNN architectures,namely 'VGG16-Spectral Enhanced', 'ResNet50-Subaru Enhanced', and 'InceptionV3-Omicron Variant', by implementing intricate preprocessing methods like anisotropic Gaussian filtering and chromatic adaptation. Design a sophisticated ranking system to choose the optimal model based on a weighted harmonic mean of F1 score, precision, and recall, while deftly handling the challenge of uneven class distributions and exotic noise patterns through a hybrid ensemble of deep learning and transfer learning techniques. Additionally, propose a multi-step strategy for data augmentation using GAN-generated samples and expert astronomer annotations for improved model robustness.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Develop a specialized algorithm for distinguishing different Andromeda Nebula sub-species, using the GalaxyZoo2-Andromeda Subset fetched from accredited data sources with hash code verification. Evaluate three CNN architectures (VGG16-Spectral Enhanced, ResNet50-Subaru Enhanced, and InceptionV3-Omicron Variant) with advanced preprocessing techniques like anisotropic Gaussian filtering and chromatic adaptation. Create a ranking system for selecting the optimal model based on a weighted harmonic mean of F1 score, precision, and recall, considering class imbalance and exotic noise patterns, and utilize a hybrid ensemble of deep learning and transfer learning. Implement a multi-step strategy for data augmentation with GAN-generated samples and expert astronomer annotations to enhance model robustness.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"GalaxyZoo2-Andromeda Subset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"sub-species\"],\n            \"specification\": {\n                \"data_sources\": [\"accredited repository 1\", \"accredited repository 2\", \"accredited repository 3\", \"accredited repository 4\", \"accredited repository 5\"],\n                \"hash_verification\": true\n            },\n            \"description\": \"A collection of Andromeda Nebula images from various data repositories, requiring rigorous authenticity check.\",\n            \"preprocessing\": [\"anisotropic Gaussian filtering\", \"chromatic adaptation\"],\n            \"augmentation\": [\"GAN-generated samples\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"VGG16-Spectral Enhanced\",\n            \"family\": \"CNN\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"ResNet50-Subaru Enhanced\",\n            \"family\": \"CNN\",\n            \"type\": \"neural networks\"\n        },\n        {\n            \"name\": \"InceptionV3-Omicron Variant\",\n            \"family\": \"CNN\",\n            \"type\": \"neural networks\"\n        }\n    ],\n    \"knowledge\": [\n        \"Exotic noise patterns, class imbalance, and the need for expert astronomer annotations require tailored handling in the algorithm and evaluation process.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"ensemble techniques\", \"transfer learning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"a multi-step strategy for data augmentation and model selection process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction and is tailored to the user's high level of expertise. The problem area is computer vision (specifically image classification), and the provided architectures are CNNs as requested. Performance metrics (F1 score, precision, recall), class imbalance handling, and data augmentation are explicitly mentioned. The specific repositories are not provided since they are to be fetched from direct search, and the detailed design steps like hash verification and ranking system are incorporated. However, the exact values of performance metrics and time complexities are missing, requiring the team to achieve them in the model development phase.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project lead of the pioneering financial sentiment analysis initiative, task the ML team to develop a next-generation Reuters-21578 news classification system that combines BERT and sophisticated convolutional neural networks. Mandate a rigorous 5-step process for sector-specific (aviation, tech, and energy) classification, targeting an impressive 95% accuracy. Incorporate an advanced noise filtration mechanism with sub-second accuracy (99.99% within 5 milliseconds), capable of differentiating genuine real-time news from groundless speculation. Alongside, design a multi-faceted benchmarking strategy to compare performance against leading competitors and conduct quarterly performance reviews, ensuring a minimum improvement rate of 2% per quarter.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Develop a next-generation Reuters-21578 news classification system combining BERT and CNNs, focusing on aviation, tech, and energy sectors. Aim for 95% accuracy, and include a noise filtration mechanism with sub-second response and 99.99% accuracy within 5ms. Implement a benchmarking strategy for quarterly performance review with a minimum 2% improvement rate.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"noise filtration accuracy within 5ms\",\n                \"value\": 0.9999\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 5,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"sector-specific\": [\"aviation\", \"tech\", \"energy\"]\n            },\n            \"description\": \"News dataset for sentiment analysis in financial context, encompassing aviation, tech, and energy sectors.\",\n            \"preprocessing\": [\"BERT embeddings\", \"CNN architecture\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-CNN combination\",\n            \"family\": \"transformer-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"5-step sector-specific classification process\"\n            },\n            \"description\": \"Advanced NLP model using BERT and CNNs for financial sentiment analysis, targeting multi-sector news classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Industry-standard benchmarking against leading competitors is essential.\",\n        \"Quarterly performance reviews and 2% improvement target are performance requirements.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"BERT embeddings\", \"CNN architecture\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"compare with competitor models\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a complex NLP project, the high expertise level, and the specific problem statement about a financial sentiment analysis system. Performance metrics like accuracy, noise filtration, and response time are included. The dataset's details for Reuters-21578 and the model's specifications are derived from the given information. The service requirements and the multi-faceted benchmarking strategy are also included. The confidence score is set to a high value as the response closely reflects the details provided.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "node classification", "instruction": "For an exclusive financial crime investigation platform, design a state-of-the-art Fused Graph Neural Network (FGNN) model that integrates blockchain and Homomorphic Encryption (HE) techniques. The model must not only identify intricate money laundering patterns in highly convoluted international transaction networks but also predict potential future illicit activities with precision. Additionally, optimize the model for zero-knowledge inference, enabling real-time risk assessment without revealing sensitive data to regulatory authorities. Deploy this solution on a highly available, quantum-resistant edge computing infrastructure, ensuring ultra-high security and millisecond-level response times, even with exabyte-scale data processing.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"cybersecurity\",\n        \"description\": \"Design a state-of-the-art Fused Graph Neural Network (FGNN) model for financial crime investigation, integrating blockchain and Homomorphic Encryption (HE) techniques. The model must identify money laundering patterns and predict future illicit activities in international transaction networks with high precision, while ensuring zero-knowledge inference for real-time risk assessment and ultra-high security on a quantum-resistant edge computing infrastructure.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"identification precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"future illicit activity prediction\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"zero-knowledge inference time\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": 1\n                }\n            },\n            {\n                \"name\": \"exabyte-scale data processing speed\",\n                \"value\": {\n                    \"unit\": \"throughput per second\",\n                    \"value\": 1000\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Financial Crime Investigation Data\",\n            \"modality\": [\"graph\", \"tabular\"],\n            \"target_variables\": [\"illicit activities\", \"money laundering patterns\"],\n            \"specification\": {\n                \"size\": {\n                    \"total_transactions\": null,\n                    \"nodes\": null,\n                    \"edges\": null\n                },\n                \"dimensions\": {\n                    \"node_features\": null,\n                    \"edge_features\": null\n                }\n            },\n            \"description\": \"A dataset containing encrypted financial transaction data with both graph and tabular representations for the investigation platform.\",\n            \"preprocessing\": [\"Homomorphic Encryption\", \"Blockchain data extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"FGNN for Financial Crime Detection\",\n            \"family\": \"Fused Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": null,\n                \"encryption_techniques\": [\"blockchain\", \"Homomorphic Encryption\"],\n                \"real_time_inference\": \"enabled\",\n                \"quantum_resistance\": true\n            },\n            \"description\": \"A high-security model capable of investigating complex financial transactions and detecting money laundering patterns using HE and Blockchain.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"data anonymization\",\n            \"transaction pattern analysis\"\n        ],\n        \"target_device\": \"quantum-resistant edge computing infrastructure\",\n        \"deployment_endpoint\": {\n            \"type\": \"private blockchain\",\n            \"access_method\": \"zero-knowledge access\"\n        },\n        \"inference_engine\": [\"HE-optimized inference engine\"],\n        \"demonstration\": \"Real-time risk assessment workflow\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the provided instruction and structured according to the schema. The intent to build a model and the high level of expertise are clear. The user's goal for integrating blockchain, HE, and zero-knowledge inference is captured. However, specific performance metrics (precision) and preprocessing steps (number of transactions, node/edge specifics) were not specified, hence they are set to null. The quantum-resistant edge computing infrastructure and millisecond response times are mentioned but not quantified.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create a highly specialized transformer-based neural machine translation model tailored for English-Russian communication, focusing on translating colloquial slang and idiomatic expressions exclusive to street culture. Optimize for a minimum BLEU score of 38 within a tight constraint of 200 milliseconds per sentence, while guaranteeing a user-centric design that enhances interactivity and maintains user satisfaction in fast-paced, informal chat exchanges.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-cultural communication\",\n        \"description\": \"Create a specialized transformer-based neural machine translation model for English-Russian communication, targeting colloquial slang and idiomatic expressions from street culture. The model must achieve a minimum BLEU score of 38 and have a user-centric design, optimizing for 200 milliseconds per sentence with a focus on interactivity and user satisfaction for fast, informal chat exchanges.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 38\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time per sentence\",\n                \"value\": 0.2,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Colloquial Slang and Idiomatic Expressions (custom)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source_language\": \"English\",\n                \"target_language\": \"Russian\",\n                \"genre\": \"street culture colloquial slang and idioms\"\n            },\n            \"description\": \"A dataset specifically curated for training the model with non-standard English-Russian slang and idiomatic expressions.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-based Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_focused_on\": \"colloquial slang and idiomatic expressions\",\n                \"optimizer\": \"for tight inference time constraint\",\n                \"user-centric_design\": true\n            },\n            \"description\": \"A highly specialized transformer model designed for translating colloquial English to Russian, prioritizing slang and idiom understanding.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transformer-based models are well-suited for handling the complex nature of slang and idioms.\",\n        \"Achieving low inference time requires efficient model optimization and lightweight architecture.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"optimized for fast inference on edge or cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time, interactive chat\"],\n        \"demonstration\": \"Highly interactive and intuitive, supporting fast-paced informal chat exchanges\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the specific requirements provided in the instruction. The problem area is NLP with machine translation, target dataset is specified for colloquial slang and idiomatic expressions, performance metrics include BLEU score, and complexity metrics address the inference time constraint. The expert level and the focus on user satisfaction and interactivity are also captured accurately.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager, you now require the ML team to design a novel attention-based DenseNet model for the PlantVillage Leaf Disease dataset, ensuring not only high-resolution segmentation for separate diseases but also real-time adaptation to newly discovered pathogens. Mandate a user-friendly, interactive interface with intuitive visualizations for non-technical horticulturists to identify patterns, forecast potential outbreaks, and prescribe targeted treatments in an efficient and easily digestible manner.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"agriculture, horticulture\",\n        \"description\": \"Design a novel attention-based DenseNet model for the PlantVillage Leaf Disease dataset, targeting high-resolution disease segmentation and real-time adaptation to new pathogens. The model should have an interactive interface suitable for non-technical horticulturists to identify patterns, forecast outbreaks, and suggest treatments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"speed of adaptation\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PlantVillage Leaf Disease dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_resolution\": null,\n                \"number_of_classes\": null\n            },\n            \"description\": \"A dataset for leaf disease detection and segmentation with high-resolution images, catering to both known and emerging diseases.\",\n            \"preprocessing\": [\n                \"image normalization\",\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\"real-time adaptation for new pathogens\"],\n            \"visualization\": [\n                \"interactive, intuitive for non-technical users\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention-based DenseNet\",\n            \"family\": \"DenseNets\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"disease-specific attention mechanism\"\n            },\n            \"description\": \"A novel deep learning model designed for real-time leaf disease segmentation with an emphasis on adapting to new pathogens and providing a user-friendly interface.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Attention mechanisms improve performance in high-resolution image segmentation.\",\n        \"Horticulturists require an intuitive and non-technical interface.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"interactive and user-friendly design\"\n        ],\n        \"target_device\": \"real-time adaptable for edge or cloud\",\n        \"deployment_endpoint\": {\n            \"target\": \"horticultural decision support platform\"\n        },\n        \"inference_engine\": [\n            \"optimization for fast response times\"\n        ],\n        \"demonstration\": {\n            \"description\": \"Interactive, real-time demonstrations for identifying patterns and disease outbreaks.\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the requirements of a build project for a machine learning model, focusing on the attention-based DenseNet, dataset specifics, user interface, and performance expectations. The missing metric values can be inferred as null due to the lack of numerical targets in the instruction. The model's ability to handle new pathogens and horticulturalists' needs is well represented in the description. However, a specific deployment environment is suggested, but not explicitly detailed.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager is now responsible for leading a state-of-the-art neural machine translation project, utilizing the intricate WMT19 corpus containing a myriad of international news commentaries in Russian and English. The mission goes beyond conventional achievements by striving for a BLEU score of 45 or higher in the specialized field of scientific literature translation. To achieve this, the team must develop an advanced Transformer-XL model, ensuring not only grammatical precision but also capturing colloquial expressions, maintaining factual accuracy, and displaying cultural adaptability. Extensive evaluation will involve a multi-faceted approach, comparing performance with transformer-based competitors, evaluating on expert-submitted technical jargon, and conducting user studies gauging acceptability in the tech community.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"scientific literature\",\n        \"description\": \"The project involves leading a state-of-the-art neural machine translation project using the WMT19 corpus, focusing on Russian to English translation. The objective is to achieve a BLEU score of 45 or higher, with a focus on capturing colloquial expressions, maintaining factual accuracy, and cultural adaptability. Evaluation methods include comparison with transformer-based competitors, expert evaluation on technical jargon, and user studies in the tech community.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 45\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"language\": [\"Russian\", \"English\"],\n                \"domain\": \"scientific literature\",\n                \"size\": \"multilingual\",\n                \"genre\": \"news commentaries\"\n            },\n            \"description\": \"Intricate corpus containing a variety of international news commentaries in Russian and English for advanced neural machine translation.\",\n            \"preprocessing\": [\n                \"Tokenization\",\n                \"Text normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-XL\",\n                \"tasks\": [\"Translation\"],\n                \"features\": [\"Advanced for colloquial expressions\", \"Factually accurate\", \"Culturally adaptable\"]\n            },\n            \"description\": \"A state-of-the-art, transformer-based model with a focus on long-range dependencies and capturing nuances for high-quality scientific literature translation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The project demands a high BLEU score in a niche domain, necessitating a customized Transformer-XL model.\",\n        \"Evaluation will encompass competitive models, technical jargon, and user acceptability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Fine-tuning for scientific literature\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Expert-submitted jargon evaluation and user studies\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent to build a machine translation model, the specified WMT19 dataset, the requirement for a high BLEU score, and the emphasis on specialized metrics and evaluation methods. However, some details such as target device and specific preprocessing techniques are not provided in the instruction, so they remain unspecified.\",\n        \"score\": 0.90\n    }\n}"}
{"task": "text classification", "instruction": "Incorporate the 'Reuters-21578' subset, focusing on niche topic classification in the finance domain. Choose a model with explainable AI techniques, like SHAP or LIME, ensuring interpretability for non-technical stakeholders. Implement a mixed-precision training strategy with mixed-precision schedules for NVIDIA GPUs, optimizing memory usage and energy efficiency under strict latency requirements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Incorporate the 'Reuters-21578' subset focusing on niche topic classification in the finance domain, using a model with explainable AI techniques like SHAP or LIME for interpretability for non-technical stakeholders.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability_score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory_usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"energy_efficiency\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"max\": null,\n                    \"min\": null\n                },\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"subset\": \"niche topic classification\",\n                \"domain\": \"finance\"\n            },\n            \"description\": \"A subset of Reuters-21578 dataset with a focus on finance domain text classification.\",\n            \"preprocessing\": [\n                \"explainable AI techniques\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Model with Explainable AI (SHAP or LIME)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability_methods\": [\"SHAP\", \"LIME\"]\n            },\n            \"description\": \"A neural network model designed for text classification with explainable AI techniques in the finance domain.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The importance of interpretability for non-technical stakeholders in the finance domain.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"mixed-precision training\",\n            \"mixed-precision schedules\"\n        ],\n        \"target_device\": \"NVIDIA GPU\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for memory and energy efficiency\"\n        ],\n        \"demonstration\": \"with strict latency requirements\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a model for the given problem. It interprets the finance domain focus and the niche topic classification. Performance metrics and complexity metrics like memory usage and latency are included as requested, with appropriate descriptions. However, actual performance targets for accuracy, interpretability, and latency are not provided as they are typically determined through experimentation. The missing values in performance metrics and model specifications should be filled in by performing the intended tasks.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "image classification", "instruction": "For the upcoming celestial object classification project, adhere to the IAU's specific taxonomy of celestial entities, such as nebulas, quasars, and dwarf galaxies. Retrieve the 'GalaxyZoo2' dataset from authoritative sources like SDSS, ALFALFA, and Pan-STARRS, ensuring its versioning and metadata integrity. Perform a thorough analysis by fine-tuning not only the traditional VGG16, ResNet50, and InceptionV3, but also explore advanced SOTA models like EfficientNet-B7 and DenseNet-201 with depthwise separable convolutions. Experiment with diverse image preprocessing methods, like progressive resizing, color normalization using standardized z-scales, and employing noise reduction algorithms like BM3D for improved texture preservation. Address the dataset's imbalance using custom techniques like SMOTEBoost and cluster undersampling, and evaluate their impact on model stability and generalization under various data perturbations. Assess the models based on not only peak accuracy but also computational footprint, inference speed, and robustness to noisy and complex real-world astronomical images.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Work on a celestial object classification project adhering to the IAU taxonomy, using the GalaxyZoo2 dataset from authoritative sources. Incorporate VGG16, ResNet50, InceptionV3, EfficientNet-B7, and DenseNet-201, focusing on advanced models with depthwise separable convolutions. Experiment with advanced preprocessing methods such as progressive resizing, color normalization, and noise reduction. Address dataset imbalance with SMOTEBoost and cluster undersampling, while evaluating model performance, computational footprint, inference speed, and robustness to real-world astronomical image variations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational footprint\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inference speed\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"GalaxyZoo2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"Celestial Entity Type\"],\n            \"specification\": {\n                \"source\": [\"SDSS\", \"ALFALFA\", \"Pan-STARRS\"],\n                \"version\": null,\n                \"metadata\": {\n                    \"integrity\": \"preserved\"\n                }\n            },\n            \"description\": \"Celestial object classification dataset sourced from authoritative astronomical databases like SDSS, ALFALFA, and Pan-STARRS, with IAU taxonomy and versioning considerations.\",\n            \"preprocessing\": [\n                \"progressive resizing\",\n                \"color normalization using z-scales\",\n                \"BM3D noise reduction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": [\"VGG16\", \"ResNet50\", \"InceptionV3\", \"EfficientNet-B7\", \"DenseNet-201\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"depthwise_separable\": true\n            },\n            \"description\": \"A list of deep learning image classification models with depthwise separable convolutions\"\n        }\n    ],\n    \"knowledge\": [\n        \"IAU taxonomy for celestial entities\",\n        \"Potential challenges with real-world astronomical image data\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"SMOTEBoost\",\n            \"cluster undersampling\"\n        ],\n        \"target_device\": null,\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": null\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the given instruction. The high expertise level is assumed given the complexity of the project. Performance metrics such as accuracy and computational requirements are included as open-ended because specific numbers were not provided. The source of the dataset is specified as 'direct-search' to emphasize that specific dataset version and metadata are important aspects. The instructions are well-explained, leading to a high confidence score.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "In your capacity as an experienced project manager for a groundbreaking AI classification project, design an advanced system using the 20 Newsgroups dataset. Mandate the integration of multiple state-of-the-art algorithms in a sophisticated ensemble, prioritizing not just peak accuracy but also computational efficiency and energy consumption. Conduct an in-depth analysis of training time dynamics across various cutting-edge hardware platforms, including GPU, TPU, and FPGAs, while employing strategies like distributed parallel processing and continuous learning. Additionally, draft a detailed deployment plan with clear milestones, taking into account real-time adjustments and ensuring compliance with strict, time-sensitive industry regulations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"machine learning research\",\n        \"description\": \"Design an advanced text classification system using the 20 Newsgroups dataset. The system should incorporate multiple state-of-the-art algorithms in an ensemble, focusing on both high accuracy and computational efficiency, while minimizing energy consumption. Perform a comprehensive analysis of training time dynamics across GPU, TPU, and FPGA hardware, using distributed parallel processing and continuous learning. Develop a detailed deployment plan with clear milestones, considering real-time adjustments and industry regulations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": \"maximize\"\n            },\n            {\n                \"name\": \"energy consumption\",\n                \"value\": \"minimize\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time (GPU)\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"training time (TPU)\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"training time (FPGA)\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"number_of_classes\": null,\n                \"document_size\": null,\n                \"vocabulary_size\": null\n            },\n            \"description\": \"A collection of 20 newsgroups for text classification tasks\",\n            \"preprocessing\": [\n                \"stop words removal\",\n                \"lemmatization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ensemble of state-of-the-art\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_components\": [],\n                \"parallelization\": true,\n                \"continuous_learning\": true\n            },\n            \"description\": \"Advanced ensemble system utilizing multiple SOTA algorithms for text classification\"\n        }\n    ],\n    \"knowledge\": [\n        \"Employing ensemble learning can improve performance and handle algorithm diversity\",\n        \"Attention to computational efficiency and energy consumption aids in practical implementation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"distributed parallel processing\"],\n        \"target_device\": [\"GPU\", \"TPU\", \"FPGA\"],\n        \"deployment_endpoint\": \"specific platform(s) to be determined\",\n        \"inference_engine\": [\n            \"GPU-accelerated\",\n            \"TPU-accelerated\",\n            \"FPGA-accelerated\"\n        ],\n        \"demonstration\": \"Real-time, dynamic, and regulatory compliant\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's intent to build an advanced AI classification project, with a focus on text classification using the 20 Newsgroups dataset. The need for state-of-the-art algorithms, ensemble, and hardware performance analysis is accurately captured. However, specifics such as exact performance metrics and hardware platform training times are not provided and assumed to require further analysis or input from the user. The service aspects like deployment milestones and regulatory compliance are clearly outlined.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "Require the AI development team to design a novel, model-agnostic technique for text classification on the DAIGT dataset. In addition to traditional feature extraction, mandate the integration of time-evolving text analysis and explainability into each step. Implement a nested cross-validation strategy and optimize for not only peak accuracy but also computational efficiency, ensuring real-time processing capacity for large-scale data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"text analytics\",\n        \"description\": \"Design a novel, model-agnostic text classification technique for the DAIGT dataset. It should integrate time-evolving text analysis and explainability, employ a nested cross-validation strategy, and optimize for both high accuracy and real-time processing capacity for large-scale data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"classification labels\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensionality\": null,\n                \"length\": null,\n                \"time-varying\": true\n            },\n            \"description\": \"A large-scale text dataset for text classification that requires consideration of time-evolving aspects.\",\n            \"preprocessing\": [\"time-evolving feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"explainability analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"agnostic\",\n            \"specification\": {\n                \"architecture\": \"flexible\",\n                \"optimization_target\": [\"accuracy\", \"computational efficiency\"]\n            },\n            \"description\": \"A model-agnostic design for text classification, incorporating time-evolving analysis and explainability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Time-evolving text analysis is crucial for DAIGT dataset.\",\n        \"Integration of explainability in the model is mandated.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"time-evolving\", \"explainable\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time processing\"],\n        \"demonstration\": \"Methodology for real-time, large-scale text classification with explainability.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the given instruction, outlining the user's intent (building a technique), expertise, and the specific requirements such as integrating time-evolving analysis and explainability, nested cross-validation, and real-time processing. The dataset, model, and additional knowledge sections are detailed to match the task. The missing performance and complexity metrics' values are set to null due to the lack of specific values in the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "As the project director for the elite deep learning development team, your mission is to design a cutting-edge image analysis system that not only deciphers 50 diverse plant species, including their sub-species, from the highly granular ' botanical seedlings dataset 2.0' but also employs efficient transfer learning. This must be achieved within a stringent framework: the model must process images in real-time on a resource-restricted embedded device, maintaining an exceptional accuracy rate of 95%, while guaranteeing a latency of under 50 milliseconds and optimizing for low-power mobile deployments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"botany\",\n        \"description\": \"Design a real-time image analysis system with transfer learning for classifying 50 diverse plant species (including sub-species) from the botanical seedlings dataset 2.0. The system must operate on resource-restricted embedded devices, maintaining an accuracy rate of 95% and a latency of under 50 milliseconds, while optimizing for low-power mobile deployments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 50\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing\",\n                \"value\": true\n            },\n            {\n                \"name\": \"resource constraints\",\n                \"value\": \"embedded devices\",\n                \"unit\": \"device type\"\n            },\n            {\n                \"name\": \"low-power\",\n                \"value\": true,\n                \"unit\": \"optimization\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"botanical seedlings dataset 2.0\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"plant species\"],\n            \"specification\": {\n                \"image_size\": \"variable (optimized for embedded deployment)\",\n                \"data_format\": \"RGB\"\n            },\n            \"description\": \"Highly granular dataset of plant seedlings for image classification with 50 diverse species and sub-species.\",\n            \"preprocessing\": [\n                \"image resizing and normalization\"\n            ],\n            \"augmentation\": [\n                \"lighting and rotation variations for transfer learning\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"cutting-edge deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"optimized for resource efficiency and transfer learning\",\n                \"parameters\": \"trained on a subset of larger dataset for efficient fine-tuning\",\n                \"layers\": \"Lightweight and efficient\"\n            },\n            \"description\": \"An image classification model designed for resource-constrained devices, capable of real-time operation and achieving high accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning can be effective with limited data and resource constraints.\",\n        \"Employing lightweight deep learning models is crucial for real-time and low-power performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"embedded device\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for mobile or embedded environments\"\n        ],\n        \"demonstration\": \"comparison with non-efficient models showcasing real-time performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is accurately constructed based on the given instruction, detailing the user's intent, problem domain, requirements for the dataset and model, and service expectations. The high-level details of the performance metrics and complexity constraints are specified. However, specific details like the model architecture and preprocessing steps can be further explored or adjusted based on available transfer learning techniques.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "The project manager of a premier machine learning team is tasked with a multifaceted deep learning endeavor. They must elevate the ImageClassifier-SLD on Stanford Cars Dataset v2.0, incorporating a state-of-the-art Inception-v4+ architecture with attention mechanisms. The model should exhibit exceptional resilience to challenging conditions, such as partial occlusions, nuanced color shifts, and dynamic lighting variations. To achieve this, the team must optimize for a strict benchmark of at least 97% mean Average Precision (mAP), while maintaining per-class precision and recall above 95% to ensure real-world applicability. Additionally, the manager is required to design an extensive augmentation strategy, document performance trade-offs, and conduct an in-depth analysis comparing their model with the latest top competitors, including ROC curves and PR AUC scores, for a comprehensive presentation to stakeholders.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"automotive or industrial design\",\n        \"description\": \"The project involves enhancing the ImageClassifier-SLD on the Stanford Cars Dataset v2.0 by adopting a state-of-the-art Inception-v4+ architecture with attention mechanisms. The model must be robust to partial occlusions, nuanced color shifts, and dynamic lighting variations. The performance target is at least 97% mean Average Precision (mAP), and per-class precision and recall must be above 95%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean Average Precision\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"per-class precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"per-class recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Dataset v2.0\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"Image Label\"],\n            \"specification\": {\n                \"size\": {\n                    \"image\": {\n                        \"width\": [224, 224],\n                        \"height\": [224, 224]\n                    }\n                },\n                \"categories\": \"Stanford Cars dataset categories\"\n            },\n            \"description\": \"Stanford Cars Dataset with images of vehicles for improved ImageClassifier using Inception-v4+ and attention\",\n            \"preprocessing\": [\n                \"Inception-v4+ input normalization\",\n                \"Data augmentation for challenging conditions\"\n            ],\n            \"augmentation\": [\n                \"partial occlusions\",\n                \"color shifts\",\n                \"dynamic lighting variations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ImageClassifier-SLD (Inception-v4+ with Attention)\",\n            \"family\": \"state-of-the-art deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Inception-v4+ with attention mechanisms\",\n                \"resilience\": \"partial occlusions, color shifts, dynamic lighting variations\"\n            },\n            \"description\": \"Advanced model for image classification on the Stanford Cars Dataset with specific performance and robustness targets.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Performance trade-offs may arise between mAP, precision, and recall.\",\n        \"Extensive augmentation strategy is key to resilience.\",\n        \"Comparison with top competitors includes ROC curves and PR AUC scores.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"attention-based feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow, PyTorch or Caffe\"],\n        \"demonstration\": \"Presentation with ROC curves and PR AUC scores for stakeholders\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the main requirements from the instruction, such as the project's scope, the dataset, and the model requirements. The expert level of the user is reflected in the specific demands. However, some details like exact number of categories, exact model specifications, or the inference engine are not specified in the instruction, so they are left for further clarification. The confidence score is high, considering the clear understanding of the task.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a highly specialized Graph Wavelet Neural Network (GWNN), tailored for discriminating an extensive taxonomy of micron-level neuronal subtypes within a high-resolution Mouse Brain Atlas, utilizing MindSpore SDK. Emphasize on improving F1-score for both extremely scarce and abundant classes, while ensuring the model can discern intricate hierarchies in an ultra-dense network, all while maintaining efficiency on a parallel GPU infrastructure with stringent constraints on computational resource utilization for big-data analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Develop a highly specialized Graph Wavelet Neural Network (GWNN) for discriminating diverse micron-level neuronal subtypes in a high-resolution Mouse Brain Atlas using MindSpore SDK. Focus on improving F1-score for both scarce and abundant classes, ensuring the model handles hierarchical structures in an ultra-dense network, while maintaining efficiency on a parallel GPU infrastructure with strict computational resource constraints for big-data analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"GPU utilization\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Atlas\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"neuronal subtypes\"],\n            \"specification\": {\n                \"resolution\": {\n                    \"type\": \"integer\",\n                    \"description\": \"High resolution\"\n                },\n                \"node_density\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Ultra-dense network\"\n                }\n            },\n            \"description\": \"A high-resolution dataset depicting neuronal subtypes in a Mouse Brain Atlas.\",\n            \"preprocessing\": [\n                \"Graph normalization\",\n                \"Feature extraction\"\n            ],\n            \"augmentation\": [\n                \"Graph augmentations to handle class imbalance\"\n            ],\n            \"visualization\": [\n                \"Graph-level visualizations\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Wavelet Neural Network (GWNN)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"GWNN\",\n                \"target_classes\": \"extensive taxonomy of neuronal subtypes\",\n                \"improvement\": \"enhanced for F1-score\"\n            },\n            \"description\": \"A custom-built Graph Wavelet Neural Network optimized for neuronal subtype classification in the Mouse Brain Atlas dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"MindSpore SDK will provide parallel processing capabilities.\",\n        \"Enhancing F1-score for scarce and abundant classes is crucial.\",\n        \"Accounting for hierarchical structures in neuronal subtypes.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Neuronal feature engineering\"\n        ],\n        \"target_device\": \"parallel GPU\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"MindSpore\"\n        ],\n        \"demonstration\": \"Ultra-dense network and hierarchical differentiation demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is derived from the provided instruction, with a high level of expertise assumed. The user's intent to build a specialized model is captured, along with the focus on graph wavelet neural networks, MindSpore SDK, and resource constraints. The performance metric is set as F1-score, and potential complexity metrics are included. However, the expected F1-score value is not provided, and additional details such as data preprocessing and model architecture would require more specific information.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager in a groundbreaking AI research initiative, you're tasked with overseeing a multidisciplinary project that combines neuroevolutionary techniques and multi-modal transfer learning for ultra-precise real-time predictive maintenance in the aerospace industry. The newly synthesized 'AeroTech' dataset, containing flight telemetry, material degradation patterns, and molecular stress indicators, has been stratified into four distinct segments: historical (60%), cross-domain adaptation (10%), fine-tuning (15%), and cutting-edge performance validation (15%) to test the model's adaptability across different aircraft models and environmental conditions. The input sequences now consist of high-dimensional sensory data (INPUT_SEQ_LEN=128, INPUT_CHANNELS=500) representing various engine states, sensor readings, and structural health. The aim is to develop an advanced model that not only forecasts the probability of component failure within the next 128 time steps (PRED_SEQ_LEN=128, PRED_CLASSES=10) with utmost accuracy, but also calculates conditional probability of failures based on unseen combinations of variables. Ensure the model demonstrates exceptional AUC-ROC score, interpretable decision-making through counterfactual explanations, and resilience against adversarial attacks on streaming data. This project should pave the way for Explainable Artificial Intelligence in critical industry operations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"aerospace industry\",\n        \"description\": \"Combine neuroevolutionary techniques and multi-modal transfer learning for ultra-precise real-time predictive maintenance in aerospace, using the AeroTech dataset. The model should handle high-dimensional sensory data, forecast failure probabilities within a given time window, calculate conditional probabilities, and be interpretable with counterfactual explanations. It must also be resilient to adversarial attacks on streaming data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AeroTech\",\n            \"modality\": [\"time series\", \"image\", \"tabular\"],\n            \"target_variables\": [\"failure probability\", \"conditional probabilities\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 128,\n                \"INPUT_CHANNELS\": 500,\n                \"PRED_SEQ_LEN\": 128,\n                \"PRED_CLASSES\": 10\n            },\n            \"description\": \"A synthesized dataset for predictive maintenance, combining flight telemetry, material degradation patterns, and molecular stress indicators, stratified for different training stages.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Neuroevolutionary Transfer Learning Model\",\n            \"family\": \"multi-modal neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"explainability\": \"yes\",\n                    \"counterfactual_explanations\": \"yes\"\n                },\n                \"adversarial_resilience\": \"yes\"\n            },\n            \"description\": \"A model combining neuroevolution and multi-modal transfer learning, designed for real-time predictive maintenance with interpretable decisions and adversarial attack resistance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Ultra-precise predictive maintenance is crucial in high-risk industries like aerospace.\",\n        \"Model interpretability and counterfactual explanations are requirements for critical decision-making in the face of potential failures.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"dimensionality reduction\", \"sequence alignment\"],\n        \"target_device\": \"edge devices with real-time computation\",\n        \"deployment_endpoint\": \" Industry-grade edge computing infrastructure\",\n        \"inference_engine\": [\"on-device\", \"streaming\"],\n        \"demonstration\": \"Real-time streaming data prediction with failure probability and explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's intent (high expertise) and the problem's description and requirements, such as neuroevolution, multi-modal learning, and specific metrics. Data modality and dataset stratification are derived from the details provided. The model, performance, and service requirements are based on the given scenario. However, there are some missing performance metric targets (accuracy and AUC-ROC), which the user might specify during the project.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "machine translation", "instruction": "As a seasoned project manager for the AI development team, your mandate is to design an avant-garde neural machine translation system that expertly translates rare and indigenous languages, like Swahili to English, leveraging Masakhane's vast, under-explored corpus. Demanding a fusion of sophisticated Transformer-XL and dual-path RNN architectures, optimize not only for translation accuracy but also for efficiency and context preservation. Your challenge is to achieve a groundbreaking 25% increase in BLEU scores over the highest benchmark, while meticulously conducting a fine-grained analysis comparing performance, computational efficiency, and cultural nuances with at least three recently published, highly competitive models, ensuring both precision and cultural sensitivity in every translation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language technology, international communication\",\n        \"description\": \"Design an avant-garde neural machine translation system targeting rare and indigenous languages, like Swahili to English, using Masakhane's under-explored corpus. Incorporate Transformer-XL and dual-path RNN architectures, optimizing for translation accuracy, efficiency, and context preservation. Aim for a 25% increase in BLEU scores over the top benchmark and conduct a thorough comparison with three competitive models on performance, computational efficiency, and cultural nuances.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cultural sensitivity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Masakhane Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"languages\": [\"Swahili\", \"English\"],\n                \"rareness\": \"high\"\n            },\n            \"description\": \"Under-explored corpus containing rare and indigenous languages\",\n            \"preprocessing\": [\n                \"Corpus preparation for machine translation\",\n                \"Data cleaning and normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL with Dual-Path RNN\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural machine translation\",\n            \"specification\": {\n                \"architecture\": [\"Transformer-XL\", \"dual-path RNN\"],\n                \"optimization\": {\n                    \"accuracy_target\": \"25% increase over benchmark\",\n                    \"context_preservation\": true\n                }\n            },\n            \"description\": \"Avant-garde model with focus on rare language translation and efficiency\"\n        }\n    ],\n    \"knowledge\": [\n        \"Swahili to English translation challenge in the language technology domain\",\n        \"Importance of precision and cultural sensitivity in machine translation of indigenous languages\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Fine-tuning for rare language adaptation\",\n            \"Transformer-XL specific architecture tuning\"\n        ],\n        \"target_device\": \"cloud or high-performance computing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for real-time translations\"],\n        \"demonstration\": \"Performance comparison and cultural impact analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is derived from the instruction, capturing the requirement for a machine translation system, the specific architecture, and the performance goals. The user's intent to build a model, high expertise, and the problem area are inferred from the task. The performance metrics and efficiency indicators are directly mentioned or inferred from the benchmark and efficiency concerns. However, exact values for BLEU score, precision, and memory requirements are not provided, as they depend on model development. Similarly, the target deployment environment and demonstration details are not fully specified, needing further clarification from the user.\",\n        \"score\": 0.92\n    }\n}"}
{"task": "link prediction", "instruction": "Develop a state-of-the-art, deep-learning-enhanced PIN (Protein Interaction Network) model that integrates multi-scale topological features and gene expression data for the highly dynamic yeast protein network. The model should not only predict unseen interactions but also rank them by confidence, reflecting functional relevance, and interpret network dynamics to facilitate novel biological discovery.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Develop a state-of-the-art deep-learning-enhanced Protein Interaction Network (PIN) model that integrates multi-scale topological features and gene expression data for yeast protein network. The model should predict unseen interactions, rank them by confidence, and facilitate novel biological discoveries by interpreting network dynamics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Yeast Protein Interaction Network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"topological_features\": \"multi-scale\",\n                \"gene_expression_data\": \"inclusive\"\n            },\n            \"description\": \"A dynamic yeast protein interaction network dataset combining topological features and gene expression data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep-learning-enhanced PIN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"integrating multi-scale topological features and gene expression\",\n                \"interpretability\": \"required\"\n            },\n            \"description\": \"A deep-learning-based model designed for protein interaction networks, with emphasis on state-of-the-art methods, confidence ranking, and biological interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model must handle the complexity of yeast protein network dynamics by integrating multi-scale features.\",\n        \"Gene expression data adds an additional layer of information for prediction and ranking.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"requirements for interpreting network dynamics and showcasing predicted interactions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intent to build a high-expertise model, the application domain (bioinformatics), and the specific requirements of a state-of-the-art PIN model with multi-scale topological and gene expression data, ranking, and biological interpretability. However, detailed performance metrics, like specific values or ranges, are not provided in the instruction. The model's architecture, specific network architecture, and detailed feature integration are also not mentioned.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "image classification", "instruction": "As a seasoned project manager, you're tasked with overseeing the implementation of an advanced, deep-learning image analysis system that goes beyond basic object recognition. The focus is on the sophisticated Galaxy Zoo dataset, which demands exceptional performance in differentiating not just between celestial bodies but also subcategories and rare phenomena. The project mandates the utilization of the state-of-the-art Vision Transformer (ViT), enhanced with attention mechanisms, to capture intricate patterns and long-range dependencies in variable astronomical imagery. It's crucial that the model surpasses benchmarks for precision, generalization, and demonstrates robustness to noisy and dynamically changing backgrounds, all while facilitating groundbreaking discoveries in the field of astrophysics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Implement an advanced deep-learning image analysis system using a state-of-the-art Vision Transformer (ViT) with attention mechanisms for the Galaxy Zoo dataset. The system should excel at differentiating celestial objects, subcategories, and rare phenomena, while achieving high precision, generalization, and robustness to noisy and dynamic backgrounds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"generalization\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"frames per second\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GPU memory\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Galaxy Zoo\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"images\": {\n                        \"train\": null,\n                        \"validation\": null,\n                        \"test\": null\n                    },\n                    \"categories\": null\n                },\n                \"types\": {\n                    \"celestial objects\": {\n                        \"subcategories\": null,\n                        \"rare_phenomena\": null\n                    }\n                }\n            },\n            \"description\": \"A sophisticated dataset for image analysis in astronomy, requiring differentiation between celestial bodies and subcategories, and managing noisy and dynamic backgrounds.\",\n            \"preprocessing\": [\"data augmentation\"],\n            \"augmentation\": [\"image transformations for variable astronomical imagery\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced ViT with Attention\",\n            \"family\": \"Vision Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A state-of-the-art deep learning model using Vision Transformer (ViT) with enhanced attention mechanisms for precise and robust analysis of the Galaxy Zoo dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilizing the latest Vision Transformer with attention for complex astronomical imagery\",\n        \"Model performance is expected to surpass benchmarks in precision and generalization\",\n        \"Robustness to noisy backgrounds and dynamic transformations is crucial\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"extracting domain-specific features from astronomical images\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"demonstration of model performance through interactive visualizations and interpretation of findings\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the given instruction, capturing the need for an advanced image analysis system using a state-of-the-art ViT with attention for the Galaxy Zoo dataset. Performance metrics, dataset specifications, and model requirements are adapted from the instruction. However, precise target values for precision and generalization, as well as inference speed and memory usage, are not provided as they would typically be determined during the development process.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning enthusiast, you've decided to delve into a highly specialized domain: classifying not just plant species but also their genetic variants and micro-ecological adaptations from high-resolution, time-lapse images. You've discovered a unique dataset called 'Phenotype Insights through Time-lapse Vegetation Analysis (PHI-VA)' containing images of plants under varying environmental conditions. Your challenge is to design an advanced deep learning architecture, incorporating temporal features and transfer learning, that can not only identify species but also detect subtle changes in gene expression and response to stress. \r\n\r\nWhat specific deep learning model would you recommend, considering the need for real-time monitoring and the requirement to achieve near-human-level accuracy while maintaining computational efficiency for remote monitoring systems? In addition to accuracy benchmarks, discuss the potential for this model to differentiate between closely related species under different growing conditions and the trade-offs in speed and resource usage for real-world deployment in precision agriculture settings.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"precision agriculture\",\n        \"description\": \"Design an advanced deep learning architecture for classifying plant species, genetic variants, and micro-ecological adaptations from high-resolution, time-lapse images. The dataset is PHI-VA, includes plants under varying conditions, and requires real-time monitoring, near-human accuracy, and computational efficiency for remote monitoring systems.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"near-human accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time monitoring\",\n                \"value\": null,\n                \"unit\": \"time (ms)\"\n            },\n            {\n                \"name\": \"resource usage\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Phenotype Insights through Time-lapse Vegetation Analysis (PHI-VA)\",\n            \"modality\": [\"image\", \"time series\"],\n            \"target_variables\": [\"species\", \"genetic variants\", \"micro-ecological adaptations\"],\n            \"specification\": {\n                \"image_resolution\": {\n                    \"width\": null,\n                    \"height\": null,\n                    \"time_steps\": null\n                },\n                \"environmental_conditions\": \"varied\",\n                \"dataset_split\": [\"train\", \"validation\", \"test\"]\n            },\n            \"description\": \"Dataset for plant classification, genetic variation, and micro-ecological adaptation from time-lapse images under different growing conditions.\",\n            \"preprocessing\": [\"temporal feature extraction\", \"image normalization\"],\n            \"augmentation\": [\"temporal augmentation\", \"image augmentation\"],\n            \"visualization\": [\"model interpretability maps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Temporal Convolutional Network (TCN) with Transfer Learning\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"model_depth\": null,\n                \"transfer_learning_base_model\": \"efficientnet-b0 or resnet-50\",\n                \"feature_extraction_layers\": \"fully connected layers\",\n                \"time_series_processing\": \"LSTM or GRU\"\n            },\n            \"description\": \"A deep architecture combining temporal convolution and transfer learning for plant classification and variations, while maintaining real-time monitoring and resource efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Temporal features and transfer learning are necessary for complex plant classification tasks\",\n        \"Difference in growing conditions may affect model's ability to differentiate closely related species\",\n        \"Careful selection of base model and lightweight architectures can balance accuracy and computational efficiency\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"image resizing\",\n            \"feature extraction from time-lapse sequences\"\n        ],\n        \"target_device\": \"remote monitoring systems (embedded or cloud-based)\",\n        \"deployment_endpoint\": \"precision agriculture platform\",\n        \"inference_engine\": [\n            \"TensorRT for real-time optimization\",\n            \"GPU-powered for efficient computation\"\n        ],\n        \"demonstration\": \"Explanation of model's predictions, attention maps, and performance under different environmental scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent for building a deep learning model, their high level of expertise, and the problem domain of computer vision for plant classification. Performance metrics and complexity trade-offs are incorporated, and the dataset, model type, and specific features are derived from the instruction details. The model recommendation, transfer learning strategy, and the potential trade-offs are based on the requirements of the problem.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "In a highly specialized project for interventional neuroradiology, we aim to develop a fine-tuned V-Net++ architecture for the BRATS 2022 dataset. The model must deliver exceptional performance with a Dice similarity coefficient surpassing 0.9 for GBM and AVM lesions, while integrating Grad-CAM interpretability layers to elucidate subtle gliomagenic features, adhering to strict FDA-clearance requirements for trust in life-critical decisions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Develop a fine-tuned V-Net++ architecture for the BRATS 2022 dataset in the context of interventional neuroradiology. The model should achieve a Dice similarity coefficient greater than 0.9 for GBM and AVM lesions, and include Grad-CAM interpretability layers to identify gliomagenic features, meeting strict FDA-clearance requirements for medical decision-making.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Dice similarity coefficient (GBM)\",\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"Dice similarity coefficient (AVM)\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BRATS 2022\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_dimensions\": {\n                    \"width\": \"variable\",\n                    \"height\": \"variable\",\n                    \"depth\": \"variable\"\n                },\n                \"label_dimensions\": {\n                    \"width\": \"variable\",\n                    \"height\": \"variable\",\n                    \"depth\": 1\n                }\n            },\n            \"description\": \"A medical imaging dataset for brain tumor segmentation, particularly focusing on GBM and AVM with FDA-clearance requirements.\",\n            \"preprocessing\": [\" skull-stripping\", \"normalization\"],\n            \"augmentation\": [\"geometric transformations\", \"brightness adjustments\"],\n            \"visualization\": [\"slice visualization\", \"confusion matrices\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"V-Net++\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"V-Net with enhanced layers\",\n                \"fine-tuning\": true,\n                \"interpretability_layers\": \"Grad-CAM\"\n            },\n            \"description\": \"A state-of-the-art deep learning model specifically designed for brain tumor segmentation, targeting GBM and AVM lesions with interpretability for critical decisions in neuroradiology.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Importance of interpretability for life-critical applications\",\n        \"FDA-clearance implications on model development and validation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"DICOM-compliant medical imaging systems\",\n        \"deployment_endpoint\": \"cloud-based medical imaging solution\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"Interactive segmentations and model interpretability visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a model (V-Net++ for BRATS 2022), their expertise level, and their specific requirements for performance, interpretability, and FDA-clearance. Performance metrics are specified for GBM and AVM. The dataset, its characteristics, and the model's architectural details are consistent with the given instruction. However, note that some aspects of preprocessing and augmentation are assumptions based on common practices.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, outline a comprehensive strategy for developing a state-of-the-art translation system utilizing the ParaCrawl dataset. The system must employ a sophisticated dual encoder architecture, with a focus on preserving both semantic meaning and nuanced emotional nuances in English to Dutch translations, while ensuring competitive performance and addressing domain-specific challenges.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language technology\",\n        \"description\": \"Outline a comprehensive strategy for developing a state-of-the-art translation system using the ParaCrawl dataset. Focus should be on a dual encoder architecture, preserving semantic meaning and emotional nuances between English and Dutch, and ensuring competitive performance with domain-specific challenge handling.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Semantic Similarity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Emotional Consistency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Training time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"Inference speed\",\n                \"value\": null,\n                \"unit\": \"translated sentences per second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"English to Dutch translation\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"valid\": null,\n                    \"test\": null\n                },\n                \"language_pair\": [\"English\", \"Dutch\"]\n            },\n            \"description\": \"A large-scale dataset for multilingual translation, containing diverse and noisy data to challenge the system\",\n            \"preprocessing\": [\n                \"Data cleaning\",\n                \"Tokenization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dual Encoder for Translation\",\n            \"family\": \"neural networks\",\n            \"type\": \"Transformer-based architecture\",\n            \"specification\": {\n                \"architecture\": \"dual encoder\",\n                \"embedding_size\": null,\n                \"attention_heads\": null,\n                \"encoder_layers\": null\n            },\n            \"description\": \"Advanced NLP model with dual encoders for simultaneously processing input and target languages preserving meaning and emotions\"\n        }\n    ],\n    \"knowledge\": [\n        \"ParaCrawl's noisy nature requires robust data handling and preprocessing techniques\",\n        \"Sophisticated architecture should address nuanced emotional nuances in translations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Domain-specific embeddings\",\n            \"Contextualized embeddings\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"online translation platform\",\n        \"inference_engine\": [\"GPU acceleration\", \"batch processing\"],\n        \"demonstration\": \"Interactive demos for system performance evaluation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's intent to build a high-expertise project, specifies the NLP area for translation, and includes performance and complexity metrics for the system. The ParaCrawl dataset is accurately represented with its focus on translating between English and Dutch, while emphasizing on semantic and emotional preservation. However, specific metric targets, model architecture details, and preprocessing steps are not explicitly given and may need further clarification or assumptions based on typical ML practices.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "You're working on a multi-faceted data science challenge involving the Concrete Strength Dataset. The task involves feature engineering, data preprocessing, and model selection. Utilize advanced regression techniques, handle potential outliers, and perform hyperparameter tuning. Your objective is to achieve high accuracy, considering not only RMSE but also correlation coefficients, and present a comprehensive analysis of the model's performance across train, validation, and test sets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"civil engineering\",\n        \"description\": \"Work on a multi-faceted data science challenge using the Concrete Strength Dataset, which includes feature engineering, preprocessing, and model selection. Employ advanced regression techniques, handle outliers, and perform hyperparameter tuning. The objective is to achieve high accuracy with consideration of RMSE and correlation coefficients, and provide a comprehensive analysis across train, validation, and test sets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"correlation coefficients\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Concrete Strength Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"concrete_strength\"],\n            \"specification\": null,\n            \"description\": \"A dataset for predicting concrete strength with emphasis on feature engineering and model performance.\",\n            \"preprocessing\": [\n                \"Handling outliers\",\n                \"Data normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Correlation matrix analysis\",\n                \"Feature importance plot\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Regression Techniques\",\n            \"type\": \"neural networks or ensemble models\",\n            \"specification\": {\n                \"hyperparameters\": {}\n            },\n            \"description\": \"A model using advanced regression techniques optimized for the Concrete Strength Dataset, including potential consideration of neural networks or ensemble models.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Potential outliers should be detected and handled to improve the model's robustness.\",\n        \"Comprehensive analysis of RMSE and correlation coefficients is required for multi-faceted evaluation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Outlier detection\",\n            \"Feature selection\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Model performance and analysis across different sets (train, validation, test) with visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the key aspects of the user's instruction, including the task at hand, dataset, and performance metrics. The user's high expertise level is assumed, and the need for a comprehensive analysis is acknowledged. However, specific model architectures or hyperparameters, pre-processing steps beyond the basics, and performance target values are not provided, as the instruction leaves room for flexibility. The confidence score is high given the clarity of the provided information.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For an advanced coral reef and delicate marine ecosystem analysis in the Maldives dataset using a state-of-the-art DeepLabV3+ model, design an adaptation that specifically accounts for dynamic turbidity gradients and underwater color distortion. The proposed solution should surpass current AUROC benchmarks by at least 15%, showcasing superior performance under low light conditions and preserving fine-grained details of the biodiversity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"marine biology\",\n        \"description\": \"Design an adapted DeepLabV3+ model for advanced coral reef and marine ecosystem analysis in the Maldives dataset. The adaptation should account for dynamic turbidity gradients and underwater color distortion. Aim to surpass current AUROC benchmarks by at least 15% and demonstrate superior performance under low light conditions, while preserving fine-grained details of biodiversity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUROC\",\n                \"value\": 1.15\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Maldives\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dynamic_turbidity\": true,\n                \"underwater_color_distortion\": true\n            },\n            \"description\": \"A dataset capturing coral reefs and marine ecosystems in the Maldives, with specific challenges due to dynamic turbidity and underwater color variations.\",\n            \"preprocessing\": [\"account for dynamic turbidity and color distortion\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepLabV3+\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptation\": \"for turbidity gradients and color distortion\",\n                \"low_light_performance\": \"optimized\"\n            },\n            \"description\": \"An advanced image segmentation model based on DeepLabV3+, modified for coral reef analysis in Maldives, targeting improved performance in low light conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Turbidity gradient adaptation is essential for reliable performance in underwater environments.\",\n        \"Underwater color distortion is a key challenge in recognizing fine-grained details.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"account for low light adaptation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"showcasing performance with low light and fine-grained biodiversity preservation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction indicates a clear request for developing a high-performing adaptation of the DeepLabV3+ model. The user's intent, expertise, and problem domain are accurately captured in the JSON response. Specific requirements, such as performance metrics, adaptation details, and dataset properties, are included. However, without further information on the precise feature engineering steps or the current AUROC benchmarks, some details may be incomplete. Overall, the response reflects the primary aspects of the task.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager needs a tailored neural machine translation system, focusing on underrepresented African languages like Kikuyu, integrating the Masakhane corpus. Explore a novel fusion of Transformer-XL and LSTM architectures with attention-guided transfer learning. Aim to elevate the state-of-the-art in Swahili-English translation by a minimum of 7% absolute BLEU, ensuring peak performance across diverse domains and dialects.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual translation\",\n        \"description\": \"The project requires a tailored neural machine translation system, focusing on underrepresented African languages like Kikuyu. The system should integrate the Masakhane corpus and employ a novel fusion of Transformer-XL and LSTM architectures with attention-guided transfer learning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 0.07\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Masakhane Corpus (specifically for Kikuyu and Swahili)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"source_language\": \"Kikuyu\",\n                \"target_language\": \"Swahili\",\n                \"corpus_size\": \"adequate for Transformer-XL and LSTM training\",\n                \"domain_diversity\": \"diverse domains and dialects\"\n            },\n            \"description\": \"A corpus containing Kikuyu and Swahili text data for training the neural machine translation system.\",\n            \"preprocessing\": [\"Tokenization, normalization, corpus integration with Masakhane\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Neural Machine Translation System\",\n            \"family\": \"Transformer-XL with LSTM fusion\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"Attention-guided transfer learning for underrepresented languages\"\n            },\n            \"description\": \"A state-of-the-art machine translation system optimized for African languages, particularly Kikuyu, using a fusion of Transformer-XL and LSTM with Masakhane corpus integration.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Fusion of Transformer-XL and LSTM models can enhance performance for under-resourced languages like Kikuyu.\",\n        \"Attention-guided transfer learning helps with language adaptation and elevates translation quality.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Domain adaptation, fine-tuning for Swahili-English\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for diverse domains and dialects\"],\n        \"demonstration\": \"Performance demonstrated across multiple domains and dialects\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the project requirements, including the target language (Kikuyu), the dataset (Masakhane corpus), the novel model architecture, and the performance target (7% improvement in BLEU). The user's intent to build a model, high expertise level, and application domain are accurately reflected. The dataset preprocessing steps and model specifications are based on the given information. However, a specific value for BLEU improvement was not provided, so it is left as a 0.07 target.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning project manager, your team needs to develop an image classifier for celestial objects, focusing on galaxies and stars. The classifier should utilize a lesser-known, challenging dataset obtained from a non-NASA source, ensuring at least 95% precision. Additionally, explore transfer learning with convolutional neural networks and compare its performance to a state-of-the-art transformer-based model.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Develop an image classifier for celestial objects, specifically targeting galaxies and stars, using a lesser-known and challenging dataset not from NASA. Aim for at least 95% precision. Investigate transfer learning with convolutional neural networks and compare it to a state-of-the-art transformer-based model.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Non-NASA Celestial Object Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"galaxy\", \"star\"],\n            \"specification\": {\n                \"source\": \"non-public, challenging\",\n                \"size\": \"\",\n                \"dimensions\": \"\",\n                \"length\": \"\"\n            },\n            \"description\": \"A lesser-known, complex dataset for celestial object recognition.\",\n            \"preprocessing\": [\"Data cleaning, augmentation (if necessary)\"],\n            \"augmentation\": [\"Specific techniques for domain adaptation\"],\n            \"visualization\": [\"Sample images and class distribution\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Convolutional Neural Network (Transfer Learning)\",\n            \"family\": \"ConvNets\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"transfer learning with a CNN\",\n                \"base_model\": \"\",\n                \"performance\": \"optimized for precision\"\n            },\n            \"description\": \"A transfer learning approach using a CNN for the celestial object classification task.\"\n        },\n        {\n            \"name\": \"Transformer-Based Model (State-of-the-art)\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art in image classification\",\n                \"model_name\": \"\",\n                \"performance\": \"\"\n            },\n            \"description\": \"A transformer-based model, designed to compete with the CNN for high precision in celestial object classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Challenging dataset enhances model generalization capabilities.\",\n        \"Transfer learning improves performance with limited data in a specific domain.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Fine-tuning for target classes\"],\n        \"target_device\": \"cloud-based or high-performance computing environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow or PyTorch\"],\n        \"demonstration\": \"Performance comparison of both models\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's instruction by specifying the intent to build, problem area, target metrics, and the particular focus on transfer learning and transformer-based models. Expertise level is assumed high due to the complexity of the task. The dataset and models are described based on the information provided, with additional considerations for non-NASA and high precision requirements. The service section covers expected post-model steps, while the confidence score takes into account the clarity of the instruction and mapping to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for a cutting-edge machine learning team, you are tasked with developing a highly sophisticated time-series forecasting model for the ETTm2 dataset. This real-world dataset is divided into intricate train, validation, and test subsets, each containing unique temporal patterns. The input sequences consist of historical data with a specific length (INPUT_SEQ_LEN=96, INPUT_DIM=7) requiring intricate feature extraction. The challenge lies in designing an algorithm that forecasts future sequences (PRED_SEQ_LEN=96, PRED_DIM=7) not only accurately but also handles seasonality and anomalies effectively. Your team must optimize for both mean squared error (MSE) and mean absolute error (MAE) to demonstrate superior performance in a competitive benchmark. Emphasize the importance of model explainability and efficiency in the face of increasing data complexity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"data science/research\",\n        \"description\": \"Develop a highly sophisticated time-series forecasting model for the ETTm2 dataset, which is split into train, validation, and test subsets with unique temporal patterns. The model should handle input sequences of length 96 and dimension 7, and forecast future sequences of the same length and dimension. Key objectives include accuracy (MSE and MAE optimization), handling seasonality and anomalies, and ensuring model explainability and efficiency given the increasing data complexity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error (MSE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error (MAE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory usage\",\n                \"unit\": \"GB\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"seconds\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 7\n            },\n            \"description\": \"A real-world time-series dataset split into intricate train, validation, and test subsets with diverse patterns, and historical data inputs requiring advanced feature extraction.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art time-series forecasting\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"AutoML for time-series\"\n            },\n            \"description\": \"A highly customized, accurate, and efficient model specifically designed for time-series forecasting, with a focus on seasonality, anomalies, interpretability, and handling complex patterns in the ETTm2 dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The importance of seasonality and anomaly detection techniques to achieve high accuracy.\",\n        \"The need for efficient algorithms and explainability in complex time-series forecasting\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"advanced feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"Visualizations of prediction accuracy, seasonality analysis, and model explainability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response captures the essence of the given task, including the user's high expertise level, the time-series analysis problem area, and specific requirements like handling seasonality, anomalies, and performance metrics. However, specific metric values are not provided due to the need for model optimization, and additional model-specific details like feature extraction techniques have been inferred. The confidence score is high considering the clarity of the input instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the project manager for our upcoming initiative, we aim to design a transformer-based multilingual translation system utilizing the Common Crawl dataset. Targeting English to Japanese, it must exhibit exceptional handling of colloquial phrases, slang, and cultural nuances, with a stringent requirement to maintain a minimum accuracy of 97% on a diverse range of informal text sources, reflecting real-world conversations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Design a transformer-based multilingual translation system specifically for English to Japanese, with a focus on handling colloquial phrases, slang, and cultural nuances. The system should maintain a minimum accuracy of 97% on informal text sources, reflecting real-world conversations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl (English-Japanese)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"diversity\": \"informal text, representing real-world conversations\",\n                \"content_types\": [\"colloquial phrases\", \"slang\", \"cultural nuances\"]\n            },\n            \"description\": \"A multilingual dataset derived from Common Crawl, specifically targeting English and Japanese, for training the translation system.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based Multilingual Translation\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"target_languages\": [\"English\", \"Japanese\"],\n                \"handling_of_variants\": \"colloquial, slang, cultural nuances\"\n            },\n            \"description\": \"A state-of-the-art neural network model designed for multilingual machine translation, optimized for English to Japanese translation with exceptional handling of colloquial elements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The system should prioritize colloquial and informal language to align with real-world conversations.\",\n        \"Handling cultural nuances is essential for accurate translation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"To showcase the system's performance on diverse informal text sources\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the project's intent to build a translation system, considering the high expertise level. The problem area, task, performance requirement, and dataset details are derived from the instruction, ensuring relevance and accuracy. The system's specific focus on handling colloquial phrases, slang, and cultural nuances aligns with the user's requirements. However, a more detailed specification of preprocessing steps, augmentation techniques, or the chosen transformer architecture would require additional information.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a seasoned project manager, your request is to develop an advanced text classification system for the DAIGT dataset. The objective is not only to classify input texts as human (0) or AI-generated (1) with high precision, but also to analyze the nuances in language patterns. The model must undergo multiple training phases, cross-validation, and fine-tuning using ensemble methods for optimal performance, with a focus on interpretability and model robustness.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"cybersecurity\",\n        \"description\": \"Develop an advanced text classification system for the DAIGT dataset, differentiating between human-generated (0) and AI-generated (1) texts. The system should emphasize high precision, language pattern analysis, multiple training phases, cross-validation, fine-tuning using ensemble methods, and interpretability with robustness.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"human_or_AI\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": null,\n                \"language patterns\": \"analyzed\"\n            },\n            \"description\": \"A dataset containing texts to be classified as human or AI-generated, suitable for the advanced classification system.\",\n            \"preprocessing\": [\"language cleaning\", \"tokenization\", \"lemmatization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"word cloud for language patterns\", \"confusion matrix\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Ensemble Text Classifier\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"robustness\": \"optimized\"\n            },\n            \"description\": \"An advanced text classification model using ensemble techniques, designed for high precision and language pattern analysis on the DAIGT dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The focus on precision and interpretability emphasizes the importance of minimizing false positives and understanding the model's decision-making process.\",\n        \"Multiple training phases and fine-tuning using ensemble methods indicate an intention to achieve superior performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection\", \"feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"web platform or API\",\n        \"inference_engine\": [\"interpretable models\"],\n        \"demonstration\": \"interactive model explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the schema, accurately reflecting the requirements from the user's instruction. High emphasis on precision, interpretability, and ensemble methods reflects the focus on the project. However, a specific precision value was not provided and should be inferred from the performance of the model after development. The details of the DAIGT dataset are described, but actual numbers are left to be filled during the development process.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "As a machine learning project manager, request the development team to design and implement a robust, multi-layered HGNN model for the Coauthorship Science dataset. Elaborate on the need for feature extraction, domain-specific node embeddings, and fine-tuning for temporal author evolution. Ensure the model demonstrates interpretability and efficiency through cross-validation and AUC-ROC analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Request the development team to design and implement a robust, multi-layered HGNN (Hypergraph Neural Network) model for the Coauthorship Science dataset. Focus on feature extraction, domain-specific node embeddings, and fine-tuning for temporal author evolution. Ensure interpretability and efficiency through cross-validation and AUC-ROC analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coauthorship Science\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": {\n                    \"nodes\": null,\n                    \"edges\": null\n                },\n                \"temporal_range\": {\n                    \"start\": null,\n                    \"end\": null\n                }\n            },\n            \"description\": \"A graph dataset capturing coauthorship relationships in the scientific community.\",\n            \"preprocessing\": [\"feature extraction\", \"node embedding\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Multi-layered HGNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_layers\": null,\n                \"time-aware\": true,\n                \"interpretability_techniques\": [\"attention mechanisms\"]\n            },\n            \"description\": \"A robust, multi-layered HGNN designed specifically for the Coauthorship Science dataset with emphasis on feature extraction, domain-specific node embeddings, and temporal author evolution.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The Coauthorship Science dataset necessitates a model that can capture complex relationships among authors.\",\n        \"Node embeddings are crucial for representing author entities in the graph context.\",\n        \"Fine-tuning for temporal evolution is important to track changes over time.\",\n        \"Interpretability aids in understanding the model's decision-making process.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"domain-specific node embeddings\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient execution\"],\n        \"demonstration\": \"cross-validation and AUC-ROC analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent for building a model, considering the project manager role. The focus on graph machine learning and feature extraction is derived from the dataset and requirements. The AUC-ROC is mentioned as a desired performance metric, but no specific value is given. The model's interpretability and efficiency are emphasized based on the instruction. The confidence score is high given the clarity of the task and the well-structured response.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "The marketing team requires a targeted, machine learning-powered image classifier for our diverse user-generated content. The system should accurately categorize photos into: (1) high-quality customer endorsements, (2) potential product improvements needing detailed response, and (3) visually striking visuals for the exclusive 'Lifestyle Chronicles' section. The algorithm must be scalable and have a minimum 95% precision to ensure brand alignment and customer satisfaction.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"A targeted machine learning image classifier for classifying user-generated content into high-quality customer endorsements, potential product improvement suggestions, and visuals for the 'Lifestyle Chronicles' section.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"User-generated Content Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"endorsements\", \"product_improvement\", \"lifestyle_chronicles\"],\n            \"specification\": {\n                \"data_distribution\": \"diverse\",\n                \"image_resolution\": \"undefined\"\n            },\n            \"description\": \"A collection of diverse user-generated images for training and testing the image classifier.\",\n            \"preprocessing\": [\"resizing\", \"data augmentation\"],\n            \"augmentation\": [\"random flips\", \"cropping\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-class classification\",\n                \"loss_function\": \"categorical crossentropy\"\n            },\n            \"description\": \"A machine learning-based image classifier specifically designed for the given classification task and high precision requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The system must balance accuracy and brand alignment by achieving at least 95% precision.\",\n        \"Handling user-generated content with varying quality is a challenge.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom AI platform\",\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"Example demonstrations showcasing each category\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON response covers the user's intent, problem area, performance metric requirements (minimum 95% precision), and scalability. The modality is specific to image classification, and the details of the dataset are assumed based on the context. The system is designed to be scalable in the cloud environment and covers necessary steps such as preprocessing and feature extraction. However, the exact CNN architecture details or the exact preprocessing steps for handling diverse user-generated content are not explicitly provided in the input instruction. The confidence score is high but not perfect due to the lack of specific architecture details.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager must now ensure the fashion item segmentation tool, utilizing a novel EfficientNet variation, not only meets a minimum accuracy of 75% but also demonstrates generalization across diverse lighting conditions and poses. Additionally, the model must be lightweight enough for seamless real-time performance in a mobile app with a stringent 30ms latency constraint for each inference.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"fashion retail or e-commerce\",\n        \"description\": \"The project aims to develop a fashion item segmentation tool using a novel EfficientNet variation. The model should have a minimum accuracy of 75%, demonstrate generalization across diverse lighting conditions and poses, and be lightweight for real-time performance in a mobile app with a 30ms latency constraint per inference.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.75\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 30,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion Item Segmentation Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_split\": {\n                    \"train\": \"for EfficientNet adaptation\",\n                    \"validation\": \"evaluating generalization\",\n                    \"test\": \"evaluating latency\"\n                }\n            },\n            \"description\": \"Dataset with diverse lighting conditions and poses for training, validation, and latency testing.\",\n            \"preprocessing\": [\"image augmentation for robustness\"],\n            \"augmentation\": [\"lighting and pose variations\"],\n            \"visualization\": [\"model performance on different lighting and pose examples\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet Variation\",\n            \"family\": \"EfficientNets\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"lightweight\",\n                \"latency_tolerant\": true\n            },\n            \"description\": \"A lightweight EfficientNet architecture specifically optimized for high accuracy and fast inference in mobile app environments.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Novel EfficientNet architecture emphasizes on both accuracy and efficiency\",\n        \"Diverse lighting and pose conditions are explicitly considered in the dataset and model evaluation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"custom augmentation for lighting and pose\"],\n        \"target_device\": \"mobile app\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"showing model accuracy and latency on representative mobile device\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a model, the EfficientNet-based segmentation tool meets the accuracy and latency requirements, and includes a clear description of the dataset, model, and service requirements. Knowledge on lighting and pose variations, as well as the need for mobile app compatibility, is well-structured in the response. A high confidence score is given due to the direct mapping to the provided information.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create a real-time, handheld TED Talks translation app utilizing Transformer-XL, targeting low-resource languages. Ensure the model demonstrates superior accuracy, low-latency, and runs on limited hardware with constraints of <=5% battery drain and <100MB memory footprint during 1-hour usage.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"mobile app development\",\n        \"description\": \"Develop a real-time, handheld TED Talks translation app using Transformer-XL, focusing on low-resource languages. The app should have high accuracy, low latency, and run efficiently on limited hardware with constraints of <=5% battery drain and <100MB memory footprint during 1-hour usage.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": {\n                    \"type\": \"comparison\",\n                    \"comparison\": \">\",\n                    \"threshold\": 0.9\n                }\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"type\": \"time\",\n                    \"unit\": \"milliseconds\",\n                    \"threshold\": null\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"battery consumption\",\n                \"value\": {\n                    \"type\": \"percentage\",\n                    \"threshold\": 5\n                },\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": {\n                    \"type\": \"size\",\n                    \"unit\": \"megabytes\",\n                    \"threshold\": 100\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TED Talks dataset (specifically for low-resource languages)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"language_type\": \"low-resource\"\n            },\n            \"description\": \"A dataset containing TED Talks transcriptions translated into the target low-resource languages.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL\",\n            \"family\": \"Transformer Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficient\": true,\n                \"low-latency\": true\n            },\n            \"description\": \"A real-time, low-resource language translation model based on Transformer-XL optimized for handheld devices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Low-resource languages may require smaller models with better transfer learning, as well as specialized techniques for data augmentation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"handheld, low-end hardware\",\n        \"deployment_endpoint\": \"TED Talks translation API\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"Interactive translation demo with latency measurements and energy consumption monitoring\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response incorporates the given requirements into a structured format. The intent to build is clear, and the expert level is assumed based on the complexity of the task. The NLP area, machine translation task, and target app domain are explicitly mentioned. The performance metric for accuracy is set to a desired high value, and latency is specified as a requirement. Complexity metrics are included with battery drain and memory constraints. Transformer-XL is chosen with the understanding that it's efficient for low-resource settings. The knowledge and final steps are also derived from the instruction. The confidence score is set to 0.97 due to the high level of confidence in interpreting the task's details.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "node classification", "instruction": "Utilize the newly discovered \"Atypical SynapsePatterns-19\" dataset, which integrates multi-modal brain activity and structural connectivity data, to design a state-of-the-art Multi-Modal Graph Convolutional Network (MGCN) for subcellular classification of neurons in the hypothalamus. The challenge is to not only enhance distinguishability between various subclasses of GABAergic and glutamatergic neurons but also decipher the dynamically changing synaptic interactions during different behavioral states. The model must exhibit exceptional interpretability and runtime efficiency while maintaining high accuracy and F1-score under limited computational resources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Design a state-of-the-art Multi-Modal Graph Convolutional Network (MGCN) for subcellular classification of neurons in the hypothalamus using the Atypical SynapsePatterns-19 dataset. The model must improve distinguishability between GABAergic and glutamatergic neurons, handle dynamically changing synaptic interactions across different behavioral states, and achieve high accuracy and F1-score with interpretability and runtime efficiency under limited computational resources.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"runtime efficiency\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Atypical SynapsePatterns-19\",\n            \"modality\": [\"image\", \"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dimension\": {\n                    \"brain activity\": { \"features\": \"unknown\", \"dimension\": \"variable\" },\n                    \"structural connectivity\": { \"features\": \"unknown\", \"dimension\": \"variable\" }\n                },\n                \"size\": \"unknown\"\n            },\n            \"description\": \"A dataset integrating multi-modal brain activity and structural connectivity data for the study of subcellular neuron classification in the hypothalamus.\",\n            \"preprocessing\": [\"data normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"network visualizations\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MGCN (for Atypical SynapsePatterns-19)\",\n            \"family\": \"Multi-Modal Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"modality integration\": \"dynamically changing\",\n                \"interpretability Techniques\": \"unknown (to be specified)\",\n                \"computational resources\": \"limited\"\n            },\n            \"description\": \"A MGCN designed specifically for the Atypical SynapsePatterns-19 dataset, targeting enhanced classification of GABAergic and glutamatergic neurons with changing synaptic interactions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Atypical SynapsePatterns-19 presents unique challenges due to its multi-modal nature and dynamic synaptic interactions.\",\n        \"Interpretability under limited resources is a focus.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for runtime efficiency\"],\n        \"demonstration\": \"visualizations of model's behavior and performance on neuron subclasses\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction, with the user's intent to build a model and high expertise level assumed. The problem area, task, and application domain are identified from the context. Performance metrics, including accuracy and F1-score, are not provided with specific values, but placeholders are set. Complexity metrics like runtime efficiency and interpretability are addressed as requirements. The Atypical SynapsePatterns-19 dataset is described as multi-modal and requires MGCN for classification, incorporating specific challenges like multi-class separation and dynamic interactions. The confidence score is high due to the direct mapping of requirements to the schema, while acknowledging that certain specific values are not provided.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Revamp the urban sound analysis project by integrating a hierarchical WaveNet architecture. The model must not only predict 24-hour ahead sound levels with high precision, but also decode and classify sub-second sound events into multiple categories, necessitating fine-grained noise forecasting and providing interpretable insights for comprehensive city planning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"Revamp the urban sound analysis project by integrating a hierarchical WaveNet architecture. The model should predict 24-hour ahead sound levels with high precision and decode and classify sub-second sound events into multiple categories for fine-grained noise forecasting and interpretable insights.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Urban Sound Dataset\",\n            \"modality\": [\"audio\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"length\": {\n                    \"daily\": null,\n                    \"sub-second\": null\n                }\n            },\n            \"description\": \"A time-series dataset for urban sound analysis, including both hourly and sub-second level sound data.\",\n            \"preprocessing\": [\n                \"resampling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"spectrogram\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical WaveNet\",\n            \"family\": \"WaveNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"layers\": {\n                    \"hierarchical_layers\": true,\n                    \"sub-second_layers\": null\n                },\n                \"accuracy_target\": \"high precision\",\n                \"interpretability\": true\n            },\n            \"description\": \"A state-of-the-art time-series forecasting model that integrates a hierarchical WaveNet architecture for precise 24-hour ahead sound level prediction and sub-second sound event classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hierarchical architectures help with fine-grained forecasting and interpretation.\",\n        \"Interpretable insights are crucial for urban planning.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"spectrogram extraction\",\n            \"feature extraction for sub-second events\"\n        ],\n        \"target_device\": \"cloud-based or edge computing\",\n        \"deployment_endpoint\": \"API for city planners and stakeholders\",\n        \"inference_engine\": [\n            \"TensorFlow serving\"\n        ],\n        \"demonstration\": \"Interactive visualization of sound event classification and forecast results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a model and high expertise level. It accurately reflects the problem area (time-series analysis), and the application domain (urban planning) with the specified downstream task (time-series forecasting). The model, integration of WaveNet architecture, and specific requirements such as precision, sub-second event classification, and interpretability are included. Dataset details, preprocessing, and augmentation techniques are deduced from the task. The confidence score is lower (around 0.8) due to the lack of exact performance metric values and some additional details that might be needed for the model's specification.\",\n        \"score\": 0.8\n    }\n}"}
{"task": "text classification", "instruction": "As a project manager for the advanced machine learning team, your objective is to tackle the multifaceted text classification challenge using the WebMD Review dataset. This task requires not only developing a high-performing sentiment analysis model but also conducting a rigorous comparative study of various algorithms, fine-tuning with domain-specific language, and assessing model explainability, while maintaining an F1-score of at least 90% to ensure industry-leading performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Tackle a multifaceted text classification task using the WebMD Review dataset for sentiment analysis. The project necessitates the development of a high-performance model with an F1-score of at least 90%, comparing multiple algorithms, fine-tuning with domain-specific language, and assessing model explainability.\",\n        \"performance_metrics\": {\n            \"F1-score\": {\n                \"value\": 0.90\n            }\n        },\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WebMD Review dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\"],\n            \"specification\": {\n                \"domain\": \"healthcare\",\n                \"size\": {\n                    \"train\": \"unknown\",\n                    \"validation\": \"unknown\",\n                    \"test\": \"unknown\"\n                },\n                \"examples\": \"various\"\n            },\n            \"description\": \"A dataset for sentiment analysis in the healthcare domain, containing reviews for text classification.\",\n            \"preprocessing\": [\"domain-specific language fine-tuning\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model performance and interpretability visualization\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"advanced machine learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_type\": [\"neural network architectures like BERT, RoBERTa, or XLNet\"],\n                \"explainability\": \"required\"\n            },\n            \"description\": \"A high-performance sentiment analysis model with the ability to compare different neural network architectures for domain adaptation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Fine-tuning with domain-specific language is essential for better performance.\",\n        \"Model explainability is crucial for trust and transparency in a healthcare context.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction, feature selection\", \"domain adaptation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"cloud-based\"],\n        \"demonstration\": \"performance and interpretability analysis on a sample of WebMD reviews\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intent as a project manager with a high level of expertise. The problem area (NLP), task (text classification), and domain (healthcare) are accurately described. The performance metric (F1-score) is included with the specified value, and the dataset and model expectations are derived from the provided details. The specific steps for preprocessing, fine-tuning, and model evaluation are identified based on the task description. The confidence is moderate due to some details, such as the size of the dataset, which are not explicitly provided.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager seeking a cutting-edge machine learning solution, can you find an AI model that not only forecasts peak service periods but also analyzes historical trends, real-time data anomalies, and weather impacts, providing at least 95% accuracy for a weekly forecasting horizon while considering regional variations?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"business forecasting\",\n        \"description\": \"Find an AI model that can forecast peak service periods, analyze historical trends, real-time data anomalies, and consider weather impacts for a weekly forecasting horizon with at least 95% accuracy, while taking regional variations into account.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"service period peak\", \"anomalies\", \"weather impact\"],\n            \"specification\": {\n                \"forecast_horizon\": \"weekly\",\n                \"time_series_frequency\": \"weekly\"\n            },\n            \"description\": \"Data with historical trends, real-time data, and weather information needed for the forecasting task, incorporating regional variations.\",\n            \"preprocessing\": [\"seasonality adjustment\", \"data normalization\"],\n            \"augmentation\": [\"time-series augmentation techniques\"],\n            \"visualization\": [\"time series plots for trend analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Time Series Forecasting Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"algorithm\": [\"ARIMA\", \"LSTM\", \"Prophet\", \"ensemble methods\"],\n                \"forecasting_method\": \"deep learning\",\n                \"regression_layers\": [\n                    {\n                        \"type\": \"RNN\",\n                        \"units\": \"custom\",\n                        \"dropout_rate\": \"custom\"\n                    }\n                ],\n                \"anomaly_detection\": \"yes\"\n            },\n            \"description\": \"A complex machine learning model, potentially incorporating multiple deep learning techniques, designed for advanced time-series forecasting with a focus on peak prediction, trend analysis, anomaly detection, and weather influence.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Performance is a critical factor, aiming for at least 95% accuracy.\",\n        \"Accounting for regional variations in the model is essential.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"feature selection\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"cloud-based\", \"real-time\"],\n        \"demonstration\": \"Demonstration of peak forecast, trend, anomaly, and weather impact analysis.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the user's request for a high-end solution, considering regional variations and advanced metrics. The model family and algorithms are generic enough to account for the requirements, but specific model details and preprocessing steps are not provided since the user didn't elaborate. The confidence score is high, assuming the instruction is clear, but may be lower if the specified accuracy target were more challenging to achieve.\",\n        \"score\": 0.92\n    }\n}"}
{"task": "text classification", "instruction": "The ambitious project for enhanced financial event analysis entails leveraging the extensive Reuters-21578 dataset, which necessitates a multi-step deep learning approach with fine-grained classification. The objective is to develop a highly efficient and robust deep contextual model, optimizing for precision above 88% in critical market news categories, while maintaining a speed-accuracy tradeoff of at least 90%. Ensure precision recall F1-score parity and address potential biases in real-time news flow.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"The project involves developing a deep learning model for enhanced financial event analysis using the Reuters-21578 dataset. The model should be multi-step and fine-grained, optimizing for precision above 88% in critical market news categories. It must maintain a speed-accuracy tradeoff of at least 90% and achieve parity in precision, recall, and F1-score. Bias mitigation in real-time news flow is also a requirement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.88\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.88\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.88\n            },\n            {\n                \"name\": \"speed-accuracy tradeoff\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"real-time news flow adaptation\",\n                \"value\": 0.9\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"financial event categories\"],\n            \"specification\": null,\n            \"description\": \"A large-scale dataset for multi-step financial event classification.\",\n            \"preprocessing\": [\"data cleaning, tokenization, and feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep contextual model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-layered, possibly including transformers or other deep learning techniques\",\n                \"optimizer\": \"Adam or similar for optimizing precision\",\n                \"training_time\": \"unknown\"\n            },\n            \"description\": \"A deep learning model designed for fine-grained classification of financial events, with a focus on precision.\"\n        }\n    ],\n    \"knowledge\": [\n        \"To achieve high precision in critical market news categories, careful feature engineering and label balancing are crucial.\",\n        \"The model must be adaptable to real-time news flow and minimize biases.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Financial domain-specific feature engineering\",\n            \"Contextual embeddings integration\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"TensorFlow serving or similar for efficient deployment\"\n        ],\n        \"demonstration\": \"Real-time performance monitoring and model explanation for news prediction\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the given schema. The user's intent is to build a model, with high expertise. The problem area is NLP and the application domain is finance. Specific requirements such as precision, speed-accuracy tradeoff, and bias mitigation are included. Performance metrics and complexity metrics are provided, but not all values or details are explicitly stated due to the missing information in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Develop a high-resolution urban green space segmentation model using the Vaihingen dataset, focusing on the challenging classes of rare and endangered vegetation. Implement an enhanced SegNet with real-time adaptation to daylight fluctuations and seasonal changes, ensuring interpretability of health indices for informed planning strategies in diverse microclimates.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"Develop a high-resolution urban green space segmentation model using the Vaihingen dataset, with a focus on rare and endangered vegetation classes. Implement an enhanced SegNet model with real-time adaptation to daylight fluctuations and seasonal changes, ensuring interpretability of health indices for informed planning strategies in different microclimates.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"overall accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity (rare vegetation)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability of health indices\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time under real-time adaptation\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint for real-time adaptation\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"green space segmentation\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"height\": null,\n                    \"width\": null,\n                    \"channels\": 3\n                },\n                \"classes_count\": null,\n                \"rare_vegetation_percentage\": null\n            },\n            \"description\": \"A high-resolution urban dataset with focus on rare and endangered vegetation\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Microclimate variations and daylight analysis\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced SegNet\",\n            \"family\": \"Encoder-Decoder Architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"SegNet with real-time adaptation\",\n                \"daylight_adaptation_techniques\": [],\n                \"seasonal_change_handling\": null,\n                \"interpretability_techniques\": null\n            },\n            \"description\": \"A SegNet model enhanced for real-time adaptation and interpretability of health indices for urban green spaces\"\n        }\n    ],\n    \"knowledge\": [\n        \"Rare and endangered vegetation classes present unique segmentation challenges in the Vaihingen dataset.\",\n        \"Real-time adaptation to daylight fluctuations and seasonal changes is crucial for high performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"GPU for efficient real-time processing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"on-the-fly adaptation engine\"],\n        \"demonstration\": \"Dynamic segmentation results showcasing adaptability and interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is tailored to the given instruction, capturing the user's intent, problem domain, dataset specifics, and model requirements. The performance metrics (accuracy, interpretability, and real-time adaptation) are inferred based on the task. The confidence score is high as the information is directly mapped from the instruction. However, missing values for performance metrics (like target values or exact implementation details) are to be provided by the user or filled in based on their expertise.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "Utilizing the large-scale Enron Email dataset, design a distributed node classification model employing a High-Order RGCN architecture. Focus on differentiating roles and subgroups within the employee network, while ensuring model parallelism for scalability. Challenge the AI by optimizing for ultra-low latency, aiming to handle millions of nodes with minimal delays in a real-time operational setting.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"security/enterprise\",\n        \"description\": \"Design a distributed node classification model using the Enron Email dataset with a High-Order RGCN architecture. The model should differentiate roles and subgroups in the employee network while ensuring parallelism for scalability. The focus is on ultra-low latency to handle millions of nodes in real-time with minimal delays.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"min\": \"ultra-low\",\n                    \"max\": \"minimal\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model_parallelism\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"microseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"nodes\": \"millions\",\n                \"edge_density\": null,\n                \"size\": null,\n                \"temporal\": false\n            },\n            \"description\": \"A large-scale graph dataset capturing the employee network of the Enron Email dataset.\",\n            \"preprocessing\": [\n                \"node labeling for roles and subgroups\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"High-Order RGCN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"distributed\",\n                \"parameter_size\": null,\n                \"flops\": null\n            },\n            \"description\": \"A distributed node classification model designed for the Enron Email dataset with a focus on High-Order RGCN architecture and scalability for real-time, low-latency performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The Enron Email dataset provides a complex network for model testing and the challenge of differentiating roles and subgroups.\",\n        \"Model parallelism is crucial for handling large-scale employee networks.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"real-time operational setting\",\n        \"inference_engine\": [\"optimized for latency\"],\n        \"demonstration\": \"real-time node classification using generated or synthetic data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intent to build a model, their high level of expertise, and the specifics of the problem (dataset, task, domain, and challenges). Performance metrics, such as accuracy and ultra-low latency, are included with a target range for the latency. The graph modality and the Enron Email dataset's complexity are reflected in the dataset description and preprocessing steps. The model's distributed and High-Order RGCN characteristics are incorporated. However, specific metric values are not provided, and some aspects like model architecture's parameter size and FLOPs or the exact edge density are not specified, leaving room for clarification.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for a premier machine learning team, your objective is to develop a cutting-edge time-series classification model. Utilizing the challenging SCP1 dataset with fine-tuned partitions for train, validation, and test sets, build an algorithm that processes variable-length sequences (INPUT_SEQ_LEN: 896, INPUT_DIM: 6) while maintaining interpretability. Your task is not only to achieve high accuracy but also to incorporate feature selection techniques and real-time anomaly detection, ensuring robustness and model efficiency. Design an evaluation protocol that assesses performance on unseen data while minimizing false positives.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"predictive maintenance\",\n        \"description\": \"Develop a cutting-edge time-series classification model for the SCP1 dataset, with emphasis on processing variable-length sequences (INPUT_SEQ_LEN: 896, INPUT_DIM: 6), interpretability, feature selection, and real-time anomaly detection. The model should achieve high accuracy, minimize false positives, and have a robust evaluation protocol for unseen data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time anomaly detection\",\n                \"value\": 1,\n                \"unit\": \"enabled\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SCP1\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 896,\n                \"INPUT_DIM\": 6,\n                \"partitioning\": {\n                    \"train\": \"fine-tuned\",\n                    \"validation\": \"fine-tuned\",\n                    \"test\": \"fine-tuned\"\n                }\n            },\n            \"description\": \"A challenging dataset for time-series analysis with fine-tuned partitioning for train, validation, and test sets.\",\n            \"preprocessing\": [\"feature normalization\", \"variable-length sequence handling\"],\n            \"augmentation\": [],\n            \"visualization\": [\"interpretability visualization\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"interpretable time-series classification algorithm\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_selection_method\": \"application-specific\",\n                \"anomaly_detection_technique\": \"real-time\"\n            },\n            \"description\": \"An advanced algorithm that processes variable-length sequences, employs feature selection, and includes real-time anomaly detection for time-series classification on the SCP1 dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Variable-length sequences and fine-tuned partitioning require tailored preprocessing and algorithm design.\",\n        \"Interpretability is crucial for understanding model decisions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction\",\n            \"transformations for interpretability\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for real-time performance\"\n        ],\n        \"demonstration\": \"Include a clear protocol for real-time anomaly detection and model explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the project manager's objective and requirements, including the user's expertise level. The problem description, dataset specifications, model requirements (interpretability, feature selection, and anomaly detection), and service expectations are all derived from the instruction. However, the performance metric (accuracy) is missing a target value, so it's set to null. The confidence score is high due to the clear mapping of the instruction to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager is tasked with developing a domain-specific neural machine translation system, focusing on the WMT19 Financial News Commentary dataset. Mandate the implementation of an advanced transformer architecture with byte-pair encoding for sentence alignment. Optimize for a rigorous F1-BLEU score evaluation, aiming to surpass a benchmark of 45 in the high-precision domain, ensuring financial terminology is accurately translated.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Develop a domain-specific neural machine translation system using the WMT19 Financial News Commentary dataset. Implement an advanced transformer architecture with byte-pair encoding for sentence alignment. Focus on optimizing for an F1-BLEU score, aiming to surpass a benchmark of 45 in the high-precision domain, ensuring financial terminology accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-BLEU\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 Financial News Commentary\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"train\": \"unknown\",\n                    \"validation\": \"unknown\",\n                    \"test\": \"unknown\"\n                },\n                \"language_pair\": \"Financial News (Source) -> English (Translation)\"\n            },\n            \"description\": \"A financial news dataset for English-translation, containing comments on financial events and news.\",\n            \"preprocessing\": [\"Byte-pair encoding for sentence alignment\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Advanced Transformer\",\n                \"sentence_alignment_method\": \"Byte-Pair Encoding\"\n            },\n            \"description\": \"A domain-specific neural machine translation model using a transformer architecture with byte-pair encoding for the WMT19 Financial News Commentary dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Financial terminology needs specialized translation to maintain accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"High-precision performance showcasing on financial terminology\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is adapted to the given instruction by detailing the user's intent to build a machine translation model, the high expertise level, and the focus on financial translation. The problem area, downstream task, and performance metric (F1-BLEU) are derived from the instruction. The dataset and its modality, preprocessing, and the selection of advanced transformer architecture are specified. However, the exact dataset size is not provided, as it would typically involve further data exploration. The high-precision domain benchmark is included with the aim of surpassing it. A specific knowledge piece about the importance of financial terminology accuracy is mentioned. The target F1-BLEU score is left as null, and the fine-tuning and deployment details require user-specific input.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "You are now working on a sophisticated, multi-faceted machine learning challenge involving time-series prediction for the Electricity dataset. The dataset is complex, containing hierarchical sub-sequences and varying patterns across train, validation, and test sets. The input sequences consist of historical data with 96 timestamps and 321 diverse features (INPUT_SEQ_LEN=96, INPUT_DIM=321). Your objective is to develop a state-of-the-art forecasting model that not only predicts the next 96-step future sequence (PRED_SEQ_LEN=96, PRED_DIM=321) but also exhibits excellent temporal context handling, seasonality awareness, and resilience to outliers. You must optimize for both Mean Squared Error (MSE) and Mean Absolute Error (MAE), ensuring model generalization and interpretability, while also conducting a rigorous comparative analysis with various baselines.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy and utilities\",\n        \"description\": \"Develop a state-of-the-art time-series prediction model for the Electricity dataset, which includes hierarchical sub-sequences and varying patterns across train, validation, and test sets. The model must forecast the next 96-step future sequence, handle temporal context, seasonality, and outliers. Optimization targets are Mean Squared Error (MSE) and Mean Absolute Error (MAE) with a focus on generalization, interpretability, and comparative analysis with baselines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Squared Error (MSE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Error (MAE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 321,\n                \"hierarchical_sub_sequences\": true,\n                \"pattern_variability\": true\n            },\n            \"description\": \"A complex time-series dataset with hierarchical structure and varying patterns across sets.\",\n            \"preprocessing\": [\"Handling missing values\", \"Normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Sequence patterns\", \"Time lags\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"State-of-the-art forecasting model (e.g., DeepAR, LSTM with attention mechanism, or Prophet)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"temporal_context\": \"excellent\",\n                \"seasonality_awareness\": true,\n                \"outlier_resilience\": true\n            },\n            \"description\": \"A sophisticated model designed for complex time-series data, addressing context, seasonality, and outliers.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hierarchical structure and varying patterns make the dataset challenging for forecasting models.\",\n        \"Optimizing for multiple metrics ensures a well-rounded evaluation.\",\n        \"Comparative analysis with baselines highlights the model's performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Feature extraction from hierarchical data\", \"Sequential feature engineering\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explainable predictions and visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction, including the user's intent (high expertise), the complexity and nature of the task (electricity dataset time-series forecasting), and the desired performance metrics and model characteristics. Specific details on preprocessing, augmentation, and model family have been inferred based on common practices for handling complex time-series data. However, due to the generality of the task, some details like specific baselines for comparison, and exact model specifications might be open to interpretation. The confidence score is high due to the clear mapping of the instruction to the schema structure.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager is tasked with developing a sophisticated question-answering system designed exclusively for children's educational enrichment. This system should leverage a specially adapted BERT model, tailored for minimal complexity and lucid explanations, derived from the KidsQA dataset. Moreover, the interface must incorporate playful and intuitive design elements, ensuring an immersive and interactive learning experience that fosters continuous engagement for the young audience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education, children's learning\",\n        \"description\": \"Develop a sophisticated question-answering system, specifically for children's educational enrichment. The system should utilize a simplified BERT model, modified for minimal complexity and clear explanations, derived from the KidsQA dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"answer accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model complexity\",\n                \"value\": \"minimal\",\n                \"unit\": \"interpretability scores\"\n            },\n            {\n                \"name\": \"interface response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"adapted_from\": \"BERT\",\n                \"suitability\": \"children-friendly\"\n            },\n            \"description\": \"A dataset for a child-oriented question-answering system, derived from BERT and adapted for minimal complexity.\",\n            \"preprocessing\": [\n                \"BERT adaptation\",\n                \"detection of simple language\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Simplified BERT for Kids\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for simplicity\",\n                \"explanation_capabilities\": \"lucid\"\n            },\n            \"description\": \"A specialized BERT model adapted for children's educational needs, ensuring minimal complexity and clear explanations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The use of adapted BERT for educational purposes can significantly improve understanding.\",\n        \"Incorporating playful design elements is essential for children's engagement.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"BERT adaptation, simplified for儿童理解\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"user-friendly and interactive interface\"],\n        \"demonstration\": \"playful and immersive learning environment\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the requirements of the provided instruction, capturing the need for a question-answering system, the adapted BERT model, and the playful interface. The high-expertise user is reflected, while the performance metrics (accuracy, interpretability) and complexity metrics (model simplicity, response time) are expected to be filled with specific numbers in practice. The dataset source is directly indicated as 'direct-search'.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As a project manager for the machine learning development team, you've been tasked with enhancing the performance of a real-world time series forecasting system. The Traffic dataset, divided into train, validation, and test subsets, features intricate, high-dimensional data (INPUT_SEQ_LEN=96, INPUT_DIM=862) representing past observations. Your challenge is to design a multi-step deep learning model that can not only predict future sequences (PRED_SEQ_LEN=96) but also exhibit exceptional adaptability to changing trends and patterns. Your team must optimize for both Mean Squared Error (MSE) and Mean Absolute Error (MAE) simultaneously, ensuring consistent accuracy across diverse data patterns. The objective is to develop a state-of-the-art model that outperforms existing methodologies in the literature.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"traffic management\",\n        \"description\": \"Enhance the performance of a real-world time series forecasting system with the Traffic dataset. Design a multi-step deep learning model for future sequence prediction (PRED_SEQ_LEN=96), targeting high adaptability to changing trends and patterns. Optimize for both Mean Squared Error (MSE) and Mean Absolute Error (MAE) simultaneously, while ensuring consistent accuracy across diverse data patterns. The goal is to create a state-of-the-art model outperforming existing literature methods.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Squared Error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Traffic\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 862\n            },\n            \"description\": \"A high-dimensional, intricate time series dataset divided into train, validation, and test subsets.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"multi-step deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adapter\": \"for high-dimensional and changing data patterns\",\n                \"sequence_prediction_length\": 96\n            },\n            \"description\": \"A state-of-the-art deep learning model designed for time series forecasting, adaptable to dynamic trends and handling large input dimensions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Focus on model architecture that allows for dynamic pattern recognition and adaptation\",\n        \"Consider using techniques like residual connections, attention mechanisms, or Transformers for improved forecasting.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's intent (building a model), their high level of expertise, and the problem domain (time series analysis for traffic management). The task description and dataset specifications have been carefully extracted from the instruction, including the input sequence length and dimensionality. Performance metrics (MSE and MAE) are included, but their target values are left unspecified as they are typically optimized during model training. The model is described as state-of-the-art, targeting adaptability and outperforming existing methods. The significance of optimizing for both MSE and MAE is noted, as it indicates a focus on accuracy and robustness.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "Develop a real-time, interactive medical Q&A prototype utilizing the challenging BioASQ dataset. Mandate the implementation of a novel fusion technique combining BioBERT with an adversarially-trained T5 model, ensuring interpretability and efficient cross-lingual compatibility while maintaining an AUC-PR score of at least 0.9. Additionally, demand integration of a sentiment analysis component to gauge patient emotions in questions for empathetic response.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Develop a real-time, interactive medical Q&A prototype using the BioASQ dataset. The system should incorporate a fusion technique combining BioBERT and an adversarially-trained T5 model, ensuring interpretability, cross-lingual compatibility, and maintain an AUC-PR score of at least 0.9.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-PR\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"multilingual\",\n                \"domain\": \"medical\",\n                \"challenging\": true\n            },\n            \"description\": \"A real-time medical Q&A dataset that requires advanced language understanding and fusion techniques.\",\n            \"preprocessing\": [\"BioBERT and T5 model fine-tuning\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model interpretability\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BioBERT + Adversarially-trained T5\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specifications\": {\n                \"fusion_technique\": \"novel\",\n                \"interpretability\": true,\n                \"cross_lingual\": true,\n                \"model_family\": [\"BioBERT\", \"T5\"]\n            },\n            \"description\": \"A Q&A system integrating BioBERT and an adversarially-trained T5 model with interpretability and cross-lingual capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"BioBERT is particularly effective in biomedical text understanding, and adversarial training improves the T5 model's performance.\",\n        \"Interpretability is crucial for a medical Q&A system, allowing for empathy and trust.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time, interactive\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient, cross-lingual\"],\n        \"demonstration\": \"context-sensitive, empathetic response generation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the provided instruction accurately, capturing the intent to build a prototype, high level of expertise, and the specific requirements (BioBERT fusion, adversarial training, AUC-PR, sentiment analysis). The dataset, model, and performance metrics are derived from the task, and relevant preprocessing and demonstration details are included. However, the actual fusion implementation details and some exact preprocessing techniques are not provided, as they would typically be filled in during the development phase.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for the machine learning development team, you've been tasked with enhancing the capabilities of an advanced forecasting system. The ILI dataset, containing intricate time-series data, has been stratified into train, validation, and test sets. Each input sequence of past data spans 36 time steps (INPUT_SEQ_LEN=36, INPUT_DIM=7) and requires sophisticated modeling to predict the subsequent 24 steps (PRED_SEQ_LEN=24, PRED_DIM=7). Your team must not only design a model that exhibits excellent performance in terms of mean squared error (MSE) and mean absolute error (MAE) but also execute an iterative process of feature engineering, fine-tuning, and cross-validation to ensure generalization across diverse patterns. Develop a multi-stage strategy for model training and evaluation, emphasizing robustness and adaptability to real-world fluctuations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"financial or business forecasting\",\n        \"description\": \"Enhance an advanced forecasting system using the ILI dataset with intricate time-series data. The model should predict 24 steps ahead, given 36 past steps, with emphasis on robustness and adaptability. Performance metrics include mean squared error (MSE) and mean absolute error (MAE), and requires iterative model training, feature engineering, fine-tuning, and cross-validation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ILI dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 36,\n                \"INPUT_DIM\": 7,\n                \"PRED_SEQ_LEN\": 24,\n                \"PRED_DIM\": 7\n            },\n            \"description\": \"Intricate time-series data stratified into train, validation, and test sets for forecasting purposes.\",\n            \"preprocessing\": [\n                \"Resampling\",\n                \"Windowing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Exploratory data analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"RNN/LSTM/Transformer (specific model yet to be determined)\",\n                \"input_shape\": [36, 7],\n                \"output_shape\": [24, 7]\n            },\n            \"description\": \"A deep learning model for time-series forecasting, emphasizing robustness and adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Iterative model development process involving feature engineering, fine-tuning, and cross-validation to ensure generalization.\",\n        \"Focus on time-series forecasting with multiple steps ahead and robustness to real-world fluctuations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"extracting seasonal patterns\",\n            \"handling missing values\",\n            \"normalization\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time forecasting\"\n        ],\n        \"demonstration\": \"Visualizing predicted results and actual data for model performance evaluation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the provided task of enhancing a forecasting system with emphasis on feature engineering, fine-tuning, and cross-validation for improved performance on the ILI dataset. The relevant areas are set to time-series analysis, and the model's high-level family and intended type are assumed to be neural networks. Performance metrics, specifications, and a multi-stage strategy are included, but exact model values and details would be filled in during actual development.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image classification", "instruction": "Develop a high-accuracy, multi-label image recognition model using state-of-the-art Vision Transformer architecture, which not only differentiates between gender but also discerns subtle age brackets (decades) in facial images, ensuring robustness against pose, occlusion, and variations in lighting for real-world deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"security, biometrics\",\n        \"description\": \"Develop a high-accuracy multi-label image recognition model, specifically using a state-of-the-art Vision Transformer architecture, for gender and fine-grained age (decades) classification in facial images. The model must be robust to variations in pose, occlusion, and lighting for real-world deployment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"gender\", \"age_decades\"],\n            \"specification\": {\n                \"image_size\": null,\n                \"number_of_classes\": {\n                    \"gender\": null,\n                    \"age_decades\": null\n                },\n                \"number_of_samples\": null\n            },\n            \"description\": \"A dataset for facial image recognition with gender and age brackets labels, designed to be robust against pose, occlusion, and lighting variations.\",\n            \"preprocessing\": [\n                \"pose normalization\",\n                \"image augmentation (e.g., random cropping, flipping, and lighting changes)\"\n            ],\n            \"augmentation\": [\n                \"random cropping\",\n                \"random flipping\",\n                \"lighting variations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Vision Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"number_of_layers\": null,\n                \"attention_heads\": null,\n                \"pre-training\": \"on a large facial image dataset\"\n            },\n            \"description\": \"A highly accurate Vision Transformer model for multi-label image recognition, focusing on gender and age detection with real-world resilience.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art Vision Transformers typically have advanced feature extraction capabilities for facial recognition tasks.\",\n        \"Considering pose, occlusion, and lighting variations is crucial for real-world deployment.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"face alignment\",\n            \"feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"demonstrate accuracy and robustness through a live demo\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the provided user's instruction by specifying a high-expertise user who wants to build a model. The area of expertise, computer vision, and downstream task of image classification are clearly captured. The model requirements, including the Vision Transformer and multi-label image recognition, are represented. Some complexities, such as metric values, are left unspecified since they depend on the data and model's performance during training, but the intention is to outline the criteria. Missing information (e.g., specific model name and dataset) could be filled upon a more in-depth understanding from the user.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Utilize the extensive YouTube-8M dataset to design a state-of-the-art video understanding system that seamlessly integrates 3D ResNet-101 and attention-based BERT models for transdisciplinary question answering. The system must demonstrate exceptional adaptability across diverse thematic categories, while maintaining real-time capabilities, precise context analysis, and temporal sensitivity to deliver timely and accurate responses to dynamically changing visual content.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"media and entertainment\",\n        \"description\": \"Design a state-of-the-art video understanding system using the YouTube-8M dataset. The system combines 3D ResNet-101 and attention-based BERT models for transdisciplinary question answering. Key requirements are adaptability across diverse thematic categories, real-time capabilities, precise context analysis, and temporal sensitivity for accurate and timely responses.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time processing\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"value\": null\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube-8M\",\n            \"modality\": [\"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_size\": null,\n                \"video_dimensions\": null,\n                \"feature_extraction\": [\"3D ResNet-101\", \"BERT\"]\n            },\n            \"description\": \"A large-scale video dataset for training a video understanding system that integrates 3D ResNet-101 and BERT models.\",\n            \"preprocessing\": [\n                \"feature extraction\",\n                \"label encoding\"\n            ],\n            \"augmentation\": [\n                \"temporal jittering\"\n            ],\n            \"visualization\": [\n                \"model performance plots\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"3D ResNet-101 + BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"3D ResNet-101\": null,\n                    \"BERT\": null\n                },\n                \"adapter_design\": \"attention-based\"\n            },\n            \"description\": \"A hybrid video understanding model combining 3D ResNet-101 and attention-based BERT for multimodal question answering.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art techniques in video and text processing\",\n        \"Adaptability to diverse thematic categories is essential\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual feature extraction\",\n            \"temporal fusion\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"example question-answering sessions on dynamically changing video content\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has been tailored based on the given requirements. The user's intent (build) and expertise (high) are clear. The area of focus is on multimodal learning, and specific datasets are mentioned (YouTube-8M). Performance metrics (accuracy and real-time capabilities) and complexity metrics are included. However, since the exact model accuracy or real-time processing values are not provided in the instruction, they are marked as null. Similarly, specific model details and preprocessing steps need more context, which is why they are outlined with placeholder values.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a specialized RNN-based neural machine translation model, utilizing the diverse OPUS-MTv2.0 dataset for colloquial English to conversational Russian translation. Emphasize on optimizing for realistic chat interactions, with a target BLEU score of 35+. Demonstrate exceptional adaptation to informal slang and idiomatic expressions while ensuring a sub-30 milliseconds response time for seamless user engagement in high-traffic scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"Develop a specialized RNN-based neural machine translation model for colloquial English to conversational Russian translation, targeting realistic chat interactions and a BLEU score of at least 35.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 35.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 30.0,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS-MTv2.0\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"multilingual\": true,\n                \"colloquial corpus\": true,\n                \"diversity of conversations\": true\n            },\n            \"description\": \"A diverse colloquial English to conversational Russian translation dataset suitable for RNN-based neural machine translation.\",\n            \"preprocessing\": [\"text normalization\", \"tokenization\"],\n            \"augmentation\": [\"back-translation\"],\n            \"visualization\": [\"attention mechanisms\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"RNN-based Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"RNN architecture\": [\"LSTM\", \"GRU\"],\n                \"sequence-to-sequence\": true,\n                \"attention mechanism\": true\n            },\n            \"description\": \"A custom RNN model optimized for colloquial English to conversational Russian translation, showcasing exceptional adaptation to informal language and slang.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating RNNs with attention mechanisms improves translation quality in diverse chat interactions.\",\n        \"Adapting to informal language requires training on a colloquial corpus.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual embeddings\", \"language adaptation\"],\n        \"target_device\": \"real-time chat application\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for speed\"],\n        \"demonstration\": \"High-traffic chat scenarios with sub-30ms response time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the given instruction, reflecting user's intent for model development, problem specifics in NLP and machine translation, diverse OPUS dataset, RNN-based model, and the target BLEU score. Performance metrics, complexity constraints, and dataset preprocessing, augmentation, and visualization are considered. The confidence score is high due to the direct mapping of the instruction to the JSON structure, while acknowledging the need for more precise model architecture details.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "For a dataset containing diverse and challenging fruit images taken under varying lighting and angles, develop a robust object recognition model that not only identifies but also localizes each fruit instance, differentiating between multiple types with high precision, even in the presence of occlusions and cluttered backgrounds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"agriculture or food industry\",\n        \"description\": \"Develop a robust object recognition model for identifying and localizing fruit instances in diverse and challenging images. The model must differentiate between multiple fruit types with high precision, even in the presence of occlusions and cluttered backgrounds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean average precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Intersection over Union (IoU)\",\n                \"value\": 0.8\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Diverse Fruit Images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": \"variable (adapted to input images)\",\n                \"color_channels\": 3,\n                \"number_of_classes\": \"multiple fruit types\"\n            },\n            \"description\": \"A challenging dataset with varying lighting, angles, occlusions, and cluttered backgrounds for fruit object detection.\",\n            \"preprocessing\": [\n                \"image resizing\",\n                \"color normalization\",\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\n                \"rotation\",\n                \"flipping\",\n                \"gaussian noise\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Object Detection (e.g., YOLO, Mask R-CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep convolutional neural network with region proposal and detection mechanisms\",\n                \"architecture_details\": \"ResNet backbone, advanced feature fusion, or attention mechanisms\"\n            },\n            \"description\": \"A deep learning model specifically designed for robust fruit object detection with high localization accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Model robustness against varying lighting and angles is crucial.\",\n        \"Handling occlusions and cluttered backgrounds demands advanced detection techniques.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction (e.g., color histograms, texture features)\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Provide a demo showcasing the model's performance on sample images.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately reflects the instruction, capturing the area of focus (computer vision), task (object detection), user's intent to build a model, and the complex requirements. Performance metrics like accuracy, mAP, and IoU are mentioned, acknowledging the need for precision. The user's high expertise level implies a focus on fine-tuning and understanding of advanced techniques. However, the model's specific architecture and required preprocessing details are not explicitly given since the instruction didn't specify a particular method.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "image classification", "instruction": "Create a sophisticated, pose-agnostic, and highly resilient deep learning system using the latest MViT-Hybrid model, specifically tailored for discriminating between micro-genders and age groups within a tight range (exact decades), while maintaining exceptional performance under challenging conditions like heavy occlusions and diverse lighting conditions, designed for seamless integration into a real-world surveillance and security application.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"security and surveillance\",\n        \"description\": \"Design a sophisticated deep learning system using the latest MViT-Hybrid model, aimed at distinguishing micro-genders and age groups within tight decades under challenging conditions like heavy occlusions and diverse lighting. The system must be designed for seamless integration into real-world surveillance and security applications.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity (for micro-genders)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sensitivity (for age groups)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness to occlusions\",\n                \"value\": null\n            },\n            {\n                \"name\": \"invariance to lighting conditions\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Custom Surveillance Data\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"micro-gender\", \"age decade\"],\n            \"specification\": {\n                \"image_size\": null,\n                \"diversity\": {\n                    \"occlusions\": \"high\",\n                    \"lighting\": \"varied\"\n                }\n            },\n            \"description\": \"A dataset containing images for micro-gender and age classification, specifically curated with heavy occlusions and diverse lighting conditions.\",\n            \"preprocessing\": [\"data augmentation for occlusion and lighting\"],\n            \"augmentation\": [\"GAN-based occlusion synthesis\", \"lighting normalization\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MViT-Hybrid\",\n            \"family\": \"Multimodal Vision Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"latest\",\n                \"specifics\": \"tailored for micro-gender and age classification\"\n            },\n            \"description\": \"A highly advanced MViT-Hybrid model specifically designed for fine-grained classification tasks in challenging surveillance conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"MViT-Hybrid's attention mechanism helps in pose-invariant feature extraction.\",\n        \"Its robustness to occlusions is due to the integration of transformers and convolutional layers.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"Edge or Cloud (取决于部署要求)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow-Serving\", \"ONNX Runtime\"],\n        \"demonstration\": \"user-defined API integration and live footage demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately represents the given instruction, capturing the user's intent to build a model, the domain (computer vision), and the specific requirements for performance, robustness, and dataset characteristics. However, values for performance metrics and complexity metrics have been set to null due to the lack of specific target values. The user's expertise is assumed to be high based on the request for a sophisticated system. The confidence score is high given the clear instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "question answering", "instruction": "Create a sophisticated medical Q&A system that manages the intricate BioASQ collection, requiring the invention of a fusion strategy marrying BioBERT and a state-of-the-art T5 model resistant to adversarial attacks. Strive for explainable AI with exceptional cross-lingual proficiency, while maintaining a minimum AUC-PR score of 0.95. Moreover, incorporate a sophisticated emotion detection module to not only interpret queries but also deeply understand and respond with empathetic nuances based on the patient's emotional state.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"Create a sophisticated medical Q&A system using the BioASQ collection. Employ a fusion strategy combining BioBERT and a T5 model with adversarial resistance. Aim for explainable AI with cross-lingual capabilities, while maintaining an AUC-PR score of at least 0.95.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-PR\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ Collection\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain\": \"bioinformatics\",\n                \"size\": \"large\",\n                \"diversity\": \"high\",\n                \"language\": \"multilingual\"\n            },\n            \"description\": \"A comprehensive medical dataset for question answering, derived from BioASQ challenge\",\n            \"preprocessing\": [\n                \"BERT tokenization\",\n                \"adversarial data augmentation\"\n            ],\n            \"augmentation\": [\"adversarial attacks\"],\n            \"visualization\": [\"model performance analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BioBERT-T5 Fusion\",\n            \"family\": \"transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_format\": \"fused BioBERT and T5\",\n                \"adversarial_resistance\": \"state-of-the-art\"\n            },\n            \"description\": \"An AI model with a fusion of BioBERT and T5, enhancing performance in medical question answering and adversarial resilience\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporate explainable AI techniques for transparency in answering complex medical queries.\",\n        \"Emphasis on cross-lingual proficiency for international use in healthcare.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"potentially a healthcare API platform\",\n        \"inference_engine\": [\"GPU-accelerated for fast response times\"],\n        \"demonstration\": \"Showcase of empathetic response to patient queries based on emotional state detection\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the intricate requirements of the medical Q&A system, capturing the complexities of the BioASQ collection, the fusion strategy, explainable AI, and emotion detection. The user's expertise level is inferred to be high based on the request. However, the exact emotion detection model specifications are not provided, nor are specific preprocessing details for the BioASQ collection. The confidence score is set to 0.95, indicating confidence in the understanding of the task but acknowledging that additional details could improve the response.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager must now engineer a child-centric question-answering system, incorporating advanced affective computing, that not only uses the tailored BERT model derived from KidsQA but also dynamically adjusts its language complexity based on the child's cognitive development. The design must include personalized gamification features, real-time progress tracking, and age-appropriate audio-visual cues, all while maintaining a visually minimalistic and touch-friendly interface for seamless learning and fostering a deep, long-term love for knowledge in the youngest users.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Engineer a child-centric question-answering system that incorporates advanced affective computing and a BERT-based model adapted from KidsQA. The system should dynamically adjust language complexity, include personalized gamification, real-time progress tracking, and age-appropriate audio-visual cues. The design focuses on a visually minimalistic and touch-friendly interface for learning enhancement and fostering long-term knowledge engagement in young users.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"user engagement\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time inference speed\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"interface simplicity\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\", \"audio\", \"multimodal\"],\n            \"target_variables\": [\"user interactions\", \"learning outcomes\"],\n            \"specification\": {\n                \"cognitive_development_levels\": [\"infant\", \"preschool\", \"elementary\"],\n                \"BERT_model_adaptation\": \"KidsQA\"\n            },\n            \"description\": \"A custom BERT-based model with age-appropriate data for question-answering, designed for child development levels.\",\n            \"preprocessing\": [\n                \"BERT adaptation for KidsQA\",\n                \"Age-appropriate text simplification\"\n            ],\n            \"augmentation\": [\n                \"Cognitive development-based content generation\"\n            ],\n            \"visualization\": [\n                \"Age-specific learning analytics\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Child-Centric Q&A System\",\n            \"family\": \"BERT-based (adapted from KidsQA)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"dynamic_language_complexity\": true,\n                \"gamification_components\": true,\n                \"minimal_interface\": true\n            },\n            \"description\": \"A tailored BERT model designed for kids, with features that adapt to cognitive development and foster a positive learning experience.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Affective computing handles emotional intelligence and personalization.\",\n        \"Gamification and progress tracking are essential for engaging young learners.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Personalized QA adaptation\", \"Emotion recognition integration\"],\n        \"target_device\": \"touch-friendly, adaptable to child's cognitive device\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Real-time analysis, adaptable for younger users\"\n        ],\n        \"demonstration\": \"Age-appropriate visual, audio, and gameplay demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's intent (building a system) and their high level of expertise. The problem area is NLP with a specific downstream task of QA. The project requirements, such as affective computing, complexity adjustments, and gamification, are represented. Some values for metrics are left as null because they were not directly specified in the instruction. The source of the dataset is inferred from the need for a tailored BERT model. The confidence score reflects the high clarity of the instructions and mapping to the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for the advanced machine learning team, your mandate is to design a highly specialized image recognition system for extragalactic entities, discriminating between galaxies and a nuanced variety of stars. The classifier must be built on a obscure, intricately labeled dataset sourced from an offbeat, non-astronomical institution, demanding a minimum precision of 97%. Furthermore, delve into a multi-layered approach by conducting an in-depth evaluation of transfer learning techniques, specifically using convolutional neural networks (CNN) against a sophisticated transformer-based model, with a thorough analysis of computational efficiency and scalability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Design a highly specialized image recognition system for extragalactic entities, differentiating between galaxies and various star types. The system should be based on an obscure, intricately labeled dataset from a non-astronomical source with a minimum precision requirement of 97%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"% decrease in FLOPs\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"relative to baseline model\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Obscure Galaxy Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"label_distribution\": {\n                    \"galaxies\": {\"count\": null, \"percentage\": null},\n                    \"stars\": {\"count\": null, \"percentage\": null}\n                },\n                \"size\": {\"total_samples\": null, \"labeled_samples\": null},\n                \"source\": \"non-astronomical institution\"\n            },\n            \"description\": \"A niche, expertly labeled dataset for image recognition of extragalactic entities.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Customized Image Classifier\",\n            \"family\": [\"convolutional neural networks\", \"transformer-based models\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"CNN vs Transformer\": {\n                    \"performance_comparison\": {\n                        \"accuracy\": null,\n                        \"efficiency\": {\n                            \"CNN\": null,\n                            \"Transformer\": null\n                        },\n                        \"scalability\": {\n                            \"CNN\": null,\n                            \"Transformer\": null\n                        }\n                    },\n                    \"analysis\": \"A thorough comparison of CNN and transformer-based approaches, focusing on computational efficiency and scalability.\"\n                }\n            },\n            \"description\": \"A multi-layered approach using both CNNs and transformer-based models for high-performance image recognition.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimization for astronomically constrained hardware\"],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a specialized image recognition system, with a focus on complexity and precision requirements. The problem area, performance metrics, and dataset properties are derived from the given instructions. However, the dataset statistics, specific model performance, and some complexity metrics are not provided in detail as they would typically be collected during the model development phase or based on prior research. The confidence score is high given that the structure follows the schema and the information provided is relevant to the task.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for a cutting-edge AI research team, your team has been tasked with a multifaceted data analytics challenge. The Concrete Compressive Strength dataset holds the key to a breakthrough in predictive modeling. Demanding excellence, you must employ state-of-the-art machine learning algorithms, address complex feature interactions, normalize data using multiple techniques, and combat outliers with advanced handling techniques. Emphasize on efficiency and generalization by optimizing model performance with not only root mean squared error (RMSE) but also precision, recall, and partial least squares regression. Furthermore, generate detailed visualizations to analyze the models' behavior across diverse subsets (rolling windows, stratified, and time-series splits), presenting not only training, validation, and test results but also the impact of different data splits on performance dynamics. Ensure a thorough examination of interpretability and bias is included in the final report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"engineering\",\n        \"description\": \"As a project manager, the team must tackle a data analytics challenge using the Concrete Compressive Strength dataset with state-of-the-art ML algorithms. Focus on addressing complex feature interactions, multiple normalization techniques, outlier handling, and optimizing for RMSE, precision, recall, and partial least squares regression. Detailed visualizations should analyze performance across rolling windows, stratified, and time-series splits, examining the impact on performance and model interpretability and bias.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"partial least squares regression\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"efficiency\",\n                \"value\": null,\n                \"unit\": \"computational resources\"\n            },\n            {\n                \"name\": \"generalization\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Concrete Compressive Strength\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"compressive strength\"],\n            \"specification\": {\n                \"rows\": null,\n                \"columns\": null,\n                \"data_types\": null,\n                \"missing_values\": null\n            },\n            \"description\": \"Dataset for predictive modeling research, focusing on concrete compressive strength with complex features and potential outliers.\",\n            \"preprocessing\": [\n                \"feature interactions\",\n                \"normalization techniques (multi-level)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                {\n                    \"type\": \"rolling windows\",\n                    \"description\": \"Performance analysis across different time windows\"\n                },\n                {\n                    \"type\": \"stratified\",\n                    \"description\": \"Performance comparison across stratified data splits\"\n                },\n                {\n                    \"type\": \"time-series\",\n                    \"description\": \"Impact of time-based splits on model performance\"\n                }\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art machine learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_depth\": null,\n                \"nodes_per_layer\": null,\n                \"optimization Algorithm\": null\n            },\n            \"description\": \"Advanced models for regression analysis with emphasis on efficient and generalizable performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Complex feature interactions, multiple normalization techniques, and handling outliers are crucial.\",\n        \"Emphasis on interpretability and model bias detection.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"complexity analysis\",\n            \"feature selection\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"generating reports and visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key aspects of the project manager's requirements, including the use of cutting-edge algorithms, performance metrics, and data handling methods. The dataset's specifics are left open for upload, and the performance dynamics and interpretability assessments are captured in the requirements. However, exact model specifications and some performance values are left unspecified as they are not provided in the instruction. The confidence score is high due to the clarity of the project scope.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As the project manager of the leading-edge AI research group, devise a multi-level deep learning system that not only distinguishes between 50 intricate plant species and their sub-variants in the highly-detailed 'Botanical Seedlings Dataset 3.0 Enhanced', but also implements nested transfer learning, Federated Learning, and differential privacy. The system must demonstrate exceptional accuracy of 99%, handle live image streams from resource-constrained IoT devices with a guaranteed latency of 10 milliseconds, and operate with extreme energy efficiency for 24/7 mobile field deployment in environments with fluctuating connectivity. Ensure strict adherence to GDPR and security protocols.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"botany and AI research\",\n        \"description\": \"Develop a multi-level deep learning system for distinguishing between 50 plant species and their sub-variants in the 'Botanical Seedlings Dataset 3.0 Enhanced'. Incorporate nested transfer learning, Federated Learning, and differential privacy techniques. The system must achieve 99% accuracy, handle live image streams from resource-constrained IoT devices with a latency of 10 milliseconds, and prioritize energy efficiency for 24/7 mobile field deployment with varying connectivity. Adhere strictly to GDPR and security protocols.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency (IoT devices)\",\n                \"value\": 10,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Botanical Seedlings Dataset 3.0 Enhanced\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species and sub-variants\"],\n            \"specification\": {\n                \"number of classes\": 50,\n                \"image resolution\": {\n                    \"width\": \"determined\",\n                    \"height\": \"determined\"\n                },\n                \"detail level\": \"high\"\n            },\n            \"description\": \"A highly-detailed dataset for plant species and sub-variants recognition.\",\n            \"preprocessing\": [\"differential privacy\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"multi-level deep learning system\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": [\n                    \"multi-level architecture\",\n                    \"nested transfer learning\",\n                    \"Federated Learning\"\n                ],\n                \"differential privacy techniques\": \"applicable\"\n            },\n            \"description\": \"A deep learning system designed for complex plant species recognition\"\n        }\n    ],\n    \"knowledge\": [\n        \"nested transfer learning, Federated Learning, and differential privacy for improved model privacy and performance.\",\n        \"24/7 mobile field deployment with resource-constrained IoT devices requires optimization for latency and energy efficiency.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained IoT devices\",\n        \"deployment_endpoint\": \"ensured in project\",\n        \"inference_engine\": [\"latency-sensitive for IoT\"],\n        \"demonstration\": \"include live image classification on IoT devices under varying connectivity\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the given instruction, with high-level specifications like task (image classification), datasets, models, and additional requirements such as GDPR compliance and security. The system's performance metric of 99% accuracy, latency of 10ms, and energy efficiency targeting are derived from the user's requirements. However, specific details like model architecture and the exact differential privacy techniques may need to be filled based on the best practices in the field.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "Develop a highly specialized financial news categorization system using the 'Reuters-21578' dataset, targeting obscure sub-sectors such as agricultural commodities derivatives and sustainable finance. Employ a state-of-the-art fusion model, integrating deep learning with symbolic AI for enhanced accuracy and explainability. Implement a real-time, adaptive training framework that utilizes model distillation and evolutionary algorithms, dynamically optimizing for performance on edge devices with limited computational power while maintaining interpretability for regulators through interactive visualizations. The system must prioritize energy efficiency, achieving at least a 30% reduction in power consumption during model inference, all while meeting ultra-low latency requirements of <100 milliseconds for high-frequency trading scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"financial news\",\n        \"description\": \"Develop a specialized financial news categorization system using the 'Reuters-21578' dataset. Focus on obscure sub-sectors like agricultural commodities derivatives and sustainable finance. Employ a state-of-the-art fusion model combining deep learning with symbolic AI for improved accuracy and interpretability. Implement a real-time adaptive training framework that utilizes model distillation and evolutionary algorithms, optimizing for edge devices with limited compute while maintaining explainability for regulators. Aim for at least a 30% reduction in power consumption during inference, adhering to ultra-low latency requirements of <100 milliseconds for high-frequency trading scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"energy efficiency (power consumption)\",\n                \"value\": 0.3\n            },\n            {\n                \"name\": \"latency (inference)\",\n                \"value\": 0.1\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational resource usage (edge devices)\",\n                \"value\": null,\n                \"unit\": \"FLOPs\"\n            },\n            {\n                \"name\": \"interpretablity (explainability to regulators)\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"custom\",\n                \"categories\": [\"agricultural commodities derivatives\", \"sustainable finance\"]\n            },\n            \"description\": \"A financial news dataset for specialized categorization, focusing on obscure sub-sectors.\",\n            \"preprocessing\": [\"model fusion\", \"sentence embeddings\"],\n            \"augmentation\": [],\n            \"visualization\": [\"interactive for regulators\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"fusion model with deep learning and symbolic AI\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": true,\n                \"real-time_training\": true,\n                \"edge_optimization\": true\n            },\n            \"description\": \"A model designed for financial news categorization with deep learning and symbolic AI integration, targeted at achieving high accuracy and interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art fusion for enhanced accuracy\",\n        \"Real-time adaptive training for edge devices\",\n        \"Model distillation and evolutionary algorithms for optimization\",\n        \"Energy efficiency for high-frequency trading\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"model adaptation for energy efficiency\"\n        ],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\", \"energy-efficient\"],\n        \"demonstration\": \"interactive visualizations for regulators\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the given instruction. It captures the user's intent to build a model, high expertise level, and the specific requirements of the financial news categorization system. The problem area, dataset details, model type, and performance/complexity metrics are derived from the instruction, ensuring accuracy. However, no specific model performance metrics were provided, and the system's energy efficiency during training was not elaborated on. The confidence score is high but not full (0.9) due to these unspecified metrics.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "Incorporate not only the challenging MUSE-MT benchmark but also sociocultural nuances into the machine translation system design. Engineer a novel, emotion-regulating siamese transformer model using the enriched ParaCrawl DeepLinguistic dataset version 5.0, which mandates simultaneous classification of five distinct emotional dimensions and cross-lingual adaptation. Strive for subword-level precision, maintaining tone consistency across lengths and domains, while adhering to tight performance metrics and real-world deployment requirements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-cultural communication\",\n        \"description\": \"Incorporate sociocultural nuances into the machine translation system design, focusing on the MUSE-MT benchmark. Develop a novel emotion-regulating siamese transformer model using the enriched ParaCrawl DeepLinguistic dataset version 5.0. The model should classify five emotional dimensions simultaneously and support cross-lingual adaptation. Aim for subword-level precision, maintaining tone consistency across varying lengths and domains, while meeting tight performance metrics and real-world deployment requirements.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Comprehension Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Emotion Consistency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory Consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl DeepLinguistic v5.0\",\n            \"modality\": [\"text\", \"multimodal\"],\n            \"target_variables\": [\"emotion dimensions\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"length\": null\n            },\n            \"description\": \"A dataset enriched for sociocultural context and emotion analysis, derived from the MUSE-MT benchmark and ParaCrawl corpus.\",\n            \"preprocessing\": [\"emotion annotation\", \"cross-lingual preprocessing\"],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [\"data distribution analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Emotion-Regulating Siamese Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"num_layers\": null,\n                \"hidden_size\": null,\n                \"heads\": null,\n                \"encoder_layers\": null,\n                \"decoder_layers\": null\n            },\n            \"description\": \"A machine translation model designed for handling sociocultural nuances and simultaneous emotion classification using the enriched ParaCrawl v5.0 dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Emphasis on sociocultural aspects for better translation accuracy\",\n        \"Requirement for cross-lingual and emotion-aware adaptation\",\n        \"Subword-level precision for better handling length and domain variations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual embeddings\", \"emotion embeddings\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-friendly inference\", \"optimized for real-world deployment\"],\n        \"demonstration\": \"Include example translations demonstrating emotion regulation and cultural sensitivity\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a machine translation model. The problem description covers the specific requirements, including the benchmark, dataset, and additional complexities like emotion recognition. Performance metrics and complexity constraints are included. The high expertise level indicates the model engineer may need more advanced tools and knowledge to handle the task. The only missing information is the target BLEU score and exact model specifications, as they are not specified in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As a leading AI consultant for \"MediCognify,\" devise a cutting-edge deep learning fusion architecture that not only analyses complex EHRs of patients grappling with ultra-rare genetic disorders, but also uncovers nuanced sub-phenomena connections between bespoke, gene-specific interventions and their long-term health prognosis. Utilize advanced deep progressive curriculum learning, integrating diverse medical datasets and domain expertise, and incorporate explainable AI with SHAP-360's next-generation Interactive explorable heatmaps. Strive for a near-perfect AUC-ROC of 0.9999, with a stringent 12% increase in predictive prowess over the previous model, ensuring absolute adherence to the stringent privacy norms (HIPAA and GDPR+) while demonstrating robust performance in a double-blind, cross-validated study with a control group of patients with both pre-existing and rare conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"Develop a deep learning fusion architecture for analyzing complex EHRs of patients with ultra-rare genetic disorders, identifying nuanced connections between gene-specific interventions and long-term health prognosis. Incorporate advanced techniques like deep progressive curriculum learning, diverse medical datasets, and explainable AI with SHAP-360 Interactive explorable heatmaps. Aim for an AUC-ROC of 0.9999 with a 12% increase in predictive power compared to the previous model. Ensure compliance with strict privacy norms (HIPAA and GDPR+) and demonstrate robust performance through a double-blind, cross-validated study involving both pre-existing and rare condition patients.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": 0.9999\n            },\n            {\n                \"name\": \"Predictive power improvement\",\n                \"value\": 0.12\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Privacy adherence\",\n                \"value\": 1.0,\n                \"unit\": \"HIPAA and GDPR+\"\n            },\n            {\n                \"name\": \"Robustness requirements\",\n                \"value\": 1.0,\n                \"unit\": \"double-blind, cross-validation\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Complex EHRs of patients with ultra-rare genetic disorders\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"health prognosis\", \"gene-specific interventions\"],\n            \"specification\": {\n                \"complexity\": {\n                    \"data variety\": \"diverse medical datasets\"\n                }\n            },\n            \"description\": \"Includes patients' EHR data with ultra-rare genetic disorders for advanced analysis and understanding.\",\n            \"preprocessing\": [\"advanced text processing for EHRs\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"SHAP-360 Interactive explorable heatmaps\"\n            ],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"Diverse medical datasets\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"not specified\",\n                \"diversity\": \"integrated\"\n            },\n            \"description\": \"Various medical datasets for progressive curriculum learning.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Learning Fusion Architecture\",\n            \"family\": \"MediCognify's custom architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep learning fusion, with explainable AI\",\n                \"techniques\": \"deep progressive curriculum learning, SHAP-360 heatmaps\",\n                \"complexity\": {\n                    \"privacy\": \"compliant with HIPAA and GDPR+\"\n                }\n            },\n            \"description\": \"A custom architecture for complex EHR analysis with explainability and stringent privacy requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced medical NLP techniques for genetic disorder context extraction.\",\n        \"Incorporating SHAP-360 for interpretability in deep learning models.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"for EHR data fusion and progressive curriculum\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"next-gen, secure\"], \n        \"demonstration\": \"double-blind, cross-validated study with control group\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's request for a sophisticated deep learning model in the medical domain. It includes the need for precision, explainability, privacy, and robust performance evaluation. The user's intention to build a model, high expertise level, and the desired performance targets are clearly defined. However, it's assumed that the double-blind, cross-validated study details were not fully specified, but would need to be further researched and detailed.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "As a seasoned project manager for an elite AI team, design and implement a high-performance node classification model on the NELL dataset, incorporating a state-of-the-art Hypergraph Neural Network with a novel hierarchical architecture. Emphasize scalability for both dense and sparse data with complex, interdependent relationships. The model must exhibit robustness against noise, achieving at least 95% accuracy and withstanding incremental adversarial attacks. Mandate a rigorous, multi-faceted hyperparameter optimization process, requiring the analysis of optimal settings across a wide spectrum of graph metrics, edge variability, and regularization effects, while ensuring reproducibility, interpretability, and the ability to generalize to unseen graph structures, all within a tight, maintainable codebase with clear documentation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"elite AI research\",\n        \"description\": \"Design and implement a high-performance node classification model on the NELL dataset using a state-of-the-art Hypergraph Neural Network with a novel hierarchical architecture. The model should be scalable for dense and sparse data with complex, interdependent relationships, and exhibit robustness against noise targeting at least 95% accuracy. It should be resistant to incremental adversarial attacks and involve a rigorous multi-faceted hyperparameter optimization process focusing on graph metrics, edge variability, and regularization effects. The model must be reproducible, interpretable, and capable of generalization to unseen graph structures, all within a maintainable codebase with comprehensive documentation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"density\": {\n                    \"sparse\": null,\n                    \"dense\": null\n                },\n                \"data_complexity\": {\n                    \"interdependence\": \"complex\"\n                }\n            },\n            \"description\": \"An advanced dataset with complex, interdependent relationships and varying edge density for node classification.\",\n            \"preprocessing\": [\"data cleaning, normalization\"],\n            \"augmentation\": [\"incremental adversarial attacks\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hypergraph Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"hierarchical\",\n                \"scalability\": {\n                    \"dense\": \"optimized\",\n                    \"sparse\": \"optimized\"\n                },\n                \"interpretability\": true,\n                \"generalization\": \"seen structures\"\n            },\n            \"description\": \"A state-of-the-art model built for node classification on NELL dataset with novel hierarchical architecture, optimized for scalability, robustness, and interpretablility.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hierarchical architecture enhances scalability and interpretability\",\n        \"Hyperparameter optimization crucial for high performance and robustness\",\n        \"Reproducibility and generalization to unseen graphs are vital\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for efficiency\"],\n        \"demonstration\": \"model performance demonstration with visual explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is built based on the given instruction, carefully mapping the user's requirements for a high-performance project. The problem area, downstream task, and performance metrics are directly derived from the task. The dataset properties are specified, and the model details are detailed accordingly. However, certain numeric values or specific code snippets for the optimization process or model demonstration are not included as the instruction does not provide them. The high confidence score is given due to the clarity of the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager is now mandated to design a cutting-edge, real-time fashion item segmentation solution, which demands the integration of an Enhanced EfficientNet-101 into a sophisticated, scalable FCN architecture. The mobile app integration must handle intricate, multi-layered garments with fine-grained accuracy, targeting a benchmark of at least 85% on the DeepFashion2 dataset, known for its intricate challenges. In addition to technical excellence, the system's speed must surpass expectations by achieving sub-millisecond latency (<1ms) at peak load, ensuring ultra-responsive performance without compromise for the world's most exclusive e-commerce platforms, while considering hardware constraints and potential edge computing implications.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"e-commerce, fashion technology\",\n        \"description\": \"Design a cutting-edge, real-time fashion item segmentation solution, integrating Enhanced EfficientNet-101 into a scalable FCN architecture, with fine-grained accuracy for intricate multi-layered garments. Target at least 85% benchmark on the DeepFashion2 dataset with sub-millisecond latency (<1ms) under peak load, considering hardware constraints and edge computing implications.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency (at peak load)\",\n                \"value\": 0.001,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepFashion2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A dataset with intricate challenges for fashion item segmentation, used for benchmarking the solution.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced EfficientNet-101 + FCN Architecture\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Enhanced EfficientNet-101 + Fully Convolutional Network\"\n            },\n            \"description\": \"An advanced neural network architecture designed for real-time fashion item segmentation, capable of handling complex multi-layered garments.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Enhanced EfficientNet-101's efficiency and DeepFashion2's complexity make it a challenging yet essential test for the system.\",\n        \"Sub-millisecond latency is crucial for a seamless user experience on e-commerce platforms.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile app, edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for mobile and edge computing\"],\n        \"demonstration\": \"world's most exclusive e-commerce platforms, ultra-responsive performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the user's instruction accurately, capturing the project requirements for a real-time fashion item segmentation solution, integrating Enhanced EfficientNet-101, performance targets, and hardware considerations. However, the model's specific specifications (FLOPs, memory footprint) and edge computing implications are not detailed in the response, as they are not clearly specified in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project manager of an elite AI team, design a cutting-edge text classification system using the BoolQ dataset that mandates not only a state-of-the-art performance with an F1-score of 98%, but must adhere to stringent real-time constraints. Alongside meeting these demands, the project necessitates:\r\n\r\n1. Implement a hybrid model integrating interpretability techniques (SHAP + partial dependence plots) to discern nuanced feature correlations.\r\n2. Optimize BERT-base for extreme efficiency, allowing for finetuning within a maximum of 3 epochs while reducing resource consumption by 20%.\r\n3. Develop an advanced explainability module that not only generates exemplar rationales but also employs counterfactual examples to demonstrate deep decision-making depth.\r\n4. Assess model robustness under both targeted and cross-cultural adversarial attacks, ensuring secure performance across a multitude of international languages.\r\n5. Create a comparative analysis report rich in visual storytelling, highlighting incremental advancements, and benchmarking against industry competitors, with a focus on presenting technical prowess and clarity in accessible, user-friendly graphics for diverse stakeholders.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI & NLP projects\",\n        \"description\": \"Design a cutting-edge text classification system using the BoolQ dataset. The system must achieve state-of-the-art performance with an F1-score of 98% and adhere to real-time constraints. The project includes specific requirements such as hybrid interpretability, efficiency optimization, explainability module, model robustness, and a comparative analysis report.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Real-time constraint\",\n                \"value\": 1, \n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Inference efficiency reduction\",\n                \"value\": 0.2,\n                \"unit\": \"percentage\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"valid\": null,\n                    \"test\": null\n                },\n                \"languages\": [\n                    \"English\"\n                ]\n            },\n            \"description\": \"A high-performance dataset for text classification tasks.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Model (BERT-base + Interpretability)\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"epochs\": 3,\n                \"resource_consumption_reduction\": 0.2\n            },\n            \"description\": \"A cutting-edge model integrating interpretability techniques for feature correlations and efficiency optimization.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Integrating SHAP and partial dependence plots for model interpretability.\"\n        },\n        {\n            \"description\": \"BERT-base finetuning with resource efficiency goals and limitations.\"\n        },\n        {\n            \"description\": \"Explainability module with exemplar rationales and counterfactual examples for decision depth.\"\n        },\n        {\n            \"description\": \"Model robustness against targeted and cross-cultural adversarial attacks, considering international language support.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"report\": \"comparative analysis with visual storytelling and user-friendly graphics\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project manager's requirements. The user's intent to build a model is clear, with high expertise level indicated. The problem area and downstream task are identified, along with the performance and complexity metrics. The dataset is specified for the text classification task. The model's specific details, requirements, and know-how requirements are all included. However, certain details like the exact BERT-base model specifications and preprocessing methods are not provided, as the instruction didn't elaborate on them.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a highly sophisticated, real-time adaptive hybrid architecture, fusing ResNet-CNN and a hierarchical LSTM, specifically designed to handle the intricate dynamics of an extensive urban traffic flow dataset. Request a multi-faceted forecasting model that demands progressive analysis for a lead time of three months. This model must not only forecast hourly traffic volumes with sub-seasonal fluctuations but also identify subtle shifts in trends and anomalies due to complex events like overlapping holidays, weekends, and major sporting events, distinguishing peak-hour behaviors during these periods with utmost precision. Ensure the model accounts for geographical variations and incorporates interpretable explainable AI components for deeper insights.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"traffic management\",\n        \"description\": \"Develop a real-time adaptive hybrid architecture combining ResNet-CNN and hierarchical LSTM for an extensive urban traffic flow dataset. The model should forecast hourly traffic volumes for a three-month lead time, capturing sub-seasonal fluctuations, and detecting shifts in trends and anomalies due to complex events like holidays, weekends, and major sporting events. It should also distinguish peak-hour behaviors during these periods and account for geographical variations. The model must include interpretable AI components for deeper insights.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"hourly forecasting accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"sub-seasonal fluctuation detection\",\n                \"value\": null\n            },\n            {\n                \"name\": \"event-based anomaly detection\",\n                \"value\": null\n            },\n            {\n                \"name\": \"peak-hour precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extensive Urban Traffic Flow Dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly traffic volumes\"],\n            \"specification\": {\n                \"length\": null,\n                \"time_frequency\": \"hours\",\n                \"seasonality\": \"seasonal and sub-seasonal\",\n                \"geographical scope\": \"urban\"\n            },\n            \"description\": \"A dataset capturing intricate dynamics of traffic flow in an extensive urban area, with variables for hourly volumes and potential geographical information.\",\n            \"preprocessing\": [\"resampling, normalization, feature extraction\"],\n            \"augmentation\": [\"time warping, synthetic events\"],\n            \"visualization\": [\"time-series heatmaps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hybrid Architecture (ResNet-CNN + Hierarchical LSTM)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"real-time, adaptive, progressive analysis\",\n                \"lead_time\": \"3 months\",\n                \"explainability\": true\n            },\n            \"description\": \"A state-of-the-art model designed for high-resolution traffic forecasting, leveraging both CNN and LSTM components.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interpretable AI components will be essential for understanding the model's reasoning and identifying influential factors.\",\n        \"Geo-sensitivity is crucial for capturing local traffic patterns.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"time windows, feature extraction\"],\n        \"target_device\": \"real-time deployment (e.g., edge computing or IoT)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"visual dashboards, event-based anomaly detection demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the given instruction's requirements accurately, including the model architecture, dataset specifics, and performance metrics. The level of complexity and specific AI components are inferred from the context. The confidence score is not specified here, but it would likely be high based on the clarity of the task description.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager now demands the machine learning team to develop a U-Net model that not only guarantees accurate and prompt furniture segmentation in real-time, but also integrates dynamic update capabilities, incorporates quantization for low-power hardware, and must meet stringent requirements of <5 milliseconds latency for seamless smart home deployment across a diverse range of IoT devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"smart home, IoT\",\n        \"description\": \"Develop a U-Net model for real-time furniture segmentation with dynamic update capabilities, optimized for low-power hardware through quantization, and must meet a latency requirement of <5 milliseconds for seamless smart home deployment across diverse IoT devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"speed\",\n                \"value\": {\n                    \"latency_constraint\": \"5 milliseconds\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": \"low-power hardware (quantization)\",\n                \"unit\": \"lower power consumption\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"latency_constraint\": \"5 milliseconds\"\n                },\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"real-time, diverse IoT devices\",\n                \"data_format\": \"suitable for low-power hardware\"\n            },\n            \"description\": \"A dataset for real-time furniture segmentation that can handle dynamic updates.\",\n            \"preprocessing\": [\"dynamic update adaptation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\" \n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net\",\n            \"family\": \"CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"real-time, low-latency\",\n                \"quantization\": true\n            },\n            \"description\": \"A U-Net model tailored for furniture segmentation in real-time, with hardware-friendly optimizations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating dynamic update capabilities and quantization for low-power deployment.\",\n        \"Meeting strict latency requirements for IoT devices across diverse environments.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adaptation for real-time updates\"],\n        \"target_device\": \"IoT devices\",\n        \"deployment_endpoint\": \"smart home deployment infrastructure\",\n        \"inference_engine\": [\"optimized for low-power hardware\"],\n        \"demonstration\": \"real-time, low-latency segmentation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction, targeting the U-Net model for furniture segmentation. The user's intent to build a model, high expertise, and specific requirements like accuracy, low latency, and quantization are captured. However, accuracy and more specific preprocessing steps for the dataset are not mentioned, and they might need to be inferred based on the requirements. The model description is general but implies a focus on real-time performance and hardware efficiency.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "As the project manager, you are now tasked with developing a multifaceted deep learning system. This should involve not only a robust Faster R-CNN architecture but also a novel fusion model that combines it with YOLOv5 for enhanced artifact and luxury garment detection. The 'Premium Limited Edition SKU-110K' dataset, expanded with additional high-resolution images, must be meticulously fine-tuned and domain-adapted for optimal performance in niche categories such as haute couture and rare vintage collectibles. The project should deliver a set of detailed performance metrics, including precision, recall, and F1 scores, presented in a visually insightful report, highlighting the trade-offs across different product subclasses and shelf environments. Additionally, outline a strategy for real-time adaptation to evolving fashion trends and shop layouts.", "resposne": "{\n  \"user\": {\n    \"intent\": \"build\",\n    \"expertise\": \"high\"\n  },\n  \"problem\": {\n    \"area\": \"computer vision\",\n    \"downstream_task\": \"object detection\",\n    \"application_domain\": \"e-commerce, fashion\",\n    \"description\": \"Develop a multifaceted deep learning system with a robust Faster R-CNN architecture fused with YOLOv5 for enhanced artifact and luxury garment detection. The 'Premium Limited Edition SKU-110K' dataset must be fine-tuned and domain-adapted for haute couture and rare vintage collectibles, with high-resolution images. Performance metrics should include precision, recall, and F1 scores in a visually insightful report, showing trade-offs across product subclasses and shop environments. Real-time adaptation to evolving fashion trends and shop layouts is required.\",\n    \"performance_metrics\": [\n      {\n        \"name\": \"precision\",\n        \"value\": null\n      },\n      {\n        \"name\": \"recall\",\n        \"value\": null\n      },\n      {\n        \"name\": \"F1 score\",\n        \"value\": null\n      }\n    ],\n    \"complexity_metrics\": [\n      {\n        \"name\": \"inference time\",\n        \"value\": null,\n        \"unit\": \"seconds\"\n      },\n      {\n        \"name\": \"memory usage\",\n        \"value\": null,\n        \"unit\": \"MB\"\n      }\n    ]\n  },\n  \"dataset\": [\n    {\n      \"name\": \"Premium Limited Edition SKU-110K\",\n      \"modality\": [\"image\"],\n      \"target_variables\": [],\n      \"specification\": {\n        \"image_resolution\": \"high\",\n        \"domain_adaptation\": true,\n        \"expanded_images\": true\n      },\n      \"description\": \"An expanded dataset with high-resolution images specifically for artifact and luxury garment detection, including haute couture and rare vintage collectibles.\",\n      \"preprocessing\": [\n        \"domain adaptation techniques\"\n      ],\n      \"augmentation\": [\n        \"high-resolution image resizing\",\n        \"object-specific transformations\"\n      ],\n      \"visualization\": [\n        \"confusion matrices\",\n        \"roc curves\"\n      ],\n      \"source\": \"user-link\"\n    }\n  ],\n  \"model\": [\n    {\n      \"name\": \"Faster R-CNN + YOLOv5 Fusion Model\",\n      \"family\": \"Deep Neural Networks (Faster R-CNN and YOLOv5)\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"architecture\": {\n          \"Faster R-CNN\": {},\n          \"YOLOv5\": {}\n        },\n        \"customization\": {\n          \"fusion strategy\": \"domain-specific\"\n        }\n      },\n      \"description\": \"A deep learning model combining Faster R-CNN and YOLOv5 for heightened artifact and luxury garment detection, adapted for specific niche categories.\"\n    }\n  ],\n  \"service\": {\n    \"feature_engineering\": [\n      \"domain adaptation techniques\",\n      \"real-time performance tuning\"\n    ],\n    \"target_device\": \"cloud for scalability and fast inference\",\n    \"deployment_endpoint\": \"custom-fashion-detection-api\",\n    \"inference_engine\": [\"TensorFlow Serving\", \"ONNX\"],\n    \"demonstration\": {\n      \"report_format\": \"visual, interactive\",\n      \"trend_adaptation\": \"incremental learning, transfer learning\"\n    }\n  },\n  \"confidence\": {\n    \"rationale\": \"The JSON response captures the essence of the project's requirements, reflecting the desired deep learning system, dataset modifications, and performance metrics. The user's intent is to build a high-level system, indicating their expertise. The specified performance metrics and real-time adaptation are aligned with the task at hand. However, exact metric values and some technical details are not provided since they are not specified in the instruction.\",\n    \"score\": 0.9\n  }\n}\n"}
{"task": "object detection", "instruction": "As a cutting-edge project manager, devise an advanced YOLOv5 architecture for the \"EcoFriendlyMart\" that integrates domain-specific fine-tuning of the GreenFoods2022 dataset. Emphasize the model's ability to detect and precisely categorize not just perishable but also sustainably sourced, rare, and seasonal produce with a minimum precision of 98% AP@0.5:0.99. Ensure the model's robustness against challenging lighting conditions, such as dawn and dusk, while targeting sub-0.1 second latency for uninterrupted, ultra-precise inventory management during high-traffic peak hours. Additionally, outline a strategy for real-world deployment, including hardware optimization and on-device inference for optimal performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail\",\n        \"description\": \"Design an advanced YOLOv5 architecture for the EcoFriendlyMart, integrating domain-specific fine-tuning on the GreenFoods2022 dataset. The model should detect and precisely categorize perishable, sustainably sourced, rare, and seasonal produce with a minimum precision of 98% AP@0.5:0.99. Ensure robustness against challenging lighting conditions (e.g., dawn and dusk) and target sub-0.1 second latency for inventory management during peak hours.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean average precision (AP)\",\n                \"value\": {\n                    \"min\": 0.98,\n                    \"max\": 0.99,\n                    \"interval\": \"0.5:0.99\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"GreenFoods2022\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"domain_specificity\": \"high\",\n                \"challenging_conditions\": \"lighting: dawn and dusk\"\n            },\n            \"description\": \"A dataset containing images of perishable, sustainably sourced, rare, and seasonal produce for fine-tuning the YOLOv5 model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5 for EcoFriendlyMart\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"latency_target\": \"sub-0.1s\",\n                \"condition_robustness\": \"challenging lighting conditions\"\n            },\n            \"description\": \"An advanced YOLOv5 architecture tailored for the EcoFriendlyMart, emphasizing inventory management for specific product categories and high-precision detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Fine-tuning on GreenFoods2022 ensures model performance for the unique produce categories.\",\n        \"Hardware optimization and on-device inference are necessary for deployment in a retail environment.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"optimization for high-traffic peak hours (e.g., embedded hardware or edge computing)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for low latency and hardware constraints\"\n        ],\n        \"demonstration\": {\n            \"focus\": \"uninterrupted ultra-precise inventory management\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents a clear understanding of the user's intent and requirements, covering the areas of computer vision, task of object detection, target performance metrics, and domain-specific fine-tuning. It fills in the template with high confidence, accounting for the user's expertise level. However, the exact link to the GreenFoods2022 dataset and specific model architecture details are missing, as they are not provided in the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a specialized machine learning system using the Distinctive Arborial Graph Neural Network (DAG-GWNN) architecture, fused with an adaptive attention module for precise identification of understudied astrocyte subtypes in the Allen Institute's ultrahigh-resolution mouse brain connectome. The proposed model must exhibit breakthrough performance, surpassing existing benchmarks, and demonstrate optimal runtime efficiency for handling voluminous datasets with fine-grained synaptic connectivity patterns characterizing cellular interplay in complex microcircuits.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Develop a specialized machine learning system using Distinctive Arborial Graph Neural Network (DAG-GWNN) with an adaptive attention module for identifying understudied astrocyte subtypes in the Allen Institute's ultrahigh-resolution mouse brain connectome. The model must outperform existing benchmarks and exhibit optimal runtime efficiency for large datasets with detailed synaptic connectivity patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"breakthrough performance\",\n                \"value\": 1.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"runtime efficiency\",\n                \"value\": null,\n                \"unit\": \"seconds per sample\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Allen Institute Mouse Brain Connectome\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"astrocyte subtypes\"],\n            \"specification\": {\n                \"dataset size\": \"ultrahigh-resolution\",\n                \"synaptic connectivity patterns\": \"fine-grained\"\n            },\n            \"description\": \"An ultrahigh-resolution dataset depicting mouse brain connectome with fine-grained synaptic connectivity patterns for astrocyte subtype identification.\",\n            \"preprocessing\": [\"fusing adaptive attention module\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DAG-GWNN with Adaptive Attention Module\",\n            \"family\": \"Distinction Arborial Graph Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"DAG-GWNN\",\n                \"attention mechanism\": \"adaptive\"\n            },\n            \"description\": \"A state-of-the-art GNN model with adaptive attention for accurate identification of astrocyte subtypes in the given brain connectome.\"\n        }\n    ],\n    \"knowledge\": [\n        \"DAG-GWNN's structure is crucial for handling complex microcircuits, and adaptive attention enhances subtype distinction.\",\n        \"Model needs to surpass existing benchmarks in astrocyte subtype classification.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"model performance on unseen astrocyte subtypes\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the given instruction by capturing the user's intent to build a machine learning model, their expertise, and the specific problem in the neuroscience domain. The problem description is detailed, including the need to surpass existing benchmarks and focus on efficiency for large datasets. The requested DAG-GWNN with adaptive attention module is included. However, exact performance targets like accuracy and runtime efficiency are left unspecified for the model performance metrics, as they require benchmark information to be set. The confidence score is high due to the thorough interpretation of the problem given.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a specialized machine learning project manager, your objective is to design an innovative state-space model tailored for predictive analytics in the ETTm2 dataset, focusing on deep industrial process monitoring. Utilize the meticulously stratified data into 80% training (ephemeral data segments of 96 timesteps, INPUT_SEQ_LEN, with enriched features: temperature, pressure, vibration, electricity, flow, level, and speed), 10% validation, and 10% testing sets. Each subset exhibits non-overlapping, time-sensitive windows, capturing intricate temporal dynamics. The task at hand is to forecast a consecutive 96-step sequence (PRED_SEQ_LEN) with precision, optimizing not only for root mean squared error (RMSLE) but also mean absolute percentage error (MAPE), while demonstrating superior performance in the niche domain of real-time fault prediction. Emphasize the development of a distinctive architecture that bridges long-range dependencies and short-term anomalies, navigating the nuances of industrial sensor noise within this highly technical time-series environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"industrial process monitoring\",\n        \"description\": \"Design an innovative state-space model for predictive analytics in the ETTm2 dataset, focusing on deep industrial process monitoring, with 80% training data (ephemeral 96-timestep segments, INPUT_SEQ_LEN, enriched with temperature, pressure, vibration, electricity, flow, level, and speed), 10% validation, and 10% testing sets. The model should forecast a consecutive 96-step sequence (PRED_SEQ_LEN), optimize for RMSLE and MAPE, and excel in real-time fault prediction while handling industrial sensor noise in a complex time-series environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"root mean squared error (RMSLE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute percentage error (MAPE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [\n                \"future_96_step_sequence\"\n            ],\n            \"specification\": {\n                \"input_window_size\": 96,\n                \"input_features\": [\n                    \"temperature\",\n                    \"pressure\",\n                    \"vibration\",\n                    \"electricity\",\n                    \"flow\",\n                    \"level\",\n                    \"speed\"\n                ],\n                \"output_sequence_length\": 96\n            },\n            \"description\": \"A dataset for industrial process monitoring with stratified, time-sensitive data segments for training, validation, and testing.\",\n            \"preprocessing\": [\n                \"stratified splitting\",\n                \"windowing with time sensitivity\"\n            ],\n            \"augmentation\": [\n                \"temporal data augmentation (e.g., time series stretching or warping)\"\n            ],\n            \"visualization\": [\n                \"temporal data visualization\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-space model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"bridging long-range dependencies with short-term anomaly detection\",\n                \"noise handling\": \"for industrial sensor noise\"\n            },\n            \"description\": \"A specialized neural network state-space model designed for real-time fault prediction in industrial process monitoring.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Addressing long-range dependencies and short-term anomalies is crucial.\",\n        \"Industrial sensor noise requires a specialized architecture\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"timestamp-based feature extraction\"\n        ],\n        \"target_device\": \"edge device or industrial IoT\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for real-time performance\"\n        ],\n        \"demonstration\": \"time-series forecasting results and fault detection examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response parses the user's instruction into a high-level structure. The intent is clear as a build task with a high expertise level. The problem area is identified as time-series analysis with the industrial process monitoring application. Performance metrics (RMSLE and MAPE) are specified, but the exact values are not provided due to the nature of the task. Complexity metrics are also expected to be optimized for real-time performance, but they are currently placeholders. The user's needs regarding the dataset splitting and preprocessing are accurately captured. However, the specific model architecture details and performance on the dataset are not yet included, as they would depend on further analysis.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "tabular regression", "instruction": "As a seasoned project manager for a niche media ad tech firm, devise a cutting-edge regression model with a hybrid feature selection algorithm. Utilizing the \"Digital Marketing Mix and ROI Dataset,\" navigate through high-dimensional, seasonally-varying data, and unique campaign permutations. Strive for not only predictive prowess with a targeted MAPE under 2% on 'Premium Advertiser Spend', but also incorporate dynamic pricing strategies and real-world market fluctuations for exceptional business acumen. Remember to optimize computational efficiency while respecting data privacy regulations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"Design a cutting-edge regression model using a hybrid feature selection algorithm for a niche media ad tech firm. The model should handle high-dimensional, seasonally-varying, and complex campaign permutations data from the 'Digital Marketing Mix and ROI Dataset' with a targeted MAPE (Mean Absolute Percentage Error) of under 2% on the 'Premium Advertiser Spend' variable. Incorporate dynamic pricing strategies and address real-world market fluctuations for business insight. Ensure computational efficiency and adhere to data privacy regulations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute percentage error (MAPE)\",\n                \"value\": 0.02\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"data privacy conformity\",\n                \"value\": \"compliant\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Digital Marketing Mix and ROI Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"Premium Advertiser Spend\"],\n            \"specification\": {\n                \"high-dimensional\": true,\n                \"seasonality\": true,\n                \"campaign_permutations\": true\n            },\n            \"description\": \"A comprehensive dataset with high-dimensional, seasonally-varying data and unique campaign permutations within the digital marketing context.\",\n            \"preprocessing\": [\"hybrid feature selection\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"hybrid regression\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"type_of_regression\": \"dynamic pricing with market fluctuations\",\n                \"accuracy_target\": \"2%\",\n                \"optimization\": \"computational efficiency\"\n            },\n            \"description\": \"A state-of-the-art regression model with a focus on predictive accuracy and dynamic pricing strategies in the context of media ad tech.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hybrid feature selection helps handle high-dimensional data and complex relationships.\",\n        \"Incorporating real-world market fluctuations and dynamic pricing strategies is essential for business intelligence.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"hybrid feature selection\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"practicality\": \"demonstrate real-world effectiveness\",\n            \"privacy_protected\": \"predictions based on anonymized data\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent (to build a model), expertise level, and the specified problem domain. The problem description is derived directly from the instruction, detailing the dataset, required metrics, and algorithmics. The high-dimensional and seasonality aspects are included, as well as the hybrid feature selection and the importance of adhering to data privacy regulations. The performance metric, MAPE, and the requirement for computational efficiency are incorporated with a target value. The model type and feature engineering steps are derived from the instruction, and confidence is high due to the clear direction given.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "For a cutting-edge AI project manager, design an intricate plan for the ML team to tackle a specific emotion-based text classification issue on the extensive Diverse Augmented International Genre-Transfer (DAIGT) dataset. In addition to conventional techniques like multi-lingual, domain-specific feature extraction, fine-grained semantic analysis, and nested cross-validation, incorporate explainable deep learning models, fair comparison with state-of-the-art algorithms, and real-time performance boost through active learning, ensuring interpretability and continuous performance enhancement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"affective computing\",\n        \"description\": \"Design an intricate plan for an AI project manager to address an emotion-based text classification issue using the Diverse Augmented International Genre-Transfer (DAIGT) dataset. The plan should involve multi-lingual, domain-specific feature extraction, fine-grained semantic analysis, nested cross-validation, explainable deep learning models, fair comparison with state-of-the-art algorithms, and real-time performance boost through active learning for interpretability and continuous performance improvement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"f1_score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory_usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Diverse Augmented International Genre-Transfer (DAIGT)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"emotion\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"dimensions\": {\n                    \"vocabulary_size\": null,\n                    \"sequence_length\": null\n                },\n                \"augmentation_size\": null\n            },\n            \"description\": \"A large-scale, multi-lingual, and genre-diverse dataset for emotion-based text classification.\",\n            \"preprocessing\": [\n                \"multi-lingual tokenization\",\n                \"domain-specific stemming/lemmatization\"\n            ],\n            \"augmentation\": [\"synonym replacement\", \"back-translation\"],\n            \"visualization\": [\"feature importance plots\", \"confusion matrix\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Explainable Deep Learning Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"architecture\": \"e.g., Transformer-based, LSTM-based\",\n                \"explainability_techniques\": [\"GradCAM\", \"LIME\", \"SHAP\"]\n            },\n            \"description\": \"An advanced deep learning model designed for emotion-based text classification, with built-in interpretability methods.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"point\": \"DAIGT dataset's multi-lingual and genre diversity demands robust feature extraction and handling of linguistic nuances.\"\n        },\n        {\n            \"point\": \"Fair comparison with state-of-the-art requires actively tracking advancements in the field.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"fine-grained feature extraction\",\n            \"domain adaptation techniques\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom model serving platform\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Apache MXNet\"],\n        \"demonstration\": \"A demonstrable system showcasing emotion-based classification and explainability.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the given schema. The high-level requirements of the project (intent, expertise, task, dataset, and model) are clearly expressed. However, specific metric targets and complexity values are not provided, as they depend on the chosen techniques and the nature of the project. The user would need to specify target performance levels, model specifics, and other details to complete the response.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "text classification", "instruction": "As an advanced project manager for an innovative AI text classification initiative, devise a sophisticated, architecture-agnostic approach that integrates state-of-the-art deep learning models like DenseNet and Inception-ResNet for convolution, ensemble SVM models combining linear and non-linear kernels, and custom gradient-boosted forests with Tree Parzen Estimators. Utilize the massive and multi-domain 20 Newsgroups dataset (100,000+ unique documents) with transfer learning for enhanced performance. Emphasize interpretability by implementing not only SHAP and LIME, but also ProtoPNet for visual explanations. Optimize hyperparameters through a hybrid optimization strategy involving Bayesian and evolutionary algorithms, considering interpretability-performance trade-offs. Evaluate scalability, efficiency, and energy efficiency across a heterogeneous infrastructure consisting of cutting-edge GPUs (NVIDIA A100 and H100), TPUv4 for accelerated tensor processing, and eco-friendly server clusters (AMD EPYC) to accommodate variable workloads. Implement real-time model refinement through federated learning with differential privacy, while ensuring compatibility with upcoming quantum computing interfaces. Constrict the project to adhere to the stringent NIST AI ethics principles and the updated ISO/IEC 9001:2022 guidelines, while maintaining data sovereignty and respecting user privacy in a global, 12-month timeline that involves cross-functional collaboration with ethical AI design principles.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"innovative AI\",\n        \"description\": \"Design a sophisticated AI text classification initiative using deep learning models (DenseNet, Inception-ResNet), ensemble SVMs (linear and non-linear), gradient-boosted forests with Tree Parzen Estimators, and transfer learning with the 20 Newsgroups dataset (100,000+ documents). Emphasize interpretability with SHAP, LIME, and ProtoPNet. Optimize hyperparameters with a hybrid Bayesian and evolutionary approach, balancing interpretability and performance. Evaluate scalability, efficiency, and energy efficiency across heterogeneous infrastructure (NVIDIA A100, H100 GPUs, TPUv4, AMD EPYC) for federated learning with differential privacy. Ensure compatibility with quantum computing and adhere to NIST AI ethics principles and ISO/IEC 9001:2022 guidelines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"intermediate performance metrics (e.g., precision, recall)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"documents/second\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"kWh/operation\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"classification labels\"],\n            \"specification\": {\n                \"number of documents\": 100000,\n                \"document size\": null,\n                \"domains\": \"multi-domain\"\n            },\n            \"description\": \"Massive and multi-domain dataset with over 100,000 unique documents for text classification.\",\n            \"preprocessing\": [\"transfer learning\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"SHAP\",\n                \"LIME\",\n                \"ProtoPNet\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DenseNet + Inception-ResNet + Ensemble SVMs + Gradient-Boosted Forests\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"details\": [\n                    {\n                        \"model component\": \"DenseNet\",\n                        \"description\": \"State-of-the-art\"\n                    },\n                    {\n                        \"model component\": \"Inception-ResNet\",\n                        \"description\": \"State-of-the-art\"\n                    },\n                    {\n                        \"model component\": \"ensemble SVMs\",\n                        \"description\": \"with linear and non-linear kernels\"\n                    },\n                    {\n                        \"model component\": \"gradient-boosted forests with Tree Parzen Estimators\",\n                        \"description\": \"custom\"\n                    }\n                ]\n            },\n            \"description\": \"A combination of advanced models for text classification\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"aspect\": \"interpretability\",\n            \"description\": \"Use of SHAP, LIME, and ProtoPNet\"\n        },\n        {\n            \"aspect\": \"optimization\",\n            \"description\": \"Hybrid Bayesian and evolutionary optimization for model selection\"\n        },\n        {\n            \"aspect\": \"infrastructure\",\n            \"description\": \"Federated learning with differential privacy across diverse hardware\"\n        },\n        {\n            \"aspect\": \"ethics and guidelines\",\n            \"description\": \"Adherence to NIST AI ethics principles and ISO/IEC 9001:2022 standards\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"preprocessing for transfer learning\"],\n        \"target_device\": [\"NVIDIA A100, H100 GPUs\", \"TPUv4\", \"AMD EPYC\"],\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"efficient tensor processing\"],\n        \"demonstration\": \"real-time model refinement and interpretability explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the main aspects of the project based on the user's description, including the various models, datasets, performance metrics, and ethical considerations. However, specific performance targets and some detailed model specifications are not provided as they are not directly stated in the given instruction. The confidence score is set to 0.9 based on the confidence in the structure and clarity of the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Create a highly scalable, fault-tolerant time series forecasting platform for the complex in-app user behavior (DAU) in our exclusive mobile game. Integrate and elevate three advanced deep learning architectures - HAttnLSTM with attention mechanism refinement, Prophet with dynamic and adaptive seasonality, and ARIMA with domain-specific and real-time external factors. Develop a sophisticated preprocessing pipeline that combines advanced techniques like Tucker tensor decomposition, DWT-SparseTSC, and hybrid wavelet-TBATS for enriched feature representation. Implement adaptive regularization techniques, such as L1, L2, and an innovative dropout strategy tailored for sparse gaming data with evolving trends. Design a new evaluation metric, \"Weighted and Time-varying Horizon Mean Absolute Percentage Error with Adaptive Windowing\" (WT-HMAPEw), which accounts for non-stationarity and varying error sensitivity across different time periods. Conduct an exhaustive comparative analysis of your model against the latest cutting-edge methodologies in a broad, cross-genre game history dataset. Prepare a visually immersive and insightful report with interactive forecasts, to showcase the model's prowess in forecasting short-term spikes and long-term user retention patterns with real-time sensitivity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"gaming\",\n        \"description\": \"Develop a highly scalable and fault-tolerant time series forecasting platform for in-app user behavior (DAU) in a mobile game, integrating HAttnLSTM, Prophet, and ARIMA. Use Tucker tensor decomposition, DWT-SparseTSC, and a hybrid wavelet-TBATS preprocessing pipeline. Implement adaptive regularization techniques and a new evaluation metric, WT-HMAPEw, accounting for non-stationarity and varying error sensitivity. Conduct comprehensive model comparison against state-of-the-art methodologies in a cross-genre game history dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Weighted and Time-varying Horizon Mean Absolute Percentage Error with Adaptive Windowing\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Game DAU Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"user_behavior\"],\n            \"specification\": {\n                \"features\": \"complex, sparse, evolving trends\",\n                \"time_range\": {\n                    \"start\": \"unknown\",\n                    \"end\": \"latest game data\"\n                }\n            },\n            \"description\": \"A non-stationary time series dataset capturing complex in-app user behavior across different time periods.\",\n            \"preprocessing\": [\n                \"Tucker tensor decomposition\",\n                \"DWT-SparseTSC\",\n                \"hybrid wavelet-TBATS\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"interactive forecasts\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HAttnLSTM, Prophet, ARIMA\",\n            \"family\": \"deep learning architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectures\": [\"HAttnLSTM with attention refinement\", \"Prophet with dynamic seasonality\", \"ARIMA with external factors\"]\n            },\n            \"description\": \"Integration of multiple advanced deep learning models for time series forecasting in the gaming domain.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced preprocessing for sparse gaming data\",\n        \"WT-HMAPEw as a new evaluation metric\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Tucker tensor decomposition\",\n            \"DWT-SparseTSC\",\n            \"wavelet-TBATS hybrid\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"visual report showcasing forecasts and sensitivity\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response parses the given instruction accurately, considering the user's intent to build a platform, their high level of expertise, and the specifics of the problem in the gaming domain. Performance metric value is left as null for WT-HMAPEw, as it's not specified in the instruction. Other details like data source and preprocessing steps are captured based on the requirements. However, since the end-of-life for deployed models and specific deployment details are not provided, confidence score is slightly lower.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager, devise a cutting-edge QA model by integrating TinyBERT with a modular, device-sensitive fusion technique specifically tailored for mobile news filtering. Mandate a NewsQA dataset benchmark surpassing 97% precision, optimizing for under 125 ms response times on resource-constrained devices, and guarantee cross-platform consistency, energy efficiency, and adherence to stringent EU data protection regulations, including GDPR.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"news and information retrieval\",\n        \"description\": \"Develop a cutting-edge QA model by integrating TinyBERT with a device-sensitive fusion technique suitable for mobile news filtering. The model should surpass a NewsQA dataset benchmark of 97% precision, with a response time of under 125 ms on resource-constrained devices, while ensuring cross-platform consistency, energy efficiency, and GDPR compliance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 125,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NewsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"benchmark\": {\n                    \"precision\": {\n                        \"minimum_value\": 0.97\n                    }\n                }\n            },\n            \"description\": \"A dataset for evaluating question answering models on mobile news filtering, targeting high precision\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT with Modular Fusion\",\n            \"family\": \"transformers (TinyBERT)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimized_for\": \"mobile devices\",\n                \"energy_efficiency\": true,\n                \"gdpr_compliant\": true\n            },\n            \"description\": \"A state-of-the-art QA model integrating TinyBERT with a device-sensitive fusion technique, designed for high precision mobile news filtering\"\n        }\n    ],\n    \"knowledge\": [\n        \"Mobile optimization is essential for fast response times.\",\n        \"TinyBERT provides efficient transformer-based performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource-constrained environments\"],\n        \"demonstration\": \"cross-platform and GDPR compliant\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent (to build a model), high expertise level, and the specific requirements for integrating TinyBERT, optimizing for mobile devices, and meeting GDPR. The problem description, performance metric (precision), and response time constraint are directly derived from the instruction. The dataset requirement and model specifications match the task, and service requirements such as energy efficiency, cross-platform consistency, and GDPR adherence are noted. However, since the exact fusion technique is not specified, it is left to an expert's discretion.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "As the head of the elite AI translation innovation lab, your directive is to develop a cutting-edge transformer architecture with not just WMT19's multilingual extensive and varied commentary, focusing on advanced cross-lingual adaptability and high-precision transfer learning through the integration of multiple parallel corpus sources. Emphasize a meticulous, iterative development process with a target of achieving a superior BLEU score of 49, while maintaining a 0.95 Pearson's correlation coefficient for semantic equivalence. Scrutinize METEOR, TER, and ROUGE-L scores, as well as incorporating TIDE for nuanced evaluation. Additionally, design an energy-efficient training strategy with a commitment to carbon neutrality, and meet the demanding deadline of a 15-month project, divided into six-phased milestones.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language technology\",\n        \"description\": \"Develop a cutting-edge transformer architecture for AI translation with WMT19's multilingual extensive commentary dataset, targeting advanced cross-lingual adaptability and high-precision transfer learning. Milestones include achieving a BLEU score of 49, a Pearson's correlation coefficient of 0.95 for semantic equivalence, and using METEOR, TER, ROUGE-L, and TIDE evaluation metrics. The project should be energy-efficient and eco-friendly with a 15-month timeline and six-phased milestones.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU Score\",\n                \"value\": 49\n            },\n            {\n                \"name\": \"Pearson's Correlation Coefficient\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Energy Efficiency\",\n                \"value\": \"required\"\n            },\n            {\n                \"name\": \"Carbon Neutrality\",\n                \"value\": \"required\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 Multilingual Commentaries\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"corpus_size\": \"extensive and varied\",\n                \"parallel_corpus_sources\": true\n            },\n            \"description\": \"A multilingual dataset for advanced translation, containing a large and diverse set of commentary for cross-lingual learning.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Transformer Architecture\",\n            \"family\": \"Transformer (including the integration of multiple parallel corpora)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"cross-lingual adaptability\": \"state-of-the-art\",\n                \"transfer learning capability\": \"high\"\n            },\n            \"description\": \"A transformer model designed for high-performance machine translation with emphasis on cross-lingual generalization and parallel corpus integration.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Targeting advanced cross-lingual adaptability and high-precision transfer learning requires exploration of diverse and parallel data sources.\",\n        \"The project should prioritize metric evaluation methods, including BLEU, METEOR, TER, ROUGE-L, and TIDE for comprehensive performance assessment.\",\n        \"Energy-efficient training and carbon neutrality are essential requirements for the development process.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"iterative development\", \"tuning for efficiency\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"phase-based and performance updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the high-level description of the project, including the objectives for the cutting-edge AI translation model. The user's intent and expertise level are specified, along with the necessary performance metrics, complexity requirements, dataset, and model specifications. However, specific preprocessing steps, energy efficiency strategies, and deployment targets were not included in the provided instruction, so they are left as unspecified in the JSON. The confidence score is high due to the clear direction provided.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "Forge a state-of-the-art node classification solution on the NELL data corpus employing apatent Hypergraph Neural Network with nested, intra-level self-attention mechanisms. Mandate the system to handle dynamic, real-time edge modifications with high resilience (97% F1 score), robustness against anomalies, and adaptability to diverse topologies. Implement a multi-faceted optimization strategy that balances edge density, centrality measures, and resistance to synthetic noise, requiring detailed explanations for each decision, accompanied by interactive visualizations and a comprehensive comparison with contemporary methods, including ensemble techniques.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic/research\",\n        \"description\": \"Forge a state-of-the-art node classification solution on the NELL data corpus using a patent Hypergraph Neural Network with nested, intra-level self-attention mechanisms. The system should handle dynamic, real-time edge modifications with a high F1 score (97%), be robust against anomalies, and adaptable to diverse topologies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL corpus\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dynamic_edges\": true,\n                \"anomaly-resistant\": true,\n                \"diverse_topologies\": true\n            },\n            \"description\": \"A graph dataset for node classification using the NELL data corpus with a focus on handling dynamic, real-time edge updates.\",\n            \"preprocessing\": [\"handle edge modifications\", \"anomaly detection\"],\n            \"augmentation\": [],\n            \"visualization\": [\"nested self-attention mechanism\", \"edge density & centrality measures\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hypergraph Neural Network\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention_mechanism\": \"nested, intra-level self-attention\",\n                \"resilience\": \"anomaly-resistant\",\n                \"adaptability\": \"diverse topology handling\"\n            },\n            \"description\": \"A highly advanced, patent-protected model designed for node classification on NELL data corpus, incorporating self-attention mechanisms and resilience against edge modifications.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Intra-level self-attention enhances information sharing across nodes.\",\n        \"Dynamic edge handling is crucial for real-time scenarios.\",\n        \"High resilience is vital for detecting and mitigating anomalies.\",\n        \"Adaptability to diverse topologies ensures model versatility.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"optimization for edge density, centrality measures, and noise resistance\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"explanations\": \"methodologies behind each decision and comparison\",\n            \"visualizations\": \"interactive, showcasing model performance and decision making\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a state-of-the-art model, incorporates the high-level expertise, and details the task, including performance metrics and multi-faceted optimization requirements. It includes data set specifics and the desired model characteristics. However, the specific ensemble techniques for comparison are not mentioned, but ensemble models are a common target for comparison in such projects.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "For the next-generation AI-driven website analytics, we need a state-of-the-art anomaly detection system capable of real-time, multi-faceted monitoring: not just source and user behavior, but also device type, geographical location, and time of day. Expect a deep-learning model that integrates historical patterns, flags probable security threats, and generates prioritized, context-rich reports with immediate response recommendations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"web analytics\",\n        \"description\": \"Develop a state-of-the-art anomaly detection system for real-time, multi-faceted monitoring of website analytics data, including source, user behavior, device type, geographical location, and time of day. The model should leverage deep learning and be capable of flagging security threats, generating prioritized reports with immediate response recommendations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Website Analytics Data\",\n            \"modality\": [\n                \"time series\"\n            ],\n            \"target_variables\": [\n                \"anomaly detection\"\n            ],\n            \"specification\": {\n                \"historical data period\": null,\n                \"data frequency\": null\n            },\n            \"description\": \"A time-series dataset with various attributes for source, user behavior, device type, location, and time of day for real-time anomaly detection.\",\n            \"preprocessing\": [\n                \"time series normalization\",\n                \"feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"anomaly detection patterns\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Learning Anomaly Detection\",\n            \"family\": \"Autoencoder or RNN-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep neural network\",\n                \"layers\": null,\n                \"training data size\": null,\n                \"model efficiency (FLOPs)\": null\n            },\n            \"description\": \"A deep learning model, potentially an autoencoder or RNN, designed for real-time anomaly detection in diverse website analytics data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art techniques for multi-faceted monitoring require advanced deep learning models\",\n        \"The model must balance precision and response time for security threat detection\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"historical pattern integration\"],\n        \"target_device\": \"real-time processing\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": \"contextualized anomaly detection dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the schema, capturing the user's intent to build a high-expertise model. The problem area (time-series analysis), downstream task (anomaly detection), and performance metrics are derived from the given task. Dataset information is deduced from the need for historical patterns and real-time monitoring. However, specific model architecture details and expected performance values are not provided as they are not explicitly mentioned in the instruction.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As the Project Manager for a cutting-edge deep learning team working on a niche mineral strength prediction project, your mission is to boost the predictive accuracy of a customized neural network model for the rare Earth element hardness dataset. This dataset comprises a complex array of minute, non-linearly correlated numerical attributes, distributed unevenly across 1,000,000 training samples, 50,000 validation instances, and a undisclosed test set for commercial confidentiality. Your strategy must streamline feature extraction using domain-specific knowledge, fine-tune parameters using a Bayesian optimization algorithm, and explore advanced ensemble techniques such as stacking with rare data handling, all while striving for a groundbreaking decrease of 1% in MedAE compared to the current industry benchmark.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"mineral industry\",\n        \"description\": \"Boost the predictive accuracy of a customized neural network model for rare Earth element hardness dataset, with complex numerical attributes and non-linear correlations. Strive for a 1% improvement in MedAE compared to the industry benchmark, leveraging domain-specific feature extraction, Bayesian optimization, and advanced ensemble techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Median Absolute Error (MedAE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Rare Earth Element Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"hardness\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": 1000000,\n                    \"validation\": 50000,\n                    \"test\": \"undisclosed for commercial confidentiality\"\n                },\n                \"attribute_distribution\": \"uneven across non-linearly correlated numerical attributes\"\n            },\n            \"description\": \"A dataset for rare earth element hardness prediction with complex attribute distribution and large sample sizes.\",\n            \"preprocessing\": [\n                \"domain-specific feature extraction\",\n                \"data normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Customized Neural Network\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"customized\",\n                \"optimization_algorithm\": \"Bayesian\",\n                \"ensemble_techniques\": [\"stacking\"],\n                \"handling_rare_data\": true\n            },\n            \"description\": \"A deep learning model specifically designed for rare earth element hardness prediction, including feature extraction and advanced optimization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Domain-specific feature extraction is critical for such a complex dataset.\",\n        \"Bayesian optimization can help fine-tune parameters for better performance.\",\n        \"Ensemble techniques with rare data handling are required to meet the 1% improvement goal.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific feature extraction\", \"feature selection\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Bayesian inference\"],\n        \"demonstration\": \"prediction improvement demonstration using performance metrics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is built based on the provided instruction, mapping the user's intention to build a model, the high expertise level, and the requirements for a regression model with complex data and domain knowledge. Performance metrics, including MedAE, are specified, and the need for optimization and ensemble techniques is highlighted. However, due to the lack of specific detail on the current industry benchmark, the value for MedAE improvement cannot be given. Inference time is marked as uncertain due to missing information.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager overseeing the \"AI Enhanced Wildlife Camera Trap Analysis System,\" your team is now responsible for leveraging the iWildCam Wildlife Image Dataset to design a scalable platform. Straddle the line between using a lightweight, interpretable Decision Tree-based model to quickly process huge volumes of wildlife photos in remote locations with limited connectivity, and a cutting-edge Federated Learning ensemble model that promises exceptional species classification accuracy but necessitates intricate privacy-preserving techniques. Conduct a meticulous comparison of the energy consumption, data security implications, and the system's ability to recognize and classify multiple species in near real-time, ensuring minimal disruption to the ecosystem and a user interface that provides biologist-friendly insights within a median latency of 1 second.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"Develop an AI Enhanced Wildlife Camera Trap Analysis System using the iWildCam Wildlife Image Dataset. The system should balance between a lightweight, interpretable Decision Tree-based model for fast processing and a Federated Learning ensemble model for high species classification accuracy, ensuring privacy, low energy consumption, near real-time capabilities, and minimal ecological impact.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"energy consumption\",\n                \"value\": null\n            },\n            {\n                \"name\": \"species classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"median latency\",\n                \"value\": 1\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"Joules per classification\"\n            },\n            {\n                \"name\": \"data security\",\n                \"value\": null,\n                \"unit\": \"Privacy preserving techniques\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"iWildCam Wildlife Image Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"dimension\": {\n                    \"image\": null\n                },\n                \"length\": null\n            },\n            \"description\": \"Dataset for wildlife image analysis, suitable for both Decision Tree and Federated Learning approaches.\",\n            \"preprocessing\": [\"image resizing\", \"cropping\", \"normalization\"],\n            \"augmentation\": [\"random crops\", \"color jitter\"],\n            \"visualization\": [\"dataset distribution\", \"species examples\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Decision Tree-based\",\n            \"family\": \"interpretable\",\n            \"type\": \"classical machine learning\",\n            \"specification\": {\n                \"complexity\": \"lightweight\",\n                \"latency\": null\n            },\n            \"description\": \"Lightweight model for rapid processing of wildlife images.\"\n        },\n        {\n            \"name\": \"Federated Learning ensemble model\",\n            \"family\": \"privacy-preserving\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": null,\n                \"privacy_protection\": null\n            },\n            \"description\": \"Cutting-edge model with high species classification accuracy and privacy measures.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Energy consumption comparison: Decision Trees vs. Neural Networks\",\n        \"Federated Learning privacy implications and data security trade-offs\",\n        \"Striking balance between processing speed and accuracy in the wild\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"data preprocessing\", \"ensemble technique\"],\n        \"target_device\": \"remote locations with limited connectivity\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"cloud-based\", \"Federated learning compatible\"],\n        \"demonstration\": \" biologist-friendly UI with near real-time classification\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been crafted based on the provided instruction. The user's intent to build a system and high level of expertise are indicated. The problem area, downstream task, and application domain are identified. The iWildCam dataset details are captured, along with specific requirements for models and performance metrics. However, missing specific values like energy consumption, accuracy, and latency were left as null since they are not explicitly mentioned in the instruction. The confidence score is high due to the clear mapping from the instruction to the JSON structure.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As the project manager, your mandate now involves not only managing a cutting-edge mobile app that integrates a sophisticated EfficientNet-fueled Fully Convolutional Network (FCN) for fashion item segmentation, but also ensuring the system can identify and differentiate intricate, densely layered clothing items within the challenging DeepFashion2 dataset. The system must consistently achieve accuracy of at least 77%, while maintaining ultra-realtime performance with a strict latency ceiling of 150ms to guarantee a frictionless user interaction experience across various device platforms and under diverse network conditions. Additionally, outline a robust quality assurance plan for cross-validation across multiple sub-datasets and scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"fashion technology\",\n        \"description\": \"As a project manager, the task is to develop a mobile app with an EfficientNet-fueled Fully Convolutional Network for fashion item segmentation, specifically focusing on intricate, densely layered clothing in the DeepFashion2 dataset. The system must achieve at least 77% accuracy, operate in ultra-realtime with a latency ceiling of 150ms, and ensure seamless user experience across various platforms and network conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.77\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 150,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepFashion2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"complexity\": \"intricate, densely layered clothing\"\n            },\n            \"description\": \"A challenging dataset for fashion item segmentation with emphasis on detailed and layered clothing.\",\n            \"preprocessing\": [\n                {\n                    \"step\": \"EfficientNet data augmentation\"\n                }\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"EfficientNet-FCN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"sophisticated, suitable for image segmentation\",\n                \"runtime-performance\": \"ultra-realtime\"\n            },\n            \"description\": \"A high-performance FCN using EfficientNet as the backbone, designed for fashion item segmentation in mobile applications.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Effective handling of DenseLayered clothing is crucial for segmentation accuracy.\",\n        \"Robustness to diverse network conditions and platforms for ultra-realtime performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"EfficientNet-specific feature extraction\"],\n        \"target_device\": [\"mobile, IoT, edge devices\"],\n        \"deployment_endpoint\": \"frictionless user interaction experience\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"cross-validation across multiple sub-datasets and scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the task, from the model choice to the performance targets and quality assurance requirements. It accounts for the user's high expertise level and the emphasis on accuracy and latency. However, some specific model specifications (like the exact architecture or the techniques for achieving ultra-realtime performance) were left unspecified for the JSON to be more generic. Similarly, though the importance of cross-validation and multiple scenarios is recognized, the details on how to implement them aren't included.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the enhanced text analytics initiative, you must now not only analyze the 20 Newsgroups dataset with five cutting-edge models (BERT, SVM, LSTM, FastText, and GNN) for accuracy, robustness, and explainability, but also conduct an in-depth comparison of their environmental impact (energy consumption) during training. Additionally, optimize model performance under limited GPU resources and track the effects of incremental fine-tuning, all while meeting a stricter deadline under dynamic market competition.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Analyze the 20 Newsgroups dataset using five cutting-edge models (BERT, SVM, LSTM, FastText, and GNN) for accuracy, robustness, and explainability. Compare their environmental impact during training and optimize performance under limited GPU resources. Also, investigate the effects of incremental fine-tuning while meeting a stricter deadline in a competitive market.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explainability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy consumption\",\n                \"value\": null,\n                \"unit\": \"kWh\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"20 Newsgroups\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"split_ratio\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                }\n            },\n            \"description\": \"A text classification dataset with newsgroup topics.\",\n            \"preprocessing\": [\"data preprocessing for NLP models\"],\n            \"augmentation\": [],\n            \"visualization\": [\"compare model performance and interpretability\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT\",\n            \"family\": \"Transformer-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        },\n        {\n            \"name\": \"SVM\",\n            \"family\": \"Support Vector Machines\",\n            \"type\": \"classical machine learning\",\n            \"specification\": null\n        },\n        {\n            \"name\": \"LSTM\",\n            \"family\": \"Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        },\n        {\n            \"name\": \"FastText\",\n            \"family\": \"N-gram models\",\n            \"type\": \"statistical models\",\n            \"specification\": null\n        },\n        {\n            \"name\": \"GNN\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null\n        }\n    ],\n    \"knowledge\": [\n        \"Dynamic market competition requires timely adaptation and model selection based on performance and efficiency.\",\n        \"Limited GPU resources may require model pruning, quantization, or model size optimization strategies.\",\n        \"Fine-tuning can impact both performance and environmental impact, so tracking these effects is crucial.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"GPU resource optimization techniques\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"energy-efficient inference methods\"],\n        \"demonstration\": \"Performance comparison and training progress tracking\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the requirements from the given instruction, including various model comparisons, environmental impact analysis, and optimization strategies. The user's intent is to build and compare models, and their expertise level is inferred as high due to the project management role. Performance metrics and model specifics are left unspecified for now, as they require specific quantitative data or models' architectural details. The confidence score is slightly lower due to the lack of precise metric values and some details that could not be directly inferred from the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager for a specialized deep learning team, your mandate is to develop a transformer-based neural machine translation system that can fluently translate between English, Japanese, Spanish, and German. The model must exhibit exceptional performance in deciphering regional variations within each language, particularly focusing on subialects used in online social media. Demand accuracy of 98% or higher for colloquial expressions from non-standard dialects in Common Crawl data, ensuring seamless cultural adaptation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Develop a transformer-based neural machine translation system targeting English, Japanese, Spanish, and German languages. The model should handle regional variations, including subdialects from online social media with a minimum accuracy of 98% for colloquial expressions from non-standard dialects in Common Crawl data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"handling non-standard dialects\",\n                \"value\": null,\n                \"unit\": \"% of in-domain data\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"languages\": [\"English\", \"Japanese\", \"Spanish\", \"German\"],\n                \"non-standard dialect data\": {\n                    \"source\": \"Common Crawl\",\n                    \"percentage\": null\n                }\n            },\n            \"description\": \"Multilingual dataset with a focus on colloquial expressions from non-standard dialects for training translation model.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"subdialect identification\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-based Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer\",\n                \"language pairs\": [\"English-Japanese, Japanese-English, Spanish-English, German-English, English-Spanish, English-German\"],\n                \"focus on regional variations\": true\n            },\n            \"description\": \"A high-performing transformer model designed for precise translation of non-standard dialects in social media content.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transformer models are effective for handling long-range dependencies and translating between multiple languages.\",\n        \"Cultural adaptation is crucial for online language translations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"linguistic adaptation techniques\",\n            \"dialect-specific adaptation\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Translation of colloquial expressions showcasing regional variations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the provided user instruction. The intent, problem area, specific language handling, high accuracy requirement, and dataset properties are all captured accurately. However, some specific model details like exact architecture and feature engineering steps were not detailed as they would typically involve the technical specifications and implementation details beyond the given task description. The confidence score is high given the clear understanding of the task but may be lowered if more specific details are required.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a highly specialized, fault-tolerant GNN architecture for the Dynamic Knowledge Graph (DKG) in OpenKE, utilizing the massive Wikidata5m dataset. The model should exhibit exceptional few-shot learning prowess, targeting an F1 score above 98% for previously unencountered entity pairs. Implement an innovative, real-time learning mechanism that ensures continuous data assimilation without significant performance decline, while demonstrating efficacy through a stringent subgraph-wise nested cross-validation protocol. Additionally, devise a dynamic resource management strategy for the model across a heterogeneous cluster, maximizing parallel efficiency and minimizing energy footprint while accounting for hardware heterogeneity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graph\",\n        \"description\": \"Develop a highly specialized, fault-tolerant GNN architecture for the Dynamic Knowledge Graph (DKG) using OpenKE, focusing on the Wikidata5m dataset. The model should exhibit exceptional few-shot learning and target an F1 score above 98% for unseen entity pairs. Implement a real-time learning mechanism for continuous data assimilation without performance drop and validate through a nested cross-validation protocol. Also, design a dynamic resource management strategy for heterogeneous clusters optimizing parallel efficiency and minimizing energy consumption.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time learning\",\n                \"value\": null,\n                \"unit\": \"continuous\"\n            },\n            {\n                \"name\": \"parallel efficiency\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"energy footprint\",\n                \"value\": null,\n                \"unit\": \"Joules\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wikidata5m\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"massive\",\n                \"dynamic\": true\n            },\n            \"description\": \"A large-scale dynamic knowledge graph dataset suitable for few-shot learning and GNN research.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Fault-tolerant GNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"few-shot learning\": true,\n                \"resource_management\": \"dynamic\"\n            },\n            \"description\": \"A specialized GNN architecture designed for dynamic knowledge graphs in OpenKE, capable of few-shot learning and high F1 score.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time learning is crucial for assimilating new data without performance degradation.\",\n        \"Few-shot learning and high F1 performance on unseen entity pairs require a robust and innovative architecture.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"heterogeneous cluster\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"nested cross-validation with subgraph-wise evaluation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent (building a specialized GNN model), their level of expertise, and the problem's details. The area, downstream task, and dataset properties are derived from the instruction. Performance metric (F1 score) and a desired value (98%) are included. The model family, type, and certain specifications are inferred. However, specific metric values like real-time learning efficiency and energy footprint are not given since they are expected to be calculated or optimized during model development. The nested cross-validation protocol and resource management strategy are also clearly defined. The confidence score is high due to the clarity of the task.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "node classification", "instruction": "For the Coauthorship Science project, develop a highly specialized node classification model that combines a hybrid pruning strategy with a dynamic, temporally adaptive spectralHGNN. The model must integrate both citation networks and domain-specific feature extraction, ensuring interpretability and resilience to evolving academic trends. In addition, optimize for precision at the subfield level with at least 95% accuracy, while demonstrating robustness across a wide range of sub-disciplines and handling evolving authorship styles.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"scholarly research\",\n        \"description\": \"For the Coauthorship Science project, design a specialized node classification model combining a hybrid pruning strategy with a dynamic, temporally adaptive spectralHGNN. The model must incorporate citation networks and domain-specific feature extraction, prioritize interpretability and resilience to academic trends, and aim for at least 95% precision at the subfield level while demonstrating cross-disciplinary robustness and adaptability to evolving authorship styles.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision (subfield level)\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coauthorship Science\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"network_structure\": \"citation and domain-specific feature graphs\",\n                \"temporal_adaptation\": true\n            },\n            \"description\": \"A dataset for node classification that integrates citation networks and domain-specific feature extraction in the academic context of Coauthorship Science.\",\n            \"preprocessing\": [\n                \"feature extraction\",\n                \"temporal pruning\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Pruned Dynamic SpectralHGNN\",\n            \"family\": \"Hybrid Pruning & Adaptive Spectral Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"incorporated\",\n                \"resilience_to_trends\": \"true\",\n                \"temporal_adaptation\": \"true\"\n            },\n            \"description\": \"A specialized model that combines pruning and adaptive spectral graph neural networks, specifically tailored for node classification in the Coauthorship Science project.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Domain-specific feature extraction enhances model performance in scholarly research.\",\n        \"Hybrid pruning strategies improve model efficiency while maintaining performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain-specific feature engineering\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"academic platform or hosting service\",\n        \"inference_engine\": [\"GPU optimized\"],\n        \"demonstration\": \"user-friendly interactive visualization of model predictions and insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the user's intent to build a model for the given Coauthorship Science project, the high level of expertise, and the requirements specified. The model description is based on the given hybrid pruning and dynamic temporal adaptations. Performance metrics, like precision at 95% accuracy, are set as per the instructions. However, to complete the confidence, specific feature engineering steps and demonstration methods could benefit from more specific details based on the user's preferences.\",\n        \"score\": 0.92\n    }\n}\n"}
{"task": "node classification", "instruction": "Task the machine learning development team to develop a state-of-the-art Graph Wavelet Neural Network (GWNN) architecture, leveraging the Mouse Brain Connectivity dataset. The objective is not only to achieve superlative node classification accuracy surpassing 97%, but also to deliver superior F1 score, scalability, and resilience to various spatial scales and outliers. Emphasize the integration of a sophisticated two-way feature fusion strategy, while maintaining model explainability amidst the intricacies of the dataset's rich connectivity structure. Additionally, devise a systematic hyperparameter tuning process to ensure optimal performance and generalization capabilities.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Task the machine learning team with developing a state-of-the-art Graph Wavelet Neural Network (GWNN) for the Mouse Brain Connectivity dataset. Aim for accuracy above 97%, superior F1 score, scalability, and resilience to different spatial scales and outliers. Incorporate a two-way feature fusion strategy and prioritize model explainability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mouse Brain Connectivity\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"nodes\": null,\n                    \"edges\": null\n                },\n                \"spatial_dimensions\": null,\n                \"connectivity_structure\": \"rich\"\n            },\n            \"description\": \"A dataset representing the complex connectivity structure of a mouse brain, suited for node classification tasks with Graph Wavelet Neural Networks.\",\n            \"preprocessing\": [\n                \"data normalization\",\n                \"denoising\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"graph layout analysis\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Wavelet Neural Network\",\n            \"family\": \"GWNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"fusion_strategy\": \"two-way\",\n                    \"explainability\": true\n                },\n                \"hyperparameters\": {\n                    \"optimization\": \"systematic tuning\",\n                    \"performance_generalization\": true\n                }\n            },\n            \"description\": \"A state-of-the-art Graph Wavelet Neural Network designed for node classification in the Mouse Brain Connectivity dataset with a focus on advanced features, scalability, and explainability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"A sophisticated two-way feature fusion is necessary to handle the rich connectivity structure.\",\n        \"Model explainability is crucial given the intricate nature of the mouse brain connectivity data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"node attribute extraction\",\n            \"structural embeddings\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"lightweight inference framework\"\n        ],\n        \"demonstration\": \"targeted visual explanations of node classifications\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's request to develop a state-of-the-art GWNN, with specific requirements for accuracy, F1 score, and model properties like scalability, explainability, and hyperparameter tuning. The dataset details, model architecture, and preprocessing steps are derived directly from the instruction, while leaving room for additional details that may need to be filled based on a more in-depth understanding of the Mouse Brain Connectivity dataset and the team's preferences.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a seasoned project manager, your mandate now involves refining the existing financial news classifier into a cutting-edge, ensemble fastText architecture. Using the comprehensive Reuters-21578 dataset, the AI development team must design a model that not only sustains state-of-the-art efficiency but also secures a statistically significant leap of 5% in precision, surpassing benchmark scores by a double-digit margin. Target an impressive 95% accuracy, particularly for niche sectors like intricate insider trading activities and vital earnings announcements, demanding a multi-faceted, error-resistant mechanism. Strive for a stringent 0.5% false positive rate across all classification categories, concurrently ensuring regulatory conformity and fostering unparalleled user confidence in risk assessments. Provide a detailed performance report with explainable AI techniques and data visualization for audit trails.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Refine an existing financial news classifier into a cutting-edge ensemble fastText architecture using the Reuters-21578 dataset. Improve precision by 5% over benchmark scores, target at 95% accuracy with focus on niche sectors like insider trading and earnings announcements, and maintain a 0.5% false positive rate for all categories.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 1.05\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": {\n                    \"total_samples\": \"unknown\",\n                    \"train_split\": \"unknown\",\n                    \"validation_split\": \"unknown\"\n                },\n                \"diversity\": \"multi-label text classification\",\n                \"categories\": [\"financial news\", \"insider trading\", \"earnings announcements\"]\n            },\n            \"description\": \"Comprehensive financial news dataset for text classification\",\n            \"preprocessing\": [\n                \"text normalization\",\n                \"stop words removal\",\n                \"lemmatization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"confusion matrix\",\n                \"ROC curve\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Financial News Classifier (FastText Ensemble)\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"fastText with ensemble of multiple models\",\n                \"performance\": \"state-of-the-art efficiency\",\n                \"accuracy_trend\": \"95% target with improved precision\"\n            },\n            \"description\": \"Advanced text classification model for financial news, focusing on niche sectors with enhanced error resistance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Ensemble models improve performance by combining multiple models.\",\n        \"fastText for text classification is efficient and captures nuances.\",\n        \"Targeting 0.5% false positive rate is crucial for regulatory compliance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"techniques for multi-faceted error reduction\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"specific platform or service\",\n        \"inference_engine\": [\"GPU-optimized\", \"TPU support\"],\n        \"demonstration\": {\n            \"examples\": \"selected news headlines and their classifications\",\n            \"focus\": \"niche sectors and accuracy explanations\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The user requires a detailed and accurate model with specified performance improvements, so the parsed JSON reflects a high level of expertise and a clear understanding of the task. However, details like specific data splits, model structure specifics, and performance improvement strategies are hypothetical and would require further clarification.\",\n        \"score\": 0.90\n    }\n}\n"}
{"task": "text classification", "instruction": "As a seasoned project manager for a cutting-edge medical sentiment analysis team, your mandate is to design a next-generation text classifier utilizing the comprehensive and nuanced WebMD Review dataset. This dataset contains granular, multi-faceted feedback with deep emotional intricacy. Your objective is to not only meet, but surpass industry benchmarks with a model demonstrating superior precision, recall, and F1-score. Emphasize on model explainability, explainable AI methods, and efficient real-time processing to facilitate easy interpretation by busy healthcare professionals seeking to instantly comprehend and act on patient experience trends. Additionally, outline a rigorous validation plan to test the model's robustness across diverse demographics and therapeutic domains.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"medicine, healthcare\",\n        \"description\": \"Design a next-generation text classifier using the WebMD Review dataset with a focus on precision, recall, and F1-score. The model must be explainable, incorporate explainable AI methods, and support real-time processing. The goal is to meet and surpass industry benchmarks while facilitating easy interpretation for healthcare professionals.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WebMD Review Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment_labels\"],\n            \"specification\": {\n                \"granularity\": \"multi-faceted\",\n                \"depth\": \"emotional intricacy\",\n                \"size\": \"\",\n                \"dimensions\": \"\"\n            },\n            \"description\": \"A comprehensive medical sentiment analysis dataset containing detailed and nuanced feedback across diverse demographics and therapeutic domains.\",\n            \"preprocessing\": [\"text preprocessing, tokenization\"],\n            \"augmentation\": [\"word embeddings, synthetic data generation\"],\n            \"visualization\": [\"confusion matrix, ROC curves\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning (e.g., transformers, BERT, or RoBERTa)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": \"XAI methods, model interpretability tools\",\n                \"real-time processing\": true\n            },\n            \"description\": \"A state-of-the-art text classifier designed for complex, multi-faceted medical sentiment analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Target industry benchmarks for precision, recall, and F1-score in medical text classification.\",\n        \"Incorporating explainability and model interpretability in the classifier design.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"handling class imbalance, noise reduction\",\n            \"incorporating domain knowledge\"\n        ],\n        \"target_device\": \"cloud or edge\",\n        \"deployment_endpoint\": \"WebMD API or a custom platform\",\n        \"inference_engine\": [\"RESTful API, real-time streaming\"],\n        \"demonstration\": \"prepared case studies, interactive visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's intent, requirements for a high-level model, and the nature of the WebMD dataset. Performance metrics are left to be filled in as the actual values are not provided. The model type, explainability, and real-time processing are based on the mention of industry benchmarks and the needs for healthcare professionals. The dataset preprocessing and augmentation techniques are derived from the description and the emphasis on robustness across diverse demographics and therapeutic domains.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a visionary project manager overseeing a team specializing in intercultural fantasy literature translation, your mandate is to create a cutting-edge neural MT system. Key objectives are: a) curating a massive, domain-specific dataset from Korean and English web novels featuring sub-genres like mythical adventures and urban fantasy, b) optimizing for near-instantaneous translation with seamless readability, using cutting-edge transformers and adaptive compression algorithms, c) exploring transfer learning strategies for Korean, considering its limited availability in translation resources, d) integrating transformer encoders and decoders with Long Short-Term Memory (LSTM) for enhanced context retention during translation, and e) designing a rigorous user-centric study involving diverse global readers to evaluate translation quality and natural dialog flow in various narrative styles. Can you outline a detailed implementation plan that prioritizes accuracy, efficiency, and user satisfaction?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"intercultural literature translation\",\n        \"description\": \"Create a cutting-edge neural machine translation system for Korean and English web novels with sub-genres of mythical adventures and urban fantasy. The project objectives include: dataset curation, optimization for speed and readability, transfer learning for limited Korean resources, integration of transformer encoders-decoders with LSTM for context retention, and a user-centric evaluation study.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"translation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"speed (per sentence)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"readability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"user satisfaction\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency for near-instantaneous translation\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint (compression algorithm)\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Custom Korean-English Web Novel Dataset\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [\"source language\", \"target language\"],\n            \"specification\": {\n                \"data size\": null,\n                \"source language type\": \"Korean\",\n                \"target language type\": \"English\",\n                \"sub-genres\": [\"mythical adventures\", \"urban fantasy\"]\n            },\n            \"description\": \"A large, domain-specific dataset obtained from Korean and English web novels targeting the mentioned sub-genres.\",\n            \"preprocessing\": [\n                \"data crawling and extraction\",\n                \"domain filtering\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Neural Machine Translation System\",\n            \"family\": \"Transformers and adaptive compression with LSTM integration\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-Encoder + Transformer-Decoder + LSTM\",\n                \"transfer learning approach\": \"exploring for Korean\",\n                \"ensemble techniques\": null\n            },\n            \"description\": \"A state-of-the-art MT system leveraging transformers, compression algorithms, and LSTM for enhanced context preservation.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"data preprocessing for translation context\",\n            \"hybrid model architecture optimization\"\n        ],\n        \"target_device\": \"cloud computing\",\n        \"deployment_endpoint\": \"custom API or platform\",\n        \"inference_engine\": [\"GPU-optimized inference\"],\n        \"demonstration\": \"user-friendly interface for personalized experience\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is parsed based on the given instruction, which specifies a high-level plan for a machine translation project. The user intent is 'build', and the expertise is assumed to be high given the complexity of the project. Details are filled in according to the objectives, such as the dataset curation, model architecture, and performance metrics. However, specific metric values are left as null since they are not provided in the instruction. The missing metrics can be filled after detailed analysis or user input. The confidence score is high, assuming the project plan is a good starting point.\",\n        \"score\": 0.9\n    }\n}"}
{"task": "node classification", "instruction": "For the advanced Coauthorship Science endeavor, design a highly specialized and scalable node classification model by refining the spectralHGNN with advanced pruning techniques, while integrating both heterogeneous citation graphs and an intricate time-variant component. Focus on refining the architecture for distinct sub-disciplines within the sprawling scientific ecosystem, demanding precise author attribution across granular research domains, and optimize for exceptional performance and interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic\",\n        \"description\": \"For the advanced Coauthorship Science project, develop a highly specialized and scalable node classification model by enhancing spectralHGNN with advanced pruning techniques, integrating heterogeneous citation graphs and a time-variant component. Focus on optimizing the architecture for specific sub-disciplines within science, aiming for precise author attribution across granular research domains, prioritizing excellent performance and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": \"high\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Heterogeneous Citation Graphs\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"heterogeneous\": true,\n                \"time-variant\": true\n            },\n            \"description\": \"A dataset representing citation networks between authors across different sub-disciplines in science, incorporating both heterogeneous data and time-variant elements.\",\n            \"preprocessing\": [\"pruning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"spectralHGNN_refined\",\n            \"family\": \"Spectral Hypergraph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_refinement\": \"advanced pruning techniques\",\n                \"heterogeneity_integration\": \"true\",\n                \"time-variant_component\": \"incorporated\"\n            },\n            \"description\": \"A highly specialized node classification model for the Coauthorship Science project, optimized for each sub-discipline with improved spectralHGNN, focused on interpretability and superior performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced pruning techniques enhance model efficiency and scalability.\",\n        \"Heterogeneous citation graphs and time-variant components require specialized architectural adjustments.\",\n        \"Strong emphasis on interpretability for insights into research domain attribution.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature refinement for sub-disciplines\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Include clear explanations for interpreting the model's predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a model for the Coauthorship Science project. The model's requirements (high-performance, interpretability, and scalability) are detailed, and the metrics have been incorporated. The source for the dataset is inferred from the problem description and the nature of the task, while the need for feature refinement and interpretability demonstration is explicitly mentioned. However, the accuracy value is left as null since it wasn't specified in the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for a high-tech deep learning unit, your mission involves enhancing a custom regression model for lithium-ion battery performance using the \"Lithium-Ion Battery Advanced Dataset\" with an additional constraint. The dataset now consists of 100 features, each requiring feature extraction and normalization. The train, validation, and test sets are not only non-uniformly distributed but also have varying temporal patterns. Your team must design a model that reduces MAPE and offers quantifiable fairness in predictions. Emphasize interpretability, generate a feature importance ranking with time-evolving coefficients, and prepare a comprehensive report on performance trade-offs and biases.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"material science\",\n        \"description\": \"Enhance a custom regression model for lithium-ion battery performance using the 'Lithium-Ion Battery Advanced Dataset'. The dataset contains 100 features, requiring feature extraction and normalization. The train, validation, and test sets have non-uniform distributions and varying temporal patterns. The goal is to reduce mean absolute percentage error (MAPE), ensure quantifiable fairness, emphasize interpretability with time-evolving feature importance, and produce a comprehensive report on performance trade-offs and biases.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Lithium-Ion Battery Advanced Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"battery performance\"],\n            \"specification\": {\n                \"number_of_features\": 100,\n                \"feature_extraction\": true,\n                \"normalization_required\": true,\n                \"temporal_distribution\": \"non-uniform\",\n                \"temporal_pattern\": \"varying\"\n            },\n            \"description\": \"The dataset for lithium-ion battery performance contains a large number of features that need preprocessing.\",\n            \"preprocessing\": [\"feature extraction\", \"normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Lithium-Ion Battery Regression Model\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true,\n                \"time-evolving_feature_importance\": true\n            },\n            \"description\": \"A deep learning model for lithium-ion battery performance with emphasis on interpretability and feature importance tracking over time.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The dataset's complexity requires feature extraction and normalization to handle large number of features.\",\n        \"Temporal patterns in data distribution necessitate consideration of temporal fairness in predictions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"normalization\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"comprehensive report on performance trade-offs and biases\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is tailored to the provided instruction, with the user's intent and expertise assumed to be high due to the project manager role. The area and downstream task for the problem are specified as tabular regression and battery performance, respectively. Performance metrics (MAPE) and interpretability requirements are included. The dataset details match the given constraints, and fairness and report generation are addressed in the service section. The confidence score is high due to the clear alignment with the provided information.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "text classification", "instruction": "As the seasoned head of the trailblazing AI research division, your directive is to architect a cutting-edge text classifier for the BoolQ dataset, with a focus on refined binary classification (0: null relationship, 1: precise concordance). Mandate the inclusion of an innovative metric, the Semantic Resilience Index, which measures model stability against nuanced semantic variations and syntactic disruptions. Embrace a multifaceted evaluation, demanding not just F1 score and precision/recall, but also incremental performance analysis across diverse domains. Furthermore, devise a cross-modal adaptation strategy to investigate its impact on overall system efficiency in real-world scenarios, ensuring robustness under transformation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Architect a cutting-edge text classifier for the BoolQ dataset, focusing on binary classification with labels 0 (null relationship) and 1 (precise concordance). The classifier should include a novel metric, the Semantic Resilience Index, measuring model stability against semantic variations and syntactic disruptions. The evaluation will involve F1 score, precision/recall, and incremental performance analysis across different domains. A cross-modal adaptation strategy must be devised to study its impact on system efficiency in real-world scenarios, ensuring robustness under transformation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\"\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Semantic Resilience Index\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"label\"],\n            \"specification\": {\n                \"type\": \"binary classification\",\n                \"labels\": [\"null relationship\", \"precise concordance\"]\n            },\n            \"description\": \"A dataset for text classification focused on binary relationships\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art NLP classifier\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"method\": \"Transformer-based architecture\",\n                \"model_innovation\": \"Semantic Resilience Index integration\"\n            },\n            \"description\": \"A cutting-edge NLP model for text classification with an emphasis on semantic resilience and binary relationship detection in the BoolQ dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Semantic Resilience Index measures model stability against nuanced variations in meaning and syntax.\",\n        \"Evaluation includes multiple performance indicators and cross-domain analysis\",\n        \"Cross-modal adaptation strategy aims for real-world system robustness under transformations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"investigation on cross-modal adaptation impact and performance analysis under transformation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately parses the user's requirements for a text classifier on the BoolQ dataset. It reflects the desired high-level architecture, the target metrics, and the cross-modal adaptation strategy. However, specific metric target values and cross-modal adaptation details are not provided, as these require detailed experimentation and might not be easily deduced from the initial instruction. The confidence score is high for the main aspects, but the absence of specific metric targets leaves room for further refinement.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "The project manager is tasked with designing a sophisticated AI-driven retail management system for elite luxury boutiques, utilizing an advanced Mask R-CNN variant tailored to the 'PremiumLuxuryRetail-250K' dataset. The system must demonstrate a minimum 98.5% precision for each of the following five niche categories: haute couture, high-end leather goods, precious gemstone accessories, luxury timepieces, and fine art. In addition to macro performance, a real-time, per-subgenre error analysis is mandatory, with an overall end-to-end error rate of less than 1.5%. This innovative solution should seamlessly integrate into current opex systems, streamlining inventory control and showcasing a return on investment through precise product identification and placement optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail management\",\n        \"description\": \"Design a sophisticated AI-driven retail management system for luxury boutiques using an advanced Mask R-CNN variant tailored to the 'PremiumLuxuryRetail-250K' dataset. Target precision for each of five niche categories (haute couture, high-end leather goods, precious gemstone accessories, luxury timepieces, fine art) must be at least 98.5%, and per-subgenre error analysis with an overall end-to-end error rate below 1.5% is mandatory. The system should integrate seamlessly with existing opex systems for inventory control and ROI optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.985,\n                \"subcategories\": [\n                    {\"name\": \"haute couture\", \"value\": 0.985},\n                    {\"name\": \"high-end leather goods\", \"value\": 0.985},\n                    {\"name\": \"precious gemstone accessories\", \"value\": 0.985},\n                    {\"name\": \"luxury timepieces\", \"value\": 0.985},\n                    {\"name\": \"fine art\", \"value\": 0.985}\n                ]\n            },\n            {\n                \"name\": \"end-to-end error rate\",\n                \"value\": 0.015\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PremiumLuxuryRetail-250K\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A computer vision dataset for advanced object detection, specifically tailored for elite luxury retail, focusing on five niche categories.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Mask R-CNN variant\",\n            \"family\": \"CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"micro\": {\n                    \"accuracy\": {\n                        \"haute couture\": \"98.5%\",\n                        \"high-end leather goods\": \"98.5%\",\n                        \"precious gemstone accessories\": \"98.5%\",\n                        \"luxury timepieces\": \"98.5%\",\n                        \"fine art\": \"98.5%\"\n                    },\n                    \"error_per_subgenre\": true,\n                    \"per_subgenre_mandatory\": true\n                },\n                \"macro\": {\n                    \"end_to_end_error_rate\": \"less than 1.5%\"\n                }\n            },\n            \"description\": \"A specialized deep learning model for object detection with a focus on luxury retail applications.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The integration with opex systems is crucial for inventory control and ROI.\",\n        \"Precision is of utmost importance given the high-end products.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Real-time per-subgenre error analysis and seamless integration with existing opex systems.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's need for a machine learning model (build) with a high level of expertise. The problem statement is well-structured, including the specific requirements for the advanced Mask R-CNN model and the performance metrics. The 'PremiumLuxuryRetail-250K' dataset is directly referenced, and the expected real-time error analysis is captured. However, without more explicit information on preprocessing steps, specific model parameters, or opex system integration details, the confidence score is slightly lower.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "The machine learning development team has been tasked with a multifaceted and high-precision image classification project. They are required to create an advanced classifier that performs hierarchical analysis for the art department, categorizing user-generated content into four distinct levels: elite appreciations, sophisticated feedback, unique 'consumer narratives,' and exceptional masterpieces. The model must exhibit exceptional discernment for diverse artistic styles, subtle product details, and cultural nuances, targeting a minimum benchmark of 99% accuracy on a meticulously curated dataset of 10,000 high-resolution images with intricate, multi-lingual captions. Moreover, the deployed API must deliver real-time responses, demonstrate dynamic adaptability to evolving content, and seamlessly integrate with a complex CMS system, all while ensuring data privacy in strict compliance with the European Union's General Data Protection Regulation (GDPR).", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"art, user-generated content analysis\",\n        \"description\": \"The machine learning team is tasked with creating an advanced image classifier for hierarchical analysis of art department user-generated content into four levels: elite appreciations, sophisticated feedback, unique consumer narratives, and exceptional masterpieces. The model must have high precision, particularly targeting a minimum benchmark of 99% accuracy on a large and diverse dataset of 10,000 high-resolution images with multi-lingual captions. It should provide real-time responses, adapt dynamically, and integrate with a complex CMS while complying with GDPR.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Art Curation Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"category_level\"],\n            \"specification\": {\n                \"size\": 10000,\n                \"image_resolution\": \"high\",\n                \"caption_type\": [\"multi-lingual\"]\n            },\n            \"description\": \"A diverse dataset of 10,000 high-resolution images with intricate multi-lingual captions, curated for a sophisticated art classification task.\",\n            \"preprocessing\": [\n                \"multi-lingual caption processing\",\n                \"image resolution enhancement\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hierarchical Convolutional Neural Network (HCNN) or a deep transfer learning model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep hierarchical, high precision\",\n                \"capable_of\": \"diverse artistic styles, subtle details, cultural nuances\"\n            },\n            \"description\": \"Advanced classifier suitable for multi-level image classification, built on HCNN or a fine-tuned deep learning model.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The challenge lies in the nuanced analysis and the need for GDPR compliance.\",\n        \"Diverse artistic styles and complex CMS integration require tailored model architecture.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"multi-lingual feature extraction\",\n            \"image feature enhancement\"\n        ],\n        \"target_device\": \"real-time, possibly edge or cloud-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"API for seamless CMS integration\", \"GDPR compliant\"],\n        \"demonstration\": \"demonstration of real-time classification and CMS integration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the complexities of the project, including the need for a high-precision model, a large curated dataset, and real-time response. The European Union's GDPR requirements are also included. However, specific model architecture and preprocessing steps may require further clarification based on the team's expertise and the best practices for the specific image classification task in the art context.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "visual question answering", "instruction": "As a renowned project manager for an elite elementary school's trailblazing science fair, you are tasked with designing a cutting-edge VQA system using the state-of-the-art M6 transformer, anchored in the extensive CLEVR-TextVQA dataset with temporal constraints. Ensure not only unparalleled accuracy and explainable AI, but also incorporate real-time interactive elements, context-sensitive proto-type adaptation, and a novel fusion of auto-contextual reasoning and deep multimodal compression. In addition to the 50-page research paper, propose a framework for continuous benchmarking against dynamically evolving competitor models, while adhering to stringent ethical guidelines, and outline an accompanying pedagogical component for student engagement, showcasing your team's exceptional innovation and commitment to ethical AI practices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Design a cutting-edge VQA system using the M6 transformer on CLEVR-TextVQA dataset with temporal constraints, incorporating accuracy, explainable AI, real-time interactivity, context-sensitive adaptation, auto-contextual reasoning, and deep multimodal compression. Include a framework for continuous benchmarking and an ethical pedagogical component for student engagement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explainable AI\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CLEVR-TextVQA\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"temporal_constraints\": true\n            },\n            \"description\": \"State-of-the-art dataset for visual question answering with temporal aspects.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6 Transformer-based VQA System\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time_interactivity\": true,\n                \"context_sensitive_adaptation\": true,\n                \"auto_contextual_reasoning\": true,\n                \"deep_multimodal_compression\": true\n            },\n            \"description\": \"A high-performance VQA system leveraging the M6 transformer, incorporating specified advanced features.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Research papers on M6 transformer and its applications in VQA should be consulted.\",\n        \"Ethical AI guidelines need to be adhered to, including transparency and fairness in model design.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"pedagogical_component\": \"showcasing ethical AI principles\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response provides a clear structure for the user's instruction, including the high-level intent to build a VQA system, expert level, and specific details in the problem description, data requirements, and model expectations. Performance metrics, complexity constraints, and educational components are identified. The response is accurate, but specific values for performance metrics are not provided due to lack of numerical targets in the instruction. Value is set to null, representing the need for the system to achieve the outlined goals.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Design an innovative deep learning fusion model, marrying an LSTM auto-encoder with a state-of-the-art vector autoregressive (VAR) module, for highly accurate forecasting of our app's daily active users. Mandate the integration of a real-time concept drift detection system, a sophisticated Seasonal-Trend decomposition approach capable of handling intricate cycles, and a sophisticated hybrid regularization technique that seamlessly integrates Bayesian priors and adaptive early stopping. Ensure the model's adaptability across diverse scenarios and maintains peak efficiency amidst dynamic data fluctuations, while also optimizing computational efficiency within stringent resource constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"app usage analytics\",\n        \"description\": \"Design a deep learning fusion model combining an LSTM auto-encoder and a state-of-the-art vector autoregressive (VAR) module for highly accurate forecasting of daily active users. The model must include real-time concept drift detection, Seasonal-Trend decomposition, a hybrid regularization technique with Bayesian priors and adaptive early stopping, ensuring adaptability, efficiency, and resource optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"forecasting precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"concept drift detection\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"App Daily Active Users\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"daily active users\"],\n            \"specification\": {\n                \"time_series_length\": null,\n                \"features\": null,\n                \"target_dimensions\": null\n            },\n            \"description\": \"A time-series dataset representing the daily active users of an app with fluctuations and seasonality.\",\n            \"preprocessing\": [\"seasonal decomposition\", \"concept drift handling\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"real-time monitoring of concept drift\",\n                \"performance comparison over different time periods\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"LSTM-Var Fusion Model\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"LSTM layers\": null,\n                    \"VAR module layers\": null,\n                    \"regularization technique\": \"hybrid (Bayesian priors and adaptive early stopping)\"\n                },\n                \"training_details\": {\n                    \"batch_size\": null,\n                    \"learning_rate\": null\n                }\n            },\n            \"description\": \"An innovative deep learning model that fuses an LSTM auto-encoder with a state-of-the-art VAR model for high-accuracy time-series forecasting, considering real-time concept drift, complex cycles, and resource efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"LSTM for capturing temporal dependencies, VAR for capturing multivariate relationships, and concept drift handling techniques for adaptability\",\n        \"Hybrid regularization for enhanced model robustness\",\n        \"Seasonal-Trend decomposition for handling intricate cycles\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"real-time series transformation\",\n            \"data preprocessing\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"in-app forecasting API\",\n        \"inference_engine\": [\"optimized for efficient deployment\"],\n        \"demonstration\": \"live dashboard showcasing forecast accuracy and resource usage\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a complex machine learning model, their expertise level, and the specific problem in time-series analysis. Performance metrics like accuracy, forecasting precision, and concept drift detection have been included. Time and space complexity metrics are noted, with inferred default values due to missing details in the instruction. Model architecture and data requirements are described, reflecting the user's requirements. However, exact numerical values for performance metrics, LSTM and VAR layer configurations, and preprocessing steps are not provided as they are not specified in the instruction.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for a cutting-edge ML team, your mandate now includes achieving state-of-the-art accuracy by developing a transfer learning model for cat species classification with sub-genre distinctions. Mandate: design a CNN-based system, fine-tune on a large-scale, diverse dataset with nuanced breed variations, and implement feature extraction and ensemble techniques, ensuring minimal computational overhead and real-time performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal classification\",\n        \"description\": \"Design a state-of-the-art transfer learning model, specifically a CNN-based system, for cat species classification with sub-genre distinctions. Fine-tune on a large-scale, diverse dataset capturing nuanced breed variations, implement feature extraction and ensemble techniques, with a focus on minimal computational overhead and real-time performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"real-time\",\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"computational overhead\",\n                \"value\": \"minimal\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Large-scale Cat Species Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\", \"sub-genre\"],\n            \"specification\": {\n                \"dataset_size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"image_dimensions\": {\n                    \"height\": null,\n                    \"width\": null,\n                    \"channels\": 3\n                },\n                \"variety_of_breeds\": \"diverse and nuanced\",\n                \"diversity\": \"large-scale\"\n            },\n            \"description\": \"A comprehensive dataset for fine-tuning a CNN model on cat species classification with sub-genre distinctions.\",\n            \"preprocessing\": [\"data augmentation\", \"image normalization\"],\n            \"augmentation\": [\"GAN-generated breed variations\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transfer Learning CNN (Pre-trained on ImageNet)\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"CNN\",\n                \"transfer_learning\": \"yes\",\n                \"base_model\": \"pre-trained on ImageNet\"\n            },\n            \"description\": \"A state-of-the-art CNN designed for fine-tuning on cat species classification with sub-genre variations\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning from large-scale datasets like ImageNet enhances generalization.\",\n        \"Fine-tuning on nuanced breed variations can improve accuracy.\",\n        \"Ensemble techniques improve performance without compromising real-time performance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"fine-tuning layers\",\n            \"dimensionality reduction\",\n            \"ensemble model selection\"\n        ],\n        \"target_device\": \"support for real-time and efficient inference\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow.js for real-time performance\"],\n        \"demonstration\": \"Execution of real-time cat species classification with sub-genre distinctions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers the user's intent (high expertise), the problem's domain and task, the specific model design, performance metrics, complexity, and the details required. The accuracy of feature extraction, ensemble techniques, and real-time performance is inferred based on the given task. The dataset is assumed to have been searched for and its properties are described. The confidence score is high as the instruction is clear about the project requirements.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "visual question answering", "instruction": "As a seasoned project manager for the upcoming Science and Innovation Fair, your mandate is to develop a sophisticated VQA system surpassing current standards, using the BERT4Vis model and expanded TextVQA++ dataset. The design must exhibit a multi-layered architecture, incorporating not just fusion of visual and textual cues but also temporal reasoning for dynamic scenes. Expect the blueprint to integrate dynamic context fusion, incremental attention mechanisms, and sophisticated meta-learning techniques for evolving performance. For a comprehensive win, prepare a 50-page research paper detailing the extensive evaluation using diverse, explainability-oriented metrics, an exhaustive algorithmic pipeline with tractable complexity analysis, and a comparative analysis with the latest eight competitors' methodologies. Emphasize breakthrough contributions and practical implications in a clear, concise, and innovative manner.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"science and innovation fair\",\n        \"description\": \"Develop a sophisticated VQA system using the BERT4Vis model and expanded TextVQA++ dataset. The system should incorporate multi-layered architecture with fusion of visual and textual cues, temporal reasoning for dynamic scenes, dynamic context fusion, incremental attention mechanisms, and meta-learning. Prepare a 50-page research paper detailing the comprehensive evaluation, algorithmic pipeline, complexity analysis, and competitor comparison.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"diversity of explainability-oriented metrics\",\n                \"value\": null\n            },\n            {\n                \"name\": \"comprehensiveness\",\n                \"value\": null\n            },\n            {\n                \"name\": \"algorithmic pipeline tractability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness to evolving performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BERT4Vis\",\n            \"modality\": [\"multimodal\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dynamic_dataset\": true,\n                \"multimodal_fusion\": true,\n                \"temporal_reasoning\": true\n            },\n            \"description\": \"An advanced model and expanded dataset for VQA tasks, incorporating visual and textual cues along with dynamic and temporal components.\",\n            \"preprocessing\": [\n                \"dynamic context augmentation\",\n                \"temporal feature extraction\"\n            ],\n            \"augmentation\": [\n                \"incremental attention sampling\",\n                \"meta-learning strategies\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"TextVQA++\",\n            \"modality\": [\"text\"],\n            \"description\": \"An expanded version of the TextVQA dataset, providing additional textual data for VQA system training.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT4Vis + TextVQA++ Fusion\",\n            \"family\": \"BERT-based VQA architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\n                    \"multi-layer fusion module\",\n                    \"temporal reasoning module\",\n                    \"dynamic context fusion\",\n                    \"incremental attention mechanism\",\n                    \"meta-learning component\"\n                ],\n                \"computational_complexity\": {\n                    \"inference_time\": null,\n                    \"memory_size\": null\n                }\n            },\n            \"description\": \"A fusion model using BERT4Vis and TextVQA++, equipped with advanced VQA features.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"dynamic context engineering\",\n            \"explanability design\"\n        ],\n        \"target_device\": \"cloud or edge computing\",\n        \"deployment_endpoint\": \"Science and Innovation Fair platform\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"Clear and concise user interface for showcasing the VQA system\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the given instruction, capturing the user's intent to build a sophisticated VQA system, their expertise level, and the specific requirements mentioned. The problem area, downstream tasks, datasets, model architecture, and related details are derived from the input. However, numerical values for performance and complexity metrics are not included due to their absence in the instruction. A research paper requirement and its outline are included. The confidence is high as the structure and details are in alignment with the schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project lead for \"InnovTech Global,\" develop a cutting-edge translation app exclusively for the Kichwa and Aymara-Spanish variants. Utilize the limited JW300 dataset, but design a hybrid architecture integrating Convolutional Neural Networks (CNN) for sentence-level understanding and Transformers for long-term dependency management. Mandate a stringent accuracy standard of 95% accuracy in informal Quechua expressions and optimize for a BLEU index of 45 in situations with scarce training data. Additionally, ensure cross-platform adaptability for wearables and internet of things devices, with a focus on energy efficiency metrics like the EER (Energy Efficiency Ratio) to minimize operational impact on eco-friendly devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"tech translation services\",\n        \"description\": \"Develop a cutting-edge translation app for Kichwa and Aymara-Spanish variants, using the JW300 dataset with a hybrid architecture combining CNN for sentence-level understanding and Transformers for long-term dependency management. Target accuracy for informal Quechua expressions is set to 95% and BLEU index for scarce data situations should be at least 45. The app must be cross-platform for wearables and IoT devices, with a focus on energy efficiency to minimize operational impact on eco-friendly devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95,\n                \"target_domain\": \"informal Quechua expressions\"\n            },\n            {\n                \"name\": \"BLEU index\",\n                \"value\": 45,\n                \"condition\": \"scarce training data\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Energy Efficiency Ratio (EER)\",\n                \"value\": null,\n                \"unit\": \"dimensionless\",\n                \"optimization_goal\": \"minimize\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"JW300\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated sentences\"],\n            \"specification\": {\n                \"source_languages\": [\"Kichwa\", \"Aymara\"],\n                \"target_language\": \"Spanish\",\n                \"availability\": \"limited\"\n            },\n            \"description\": \"Limited dataset suitable for translation between Kichwa and Aymara-Spanish, particularly focusing on informal Quechua expressions.\",\n            \"preprocessing\": [\"CNN for sentence-level understanding\", \"Transformer for long-term dependency management\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Architecture (CNN + Transformers)\",\n            \"family\": \"Neural Machine Translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"sentence_level\": \"CNN\",\n                \"long_term_dependency\": \"Transformers\"\n            },\n            \"description\": \"A hybrid NMT model utilizing CNNs for sentence-level understanding and Transformers for long-term dependency management in the Kichwa and Aymara-Spanish translation task.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hybrid architecture is suitable for handling diverse languages with limited data.\",\n        \"Stricter accuracy standards for informal Quechua expressions and optimized BLEU for scarce data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"cross-platform compatibility for wearables and IoT devices\",\n            \"energy efficiency in IoT applications\"\n        ],\n        \"target_device\": [\"wearables\", \"IoT devices\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"translation app showcasing energy efficiency on eco-friendly devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the requirements of the user, capturing the project focus on a hybrid architecture, language variants, performance targets, and cross-platform compatibility. The model type, dataset, and metrics were chosen based on the instruction. However, specific details of feature engineering and energy efficiency calculations are not included as they would require more in-depth analysis or specific information. The confidence score is high given the clear instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the ML team, your task is to design a highly sophisticated text classification model, employing the BoolQ dataset. The model must deliver exceptional performance by surpassing a precision-recall tradeoff benchmark of 0.95, while maintaining transparency. Incorporate LIME (Local Interpretable Model-Agnostic Explanations) for feature importance analysis within a stringent budget of only 5 epochs using a Hugging Face's BERT-base model, demanding efficient resource management and interpretability at each decision-making step.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Design a highly sophisticated text classification model using the BoolQ dataset. The model must achieve a precision-recall tradeoff benchmark of 0.95 and include transparency, particularly through LIME for feature importance analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"epochs\",\n                \"value\": 5,\n                \"unit\": \"epochs\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"not specified\",\n                \"language\": \"English\"\n            },\n            \"description\": \"A dataset for text classification tasks, possibly multiple-choice or question answering.\",\n            \"preprocessing\": [\n                \"fine-tuning with Hugging Face's BERT-base model\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"BERT-base\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"resource_management\": \"efficient\",\n                \"interpretability\": \"incorporating LIME\"\n            },\n            \"description\": \"A BERT-base model with interpretability measures to achieve high performance on the BoolQ dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transparency with LIME will help in understanding feature importance during classification decisions.\",\n        \"Strict budget of 5 epochs calls for efficient resource utilization and fast training.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Bert fine-tuning for 5 epochs\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Model explanations using LIME\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the instruction with a high level of sophistication required. The user's intent, problem area, performance metrics, and budget constraints have been incorporated. The BERT-base model with LIME for interpretability is clearly stated, and dataset preprocessing is specified. However, specifics like the model's exact performance, the preprocessing steps for the dataset, and the target device were not provided as the instruction did not specify them.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for the avant-garde micro-sized BERT adaptation team, your challenge is to develop a portable and user-centric Question Answering (QA) model derived from TinyBERT. Target the specialized Financial News NLP dataset, maintaining an impressive precision of over 95%, with a focus on attaining sub-150 ms response durations. Guarantee cross-platform compatibility on resource-constrained devices, including smartwatches and low-end smartphones, while adhering to stringent energy efficiency standards.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"financial services\",\n        \"description\": \"Develop a portable and user-centric Question Answering model derived from TinyBERT, targeting the specialized Financial News NLP dataset. Aim for precision over 95% and sub-150 ms response durations, while ensuring compatibility on resource-constrained devices like smartwatches and low-end smartphones, with strict adherence to energy efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"response time (ms)\",\n                \"value\": 150,\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"watts\"\n            },\n            {\n                \"name\": \"device compatibility\",\n                \"value\": [\"smartwatches\", \"low-end smartphones\"],\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": \"portable\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Financial News NLP dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"answers\"],\n            \"specification\": {\n                \"size\": \"custom\",\n                \"datatype\": \"financial news\",\n                \"cross-platform\": true\n            },\n            \"description\": \"A specialized dataset for financial news question answering, adapted from TinyBERT.\",\n            \"preprocessing\": [\"adaptation for TinyBERT\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT adaptation for QA\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"micro\",\n                \"training_speed\": \"optimized for mobile platforms\",\n                \"inference_speed\": {\n                    \"response_time\": 150,\n                    \"unit\": \"ms\"\n                }\n            },\n            \"description\": \"A lightweight and user-friendly TinyBERT model adapted for fast and energy-efficient financial news question answering.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cross-platform compatibility is crucial for resource-constrained devices.\",\n        \"Adapting TinyBERT for financial news domain may require fine-tuning to achieve high precision.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"lightweight for resource-constrained devices\"],\n        \"target_device\": [\"smartwatches\", \"low-end smartphones\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for mobile\"],\n        \"demonstration\": \"user-centric and fast QA on constrained devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key aspects from the user's instruction, focusing on the task of creating a QA model from TinyBERT, the dataset, performance metrics, and compatibility requirements. It assumes a high level of expertise and that the model needs to be fine-tuned for precision. Energy efficiency and compatibility are emphasized. The confidence score is high based on a clear mapping to the schema and the explicit details provided in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "community detection", "instruction": "Develop an advanced, multi-layered learning algorithm that fuses the Sunbelt dataset of CiteSeer citation network with a temporal attention mechanism, for real-time detection of subdiscipline-specific, subtemporal research communities. Implement a differentiable neural representation learning model that not only captures the dynamic evolution of research themes but also anticipates future collaborations by incorporating latent knowledge graph embeddings. Ensure the system has an adaptive resilience to sudden shifts in research trends, and optimizes resource allocation for onboarding new researchers while maintaining a sub-linear computational complexity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"scholarly research\",\n        \"description\": \"Develop an advanced multi-modal learning algorithm for real-time detection of subdiscipline-specific, subtemporal research communities using the Sunbelt CiteSeer citation network. The algorithm should integrate a temporal attention mechanism and a differentiable neural representation learning model that captures dynamic research themes, predicts future collaborations, and incorporates latent knowledge graph embeddings. The system must be resilient to sudden changes in research trends and optimize resource allocation for new researchers with sub-linear computational complexity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"future collaboration prediction rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"adaptability to sudden shifts\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"computational complexity\",\n                \"value\": \"sub-linear\",\n                \"unit\": \"time complexity\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sunbelt CiteSeer citation network\",\n            \"modality\": [\"graph\", \"text\"],\n            \"target_variables\": [\"citation count\", \"research community\"],\n            \"specification\": {\n                \"size\": null,\n                \"Temporal dimension\": null\n            },\n            \"description\": \"Dataset for detecting subdiscipline-specific and subtemporal research communities in the CiteSeer citation network, incorporating both citation patterns and textual data.\",\n            \"preprocessing\": [\n                \"Temporal filtering\",\n                \"Graph preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Multi-modal Differentiable Neural Representation Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"Layers\": {\n                    \"temporal attention\": {},\n                    \"knowledge graph embeddings\": {}\n                },\n                \"Training\": {\n                    \"future collaboration prediction\": true\n                }\n            },\n            \"description\": \"A complex model using temporal attention, neural representation learning, and latent knowledge graph embeddings for multi-layered learning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Multi-layer fusion of citation network data and temporal attention\",\n        \"Latent knowledge graph embeddings for capturing dynamic research themes\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"time-series analysis on citation patterns\",\n            \"community detection algorithms\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"prediction of future collaborations and community evolution\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response represents the given user instruction as a valid machine learning project. It includes the user's intent to build a complex model, high expertise level, the problem's domain, specific details on data fusion, temporal attention, and future collaboration prediction. Performance metrics are mentioned as placeholders, and complexity metrics are specified with a sub-linear requirement. The source, preprocessing steps, and other details are accurately captured based on the instruction.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "machine translation", "instruction": "Act as the manager of a specialized AI development team tasked with enhancing a cutting-edge translation model for our global clientele. Instruct the team to meticulously adapt a transformer model on the extensive, de-identified Common Crawl dataset, focusing on English and Japanese dialects, slang, and non-standard vocabulary. Demand a superior model that surpasses 98% accuracy in deciphering and rendering informal, conversational phrases across diverse cultural contexts, ensuring seamless translation in sub-genres like pop culture and local slang.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"global business communication\",\n        \"description\": \"Enhance a cutting-edge translation model, specifically for English and Japanese dialects, focusing on slang, non-standard vocabulary, and informal conversational phrases. The model should surpass 98% accuracy in translating diverse cultural contexts, including sub-genres like pop culture and local slang.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl (English and Japanese)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_size\": \"extensive\",\n                \"de-identification\": \"true\",\n                \"languages\": [\"English\", \"Japanese\"]\n            },\n            \"description\": \"A large-scale, de-identified dataset from Common Crawl containing informal and conversational content for English and Japanese dialects.\",\n            \"preprocessing\": [\"text cleaning, tokenization\"],\n            \"augmentation\": [\"data augmentation using slang and non-standard vocabulary\"],\n            \"visualization\": [\"language distribution and coherence analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based Model\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art transformer\",\n                \"language_specialization\": \"English and Japanese dialects\"\n            },\n            \"description\": \"A high-performance, adapted transformer model targeting translation of informal and slang content in English and Japanese.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Understanding of various English and Japanese dialects, slang, and non-standard vocabulary is crucial for improved translation.\",\n        \"Incorporating cultural context and diversity into translation quality assessment is essential.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware language models\",\n            \"linguistic variations handling\"\n        ],\n        \"target_device\": \"cloud-based for efficient processing and scalability\",\n        \"deployment_endpoint\": \"optimized for global content platforms\",\n        \"inference_engine\": [\"GPU-optimized for high performance\"],\n        \"demonstration\": \"structured web-based interface showcasing seamless translation examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This response reflects a high confidence based on the clear instructions to enhance a translation model, targeting specific language features, and achieving a high accuracy target. The JSON format captures the user's intent to build, the dataset with its unique characteristics, the transformer model type, and the requirements for preprocessing, data augmentation, and model enhancement. However, it assumes that the team has the necessary resources to achieve the 98% accuracy, and the source is inferred from the context of a search.\",\n        \"score\": 0.97\n    }\n}"}
{"task": "image classification", "instruction": "As an elite project director, mandate the AI team to develop an intricate plant species classifier using 'Advanced Plant Seedlings Dataset with Adversarial Environmental Variations,' incorporating advanced ResNeXt-CNN architecture and multi-database transfer learning. Achieve a steadfast 99%+ accuracy, ensuring illumination-invariant performance, and provide a hardware-optimized model with real-time inference (<100ms latency) for mobile devices with restricted energy consumption. Also, devise a strategy for dynamic data augmentation, model transparency, and explainability for domain experts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"agriculture or botany\",\n        \"description\": \"Develop an intricate plant species classifier using the 'Advanced Plant Seedlings Dataset with Adversarial Environmental Variations' employing the ResNeXt-CNN architecture and multi-database transfer learning. Aim for a minimum accuracy of 99%, ensuring illumination-invariant performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.1,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"energy consumption\",\n                \"value\": null, \n                \"unit\": \"mobile device-specific\" \n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Advanced Plant Seedlings Dataset with Adversarial Environmental Variations\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\"],\n            \"specification\": {\n                \"data_format\": \"image\",\n                \"environmental_variations\": \"adversarial\",\n                \"size\": \"mobile-compatible\"\n            },\n            \"description\": \"A dataset designed for plant classification, featuring adversarial environmental variations and optimized for mobile devices.\",\n            \"preprocessing\": [\"dynamic data augmentation\"],\n            \"augmentation\": [\"adapted for environmental variations\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Plant Species Classifier\",\n            \"family\": \"ResNeXt-CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ResNeXt-CNN\",\n                \"transfer_learning\": \"multi-database\",\n                \"optimized_for\": \"mobile devices\",\n                \"latency\": {\n                    \"value\": \"<100ms\",\n                    \"unit\": \"milliseconds\"\n                }\n            },\n            \"description\": \"An advanced classifier using ResNeXt-CNN, leveraging transfer learning, for plant species identification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Illumination-invariant performance is crucial for robustness.\",\n        \"Dynamic data augmentation is required for adapting to the adversarial environmental variations.\",\n        \"Model transparency and explainability are essential for domain experts to understand the classifier's decisions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"multi-database transfer learning tuning\"],\n        \"target_device\": \"mobile devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time inference\"],\n        \"demonstration\": \"including visualization of class activation maps for explainability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a complex classifier, their high level of expertise, and the specific requirements such as accuracy, performance, and constraints. The dataset and model details are in line with the instructions. The hardware optimization and explainability strategies are taken into account. The confidence score is high due to a clear mapping to the provided schema.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "image classification", "instruction": "As a forward-thinking space AI project director, command your interdepartmental team to develop an ultra-precise galaxy morphology classifier. Mandate the retrieval of the 'GalaxyZoo2' dataset from trusted digital archives, ensuring secure and validated data. Scrutinize the fusion of state-of-the-art CNN architectures (VGG16, ResNet50, and InceptionV3) with advanced preprocessing tactics like spatial attention, wavelet transformation, and custom color space manipulation. Design a sophisticated optimization process that balances model selection based on F1 score, precision at top-k predictions, and macro-recall, accounting for skewed class distributions, noisy labels, and effective use of semi-supervised and transfer learning techniques for enhanced performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Develop an ultra-precise galaxy morphology classifier using state-of-the-art CNN architectures (VGG16, ResNet50, and InceptionV3) with advanced preprocessing techniques like spatial attention, wavelet transformation, and custom color space manipulation. Ensure the data from GalaxyZoo2 is securely retrieved from reliable digital archives and processed. Optimize for F1 score, precision at top-k predictions, and macro-recall, accounting for class imbalance, noisy labels, and semi-supervised and transfer learning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1_score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision_at_top_k\",\n                \"value\": null\n            },\n            {\n                \"name\": \"macro_recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"GalaxyZoo2\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_size\": null,\n                \"data_distribution\": \"imbalance and noisy\",\n                \"source\": \"trusted digital archives\"\n            },\n            \"description\": \"Securely retrieved galaxy images from the Galaxy Zoo 2 dataset for precise image classification tasks\",\n            \"preprocessing\": [\"spatial attention\", \"wavelet transformation\", \"custom color space manipulation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced CNN Ensemble\",\n            \"family\": [\"VGG16\", \"ResNet50\", \"InceptionV3\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": true,\n                \"semi-supervised_learning\": true\n            },\n            \"description\": \"Combination of VGG16, ResNet50, and InceptionV3 CNNs with advanced preprocessing techniques for enhanced galaxy morphology classification\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"To be specified with a clear methodology\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response follows the schema and represents the details extracted from the instruction. The user's intent to build a classifier, high expertise level, and the focus on galaxy image classification from GalaxyZoo2 are captured. The need for sophisticated preprocessing and model optimization techniques, such as CNN ensembles, F1 score, and accounting for imbalanced data, is included. However, the precise metric values, target device, and the demonstration process are not provided due to the lack of details in the instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "In your capacity as the visionary project manager for the premier financial sentiment analytics platform, empower the ML team to design and integrate a highly sophisticated Reuters-21578 news categorization framework, leveraging cutting-edge deep learning techniques (Transformer-based BERT and hybrid CNN-LSTM architectures). Aim for an unprecedented accuracy of 95% with granular sector-specific analysis (aviation, tech, and renewable energy), and incorporate an advanced noise-cancellation module to distinguish factual, real-time breaking news from misinformation at a lightning-fast 1-millisecond response time. Furthermore, develop a robust, multi-dimensional benchmarking strategy to outperform rival platforms and demonstrate continuous improvement. Document the entire process with detailed project milestones and whitepapers for academic scrutiny.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"financial sentiment analytics\",\n        \"description\": \"Design and integrate a Reuters-21578 news categorization platform with advanced Transformer-based BERT and hybrid CNN-LSTM architectures. Target accuracy must reach 95% with granular sector analysis (aviation, tech, renewable energy). Implement a noise-cancellation module for real-time factual detection at 1-millisecond speed. Establish a benchmarking strategy to outperform competitors and document with project milestones and whitepapers.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"sector-specific accuracy\",\n                \"value\": 0.95,\n                \"description\": [\"aviation\", \"tech\", \"renewable energy\"]\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.001,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"news category\"],\n            \"specification\": {\n                \"type\": \"news dataset\",\n                \"granularity\": \"sector-specific\"\n            },\n            \"description\": \"A large-scale news dataset for text classification with a focus on financial sentiment.\",\n            \"preprocessing\": [\n                \"BERT tokenization\",\n                \"hybrid CNN-LSTM architecture\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"News Categorization\",\n            \"family\": \"Transformer-based BERT & CNN-LSTM\",\n            \"type\": \"neural networks\"\n        }\n    ],\n    \"knowledge\": [\n        \"Leverage cutting-edge deep learning techniques for high accuracy\",\n        \"Noise-cancellation module: distinguishing factual vs misinformation\",\n        \"1-millisecond response time\",\n        \"Benchmarks for continuous improvement\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Fine-tuning for sector-specific analysis\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"financial sentiment platform\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"Project milestones and whitepapers for academic review\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers all key aspects from the instruction: user intent, problem domain (financial sentiment analysis), target accuracy, noise-cancellation, benchmarking, and platform deployment. Specifics of preprocessing, model, and service requirements are extracted from the provided requirements. However, the link for fetching the Reuters-21578 dataset is assumed to be provided by the user, as the instruction does not detail a source.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "tabular classification", "instruction": "As a project manager for a specialized tabular classification team, design an advanced model to forecast multiple student performance indicators in diverse subject areas, accounting for individual learning styles, academic history, and real-time extracurricular engagement, using limited labeled data and optimized feature engineering.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"education\",\n        \"description\": \"Design an advanced model to forecast multiple student performance indicators in diverse subject areas, accounting for individual learning styles, academic history, and real-time extracurricular engagement using limited labeled data and optimized feature engineering.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Student Performance Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"performance indicators\", \"learning styles\", \"academic history\", \"extracurricular engagement\"],\n            \"specification\": {\n                \"sample_size\": null,\n                \"feature_count\": null,\n                \"labeled_data_ratio\": null\n            },\n            \"description\": \"A dataset containing student information with diverse subject data and limited labeled performance indicators, addressing the unique learning styles and real-time activities.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"handling imbalance\",\n                \"feature selection\"\n            ],\n            \"augmentation\": [\n                \"synthetic data generation\"\n            ],\n            \"visualization\": [\n                \"variable importance plots\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble models or deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-output neural network\",\n                \"hyperparameters\": {\n                    \"learning rate\": null,\n                    \"batch size\": null,\n                    \"number of layers\": null\n                }\n            },\n            \"description\": \"An advanced model to forecast student performance indicators using individual characteristics and real-time engagement, optimized for limited labeled data and efficient feature engineering.\"\n        }\n    ],\n    \"knowledge\": [\n        \"leveraging transfer learning\",\n        \"ensemble methods to improve performance with limited data\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature scaling\",\n            \"cross-validation for hyperparameter tuning\"\n        ],\n        \"target_device\": \"cloud server\",\n        \"deployment_endpoint\": \"custom learning platform API\",\n        \"inference_engine\": [\n            \"TensorFlow Serving\",\n            \"cloud-based deployment\"\n        ],\n        \"demonstration\": \"dashboard showcasing model performance and insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intention to build an advanced model, considering the high expertise level. The problem area and task are tabular classification with education focus. Performance metrics like accuracy, precision, and recall are not explicitly provided but would typically be sought in the context. Model-related details are left vague to accommodate various advanced approaches. The missing metrics and specific values in feature engineering, target device, and deployment imply a need for additional information to make precise decisions.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "node classification", "instruction": "As a machine learning project manager for a specialized node classification task, our team must initiate with the CiteSeer graph dataset, preprocess it for node features, employing Graph Convolutional Networks. Aim to achieve high precision and recall in citation network categorization, targeting subfields like computer science theory and information retrieval. What steps and performance milestones should we set?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Initiate a machine learning project using CiteSeer graph dataset for node classification in a specialized context, focusing on computer science theory and information retrieval. Target precision and recall, and establish preprocessing steps for node features, leveraging Graph Convolutional Networks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CiteSeer graph dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"subfields (computer science theory, information retrieval)\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"edge_types\": null\n            },\n            \"description\": \"A graph dataset for academic papers with citation relationships, focusing on computer science theory and information retrieval.\",\n            \"preprocessing\": [\"node feature extraction\", \"node normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"data exploration visualization\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Graph Convolutional Networks (GCN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"number of layers, hidden units per layer\"\n            },\n            \"description\": \"A GCN model for node classification in the CiteSeer graph, targeting high precision and recall in computer science theory and information retrieval fields.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature engineering techniques specific to GCN\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU accelerated for speed\"],\n        \"demonstration\": \"reporting of precision and recall at key milestones\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the user's role as a project manager, the focus on node classification in the CiteSeer dataset, and the use of Graph Convolutional Networks. Performance milestones are set on precision and recall without specific values, as they will likely be decided collaboratively based on the project's progress. Other aspects like preprocessing, model architecture, and deployment specifics are left intentionally vague as per the instructions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "Utilize a time-series-specific rolling mean and exponential weighted moving average (RMA) approach on the 'AtmosphericCO2Levels' dataset from the mid-20th century to late 21st century, considering seasonal patterns and long-term trends. The task necessitates detecting sudden spikes above 2 standard deviations from the Lower and Upper Control Limits (LCL & UCL) in a 3-month moving window, while accounting for the autocorrelation and non-stationarity of the data to effectively isolate anthropogenic anomalies for actionable insights on climate change.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"climate research\",\n        \"description\": \"Use a time-series-specific rolling mean (rolling mean) and exponential weighted moving average (RMA) on the 'AtmosphericCO2Levels' dataset from the mid-20th to late 21st century. Analyze for seasonal patterns, long-term trends, and account for autocorrelation and non-stationarity to isolate sudden spikes above 2 standard deviations from LCL and UCL in a 3-month moving window for actionable insights on climate change.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute deviation (MAD) from LCL and UCL\",\n                \"value\": 2.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time (accounting for autocorrelation)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory footprint for handling non-stationarity\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AtmosphericCO2Levels\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"CO2 levels\"],\n            \"specification\": {\n                \"start_year\": \"mid-20th century\",\n                \"end_year\": \"late 21st century\",\n                \"time_frequency\": \"3-monthly\"\n            },\n            \"description\": \"Dataset capturing CO2 levels over time with mid-20th century to late 21st century data, including seasonality and trends.\",\n            \"preprocessing\": [\"seasonal decomposition\", \"de-trending\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time series plots for patterns\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"rolling mean and EWM (RMA)\",\n            \"type\": \"time-series analysis techniques\",\n            \"specification\": {\n                \"window_size\": 3,\n                \"seasonality\": \"accounted for\"\n            },\n            \"description\": \"Model using a 3-month rolling mean and RMA on AtmosphericCO2Levels, considering seasonality, long-term trends, and specific anomalous detection settings.\"\n        }\n    ],\n    \"knowledge\": [\"autocorrelation and non-stationarity handling techniques\"],\n    \"service\": {\n        \"feature_engineering\": [\n            \"lagged variables (autocorrelation)\",\n            \"stationarity tests (ADF, KPSS)\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for time-sensitive tasks\"],\n        \"demonstration\": \"Visualization of identified anomalies along with trend and seasonal components\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's instruction for a time-series anomaly detection project. It specifies the dataset, data pre-processing steps, models, and the desire to account for relevant complexities like autocorrelation and non-stationarity. Performance expectations around MAD from LCL and UCL are set, but actual metric values are not provided. The model and service sections have not been fully populated due to the complexity and reliance on specific techniques not explicitly mentioned.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "The project's objective is to enhance the state-of-the-art image recognition system for fine-grained car classification using the Stanford Cars Dataset. Inception-v3 must be fine-tuned with advanced augmentation techniques for panoramic, low-light, and complex background variations. Specifically, we need to design a model capable of differentiating between rare vintage models and modern supercars with precision and recall exceeding 95% for each subcategory, providing detailed per-class evaluation metrics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"automotive industry\",\n        \"description\": \"The objective is to improve an image recognition system for fine-grained car classification using the Stanford Cars Dataset. Focus on fine-tuning Inception-v3 with advanced augmentation techniques for panoramic, low-light, and complex background variations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"car type (fine-grained categories)\"],\n            \"specification\": {\n                \"image_size\": \"square cropping or resizing to match Inception-v3 input\",\n                \"image_diversity\": \"panoramic, low-light, complex backgrounds\"\n            },\n            \"description\": \"A dataset for fine-grained car classification, including rare vintage and modern supercars.\",\n            \"preprocessing\": [\"fine-tuning Inception-v3\"],\n            \"augmentation\": [\"advanced techniques for specified variations\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Inception-v3\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"base_model\": \"inception-v3 architecture\",\n                \"pre-trained\": \"likely fine-tuned from imagenet\",\n                \"number_of_classes\": \"customized for fine-grained car classification\"\n            },\n            \"description\": \"A CNN model fine-tuned for car classification with high precision and recall, targeting vintage and modern supercars.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"requirement for detailed per-class evaluation metrics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's instruction by focusing on the objective of enhancing an image recognition system, fine-tuning Inception-v3, and specifying performance and complexity metrics. The expert-level expertise of the user is reflected. However, specific values or constraints for some metrics like inference time and memory size are not provided, as the user only mentions the need for optimization. The instructions for detailed per-class evaluation are clear.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular classification", "instruction": "As a project manager for the Titanic passenger survival analysis, you now require the team to develop a tabular classification model using ensemble methods. Additionally, ensure interpretability of features and a constraint on max 80% leak in training data preprocessing. Report not only accuracy but also AUC-ROC and F1 scores on the eval set, maintaining interpretability of feature importance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"data analysis\",\n        \"description\": \"Develop a tabular classification model for Titanic passenger survival analysis using ensemble methods. Ensure interpretability of feature importance and limit data preprocessing leaks to no more than 80% in the training data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\"\n            },\n            {\n                \"name\": \"AUC-ROC\"\n            },\n            {\n                \"name\": \"F1 Score\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"data preprocessing leak rate\",\n                \"value\": 0.8,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Titanic passenger dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"Survived\"],\n            \"specification\": {\n                \"dataset_size\": null,\n                \"dimension\": null\n            },\n            \"description\": \"A dataset containing information about passengers on the Titanic for survival prediction.\",\n            \"preprocessing\": [\n                \"Limited to 80% data leak prevention\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble methods\",\n            \"type\": \"classical machine learning\",\n            \"specification\": {\n                \"interpretability\": \"feature importance\"\n            },\n            \"description\": \"An ensemble model for tabular classification task of predicting passenger survival on the Titanic, focusing on interpretability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Maintain interpretability of feature importance during model evaluation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the given instruction. It covers the ensemble model development for passenger survival analysis, interpretability of feature importance, and the specified constraint on data preprocessing leakage. Performance metrics include accuracy, AUC-ROC, and F1 scores. The missing values in specifications like dataset size and model family name are due to the lack of information provided in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a data science project manager, request the machine learning team to develop a robust regression model for the Media Campaign Cost Dataset. Emphasize the need for step-by-step feature engineering, cross-validation, and iteratively fine-tuning parameters to optimize performance. Additionally, mandate the use of advanced techniques like stacking or time-series analysis for time-dependent costs, while maintaining a focus on the RMSLE metric for robust evaluation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"Develop a robust regression model for the Media Campaign Cost Dataset. Focus on step-by-step feature engineering, cross-validation, and parameter tuning. Incorporate advanced techniques like stacking or time-series analysis for time-dependent costs. Measure performance using RMSLE as the primary metric.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Media Campaign Cost Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"cost\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing campaign costs and associated features for predicting media campaign costs.\",\n            \"preprocessing\": [\"feature scaling\", \"datetime feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature importances\",\n                \"correlation matrices\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced regression models (may include ensembles, neural networks, or time-series specific models)\",\n            \"type\": \"ensemble models | neural networks\",\n            \"specification\": null,\n            \"description\": \"A regression model with emphasis on feature engineering, cross-validation, and parameter optimization for the Media Campaign Cost Dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Use stacking to combine predictions from multiple models, and incorporate time-series analysis for handling time-dependent costs.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection\", \"engineered features\", \"datetime-based features\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"ensemble inference\", \"real-time processing\"],\n        \"demonstration\": \"Performance analysis report and visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's requirements, highlighting the importance of advanced techniques, feature engineering, cross-validation, and performance optimization. The user's high expertise level indicates a need for more detailed information, but this is left intentionally vague to accommodate possible customization based on team's expertise and available resources. RMSLE as a metric, specific model type, and some performance-related constraints are not provided due to the lack of details in the instruction.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "tabular regression", "instruction": "As a premier project manager for a cutting-edge machine learning team, your objective is to develop a state-of-the-art regression model for the Wild Blueberry Yield dataset. This dataset, rich in high-dimensional and correlated features, has been meticulously stratified into train, validation, and test sets with a focus on precision. Your challenge is to not only minimize the mean absolute error (MAE) but also optimize for computational efficiency and model interpretability, while adhering to strict scalability requirements for large-scale production deployments.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Develop a state-of-the-art regression model for the Wild Blueberry Yield dataset. The dataset is high-dimensional and correlated, stratified into train, validation, and test sets with emphasis on precision. Aim for low MAE, optimized computational efficiency, and model interpretability, while ensuring scalability for large-scale production deployments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Absolute Error (MAE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Model Size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"Scalability\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wild Blueberry Yield\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"yields\"],\n            \"specification\": {\n                \"dimension\": {},\n                \"size\": {}\n            },\n            \"description\": \"A high-dimensional and correlated dataset with stratified train, validation, and test sets for blueberry yield prediction.\",\n            \"preprocessing\": [\n                \"High-dimensional feature reduction\",\n                \"Correlation handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Interpretable model visualization\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"State-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": {},\n                \"algorithm_details\": {}\n            },\n            \"description\": \"A regression model optimized for precision, computational efficiency, and interpretability for Wild Blueberry Yield prediction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced feature selection techniques\",\n        \"Model compression and interpretability methods\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Feature scaling\", \"Feature engineering for model interpretability\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Distributed inference framework\"],\n        \"demonstration\": \"Interactive visualization of model performance and predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the user's instruction by targeting a state-of-the-art regression model for the Wild Blueberry Yield dataset with high-dimensional features. Performance metrics focus on minimizing MAE, computational efficiency, and scalability. Specifics of the model, dataset preprocessing, feature engineering, and deployment requirements reflect the objectives and constraints provided in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "For a prestigious real estate analytics project, analyze a vast, multi-source housing dataset with temporal and geographic variations. Develop innovative feature interactions, handle missing values with state-of-the-art imputation techniques, and apply multiple regression models with ensemble methods. Compare performance using not only RMSE but also MAE and AIC, and present a detailed variance decomposition study on the eval set's log-transformed prices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"real estate analytics\",\n        \"description\": \"Analyze a large, multi-source housing dataset with temporal and geographic variations. Focus on feature interactions, impute missing values using state-of-the-art techniques, and apply ensemble methods for regression. Compare performance with RMSE, MAE, and AIC. Perform a detailed variance decomposition study on the eval set's log-transformed prices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AIC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vast multi-source housing dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"price\"],\n            \"specification\": {\n                \"size\": \"large\",\n                \"temporal_variations\": true,\n                \"geographic_variations\": true\n            },\n            \"description\": \"A comprehensive housing dataset with temporal and geographic aspects for a real estate analytics project.\",\n            \"preprocessing\": [\"feature interactions\", \"missing value imputation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble\",\n            \"type\": \"classical machine learning\",\n            \"specification\": {\n                \"model_types\": [\"regression\"],\n                \"ensemble_method\": true\n            },\n            \"description\": \"An ensemble of regression models for the housing dataset analysis project\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Innovative feature interactions and state-of-the-art imputation techniques will be employed to handle the complexity of the dataset.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"advanced interaction features\",\n            \"missing value handling\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"detailed variance decomposition on log-transformed eval prices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON accurately represents the user's requirements, detailing the requirements for data analysis, model development, performance metrics, and preprocessing. The user's high expertise level suggests a focus on intricate details, such as ensemble methods and variance decomposition. Some metric values are left unspecified as the user doesn't provide specific values, but the general approach is well-covered.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager, you need to develop an advanced sentiment analysis model for fine-grained IMDB movie review categorization. Target a nuanced understanding with multiple categories (e.g., extremely positive, moderately positive, neutral, moderately negative, extremely negative), ensuring at least a macro-average F1-score of 95% and integration with a custom named entity recognizer. Utilize pre-trained BERT variants and optimize for efficiency on resource-constrained devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"film industry, sentiment analysis\",\n        \"description\": \"Develop an advanced sentiment analysis model for fine-grained IMDB movie review categorization with categories like extremely positive, moderately positive, neutral, moderately negative, extremely negative.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"macro-average F1-score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds per sample\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"IMDB movie review dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment categories\"],\n            \"specification\": {\n                \"training_set_size\": null,\n                \"testing_set_size\": null,\n                \"data_format\": \"sentence-level\"\n            },\n            \"description\": \"A movie review dataset from IMDB for fine-grained sentiment analysis, including movie reviews and their corresponding sentiment categories.\",\n            \"preprocessing\": [\"BERT tokenization\", \"sentence splitting\"],\n            \"augmentation\": [],\n            \"visualization\": [\"data exploration, distribution of sentiments\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"BERT variants\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT\",\n                \"pretrained_weights\": \"Fine-tuning\",\n                \"device\": \"resource-constrained\",\n                \"inference_speed_target\": \"optimized\"\n            },\n            \"description\": \"A pre-trained BERT model fine-tuned for an advanced sentiment analysis model, tailored for resource-constrained devices and achieving at least a 95% macro-average F1-score in IMDB movie review classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Custom named entity recognition integration required for a more nuanced understanding.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"custom entity embeddings\"],\n        \"target_device\": \"resource-constrained\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"on-device inference using optimized model\"],\n        \"demonstration\": \"Explain the model's performance and sensitivity to named entities\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's request for a sentiment analysis model development project with high-level expertise, specific performance metrics, and consideration for resource efficiency. The details of the model, dataset, and preprocessing align with the instruction. The missing values, like model size, denote that they need to be filled based on optimizations for the resource-constrained devices. The custom named entity recognition and demonstration requirements are pointed out as additional requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The medical imaging project demands a state-of-the-art Enhanced V-Net architecture for BraTS dataset analysis. Mandate a multi-stage optimization process to achieve a minimum Dice coefficient of 0.90, with real-time Grad-CAM explanations for key feature attribution and a user-friendly interpretability report for clinical validation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"medical imaging\",\n        \"description\": \"The task is to analyze the BraTS dataset using a state-of-the-art Enhanced V-Net architecture.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Dice coefficient\",\n                \"value\": 0.90\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BraTS dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"lesion segmentation mask\"],\n            \"specification\": {\n                \"size\": \"undisclosed\",\n                \"modality\": \"MRI\"\n            },\n            \"description\": \"A medical imaging dataset for brain tumor segmentation\",\n            \"preprocessing\": [\"resized to common input size\"],\n            \"augmentation\": [\"data augmentation techniques for enhancing generalization\"],\n            \"visualization\": [\"slice-level and instance-level visualizations\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced V-Net\",\n            \"family\": \"V-Net variants\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art, multi-stage optimization\",\n                \"explainability\": \"with Grad-CAM for feature attribution and interpretability report\"\n            },\n            \"description\": \"A high-performance Enhanced V-Net tailored for BraTS dataset with real-time interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Multi-stage optimization, Dice coefficient target, and Grad-CAM for clinical validation.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"customized for Enhance V-Net\"],\n        \"target_device\": \"user-defined for real-time performance\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimization for speed on selected device\"],\n        \"demonstration\": \"clinician-friendly interpretability report and segmentation examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's advanced expertise, focusing on a state-of-the-art architecture, performance metrics, and interpretability requirements for a medical imaging project. Key details, such as model architecture specifics, are included, along with user-defined target device and deployment considerations. However, some fields, like exact preprocessing steps and data size, are left open-ended as they may depend on user's specific requirements and availability.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a sophisticated multilingual translation system utilizing the ParaCrawl database, employing a hybrid dual encoder architecture specifically tailored for English to Dutch translations. The system should not only guarantee high-precision word-for-word conversions but also adeptly capture and preserve the nuanced emotional nuances and cultural context from the source content.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Develop a sophisticated multilingual translation system, focusing on English to Dutch translations, using the ParaCrawl database. Employ a hybrid dual encoder architecture that prioritizes high precision and nuanced emotional and cultural context preservation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Semantic Similarity Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl database\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"English text\", \"Dutch translation\"],\n            \"specification\": {\n                \"size\": {\n                    \"training\": null,\n                    \"validation\": null,\n                    \"testing\": null\n                },\n                \"language\": [\"English\", \"Dutch\"]\n            },\n            \"description\": \"Large-scale parallel corpus for English to Dutch translations sourced from ParaCrawl.\",\n            \"preprocessing\": [\n                \"Text normalization\",\n                \"Sentence segmentation\"\n            ],\n            \"augmentation\": [\n                \"Back-translation\"\n            ],\n            \"visualization\": [\n                \"Neural network interpretability\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Dual Encoder for English to Dutch Translation\",\n            \"family\": \"neural networks\",\n            \"type\": \"Transformer-based\",\n            \"specification\": {\n                \"architecture\": {\n                    \"encoder_layers\": null,\n                    \"decoder_layers\": null,\n                    \"attention_heads\": null\n                },\n                \"training_details\": {\n                    \"optimizer\": \"Adam\",\n                    \"learning_rate\": null\n                }\n            },\n            \"description\": \"A state-of-the-art translation model using a hybrid dual encoder specifically designed for English to Dutch translations with a focus on precision and cultural context.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"title\": \"Cultural context preservation techniques\",\n            \"description\": \"Recent research on incorporating cultural and emotional context in multilingual NLP.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Contextual embeddings\",\n            \"Emotion detection components\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"Language translation API\",\n        \"inference_engine\": [\n            \"GPU-optimized inference\"\n        ],\n        \"demonstration\": \"Interactive translation dashboard showcasing cultural context preservation.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON reflects the user's request for a machine translation system between English and Dutch, using a ParaCrawl database. The system's requirements align with high precision and preservation of cultural nuances. Specific metrics are not provided (e.g., BLEU Score, semantic similarity), but the structure hints at custom evaluation for these. The complexity metric represents the need for optimized inference in a cloud environment. The knowledge piece indicates potential for advanced techniques in the model. The confidence score is set to medium due to the lack of specific model and performance metric details.\",\n        \"score\": 0.7\n    }\n}"}
{"task": "image segmentation", "instruction": "As an archaeologist unearthing a trove of ancient Mayan codices, we seek a cutting-edge deep learning model capable of segmenting not only intricate hieroglyphic images but also the distinct layers of pigments, faded murals, and hand-drawn annotations in these fragile, ethereal texts. This solution must not only recognize boundaries but also preserve the cultural and historical nuances in the form of a visually interpretable, pixel-level segmentation map.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"archaeology\",\n        \"description\": \"Seeking a deep learning model for segmenting ancient Mayan codices, with focus on hieroglyphic images, layer separation of pigments, murals, and hand-drawn annotations. The model should be able to recognize boundaries while maintaining cultural and historical nuances for visually interpretable, pixel-level segmentation maps.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mIoU (mean Intersection over Union)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"FPS (frames per second)\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mayan codices dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"hieroglyphics\", \"pigment layers\", \"murals\", \"annotations\"],\n            \"specification\": {\n                \"image_size\": \"variable\",\n                \"image_channels\": \"RGB + infrared (if available)\",\n                \"labels\": \"pixel-level segmentation map\"\n            },\n            \"description\": \"A dataset containing high-resolution images of ancient Mayan codices, with multi-modal data if available, for model training and testing.\",\n            \"preprocessing\": [\"image resizing, normalization, and augmentation\"],\n            \"augmentation\": [\"random cropping, flips, and color jittering\"],\n            \"visualization\": [\"visualization of segmentation maps before and after model output\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom deep convolutional neural network (CNN) or advanced segmentation architecture\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"e.g., U-Net, Mask R-CNN, or tailored for the task\",\n                \"depth_layers\": \"suitable for complex image segmentation\",\n                \"initialization\": \"e.g., pre-trained on ImageNet for transfer learning\"\n            },\n            \"description\": \"A deep learning model specifically designed to handle the intricate segmentation of Mayan codices.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Historical and cultural significance of Mayan art and its unique symbolism in codices\",\n        \"State-of-the-art techniques in deep image segmentation for fine-grained analysis\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Fine-tuning with specific preprocessing methods for Mayan codices\"\n        ],\n        \"target_device\": \"Cloud-based or high-performance computing for handling complex data and computations\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized libraries like TensorFlow or PyTorch\"],\n        \"demonstration\": \"Interactive web-based tool for researchers to visualize and interpret segmentation results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's requirements for a high-level deep learning project targeting Mayan codice segmentation, factoring in the user's expertise as a high-level expert. The model, performance metrics, and complexity metrics are tailored to the task, and the dataset's description and preprocessing steps consider the complexities of ancient texts. However, specific performance values are left open-ended to reflect the user's desire for flexibility.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for the advanced concrete material research team, your objective is to develop a highly specific tabular regression model using the High-Performance Concrete Dataset, which contains 15 distinct chemical composition variables. The dataset has been meticulously stratified into train, validation, and test sets with a focus on predicting compressive strength in megapascals (MPa) with an aim to minimize the mean absolute percentage error (MAPE) for exceptional predictive accuracy. Implement a state-of-the-art feature engineering pipeline and explain its impact on performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"materials science\",\n        \"description\": \"Develop a highly specific tabular regression model for predicting compressive strength of concrete in megapascals (MPa) using the High-Performance Concrete Dataset with 15 chemical composition variables. Strive for minimal mean absolute percentage error (MAPE) to achieve exceptional predictive accuracy. Feature engineering and its impact on performance are expected.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"runtime\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High-Performance Concrete Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"compressive strength (MPa)\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"dimensions\": {\n                    \"chemical composition variables\": 15\n                }\n            },\n            \"description\": \"A tabular dataset with chemical composition data for concrete, stratified into train, validation, and test sets.\",\n            \"preprocessing\": [\n                \"stratified splitting\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art tabular regression model\"\n            },\n            \"description\": \"A specialized model for predicting concrete compressive strength using advanced feature engineering.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Impact of feature engineering on performance: Enhancing feature representation, addressing multicollinearity, and selecting relevant features can significantly improve model performance and minimize MAPE.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"state-of-the-art techniques\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explanation of how feature engineering contributes to prediction accuracy\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the user's objective to develop a specialized regression model targeting concrete compressive strength with high accuracy. The performance metric MAPE is included, and the user's high expertise level suggests they understand the importance of feature engineering. The stratification and dataset specifics are emphasized to reflect the requirements. However, the detailed model architecture, exact performance values, and some missing target device details are areas that can be completed with further information from the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a machine learning project manager for a cutting-edge retail brand, our aim is to develop an advanced image segmentation model that can analyze the emotional engagement of customers in real-time during in-store events. Can you design a deep learning system capable of not only separating individuals from their dynamic, high-resolution event backgrounds but also detecting subtle facial expressions, body language, and environmental elements that indicate interest or satisfaction? This will enable us to optimize store layouts, product placement, and marketing strategies based on meaningful user insights.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"retail and marketing\",\n        \"description\": \"The user wants to develop an advanced image segmentation model for a retail brand to analyze emotional engagement of customers in real-time during in-store events. The model should be able to separate individuals from backgrounds, detect facial expressions and body language, and interpret interest or satisfaction levels.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score (for facial expressions and body language)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Average Precision (for image segmentation)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time (on edge devices)\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory footprint during inference\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"emotion labels, facial expressions, body language\"],\n            \"specification\": {\n                \"data_format\": \"dynamic high-resolution\",\n                \"sampling_rate\": null,\n                \"label_distribution\": null\n            },\n            \"description\": \"A dataset of real-time in-store event footage with annotations for customer emotional engagement and related features.\",\n            \"preprocessing\": [\"data augmentation\"],\n            \"augmentation\": [\"motion blur\", \"distortions\", \"noise addition\"],\n            \"visualization\": [\"real-time model performance monitoring\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image Segmentation and Emotion Detection Model\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"suitable for image segmentation and multi-label detection\",\n                \"training_layers\": null,\n                \"techniques\": [\"convolutional layers\", \"attention mechanisms\", \"multi-task learning\"]\n            },\n            \"description\": \"A deep learning model designed to perform image segmentation, facial expression and body language recognition, aimed at providing real-time customer engagement insights.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advantages of using transfer learning for faster model training\",\n        \"Incorporating explainable AI to interpret model outputs\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"custom emotion recognition features extraction\"],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"custom in-store analytics platform\",\n        \"inference_engine\": [\"real-time processing pipeline\"],\n        \"demonstration\": \"interactive dashboards showcasing user insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction covers designing a complex machine learning project, specifying the need for an advanced deep learning model and its requirements. The user's expertise level is high, indicating a solid understanding of the task. The requested model's features align with the problem, and the performance and complexity metrics demonstrate a focus on accuracy and real-world deployment constraints. However, values for specific metrics and implementation details are yet to be filled.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For an advanced underwater ecological imaging project, design and implement a state-of-the-art Multi-scale Dense Attention Pyramid (MDAP) model integrated with a custom noise reduction algorithm, specifically tailored for the CoralNet dataset. The model should address challenges in low light conditions, turbidity, and photochromic effects, while targeting a groundbreaking 20% boost in pixel-level accuracy compared to the current best results and require detailed evaluation of its resilience under extreme environmental scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"underwater ecological research\",\n        \"description\": \"Design and implement a state-of-the-art Multi-scale Dense Attention Pyramid (MDAP) model with a custom noise reduction algorithm for the CoralNet dataset. Target pixel-level accuracy improvement by 20% and address challenges like low light conditions, turbidity, and photochromic effects. Include evaluation under extreme environmental scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"pixel-level accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CoralNet dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"Underwater environmental factors\"],\n            \"specification\": {\n                \"data_format\": \"multi-modal (images and metadata)\",\n                \"image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"metadata\": {\n                    \"environmental_conditions\": [\"low light\", \"turbidity\", \"photochromic effects\"]\n                }\n            },\n            \"description\": \"Dataset for underwater ecological imaging focusing on coral detection under challenging conditions.\",\n            \"preprocessing\": [\"noise reduction\", \"multi-scale data augmentation\"],\n            \"augmentation\": [\"random brightness and contrast adjustments for low light conditions\"],\n            \"visualization\": [\"segmentation masks for qualitative analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MDAP with custom noise reduction\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Multi-scale Dense Attention Pyramid\",\n                \"customization\": \"noise reduction algorithm\"\n            },\n            \"description\": \"State-of-the-art deep learning model that integrates MDAP and a custom noise reduction technique specifically designed for the CoralNet dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Challenges: low light, turbidity, photochromic effects\",\n        \"Performance goal: 20% boost in pixel-level accuracy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"coral morphology feature extraction\"],\n        \"target_device\": \"high-performance compute cluster for large-scale training\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-based inference\"],\n        \"demonstration\": \"Qualitative and quantitative results showcasing model resilience under extreme conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been parsed into a JSON format accordingly. The user's expertise is assumed high, indicating they expect detailed and advanced design choices. The performance metrics, challenges, and specific data preprocessing steps are clearly stated. A state-of-the-art MDAP model with a custom noise reduction algorithm is specified. However, some fields like inference time and memory usage have not been provided with specific values, as they require more detailed environment information. The confidence score is set to 0.9 based on the clarity of the instruction and the coverage of the required information.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "For a challenging image classification project, I need a state-of-the-art deep learning model that can accurately distinguish between 50 distinct breeds of cats, considering variations in color, fur patterns, and subtle visual differences. Please recommend an architecture and suggest necessary preprocessing steps for optimal performance on my extensive dataset of high-resolution cat photographs.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal classification\",\n        \"description\": \"A project to develop a deep learning model for distinguishing between 50 distinct breeds of cats, accounting for color variations, fur patterns, and subtle differences. The dataset consists of high-resolution cat photographs.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High-resolution cat photographs\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": \"high\",\n                    \"height\": \"high\"\n                },\n                \"number_of_images\": \"extensive\",\n                \"variability\": [\n                    \"color variations\",\n                    \"fur patterns\",\n                    \"subtle visual differences\"\n                ]\n            },\n            \"description\": \"A dataset containing diverse, high-resolution images of 50 different cat breeds.\",\n            \"preprocessing\": [\n                \"Resizing to a standardized size (e.g., using a fixed input size or adaptive resizing techniques)\",\n                \"Normalization (subtracting the mean and dividing by standard deviation to adjust pixel values)\",\n                \"Data augmentation (e.g., rotation, flipping, and small transformations to increase model robustness)\"\n            ],\n            \"augmentation\": [\"rotation\", \"flipping\", \"color jittering\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"State-of-the-art deep learning architecture for image classification (e.g., ResNet50, DenseNet, or EfficientNet)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"Considering the challenge of subtle visual differences, a deeper and more sophisticated model may be beneficial\"\n            },\n            \"description\": \"A deep convolutional neural network designed for high accuracy in distinguishing between diverse cat breeds.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning could be useful, starting from a pre-trained model on a large dataset like ImageNet, and fine-tuning on the specific cat breed dataset\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature extraction using pre-trained layers (e.g., VGG or ResNet) for better performance\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstration of the model's accuracy on a sample set of images\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's request for a state-of-the-art model for an image classification project with high accuracy for 50 cat breeds. The performance metrics and preprocessing steps are suggested based on the specified problem. The choice of a deep learning architecture and the use of transfer learning indicate an understanding of the complexities and the user's high expertise level. Some metric values are left unspecified for optimization, as it is not directly provided. The confidence score reflects the comprehensiveness of the response given the information in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a highly scalable node classification model utilizing the intricate NELL dataset with a state-of-the-art Hypergraph Neural Network architecture. Emphasize on robustness against adversarial noise, effective handling of multi-relational edges, and efficiency in large-scale graphs. Aim for an accuracy of at least 90%, while conducting a comprehensive hyperparameter sensitivity analysis with extensive benchmarking, and provide a detailed report on performance trade-offs.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graph\",\n        \"description\": \"Develop a highly scalable and robust node classification model using the NELL dataset with a Hypergraph Neural Network (HGNN) architecture. The model should be resistant to adversarial noise and effectively handle multi-relational edges. Focus on performance in large-scale graphs with a target accuracy of at least 90%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node labels\"],\n            \"specification\": {\n                \"size\": null,\n                \"max_edge_degree\": null,\n                \"edge_types\": \"multi-relational\"\n            },\n            \"description\": \"A large-scale, intricate dataset for knowledge graphs, containing multi-relational edges and a focus on scalability.\",\n            \"preprocessing\": [\n                \"graph cleaning\",\n                \"node feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hypergraph Neural Network (HGNN)\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"HGNN\",\n                \"resistance_to_adversarial\": true,\n                \"multi-relational_edge_handling\": true\n            },\n            \"description\": \"An advanced model specifically designed for node classification in large, intricate hypergraphs.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"aspect\": \"robustness\",\n            \"insight\": \"HGNN has demonstrated resilience against adversarial noise.\"\n        },\n        {\n            \"aspect\": \"multi-relational edge handling\",\n            \"insight\": \"HGNN effectively handles multi-relational edges in knowledge graphs.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"hyperparameter tuning\",\n            \"benchmarking\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A detailed performance report with trade-offs on accuracy and computational efficiency.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the user's instruction, including details about the dataset, model architecture, performance goals, and desired analysis. The expert-level expertise and specific requirements of NELL dataset, HGNN, and scalability are captured. Although some details like exact metric values are not provided, the high confidence level reflects a general understanding of the requirements and the need for comprehensive analysis.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a senior project manager for an advanced space image analysis team, we need to identify not only a specific, obscure NASAdataset (e.g., Hubble Deep Field or Galaxy Zoo) for fine-grained galaxy and star classification. The task requires developing a deep convolutional neural network, combining transfer learning with custom data augmentation, while considering the challenges of class imbalance and architectural optimization. Can you advise on the best model architecture and expected hurdles in attaining state-of-the-art accuracy?", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"space research\",\n        \"description\": \"Identify a specific, obscure NASA dataset (e.g., Hubble Deep Field or Galaxy Zoo) for fine-grained galaxy and star classification. Develop a deep convolutional neural network using transfer learning, custom data augmentation, and address class imbalance and architectural optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"state-of-the-art accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NASAdataset (Hubble Deep Field or Galaxy Zoo)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"galaxy and star classifications\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"height\": \"variable\",\n                    \"width\": \"variable\"\n                },\n                \"image_channels\": [\"RGB\"],\n                \"dataset_size\": null,\n                \"imbalance_details\": {\n                    \"class_distribution\": null\n                }\n            },\n            \"description\": \"A fine-grained dataset for space images to classify galaxies and stars.\",\n            \"preprocessing\": [\"transfer learning\"],\n            \"augmentation\": [\"custom\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep convolutional neural network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"transfer learning with custom data augmentation\",\n                \"optimization\": [\"class imbalance handling\"],\n                \"parameters\": null,\n                \"custom_details\": {\n                    \"base_model\": \"pre-trained on ImageNet\",\n                    \"architecture_layers\": {\n                        \"convolutional\": [],\n                        \"pooling\": [],\n                        \"dense\": []\n                    }\n                }\n            },\n            \"description\": \"Deep CNN using transfer learning and customized data augmentation for galaxy and star classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Challenges: class imbalance may require techniques like SMOTE, focal loss, or weighted sampling; architectural optimization might involve NAS (Neural Architecture Search) or tweaking hyperparameters for improved efficiency.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction from pre-trained model\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow, PyTorch\"],\n        \"demonstration\": {\n            \"example_images\": \"showcase of fine-grained galaxy and star classifications\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the user's intention to consult a build task, with the user's high expertise level. The problem section specifies a challenging space image analysis task and the necessity for a deep CNN. Performance metric focuses on state-of-the-art accuracy. Challenges and architectural considerations are addressed in the knowledge section. The model section describes a deep CNN that uses transfer learning and custom data augmentation. Class imbalance is acknowledged, along with the possibility of needing specific techniques to handle it.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The task for the machine learning development team is to design a state-of-the-art, real-time question-answering system for e-commerce customer service, leveraging the comprehensive AmazonQA dataset. Utilize a fine-tuned DistilBERT model that exhibits exceptional accuracy, sentiment analysis, and deep intent recognition, all while achieving sub-second response times for uninterrupted user experiences.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"e-commerce customer service\",\n        \"description\": \"Design a state-of-the-art real-time question-answering system for e-commerce customer service using the AmazonQA dataset. The system must employ a fine-tuned DistilBERT model with high accuracy, sentiment analysis, and deep intent recognition, ensuring sub-second response times.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Sentiment analysis\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Deep intent recognition\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Response time\",\n                \"value\": \"sub-second\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AmazonQA dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"answers\", \"sentiment\", \"intent\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensionality\": null,\n                \"format\": \"structured\"\n            },\n            \"description\": \"A comprehensive dataset for e-commerce question answering with structured data for accurate fine-tuning of the DistilBERT model.\",\n            \"preprocessing\": [\"tokenization\", \"sentence splitting\"],\n            \"augmentation\": [\n                \"synthetic data generation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Fine-tuned DistilBERT\",\n            \"family\": \"transformers (BERT)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": null,\n                \"accuracy\": null,\n                \"number_of_parameters\": null,\n                \"FLOPs\": null\n            },\n            \"description\": \"A state-of-the-art, fine-tuned DistilBERT model designed for question answering with high accuracy, sentiment analysis, and intent recognition.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced NLP techniques, specific tuning for AmazonQA context\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware encoding\",\n            \"intent-specific embeddings\"\n        ],\n        \"target_device\": \"cloud-based inference\",\n        \"deployment_endpoint\": \"API-based, RESTful architecture\",\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"Live chat or mock interface showcasing real-time performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intent to build a high-end question-answering system with specific requirements, such as accuracy, sentiment analysis, and fast response times. The domain (e-commerce customer service) and the dataset (AmazonQA) are defined accurately. Some complexities like the specific metric values, preprocessing details, and deployment specifics are left to be filled with more specific information by the machine learning team based on detailed analysis and architecture choices.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a node classification model using the YouTube Algebric Connectivity dataset with Significance Graph Neural Networks (SGNNs), incorporating a novel feature extraction technique to handle extreme sparsity. Demand a minimum 85% accuracy and showcase interpretability in distinguishing diverse content creator categories.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"social media (YouTube)\",\n        \"description\": \"Develop a node classification model using the YouTube Algebric Connectivity dataset with Significance Graph Neural Networks (SGNNs), focusing on extreme sparsity with a novel feature extraction technique. Target is to achieve at least 85% accuracy and demonstrate interpretability in content creator category distinction.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube Algebric Connectivity\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"content creator categories\"],\n            \"specification\": {\n                \"sparsity_level\": \"extreme\",\n                \"node_labels\": [\"content creator categories\"]\n            },\n            \"description\": \"A dataset representing connections between YouTube content creators, focusing on extreme sparsity with the need for novel feature extraction.\",\n            \"preprocessing\": [\"address extreme sparsity\"],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance graphs\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Significance Graph Neural Networks (SGNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novel_feature_extraction\": true,\n                \"interpretability_techniques\": [\"explainable AI\", \"attention mechanisms\"]\n            },\n            \"description\": \"A GNN model using Significance Graph Neural Networks for node classification, leveraging a novel feature extraction technique to handle extreme sparsity and showcasing interpretability for content creator categorization.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"novel feature extraction algorithm\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Visualize significant node connections and feature importance in content creator categories\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been structured to match the user's requirement for a node classification model using the YouTube Algebric Connectivity dataset with SGNNs. The emphasis on extreme sparsity, novel feature extraction, accuracy, and interpretability is captured in the problem description and performance metrics. While there are some placeholders for missing information (like the model name, direct search source for the dataset, and device deployment), the response captures the essence of the user's instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for the travel app enhancement, you've been tasked with integrating a cutting-edge, real-time domain-specific question-answering system that leverages thePushMatrix.AI dataset. The new feature should be a streamlined, mobile-optimized adaptation of the ScholarlyT5 model, specializing in rapid delivery of concise and trustworthy travel insights, historical trivia, and personalized itinerary suggestions based on users' preferences and previous interactions. Given the app's target market of frequent business travelers, the system must maintain an F1 score above 95% while ensuring responses are generated in under 200 milliseconds for a seamless user experience. Develop a project roadmap and allocate resources efficiently to meet these stringent requirements while also considering the impact on battery life and data usage.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"travel app\",\n        \"description\": \"Integrate a real-time domain-specific question-answering system, leveraging PushStack.AI dataset, to create a mobile-optimized ScholarlyT5 model for providing travel insights, historical trivia, and personalized itinerary suggestions based on user preferences and interactions.\",\n        \"performance_metrics\": {\n            \"F1 score\": {\n                \"name\": \"F1 score\",\n                \"value\": 0.95,\n                \"unit\": \"%\"\n            },\n            \"response time\": {\n                \"name\": \"Response Time\",\n                \"value\": 200,\n                \"unit\": \"milliseconds\"\n            }\n        },\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Battery Impact\",\n                \"value\": null,\n                \"unit\": \"Impact on battery life\"\n            },\n            {\n                \"name\": \"Data Usage\",\n                \"value\": null,\n                \"unit\": \"Data consumption per query\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PushStack.AI dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_size\": \"large\",\n                \"domain_specificity\": \"travel related\"\n            },\n            \"description\": \"A real-time, travel domain-specific dataset for training and testing the question-answering system.\",\n            \"preprocessing\": [\"data cleaning, normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ScholarlyT5 (Mobile-Optimized)\",\n            \"family\": \"Transformer model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"mobile-friendly\",\n                \"architecture\": \"ScholarlyT5 adaptation\",\n                \"deployment\": \"mobile environment\"\n            },\n            \"description\": \"A real-time, mobile-optimized version of the ScholarlyT5 model tailored for travel-related information and personalized suggestions.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"domain_relevance\": \"Focused on frequent business travelers\",\n            \"model_specialization\": \"Customized for travel app needs\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"personalization algorithms\",\n            \"real-time adaptation\"\n        ],\n        \"target_device\": \"mobile devices\",\n        \"deployment_endpoint\": \"API integration with travel app\",\n        \"inference_engine\": [\"GPU-accelerated for speed\"],\n        \"demonstration\": \"mobile app usability testing\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's intent for building a model with specific performance requirements and resource allocation considerations. Performance metrics include F1 score and response time. The model, dataset, and service-related details are aligned with the travel app context and target market. However, since the complexity metrics are unspecified, it's inferred that they would be crucial aspects of the project, and more information is needed.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Employ a deep residual LSTM incorporating multi-seasonality and exogenous factors for our historical solar- and wind-powered energy dataset. Forecast hourly consumption for the next 72 hours, maintaining a real-time capacity of 99% accuracy and an optimization target of latencies under 100 microseconds per prediction, ensuring model efficiency on resource-constrained edge devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy production and consumption\",\n        \"description\": \"Forecast hourly consumption for the next 72 hours using a deep learning model with multi-seasonality and exogenous factors for a historical solar- and wind-powered energy dataset, maintaining a real-time accuracy of 99% and prediction latencies below 100 microseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"Latency per prediction\",\n                \"value\": 0.0001\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"microseconds\"\n            },\n            {\n                \"name\": \"Model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Historical solar- and wind-powered energy dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly consumption\"],\n            \"specification\": {\n                \"time_series_length\": null,\n                \"data_size\": null\n            },\n            \"description\": \"Dataset containing historical hourly consumption data for solar and wind energy, including multi-seasonality patterns and exogenous factors.\",\n            \"preprocessing\": [\n                \"multi-seasonality decomposition\",\n                \"exogenous feature engineering\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Residual LSTM\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"lstm with multi-seasonality and exogenous inputs\",\n                \"parameters\": {\n                    \"embedding_size\": null,\n                    \"hidden_layers\": null,\n                    \"seasonality_factors\": null\n                }\n            },\n            \"description\": \"A deep learning model that incorporates residual connections and LSTM cells, designed for multi-seasonal time-series forecasting while considering exogenous factors.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonality handling\",\n            \"exogenous factor incorporation\"\n        ],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient inference optimized for edge devices\"],\n        \"demonstration\": \"Real-time, interactive performance monitoring\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a deep residual LSTM model for energy forecasting, with high accuracy and low latency requirements. The model family and task align with the time-series analysis area, and specific performance and complexity metrics are defined. The dataset is described based on the need for multi-seasonality and exogenous factors, with preprocessing steps that match the user's requirements. The target device is mentioned, but specific details are open-ended. The model description is generic, leaving room for specific configuration details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "For the avant-garde machine learning research project, we must develop a highly specialized question-answering system leveraging HFT-BERT (Hierarchical Fusion Transformer with Enhanced Entity Retrieval), a cutting-edge variation of BERT. This model should be fine-tuned exclusively on the esoteric 'Extravaganza of obscure Quandaries' (EQOQ) dataset, which consists of questions with rare historical references and esoteric knowledge. We require not only an F1 score of over 95% in discerning intricate details, but also demonstrate near real-time response time of less than 150 milliseconds, ensuring optimal user experience even when exploring the most obscure corners of knowledge. Furthermore, the system must handle multilingual queries seamlessly, maintaining accuracy across five lesser-known languages simultaneously.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"avant-garde research\",\n        \"description\": \"Develop a highly specialized question-answering system utilizing HFT-BERT for an avant-garde project with emphasis on esoteric historical and obscure knowledge. The model should be fine-tuned on the EQOQ dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95,\n                \"target\": \"intrinsic performance\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 150,\n                \"unit\": \"milliseconds\",\n                \"target\": \"real-time response\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extravaganza of Obscure Quandaries (EQOQ)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"answer\"],\n            \"specification\": {\n                \"domain\": \"esoteric historical references\",\n                \"languages\": [\"multiple (5 lesser-known)\"]\n            },\n            \"description\": \"A dataset consisting of questions with rare historical and esoteric knowledge for fine-tuning the model.\",\n            \"preprocessing\": [\"fine-tuning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HFT-BERT\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"enhancements\": \"Hierarchical Fusion Transformer with Enhanced Entity Retrieval\",\n                \"language_models\": \"BERT\",\n                \"languages\": [\"multiple (5 lesser-known)\"]\n            },\n            \"description\": \"A highly specialized HFT-BERT model tailored for question answering with esoteric knowledge, fine-tuned on the EQOQ dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for multilingual, real-time performance\"],\n        \"demonstration\": \"Smooth and near real-time response for multilingual queries\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response captures the core requirements specified in the user's instruction. It includes details about the cutting-edge model (HFT-BERT), fine-tuning on the specific dataset (EQOQ), performance metrics (F1 score and near real-time response time), and multilingual support. The model is expected to perform well on complex and obscure questions due to its sophisticated architecture and the EQOQ dataset's focus. The only missing information is the deployment endpoint and potential specifics about feature engineering, which could be inferred from additional context or clarification from the user.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "machine translation", "instruction": "Revise the project for the machine translation team, targeting a specialized TEDx Talks dataset for dialectal Brazilian Portuguese. Mandate the adoption of state-of-the-art M6 model, incorporating SentencePiece tokenization for improved vocabulary handling, and restrict the implementation to meet the constraints of ultra-portable devices with sub-1GB RAM and minimal 10-hour continuous operation without recharging.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"conference talks\",\n        \"description\": \"Revise the machine translation project for the TEDx Talks dataset, specifically targeting dialectal Brazilian Portuguese. Mandate the use of the state-of-the-art M6 model and include SentencePiece tokenization for enhanced vocabulary handling. The implementation should be optimized for ultra-portable devices with sub-1GB RAM and the ability to operate continuously for at least 10 hours without recharging.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": 1.0,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TEDx Talks (Dialectal Brazilian Portuguese)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"specialized\",\n                \"language\": \"dialectal Brazilian Portuguese\"\n            },\n            \"description\": \"Dataset containing TEDx Talks content in dialectal Brazilian Portuguese for machine translation.\",\n            \"preprocessing\": [\"SentencePiece tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6\",\n            \"family\": \"neural machine translation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"tokenization_method\": \"SentencePiece\"\n            },\n            \"description\": \"Advanced M6 model tailored for dialectal Brazilian Portuguese machine translation, optimized for ultra-portable devices with limited memory and processing power.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"ultra-portable, sub-1GB RAM\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"continuous operation for 10 hours without recharging\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the user's request for a machine translation project update using the M6 model, with consideration for SentencePiece tokenization and constraints for ultra-portable devices. While performance metrics are not explicitly stated, the problem statement suggests optimizing for inference time and memory constraints. Some specific details are left open-ended, such as the model's exact specifications and deployment options, as the user's requirements are primarily about the chosen model and constraints.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "For a cutting-edge machine translation project, design a highly optimized, layered bidirectional LSTM architecture utilizing the specialized Europarl v7 dataset (version 7.1 with domain-specific dialects) for seamless, near-native Spanish-English translation. Emphasize on minimizing latency to <250 milliseconds, targeting real-time, interactive chat applications with stringent efficiency requirements. Ensure model efficiency through pruning techniques and parallel computation strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation, real-time communication\",\n        \"description\": \"Design a cutting-edge machine translation model with a highly optimized, layered bidirectional LSTM architecture for seamless Spanish-English translation on the Europarl v7 dataset (version 7.1 with domain-specific dialects). The focus is on minimizing latency to below 250 milliseconds, targeting real-time interactive chat applications with strict efficiency requirements. The model should incorporate pruning techniques and parallel computation strategies for improved efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 250\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"parallel computation speedup\",\n                \"value\": null,\n                \"unit\": \"X\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Europarl v7.1 (Spanish-English)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": {\n                        \"sentences\": null,\n                        \"tokens\": null\n                    },\n                    \"eval\": {\n                        \"sentences\": null,\n                        \"tokens\": null\n                    }\n                },\n                \"dialects\": \"domain-specific\",\n                \"languages\": [\"Spanish\", \"English\"]\n            },\n            \"description\": \"A dataset for machine translation that includes Spanish-English translation pairs from Europarl v7.1 dataset with dialect-specific content.\",\n            \"preprocessing\": [\n                \"tokenization\",\n                \"text normalization\"\n            ],\n            \"augmentation\": [\n                \"backtranslation\"\n            ],\n            \"visualization\": null,\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"bidirectional LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"layered\",\n                \"optimizer\": {\n                    \"name\": \"Adam\",\n                    \"learning_rate\": null\n                },\n                \"pruning_techniques\": \"used\"\n            },\n            \"description\": \"A high-performance machine translation model using bidirectional LSTM with emphasis on latency minimization and efficiency through pruning and parallel computation\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"The project should incorporate state-of-the-art sequence-to-sequence models, such as transformer architectures for improved translation accuracy\"\n        },\n        {\n            \"text\": \"Efficient batching is crucial for real-time processing\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"sentence-level features\",\n            \"pos tagging\",\n            \"dependency parsing\"\n        ],\n        \"target_device\": \"real-time interactive chat applications\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"batch processing\",\n            \"GPU acceleration\"\n        ],\n        \"demonstration\": \"Interactive chat application demonstration with real-time translations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers all aspects of the task, indicating a highly skilled user. Performance metrics include latency and specific architecture details. Dataset preprocessing, augmentation, and model knowledge are also adapted to the requirements. Pruning and parallel computation strategies are taken into account. However, some missing values might require additional research or clarification from the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "For the comprehensive analysis, you need the machine learning development team to apply a multi-step, ensemble-based approach using lightweight transformers on the diverse 250 Hexagon ML/UCR Time Series Anomaly Detection datasets. Aim for not only average accuracy exceeding 70% but also optimize model efficiency and interpretability simultaneously.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"data analysis and monitoring\",\n        \"description\": \"Apply a multi-step, ensemble-based approach using lightweight transformers on 250 diverse Hexagon ML/UCR Time Series Anomaly Detection datasets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average accuracy\",\n                \"value\": 0.70\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hexagon ML/UCR Time Series Anomaly Detection datasets\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"anomaly labels\"],\n            \"specification\": null,\n            \"description\": \"A diverse collection of datasets for time series anomaly detection, including multiple datasets from Hexagon ML and UCR repository.\",\n            \"preprocessing\": [\n                \"resampling\",\n                \"normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"per-instance visualization\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble-based with lightweight transformers\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"model_components\": \"multiple lightweight transformer models\",\n                \"interpretability_layers\": \"enabled\"\n            },\n            \"description\": \"A multi-step ensemble model leveraging lightweight transformers for high performance and interpretability in anomaly detection on diverse time series data.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Consider using ensemble techniques like stacking or blending to boost performance while maintaining interpretability.\"\n        },\n        {\n            \"description\": \"Optimize for lightweight transformers by focusing on model pruning, efficient training, and parameter tuning.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\",\n            \"feature extraction\"\n        ],\n        \"target_device\": \"optimized for edge computing or real-time IoT scenarios\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized TensorFlow or PyTorch\"\n        ],\n        \"demonstration\": \"Include clear visual explanations and a model explanation report.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON accurately represents the user's request, indicating a need for a machine learning project using a multi-step ensemble approach with lightweight transformers, targeting specific accuracy and efficiency goals for anomaly detection on diverse time series data. The lack of specific model performance targets for inference time and memory usage leaves room for optimization. The included steps for feature engineering, deployment, and interpretability align with the user's instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "Could the machine learning team develop an efficient, interpretable model that not only forecasts connections in the Citeseer citation network but also identify key features contributing to link formation, while maintaining an accuracy of at least 90% and handling a high volume of sparse data?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Develop an efficient, interpretable model for forecasting connections in the Citeseer citation network, with a focus on identifying key features contributing to link formation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.90\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory consumption\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"Handling sparse data\",\n                \"value\": true\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Citeseer citation network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"connections\"],\n            \"specification\": null,\n            \"description\": \"A large graph dataset with sparse data, where nodes are research papers and edges represent citations. The task involves link prediction and understanding feature importance.\",\n            \"preprocessing\": [\"handling sparse data\"],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance analysis\"],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Interpretable and efficient\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability Techniques\": \"model-agnostic techniques\"\n            },\n            \"description\": \"A graph neural network model optimized for high accuracy (90%) in link prediction, capable of feature interpretability and handling sparse data.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection based on interpretability\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for sparse data\"],\n        \"demonstration\": \"Demonstrate interpretability of key features contributing to link formation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has been populated with information based on the user's instruction. The problem area, downstream task, and accuracy goal have been specified. The model requirements for efficiency, interpretability, and handling sparse data are incorporated. The dataset source and preprocessing steps are aligned with the sparse data and need for key feature identification. However, specific values for complexity metrics are not provided, and some fields are left blank as the instruction does not give explicit details, such as model name or desired deployment platform.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a sophisticated deep convolutional generative adversarial network (DCGAN) hybridized with a recurrent autoencoder for the comprehensive analysis of the diverse urban sounds dataset. The objective is to not only predict sound levels for the next 48 hours with high accuracy, but also discern and visualize the evolving patterns of various sound types over time, requiring multi-step temporal reasoning and a detailed understanding of sound event segmentation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"urban sound analysis\",\n        \"description\": \"The objective is to develop a sophisticated deep convolutional generative adversarial network (DCGAN) hybridized with a recurrent autoencoder for comprehensive analysis of the diverse urban sounds dataset. The task requires high accuracy in predicting sound levels for the next 48 hours, and the ability to discern and visualize evolving patterns of various sound types over time through multi-step temporal reasoning and sound event segmentation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Urban Sounds Dataset\",\n            \"modality\": [\"audio\", \"time series\"],\n            \"target_variables\": [\"sound levels\"],\n            \"specification\": {\n                \"size\": null,\n                \"duration\": {\n                    \"hours\": 48,\n                    \"minutes\": 0\n                },\n                \"features\": {\n                    \"sound event segments\": {\n                        \"type\": \"multiclass\",\n                        \"count\": null\n                    }\n                }\n            },\n            \"description\": \"A dataset containing diverse urban sound recordings for analysis, emphasizing multi-step temporal reasoning.\",\n            \"preprocessing\": [\"sound event segmentation\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"evolving patterns of sound types over time\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DCGAN + Recurrent Autoencoder\",\n            \"family\": \"neural networks\",\n            \"type\": \"hybrid\",\n            \"specification\": {\n                \"architecture\": {\n                    \"DCGAN\": {\n                        \"convolution_layers\": null,\n                        \"deconvolution_layers\": null\n                    },\n                    \"Recurrent Autoencoder\": {\n                        \"cell_type\": \"LSTM\",\n                        \"latent_dim\": null,\n                        \"input_size\": null\n                    }\n                },\n                \"accuracy\": null,\n                \"FLOPs\": null,\n                \"other_details\": \"Combines CNN and RNN for accurate time-series predictions\"\n            },\n            \"description\": \"A sophisticated model using DCGAN and a recurrent autoencoder for long-term sound prediction and segmentation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal feature extraction\",\n            \"sequence modeling\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Keras\"],\n        \"demonstration\": \"Plots and explanations of generated predictions and segmentation results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's request to develop a complex model using DCGAN and a recurrent autoencoder. The expert-level expertise suggests a thorough understanding of the requirements. The areas of multi-step temporal reasoning and sound event segmentation have been captured in the problem description and performance metrics. However, some details such as specific model performance values, detailed preprocessing, and target deployment remain open due to missing specifics in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "Sure, let's develop a machine learning-driven, time-series tabular regression model that integrates multiple project variables (e.g., team size, complexity, technology stack) to forecast precise budget distributions and duration estimates for upcoming initiatives, ensuring historical outliers are accounted for and seasonality factors are analyzed.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"project management\",\n        \"description\": \"Build a machine learning-driven time-series model for tabular regression to forecast budget distributions and duration estimates for projects based on project variables such as team size, complexity, and technology stack. Incorporate handling of historical outliers and analysis of seasonality factors.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE (Budget)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSE (Duration)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"project data\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"budget distribution\", \"duration estimates\"],\n            \"specification\": {\n                \"dimension\": {},\n                \"length\": {}\n            },\n            \"description\": \"A dataset containing historical project data including variables like team size, complexity, and technology stack, along with budget distributions and duration estimates.\",\n            \"preprocessing\": [\"Handle historical outliers\", \"Seasonality analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Feature importance\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"time-series tabular regression model integrating multiple input variables\"\n            },\n            \"description\": \"A deep learning model specifically designed for time-series tabular regression, incorporating factors like team size, complexity, and technology stack for forecasting budget and duration.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"Scale features\", \"Feature normalization\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Interactive dashboard showcasing model predictions and analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response covers the key elements of the user's instruction. A build intent is set, along with the medium expertise level. The model requirements, project data details, and performance metrics are described based on the instruction. The request for handling outliers and seasonality analysis in preprocessing and a potential focus on performance are highlighted. However, specific metric values, model architecture, and deployment details are not detailed as they would require more context.\",\n        \"score\": 0.88\n    }\n}\n"}
{"task": "question answering", "instruction": "The task for the machine learning development team involves designing a portable QA model based on TinyBERT, optimized for resource-constrained mobile devices. Targeting the NewsQA dataset, you must achieve not only a stringent accuracy benchmark of at least 90%, but also demonstrate superior efficiency by meeting a sub-400 millisecond response time while maintaining model efficiency. Conduct multi-step performance tuning and present a detailed analysis of trade-offs in the design decisions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"information retrieval\",\n        \"description\": \"Design a portable QA model based on TinyBERT optimized for resource-constrained mobile devices, targeting the NewsQA dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.9,\n                \"comparison_operator\": \">=\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Response time\",\n                \"value\": 0.4,\n                \"unit\": \"seconds\",\n                \"comparison_operator\": \"<\"\n            }\n        ],\n        \"trade-offs\": {\n            \"description\": \"Analyze design trade-offs while optimizing for accuracy and response time, focusing on model efficiency for mobile devices.\"\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NewsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"answers\"],\n            \"specification\": {\n                \"size\": \"large\",\n                \"domain\": \"news articles\"\n            },\n            \"description\": \"A question answering dataset built on news articles, suitable for TinyBERT-based QA model development.\",\n            \"preprocessing\": [\"tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"data distribution\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT\",\n            \"family\": \"Transformer-based QA models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": {\n                    \"number_of_layers\": 4,\n                    \"hidden_size\": 128\n                },\n                \"inference_speed\": {\n                    \"target_device\": \"mobile\",\n                    \"FLOPs\": null\n                }\n            },\n            \"description\": \"A lightweight TinyBERT model tailored for resource-constrained mobile devices.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"Mobile-friendly feature extraction\", \"Model pruning\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"cross-platform API\",\n        \"inference_engine\": [\"Hardware acceleration, if available\"],\n        \"demonstration\": \"Real-time performance with device settings on a sample dataset\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON covers the core aspects of the user's instruction, including specifying TinyBERT as the model, NewsQA dataset, and resource-constrained mobile devices. It addresses the accuracy requirement, response time constraint, and emphasizes multi-step performance tuning with trade-offs. The performance metrics include accuracy and response time with specified threshold values. The dataset and service sections capture key information like preprocessing, data sources, and deployment strategies.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager, you are asked to develop a cutting-edge AI system that not only enhances real-time sports analytics by detecting equipment and player actions in sports broadcasts, but also needs to incorporate a tiered reasoning approach. The team must refine the existing RetinaNet model into anovel, latency-optimized version (RetinaNetX) tailored for the 'Sports-1M' dataset. This upgraded model must demonstrate exceptional multi-stage understanding to differentiate between various equipment types and complex player maneuvers, ensuring seamless integration into fast-paced live coverage.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"sports analytics\",\n        \"description\": \"Develop a cutting-edge AI system that uses a latency-optimized RetinaNetX, a novel variant of RetinaNet, specifically tailored for the 'Sports-1M' dataset. The system must detect equipment and identify complex player actions in real-time sports broadcasts, incorporating tiered reasoning for enhanced analytics.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sports-1M\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"equipment types\", \"player actions\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": {\n                    \"height\": null,\n                    \"width\": null\n                },\n                \"length\": null,\n                \"specific体育直播场景\": null\n            },\n            \"description\": \"A dataset containing sports broadcasts for real-time equipment detection and player action recognition.\",\n            \"preprocessing\": [\"latency optimization for real-time processing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RetinaNetX\",\n            \"family\": \"RetinaNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"latency-optimized\",\n                \"accuracy vs. latency tradeoff\": \"optimized for real-time sports analytics\"\n            },\n            \"description\": \"A novel, latency-optimized RetinaNet-based model specifically designed for detecting sports equipment and complex player actions in real-time sports broadcasts.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"fast-paced live coverage environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time optimized engines\"],\n        \"demonstration\": \"streamlined integration into live sports broadcasts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the user's requirements for a build task, with a focus on a high-level project manager's perspective. The expert level of expertise and detailed description of the system requirements, including tiered reasoning, indicate a comprehensive understanding of the problem. The metrics specified for optimization target real-time performance. The dataset and model sections closely align with the 'Sports-1M' dataset and the RetinaNetX model adaptation. However, the specific performance metrics or the deployment environment's inference engines are not detailed enough, leaving room for further clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "Develop a real-time, personalized hotel recommendation engine using the 'Extended Trivago Global Tourism' dataset, which encompasses diverse user behaviors, location history, and seasonal trends. Implement a Graph Attention Network (GAT) architecture to capture complex user-item interactions and geographical dependencies, ensuring minimal latency for seamless in-app experience on mobile devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"travel and hospitality\",\n        \"description\": \"Create a real-time, personalized hotel recommendation engine using the 'Extended Trivago Global Tourism' dataset, incorporating diverse user behaviors, location history, and seasonal trends.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency (mobile devices)\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extended Trivago Global Tourism dataset\",\n            \"modality\": [\"multimodal\", \"time series\", \"graph\"],\n            \"target_variables\": [\"user behaviors\", \"location history\"],\n            \"specification\": {\n                \"dataset_size\": null,\n                \"feature_dimensions\": null,\n                \"time_series_length\": null\n            },\n            \"description\": \"A comprehensive dataset with user behavior data, location history, and seasonal trends for hotel recommendations.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"user behavior patterns\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Graph Attention Networks (GAT)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"GAT for sequential recommendations\",\n                \"complexity\": null,\n                \"attention_heads\": null\n            },\n            \"description\": \"A GAT-based model that captures complex user-item interactions and geographical dependencies in real-time for the hotel recommendation engine.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"importance\": \"mobile devices, low latency\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"personalization\", \"real-time updates\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"optimized for mobile\"\n        ],\n        \"demonstration\": \"in-app interactions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure is well-aligned with the given instruction, detailing a build task for a recommendation engine, utilizing the Extended Trivago dataset, and specifying GAT architecture with mobile latency considerations. Performance metrics include latency and the requirement for mobile-specific optimizations. The dataset includes relevant modalities and preprocessing steps. The user's expertise level is considered high, and a few key factors for the service component are mentioned, though some specific values are left open to clarify based on detailed project requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Our project is focused on differentiating five specific subtypes of skin melanocytic lesions (Benign Melanocytic, Melanoma-in-Situ, Nv Melanoma, Basal Cell Carcinoma, and Actinic Keratosis) in the highly diverse and challenging HAM10000 dataset using the EfficientNet-B7 architecture. The model must achieve at least 99% precision, maintaining a balanced F1-score, and have <5% false negatives. In addition to accuracy, we need a model with interpretable feature attributions using Integrated Gradients, which must align with dermatologists' diagnostic reasoning for accurate clinical decision support.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medical imaging\",\n        \"description\": \"Differentiate five skin melanocytic lesion subtypes (Benign Melanocytic, Melanoma-in-Situ, Nv Melanoma, Basal Cell Carcinoma, and Actinic Keratosis) using the HAM10000 dataset with EfficientNet-B7. Aim for 99% precision, balanced F1-score, and <5% false negatives. Require interpretability with Integrated Gradients for dermatologists' diagnostic support.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"False negatives rate\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Model inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HAM10000 skin lesion dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"Melanocytic lesion subtype\"],\n            \"specification\": {\n                \"image_size\": \"input required for EfficientNet-B7\",\n                \"dataset_division\": \"train/test/validation split\"\n            },\n            \"description\": \"A diverse and challenging dataset for skin lesion classification, containing images and labels for the five specified subtypes.\",\n            \"preprocessing\": [\"Size normalization for EfficientNet-B7\"],\n            \"augmentation\": [\"Data augmentation techniques\"],\n            \"visualization\": [\"Sample images with predicted labels\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet-B7\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"architecture\": \"EfficientNet\",\n                \"depth\": \"B7\",\n                \"parameters\": \"to be optimized for high accuracy\",\n                \"flops\": \"to minimize while maintaining performance\"\n            },\n            \"description\": \"A deep learning model using EfficientNet-B7 for image classification in the context of dermatology.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Integrate dermatologists' diagnostic reasoning for interpretability\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Integrated Gradients for feature attributions\"\n        ],\n        \"target_device\": \"AI-aided dermatology devices or platforms\",\n        \"deployment_endpoint\": \"Institutional dermatology clinics or a secure cloud platform\",\n        \"inference_engine\": [\"Optimized for medical imaging analysis\"],\n        \"demonstration\": \"Clinical use case examples demonstrating alignment with diagnostic reasoning\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's request for a machine learning project with specific performance targets, dataset requirements, and interpretability constraints. The provided metrics, such as precision and false negative rate, reflect the high performance desired, and the expert level of the user suggests an understanding of the complexity of the task and the chosen deep learning architecture.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "The task at hand is to design a specialized text classification system for the arXiv repository, targeting niche fields within Physics, Math, and Computer Science. Utilize a domain-specific HAN+BERT fusion model, optimizing for micro- and macro-averaged F1 scores above 90%, and ensure seamless real-time adaptations to handle the dynamic growth of the dataset, with minimal latency for new paper submissions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"academic research (arXiv)\",\n        \"description\": \"Design a specialized text classification system for niche fields within Physics, Math, and Computer Science using a domain-specific HAN+BERT fusion model.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"micro-avg F1 score\",\n                \"value\": 0.9\n            },\n            {\n                \"name\": \"macro-avg F1 score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency for new paper submissions\",\n                \"value\": \"minimal\",\n                \"unit\": \"real-time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv repository\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"paper categories (Physics, Math, Computer Science)\"],\n            \"specification\": null,\n            \"description\": \"A dynamic dataset of academic papers from Physics, Math, and Computer Science for text classification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HAN+BERT fusion model (domain-specific)\",\n            \"family\": \"neural networks\",\n            \"type\": \"fusion models\",\n            \"specification\": {\n                \"performance optimizations\": [\"real-time adaptation\"]\n            },\n            \"description\": \"A domain-specific hybrid model combining Hierarchical Attention Networks (HAN) and BERT for text classification in niche fields within Physics, Math, and Computer Science.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"To handle the dynamic growth of the dataset, the model must be designed to seamlessly adapt in real-time.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing engine\"],\n        \"demonstration\": \"Demonstrate real-time and seamless classification for new paper submissions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided information includes a clear task of building a specialized text classification system, the model type and fusion, and the desired performance metrics. The user's high expertise level indicates a deeper understanding. The dataset and source are inferred based on the arXiv repository. The response also highlights the need for real-time adaptation and latency minimization. Missing information would include a more precise model specification and a deployment endpoint.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for our expanding retail operations, you're tasked with overseeing the implementation of a state-of-the-art AI-driven inventory management system. We require a custom Faster R-CNN model, enhanced with advanced objection detection algorithms, to accurately detect and count a diverse range of SKU-110K products on shelves in complex store environments. The model must demonstrate exceptional precision, maintaining an error rate below 1%, and exhibit nuanced differentiation between categories such as perishable goods and electronics. In addition to the model development, provide a comprehensive performance analysis report that showcases the model's F1 score and how it adapts to seasonal product fluctuations. Ensure the report includes real-world scenario testing and a comparative evaluation with existing industry benchmarks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"retail inventory management\",\n        \"description\": \"Implement a custom Faster R-CNN model for SKU-110K product detection and counting in complex store environments, prioritizing high precision with an error rate below 1%. The model should differentiate between perishable goods and electronics. Require a performance analysis report showcasing F1 score, adapting to seasonal product fluctuations, and compare with industry benchmarks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.99\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"error rate\",\n                \"value\": 0.01,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \" SKU-110K Retail Product Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product type (perishable vs. electronics)\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"height\": null,\n                    \"width\": null\n                },\n                \"SKU variety\": 110000\n            },\n            \"description\": \"A large-scale dataset containing images of various products in complex store environments for SKU-level detection.\",\n            \"preprocessing\": [\"image normalization, augmentation\"],\n            \"augmentation\": [\"random rotations, flipping\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom Faster R-CNN\",\n            \"family\": \"Faster R-CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"high\",\n                \"seasonal adaptability\": true,\n                \"industry benchmark comparison\": true\n            },\n            \"description\": \"An advanced object detection model specifically for SKU-110K product detection in retail with strong emphasis on precision and nuanced differentiation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Seasonal fluctuations in product availability and demand, benchmark models in the retail inventory management domain.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom in-house infrastructure\",\n        \"inference_engine\": [\"GPU-based\"],\n        \"demonstration\": \"Real-world tests in store environments and comparative analysis visuals\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's request for a high-level custom model with specific precision requirements, complex scenario testing, and benchmark comparisons. The user's expertise level is reflected in the high-end objectives and performance analysis requirements. The details on the dataset, model, and service components indicate a comprehensive response.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a time-series forecasting expert, your team has been tasked with developing a cutting-edge algorithm for a specialized solar energy management system. The project aims to not only predict daily peak electricity generation hours based on weather patterns, solar panel efficiency, and historical data, but also forecast potential energy shortages during maintenance periods or equipment failures. The algorithm should integrate real-time data from multiple solar farms spread across different geographical regions, accounting for seasonal fluctuations, and propose power distribution strategies to ensure grid stability. Moreover, it should provide predictive insights for energy consumption during extreme weather events to aid in disaster response planning. How would you design and implement this complex, scalable, and accurate forecasting system, while adhering to strict data privacy regulations and maintaining real-time adaptability?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy management\",\n        \"description\": \"Develop a cutting-edge algorithm for a solar energy management system predicting daily peak electricity generation hours, forecasting energy shortages during maintenance or equipment failures, integrating real-time data from multiple solar farms, accounting for seasonal fluctuations, and proposing power distribution strategies for grid stability. The system should handle extreme weather events, adhere to data privacy regulations, and maintain real-time adaptability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Scalability\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"Real-time adaptability\",\n                \"value\": \"yes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Solar farm data (multi-region, historical and real-time)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"Peak electricity generation\", \"Potential energy shortages\"],\n            \"specification\": {\n                \"data sources\": [\"weather patterns\", \"solar panel efficiency\", \"historical data\"],\n                \"geographical coverage\": \"multiple regions\"\n            },\n            \"description\": \"Data from various solar farms with weather, solar panel efficiency, and historical electricity generation data for accurate forecasts.\",\n            \"preprocessing\": [\"seasonal adjustment\", \"feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time-series patterns\", \"data correlations\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Time-Series Forecasting Algorithm\",\n            \"family\": \"Deep Learning (RNN, LSTM, or Transformer variants)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"complexity\": \"high\",\n                \"data privacy measures\": \"GDPR compliant\"\n            },\n            \"description\": \"A highly accurate and scalable algorithm using deep learning techniques to handle real-time data and account for complex factors.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Energy consumption patterns during extreme weather events\",\n        \"Ad-hoc data privacy techniques for data integration\",\n        \"Methods for real-time algorithm updates\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"seasonality handling\", \"geographical clustering\"],\n        \"target_device\": \"cloud-based or edge computing\",\n        \"deployment_endpoint\": \"cloud-based energy management platform\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Apache Kafka for real-time updates\"],\n        \"demonstration\": \"Data visualization and dashboards for stakeholders\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the key elements of the complex project, such as high-level algorithm, multi-region data, real-time adaptability, and data privacy. It addresses specific tasks like peak prediction, energy shortage forecasting, and power distribution strategies. Performance metrics are suggested without fixed values, acknowledging the requirement for accuracy. Complexity metrics are explicitly stated, and relevant data sources and tools are identified. Potential challenges and requirements for demonstration and deployment are also considered.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a machine learning project manager, I need a highly accurate and real-time object detection system for identifying various wild animals crossing a residential front yard. The model must adhere to strict energy efficiency standards and demonstrate high precision in low-light conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife monitoring\",\n        \"description\": \"A machine learning project for a highly accurate and real-time object detection system, specifically targeting various wild animals crossing a residential front yard. The model must prioritize energy efficiency and demonstrate high precision, especially in low-light conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": \"high\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"optimized for real-time\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": \"optimized\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Residential front yard animal crossing dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"animal type\"],\n            \"specification\": {\n                \"image_size\": \"variable (optimized for real-time)\",\n                \"image_format\": \"RGB or grayscale\"\n            },\n            \"description\": \"A dataset containing images of wild animals crossing residential front yards with varying lighting conditions.\",\n            \"preprocessing\": [\n                \"low-light enhancement\",\n                \"image resizing\"\n            ],\n            \"augmentation\": [\n                \"data augmentation for low-light conditions\"\n            ],\n            \"visualization\": [\n                \"real-time detection accuracy visualizations\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Real-time object detection\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Lightweight, efficient deep learning model (e.g., YOLOv3, SSD-MobileNet)\",\n                \"optimizer\": \"Faster convergence, low memory footprint\"\n            },\n            \"description\": \"A real-time, energy-efficient object detection model optimized for animal identification in low-light residential front yard scenarios.\"\n        }\n    ],\n    \"confidence\": {\n        \"rationale\": \"This JSON response captures the user's requirement for a machine learning project that involves a highly accurate and real-time object detection system, focusing on wild animal identification. The user's expertise level suggests they have a deep understanding of the project's technical aspects. Performance metrics demand high accuracy and precision, while energy efficiency and low-light condition handling are addressed in the complexity metrics. The provided dataset description indicates a need for real-life scenarios with augmentation for low-light situations. The model selected is lightweight and optimized for energy efficiency while maintaining accuracy.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series classification", "instruction": "You are now tasked with developing a cutting-edge machine learning model for recognizing and forecasting complex kinesthetic movements using the UWave Gesture Library dataset. This dataset consists of high-resolution time series data collected from specific wearable devices (INPUT_SEQ_LEN=945 samples with 8 unique channels, INPUT_DIM=8). The temporal sequences are temporally partitioned into a train set with 70% data, a validation set for model fine-tuning (15%), and a test set for robust generalization (15%). Your objective is to design a sophisticated deep-learning architecture that leverages long short-term memory (LSTM) networks and attention mechanisms, achieving a minimum accuracy of 95% on unseen test sequences while maintaining interpretability and real-time performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"human motion recognition\",\n        \"description\": \"Develop a cutting-edge machine learning model using LSTM and attention mechanisms for recognizing and forecasting complex kinesthetic movements from the UWave Gesture Library dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"milliseconds\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UWave Gesture Library dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"complex kinesthetic movements\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 945,\n                \"INPUT_DIM\": 8,\n                \"partitioning\": {\n                    \"train\": 70,\n                    \"validation\": 15,\n                    \"test\": 15\n                }\n            },\n            \"description\": \"A high-resolution time series dataset from wearable devices, capturing 8 unique channels of data for kinesthetic movement recognition.\",\n            \"preprocessing\": [\"data normalization\", \"temporal partitioning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"LSTM with Attention\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep LSTM network with integrated attention mechanism\",\n                \"interpretability\": \"explainable model\"\n            },\n            \"description\": \"A sophisticated deep learning model employing LSTM networks and attention mechanisms for time-series classification of complex kinesthetic movements from the UWave Gesture Library dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"data normalization for time series\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimization for real-time performance\"],\n        \"demonstration\": \"Visualizations and interpretability metrics for model predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the user's request for a cutting-edge model using LSTM and attention mechanisms, targeting a high accuracy and maintaining interpretability. The dataset details are accurately represented, and the user's expertise level is reflected in the task. The performance and complexity metrics include realistic goals, though specific values are not specified. The knowledge, service, and confidence components align well with the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the advanced financial sentiment analysis team, you've been assigned to refine the Reuters-21578 dataset to classify news articles into intricate financial categories with sub-genres, ensuring a state-of-the-art fastText model is employed. Your goal is to achieve at least 90% precision in market-moving news detection while maintaining an overall accuracy of 87%, all within a strict deadline. Additionally, propose a hybrid evaluation metric to balance true positives, false positives, and false negatives for nuanced topic classification.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Refine the Reuters-21578 dataset to classify news articles into intricate financial categories with sub-genres using a fastText model. Focus on market-moving news detection with a precision target of at least 90% and an overall accuracy of 87% within a strict deadline.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.90\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.87\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Reuters-21578 dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"financial categories with sub-genres\"],\n            \"specification\": null,\n            \"description\": \"An advanced financial sentiment dataset requiring refinement for nuanced topic classification and market-moving news detection.\",\n            \"preprocessing\": [\"refinement for intricate categories\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"fastText\",\n            \"family\": \"word embeddings\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"state-of-the-art\"\n            },\n            \"description\": \"Employed for advanced financial sentiment analysis and market-moving news detection\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Proposed hybrid evaluation metric to balance true positives, false positives, and false negatives for accurate and nuanced topic classification.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Narrative about how to showcase the model's performance with a focus on the hybrid evaluation metric\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the project manager's intent to refine the dataset, build a fastText model, and achieve specified performance metrics. The hybrid evaluation metric is included as a knowledge piece. The user's high expertise level justifies the assumption of a detailed understanding of the requirements. However, some metric values and further details are missing due to the vagueness around inference time or the model's specific specifications.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For a cutting-edge urban planning project, develop a novel variant of SegNet on the Vaihingen dataset that demands exceptional accuracy in distinguishing not only diverse vegetation types but also assessing their specific species, age, and stress levels. The model should demonstrate robust performance across diverse lighting angles, seasons, and weather patterns, while incorporating climate resilience measures, and present in-depth analysis for municipal green space management.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"urban planning\",\n        \"description\": \"For a cutting-edge urban planning project, develop a novel SegNet variant that focuses on distinguishing diverse vegetation types, their species, age, and stress levels on the Vaihingen dataset. The model should be robust to lighting angles, seasonal variations, weather patterns, and include climate resilience measures.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Species classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Age classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Stress level classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Robustness to lighting angles\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Robustness to seasons and weather\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\n                \"Vegetation type\",\n                \"Species\",\n                \"Age\",\n                \"Stress level\"\n            ],\n            \"specification\": null,\n            \"description\": \"A high-resolution aerial imagery dataset for urban planning, specifically focusing on vegetation details.\",\n            \"preprocessing\": [\n                \"Orthorectification\",\n                \"Radiometric normalization\"\n            ],\n            \"augmentation\": [\n                \"Simulated lighting variations\",\n                \"Weather augmentation\"\n            ],\n            \"visualization\": [\n                \"Cross-section analysis\",\n                \"Heatmaps of feature importance\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Novel SegNet variant\",\n            \"family\": \"SegNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"encoder_layers\": \"...\",\n                    \"decoder_layers\": \"...\",\n                    \"attention mechanism\": \"yes\"\n                },\n                \"hyperparameters\": {\n                    \"dropout rate\": 0.2,\n                    \"learning rate\": null,\n                    \"optimizer\": \"Adam\"\n                }\n            },\n            \"description\": \"A state-of-the-art SegNet architecture tailored for precise urban vegetation analysis, considering climate resilience measures.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Climate resilience literature\",\n        \"Species-specific vegetation classification techniques\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Environmental feature extraction\",\n            \"Species-specific features\"\n        ],\n        \"target_device\": \"Cloud-based for larger dataset and processing power\",\n        \"deployment_endpoint\": \"的城市绿化管理部门 or smart city infrastructure\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Cloud AutoML\"],\n        \"demonstration\": \"Interactive 3D visualization of urban green spaces with species, age, and stress level information\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON accurately captures the user's intent to build a high-performance SegNet model for a complex urban planning project. It covers the necessary aspects such as dataset requirements, performance metrics (implied due to the need for species and stress level classification), and deployment considerations. However, specific metric values are not provided as the user's instruction focuses on the novel model development process and the broader context of municipal green space management. The confidence score is high, considering the well-defined problem and requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Revamp the Enron Email dataset task for node classification using a Hierarchical RGCN architecture designed for large-scale enterprise communication networks. Classify employees into distinct clusters based on their encrypted email exchanges, accounting for temporal dependencies and real-time updates, while ensuring minimal latency and high computational efficiency over a distributed graph database with millions of edges.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"corporate communication\",\n        \"description\": \"Revamp the Enron Email dataset task using a Hierarchical RGCN architecture for classifying employees into clusters based on encrypted email exchanges. Account for temporal dependencies and real-time updates while ensuring minimal latency and high computational efficiency on a distributed graph database with millions of edges.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"employee clusters\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": {\n                    \"nodes\": null,\n                    \"edges\": null\n                },\n                \"time_series\": null\n            },\n            \"description\": \"A large-scale enterprise communication network dataset reflecting encrypted email exchanges between employees.\",\n            \"preprocessing\": [\n                \"data anonymization\",\n                \"temporal feature extraction\"\n            ],\n            \"augmentation\": [\n                \"real-time updates\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical RGCN\",\n            \"family\": \"neural networks\",\n            \"type\": \"graph neural networks\",\n            \"specification\": {\n                \"architecture\": \"Hierarchical\",\n                \"RGCN_layers\": null,\n                \"latency optimization\": true\n            },\n            \"description\": \"A Hierarchical Graph RNN model specifically designed for large-scale node classification with temporal dependencies and real-time updates in a distributed graph database.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Distributed graph database management for handling millions of edges.\"\n        },\n        {\n            \"content\": \"Emphasis on minimal latency for real-time updates.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal feature extraction\",\n            \"encryption-related feature engineering\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"high-performance distributed environment\",\n        \"inference_engine\": [\n            \"optimized graph processing engine\"\n        ],\n        \"demonstration\": \"real-time, live performance visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's request for building a model with a Hierarchical RGCN architecture, addressing the large-scale nature of the task with considerations for temporal dependencies, real-time updates, and performance metrics. However, some values remain unspecified for performance and complexity metrics due to the lack of concrete numerical targets. The user's high expertise level is indicated, which suggests a reasonable level of detail in the provided information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "The machine learning development team is tasked with enhancing an image classification system for the Fashion-MNIST dataset using Capsule Networks. The objective is to achieve at least 95% accuracy while demonstrating resilience to affine transformations. In addition, the project must include a thorough analysis of capsule network performance versus Convolutional Neural Networks (CNNs), emphasizing efficiency and robustness trade-offs in a visually interpretable manner.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion industry\",\n        \"description\": \"Enhance the image classification system for the Fashion-MNIST dataset using Capsule Networks, aiming for at least 95% accuracy and demonstrating resilience to affine transformations. Include a performance comparison between Capsule Networks and Convolutional Neural Networks in terms of efficiency and robustness in a visually interpretable manner.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"efficiency vs. robustness trade-off\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"class labels\"],\n            \"specification\": {\n                \"image_size\": [28, 28],\n                \"number_of_classes\": 10\n            },\n            \"description\": \"A dataset for fashion image classification, containing grayscale images of clothing items.\",\n            \"preprocessing\": [\"resizing to a standard size\", \"normalization\"],\n            \"augmentation\": [\"affine transformations\"],\n            \"visualization\": [\"comparison with CNN performance\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Capsule Network\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"capsule-based\",\n                \"accuracy target\": 0.95,\n                \"resistance to affine transformations\": true\n            },\n            \"description\": \"A state-of-the-art model utilizing Capsule Networks for enhanced image classification in the face of affine transformations.\"\n        },\n        {\n            \"name\": \"Convolutional Neural Networks (Baseline comparison)\",\n            \"description\": \"A baseline model, comparing capsule networks' efficiency and robustness with traditional CNNs.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Affine transformations: translation, scaling, rotation, and shear\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient inference\"],\n        \"demonstration\": \"visual explanations and comparison of models\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's requirements for a machine learning project, including the task, dataset, model architecture, accuracy target, and a comparison with CNNs. The performance metrics reflect the user's emphasis on accuracy, while the efficiency vs. robustness trade-off is indicated without a fixed value. Some details, such as the exact model efficiency figure and feature engineering steps, are left unspecified as they are not provided in the instruction, but the overall structure and context are valid.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "The project demands the development of a tailored GraphSAGE model, leveraging EdgeConv layers with receptive field sampling for the DBLP citation network. The model must classify papers into niche academic disciplines with precision and recall above 0.90, while optimizing for low computational footprint in a resource-constrained environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"The project requires developing a GraphSAGE model with EdgeConv layers and receptive field sampling for the DBLP citation network. The focus is on classifying papers into niche academic disciplines with precision and recall of at least 0.90, while optimizing for low computational complexity in a resource-constrained environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.90\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": 0.90\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"unit\": \"milliseconds\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DBLP citation network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"academic disciplines\"],\n            \"specification\": {\n                \"graph_structure\": \"DBLP citation links\",\n                \"node_count\": \"N (to be determined)\",\n                \"edge_count\": \"M (to be determined)\"\n            },\n            \"description\": \"A graph dataset based on the DBLP citation network, where nodes represent papers and edges denote citations.\",\n            \"preprocessing\": [\"Handling missing data\", \"Node feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Visualizing the graph structure and distribution of classes\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GraphSAGE with EdgeConv\",\n            \"family\": \"GraphSAGE\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"EdgeConv layers\": \"Receptive field sampling\"\n                },\n                \"parameters\": {\n                    \"Model size\": \"Small/efficient\",\n                    \"Learning rate\": \"Optimized for resource-constrained environment\"\n                }\n            },\n            \"description\": \"A tailored GraphSAGE model utilizing EdgeConv layers with receptive field sampling for the DBLP citation network, aiming for low resource usage.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Performance optimization techniques for resource-constrained environments\",\n        \"Importance of feature sampling in EdgeConv layers for niche classes\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Feature importance-based feature selection\"],\n        \"target_device\": \"resource-constrained environment (e.g., IoT, edge device)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Lightweight, optimized for resource-constrained environments\"],\n        \"demonstration\": \"Comparing model performance with efficiency trade-off\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's demand for a machine learning project, focusing on building a GraphSAGE model with EdgeConv and specific performance targets. The resource constraints are considered in various aspects of the project, from data processing to model architecture and deployment. However, some details like exact node count, edge count, and specific learning rate are not provided and can be filled based on more specific guidelines.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager now seeks to develop a high-performance neural machine translation system for the WMT19 datasets focused on legal document translation from Russian to English. Emphasize on utilizing advanced transformer architectures, like MBERT or T5, and incorporate domain-specific language modeling. Ensure a rigorous evaluation using case-sensitive detokenized BLEU, aiming for a minimum score of 45, with a focus on maintaining fluency and terminology accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"legal document translation\",\n        \"description\": \"The project aims to develop a high-performance neural machine translation system using advanced transformer architectures, specifically MBERT or T5, for legal document translation from Russian to English. The focus is on domain-specific language modeling and achieving a minimum case-sensitive detokenized BLEU score of 45 while ensuring fluency and terminology accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score (case-sensitive, detokenized)\",\n                \"value\": 45\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"sentences per second\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 legal translation (Russian to English)\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"training_samples\": null,\n                \"validation_samples\": null,\n                \"test_samples\": null\n            },\n            \"description\": \"A dataset for legal document translation from Russian to English, used for training and evaluation.\",\n            \"preprocessing\": [\n                \"domain adaptation for legal text\",\n                \"sentence segmentation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Transformer (MBERT or T5)\",\n            \"family\": \"transformer architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": null,\n                \"architecture_details\": {\n                    \"attention_heads\": null,\n                    \"hidden_units\": null\n                }\n            },\n            \"description\": \"A neural machine translation model based on MBERT or T5, optimized for legal document translation with domain-specific language modeling.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"resource\",\n            \"description\": \"Legal terminology and domain-specific jargon to be included in the model's training data.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain adaptation for legal text\", \"sentence segmentation\", \"tokenization\"],\n        \"target_device\": \"high-performance computing (GPU or cloud)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-CPU hybrid\", \"TPU\"],\n        \"demonstration\": \"Case study of sample translations demonstrating fluency, terminology accuracy, and BLEU score results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's requirements to build a high-performance neural machine translation system for legal document translation. It specifies advanced transformer architectures, domain-specific language modeling, and performance metrics. Some details like specific model size and evaluation setup are left open-ended to account for potential choices between MBERT and T5. The model training pipeline and service requirements are also tailored to the user's expectations.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager, you are overseeing the development of a sophisticated deep learning system for multi-step advanced time-series forecasting in a highly competitive environment. The Electricity dataset, now decomposed into train, validation, and test sets with intricate patterns, requires a model to handle input sequences of historical data (96 timesteps, 321 features, INPUT_SEQ_LEN=96, INPUT_DIM=321) for not just immediate prediction but to forecast the subsequent 96-step ahead sequences (PRED_SEQ_LEN=96, PRED_DIM=321). Emphasize on improved performance through enhanced versions of mean squared error (MSE) and mean absolute error (MAE) metrics, while ensuring interpretability and scalability for real-world grid management applications. The team must present a comprehensive strategy and iterative prototype before deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"electricity grid management\",\n        \"description\": \"Develop a sophisticated deep learning system for multi-step advanced time-series forecasting in the Electricity dataset. The model must handle historical sequences of 96 timesteps and 321 features. Target is to forecast 96-step ahead sequences with a focus on improved MSE and MAE performance, while ensuring interpretability and scalability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean squared error (MSE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error (MAE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"future sequences\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321\n            },\n            \"description\": \"Decomposed into train, validation, and test sets with complex patterns for multi-step forecasting\",\n            \"preprocessing\": [\"time series decomposition\", \"data normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time-series plots\", \"feature importance\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning (specifically LSTM or Transformer-based architecture)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-step forecasting\",\n                \"interpretability techniques\": \"LIME or SHAP\"\n            },\n            \"description\": \"A deep learning model designed for multi-step advanced time-series forecasting in the Electricity dataset, targeting enhanced MSE and MAE with interpretability features.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Use transfer learning from similar domains to improve performance.\",\n        \"Implement model ensembles for improved stability and accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonality analysis\",\n            \"trend analysis\",\n            \"cyclical patterns identification\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated environment\"],\n        \"demonstration\": \"Iterative prototyping, performance tracking, and interpretability workshops\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects a detailed instruction where the user is an expert, focusing on a specific time-series forecasting task with complex requirements. The dataset decomposition, input and output sequences, and performance metrics are clearly described. However, since the specific model architecture and exact performance targets are not provided, there may be a margin of uncertainty. The comprehensive strategy involves feature engineering, deployment settings, and interpretability considerations.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "Develop a targeted text classification model for detecting financial phishing SMS using theCredit Card Fraud Detection dataset. The ideal architecture combines a hierarchical CNN and Attention mechanism to handle short, high-precision text. Aim for precision above 98% and a mobile-specific latency of less than 5 milliseconds, ensuring real-time, device-friendly operation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance and security\",\n        \"description\": \"Develop a targeted text classification model for detecting financial phishing SMS using the Credit Card Fraud Detection dataset. The model should use a hierarchical CNN with Attention mechanism for high-precision performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 5,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Credit Card Fraud Detection dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"financial phishing SMS detection\"],\n            \"specification\": {\n                \"type\": \"structured data with text\",\n                \"length\": \"short to medium\"\n            },\n            \"description\": \"Dataset for detecting financial phishing SMS containing credit card fraud data.\",\n            \"preprocessing\": [\"tokenization\", \"lowercasing\"],\n            \"augmentation\": [\n                \"synthetic data generation for SMS text to increase variety\"\n            ],\n            \"visualization\": [\"word frequency and co-occurrence analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical CNN with Attention\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"architecture\": {\n                    \"CNN layers\": \"...\",\n                    \"Attention mechanism\": \"mobile-friendly\"\n                },\n                \"train_speed\": \"optimally for short text latency\",\n                \"inference_time\": null\n            },\n            \"description\": \"A deep learning model incorporating a hierarchical CNN and Attention mechanism for financial phishing SMS detection, with mobile-specific latency constraints.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider text length limitations, SMS-specific language and formatting, and the impact of false positives on mobile users.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual embedding for SMS text\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"on-device inference for real-time operation\"],\n        \"demonstration\": \"phishing SMS detection live demo using test SMS samples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON captures the user's intent to build a text classification model, with high expertise, and adherence to the credit card fraud detection dataset and the requirements of a hierarchical CNN with attention mechanism. Performance metrics are specified, and complexity is accounted for with mobile latency and other constraints. Attention is given to device-friendly operation and data preprocessing suitable for SMS text data. The confidence score is high due to the clear alignment with the instruction's details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "For a comprehensive real estate analytics platform, design an advanced image recognition system that not only differentiates between various property amenities (like solar panels, co-working spaces, and backyard entertainment areas) but also analyzes context, weather effects, and seasonal variations. The system must integrate with the existing CRM, provide detailed reports, and continuously learn to improve its recognition with minimal human intervention, ensuring a seamless buyer experience and optimizing property listings.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"real estate analytics\",\n        \"description\": \"Design an advanced image recognition system for property amenities, context analysis, weather effects, and seasonal variations within a comprehensive real estate analytics platform. The system should integrate with CRM, generate detailed reports, and support minimal human intervention for continuous improvement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"real estate property images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"amenities\", \"context\", \"weather effects\", \"seasonal variations\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"image_dimensions\": null,\n                \"number_of_classes\": 4\n            },\n            \"description\": \"A collection of real estate property images with labels for various amenities, context, weather, and seasonal attributes.\",\n            \"preprocessing\": [\"image resizing\", \"data augmentation\"],\n            \"augmentation\": [\"random cropping\", \"color jittering\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (CNN) or advanced deep learning architectures\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": null,\n                \"number_of_layers\": null,\n                \"learning_rate\": null\n            },\n            \"description\": \"An advanced image recognition model that integrates context, weather effects, and seasonal variations for property amenity analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Contextual features are essential for accurate recognition. Incorporating weather and seasonal data from external APIs will enhance the system's performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"CRM integration\", \"report generation\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-powered cloud environment\"],\n        \"demonstration\": \"interactive property browsing with real-time recognition updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the user's request for an advanced image recognition system in the real estate domain, with a focus on various property amenities, context, weather, and seasonality. Performance metrics are broad, reflecting the need for continuous improvement with minimal human intervention. The system is designed to be integrated with CRM and demonstrate a seamless buyer experience. The missing metric values indicate that specific targets are yet to be defined, reflecting the real-world nature of the project where those would likely be determined during implementation.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Considering our expanding dataset of high-resolution aerial images for property inspections, develop a real-time, accurate image segmentation model that not only differentiates between buildings, trees, and roads, but also accurately detects changes over time, incorporating a machine learning pipeline with temporal coherence and strict performance benchmarks to minimize human intervention.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"property inspection\",\n        \"description\": \"Develop a real-time, accurate image segmentation model that differentiates between buildings, trees, and roads, while detecting changes over time with temporal coherence. Incorporate a machine learning pipeline and strict performance benchmarks for minimal human intervention.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"change detection precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"change detection recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"high-resolution aerial images for property inspections\",\n            \"modality\": [\"image\", \"time series\"],\n            \"target_variables\": [\"building\", \"trees\", \"roads\"],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"time_series_length\": null\n            },\n            \"description\": \"A dataset with high-resolution images for property inspections, capturing changes over time for machine learning analysis.\",\n            \"preprocessing\": [\n                \"temporal alignment\",\n                \"image normalization\"\n            ],\n            \"augmentation\": [\n                \"spatial transforms\",\n                \"temporal shifts\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"real-time, sequential model with temporal coherence\",\n                \"model_depth\": null,\n                \"architecture_details\": \"temporal convolutions\"\n            },\n            \"description\": \"A real-time image segmentation model using deep learning with a focus on temporal change detection and high performance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Temporal coherence techniques such as LSTMs or 3D convolutions are crucial\",\n        \"Performance optimization to reduce false negatives in change detection\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction from multi-temporal images\",\n            \"time-series representation learning\"\n        ],\n        \"target_device\": \"embedded system with low-latency requirements\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized C++ libraries for real-time performance\"\n        ],\n        \"demonstration\": \"live interactive property inspection tool\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON captures the essence of the user's request, focusing on building a real-time image segmentation model with change detection, temporal coherence, and performance benchmarks. The user's high expertise level is represented in the parsed data. However, specific performance values, model architecture details, and deployment information are left undefined to account for the lack of precise metrics in the original instruction.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As a project manager for the advanced AI time-series anomaly detection team, your mandate is to develop a novel hybrid architecture using LSTM and Autoencoders. The 'Aircraft Engine Telemetry' dataset should be comprehensively analyzed to create a real-time, high-resolution predictive model with AUC-PR greater than 0.95. This must efficiently identify precursor patterns to minimize false positives, ensuring optimal aircraft safety and operational efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"aviation safety and operations\",\n        \"description\": \"Develop a novel hybrid architecture combining LSTM and Autoencoders for real-time, high-resolution aircraft engine anomaly detection using the 'Aircraft Engine Telemetry' dataset. Aim for an AUC-PR score of 0.95 or higher to minimize false positives, ensuring optimal safety and operational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-PR\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"time constraints\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Aircraft Engine Telemetry\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"anomaly flags\"],\n            \"specification\": {\n                \"data_resolution\": \"high\",\n                \"data_length\": \"variable\"\n            },\n            \"description\": \"A dataset containing time-series data from aircraft engine telemetry for anomaly detection, with the goal of real-time prediction and high-resolution analysis.\",\n            \"preprocessing\": [\"data cleaning, normalization, and feature extraction\"],\n            \"augmentation\": [\"time-series augmentation techniques\"],\n            \"visualization\": [\"anomaly pattern visualization\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Hybrid LSTM & Autoencoder\",\n            \"type\": \"neural networks\",\n            \"specification\": \"A novel architecture combining LSTM for sequence modeling and Autoencoders for anomaly detection in real-time\",\n            \"description\": \"A high-resolution predictive model designed to identify precursor patterns and minimize false positives for aircraft engine anomalies.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Trends in aircraft telemetry data and best practices in hybrid models for anomaly detection in time-series data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"optimized for real-time inference\"],\n        \"target_device\": \"embedded systems and IoT for aircraft monitoring\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for resource-constrained environments\"],\n        \"demonstration\": \"Generated reports showcasing predictive model performance and anomaly detection results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the user's expertise level, describes the time-series analysis and anomaly detection task, performance expectations, and emphasizes real-time operation and false positive reduction. The metrics capture the efficiency and effectiveness objectives, while the dataset and model sections adapt to the specific requirements. The model's hybrid approach, LSTM for sequence understanding and Autoencoders for anomaly detection, aligns with the instruction's mandate. The missing values in complexity metrics show a need for optimization without rigid constraints.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "As a project manager for a cutting-edge machine learning team, I need you to explore the feasibility of employing transformer models, specifically considering their adaptability and performance enhancement in discerning complex forged video manipulations from the Celeb-DF dataset. Can you design an innovative method that outperforms random baselines and demonstrates the potential for cross-modal analysis in the realm of deepfake detection?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"cybersecurity, video analysis\",\n        \"description\": \"Investigate the feasibility of using transformer models, specifically for detecting complex video manipulations from the Celeb-DF dataset, with a focus on adaptability and performance improvement compared to random baselines. Design an innovative method showcasing cross-modal analysis potential in deepfake detection.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Area Under the Curve (AUC-ROC)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"parameters\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"forgery label\"],\n            \"specification\": {\n                \"data_size\": null,\n                \"image_dimensions\": null,\n                \"video_duration\": null\n            },\n            \"description\": \"A multimodal dataset containing real and forged videos of Celebrities, suitable for assessing cross-modal performance in deepfake detection.\",\n            \"preprocessing\": [\"data normalization\", \"modal fusion\"],\n            \"augmentation\": [\"random cropping\", \"temporal jittering\"],\n            \"visualization\": [\"model attention maps\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer with adaptations for multimodal input\",\n                \"adapter layers\": null\n            },\n            \"description\": \"A transformer-based model designed for complex video forgery detection, leveraging cross-modal analysis and improving upon random baselines.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Current limitations and state-of-the-art performance in multimodal deepfake detection\"\n        },\n        {\n            \"description\": \"Research on transformer models in detecting audio-visual anomalies\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific embeddings\", \"temporal coherence analysis\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized inference framework\"],\n        \"demonstration\": \"visual explanations and cross-modal alignment examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the user's instructions, emphasizing the exploration of transformer models in video forgery detection and the potential for cross-modal analysis. Performance metrics such as accuracy, F1 score, and AUC-ROC are included with placeholders for concrete values. The lack of specific details leaves room for assumption while capturing the main objective. Given the user's high expertise level, the provided information is likely to be comprehensive.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a healthcare AI consultant, devise a specialized question-answering model that not only utilizes the BioASQ dataset for medical diagnosis but also incorporates F1-score optimization and domain-specific entity linking. Mandate real-time updates with PubMed articles, ensuring patient confidentiality in data handling, and strict adherence to HIPAA regulations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Design a specialized question-answering model for medical diagnosis using the BioASQ dataset, optimizing for F1-score, and incorporating domain-specific entity linking. Ensure real-time updates with PubMed articles while maintaining patient confidentiality under HIPAA regulations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"real-time\"\n            },\n            {\n                \"name\": \"data privacy\",\n                \"value\": \"HIPAA compliant\",\n                \"unit\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"medical entities\", \"diagnosis-related questions\"],\n            \"specification\": {\n                \"dataset_size\": \"Dataset specific to medical domain\",\n                \"entity_types\": \"Medical concepts, entities\"\n            },\n            \"description\": \"A dataset for medical question answering, including questions and relevant PubMed articles.\",\n            \"preprocessing\": [\n                \"Privacy-preserving data handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"PubMed articles\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"PubMed IDs, real-time updates\"],\n            \"specification\": null,\n            \"description\": \"Real-time access to PubMed articles for model updates and entity linking.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"question-answering\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"Incorporating entity linking and F1-score optimization\",\n                \"PubMed integration\": true,\n                \"HIPAA compliance\": true\n            },\n            \"description\": \"A specialized NLP model designed for medical diagnosis question answering with entity linking, real-time PubMed integration, and HIPAA compliance.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Adherence to HIPAA regulations for handling patient data.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Privacy-preserving techniques\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"HIPAA-compliant system demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's request for a question-answering model with specific details on the dataset, task, and performance metrics. Privacy and regulatory requirements are adequately addressed. However, specifics like model architecture and exact F1-score value are not included due to the complex nature of the task and need for further research or consultation.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for a high-precision materials research team, you must develop a tabular regression model using the recently procured Mohs Hardness Dataset. Split it into train, validation, and test sets, ensuring a 10% stricter prediction accuracy based on median absolute error (MedAE) compared to industry benchmarks. Implement a feature importance analysis and optimize model interpretability without compromising performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"materials research\",\n        \"description\": \"Develop a tabular regression model using the Mohs Hardness Dataset for materials research, aiming for at least 10% stricter prediction accuracy based on median absolute error compared to industry benchmarks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Median Absolute Error (MedAE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mohs Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"hardness\"],\n            \"specification\": {\n                \"split_distribution\": [\n                    {\n                        \"train\": 0.6,\n                        \"validation\": 0.2,\n                        \"test\": 0.2\n                    }\n                ],\n                \"missing_values_handling\": \"imputation\",\n                \"data_size\": null\n            },\n            \"description\": \"A dataset collected for materials research, containing features related to the hardness of materials and the target variable to be predicted.\",\n            \"preprocessing\": [\"handling missing values\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature importance analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"tabular regression model\",\n            \"type\": \"classical machine learning\",\n            \"specification\": {\n                \"interpretability\": true,\n                \"performance_tradeoff\": \"interpretable\"\n            },\n            \"description\": \"A tabular regression model tailored for predicting hardness in materials research, with emphasis on interpretability while maintaining accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Include a clear report or dashboard showcasing model performance and feature importance.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON is structured according to the user's role as a project manager with high expertise. The request for tabular regression, data splitting, and 10% stricter MedAE is reflected in the problem description and performance metrics. Model interpretability is explicitly mentioned. The missing details like the model name, exact data size, and benchmark performance will be inferred from industry norms or specified by the user in a more specific instruction.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image segmentation", "instruction": "For the upgraded PlantVillage project, develop a deep learning pipeline that combines multiple attention mechanisms in a hierarchical DenseNet architecture for precise segmentation of leaf diseases. Ensure interpretability through Grad-CAM heatmaps, layer-wise relevance propagation, and a user-friendly visual summary, catering to non-technical stakeholders for accurate plant health diagnostics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"agriculture/PlantVillage\",\n        \"description\": \"Develop a deep learning pipeline using a hierarchical DenseNet architecture with multiple attention mechanisms for leaf disease segmentation in the upgraded PlantVillage project. Focus on interpretability with Grad-CAM heatmaps, layer-wise relevance propagation, and a visual summary for non-technical stakeholders.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Segmentation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Interpretability score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PlantVillage (leaf disease dataset)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"leaf disease labels\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"classes\": null\n            },\n            \"description\": \"Dataset with plant leaf images and corresponding disease labels for disease segmentation in the upgraded PlantVillage project.\",\n            \"preprocessing\": [\"image resizing, normalization\"],\n            \"augmentation\": [\"random crops, horizontal flips\"],\n            \"visualization\": [\"image and heatmap visualizations\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical DenseNet with Multi-Attention\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"DenseNet with attention modules\",\n                \"interpretability techniques\": [\"Grad-CAM\", \"layer-wise relevance propagation\"],\n                \"target audience\": \"non-technical stakeholders\"\n            },\n            \"description\": \"A deep learning model using DenseNet with multiple attention mechanisms specifically designed for precise leaf disease segmentation in the upgraded PlantVillage project.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"interpretability techniques\",\n            \"description\": \"The user is aware of the importance of interpretability in the context of machine learning for non-technical stakeholders.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretable AI-friendly inference\"],\n        \"demonstration\": \"User-friendly visual summary of model results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction has been parsed into a JSON response with a clear focus on the build task, model specifications, and interpretability for non-technical users. Specific metrics for performance and complexity are left undefined, allowing for optimization based on the user's priorities. The model details address the requested attention mechanism in a DenseNet architecture and the emphasis on interpretability techniques. However, more specific details like dataset size and model specifications could be added to provide a more complete picture.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "Develop a state-of-the-art object detection system for recognizing and localizing not only a diverse range of fruits, including rare and exotic varieties, but also their subcategories such as seeds, stems, and blemishes on ripened fruits, in high-resolution images taken under varying lighting conditions and angles. The project should incorporate advanced algorithms like Mask R-CNN with focal loss for improved performance and real-time tracking in video streams. Additionally, design an interactive user interface for agricultural experts to easily annotate and fine-tune the model with domain-specific knowledge, ensuring consistently accurate counts and classifications across different seasons and global harvest regions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"agriculture and AI in food industry\",\n        \"description\": \"Develop a state-of-the-art object detection system to recognize and localize fruits, their subcategories, seeds, stems, and blemishes in high-resolution images with varying lighting and angles. Incorporate Mask R-CNN with focal loss and real-time tracking for video streams. Create an interactive interface for agricultural experts to annotate and fine-tune the model with domain knowledge.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mAP\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Intersection over Union (IoU)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Real-time tracking efficiency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"fruit recognition dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"fruit categories, subcategories, seeds, stems, and blemishes\"],\n            \"specification\": {\n                \"image_size\": \"high-resolution\",\n                \"variety\": \"diverse, including rare and exotic fruits\",\n                \"lighting_conditions\": \"varied\",\n                \"angle_range\": \"wide\"\n            },\n            \"description\": \"A dataset containing high-resolution images with various fruits and their subcategories under different conditions.\",\n            \"preprocessing\": [\"image normalization\", \"augmentation\"],\n            \"augmentation\": [\"geometric transformations, photometric changes\"],\n            \"visualization\": [\"model performance heatmaps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Fruit Detection System\",\n            \"family\": \"Mask R-CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"algorithm\": \"Mask R-CNN with focal loss\",\n                \"real-time_tracking\": \"enabled\"\n            },\n            \"description\": \"Advanced object detection model for fruit recognition and localization with subcategories\"\n        }\n    ],\n    \"knowledge\": [\n        \"Agricultural domain knowledge and seasonal/global harvest region-specific adjustments\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific annotations\"],\n        \"target_device\": \"cloud or edge for real-time tracking\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"lightweight for real-time performance\"],\n        \"demonstration\": \"interactive, user-friendly interface for agricultural experts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response parses the user's instruction to build a high-end object detection system with Mask R-CNN and focus on performance, real-time tracking, and an interactive user interface. It captures the need for diverse fruit recognition, seasonal adjustments, and global regions. Some specific metric values are left unspecified (denoted as null) to allow flexibility in defining optimization targets.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular classification", "instruction": "As a data science project manager for a specialized machine learning team, you've been tasked with tackling a unique and complex tabular classification challenge. The Rare Insect Behavior Identification Dataset, derived from ecological research, presents an extraordinary set of features capturing minute details about insect morphology and environmental factors. The data has been meticulously divided into train, validation, and test partitions to maintain ecological integrity. Your objective is to accurately forecast not only theBugType labels (limited to five distinct categories) but also the rare behavioral traits 'Endangered Habitat Indicator' and 'Coexistence Strategy Score', both with a ternary classification system (0, 1, and 2). The performance will be assessed using F1 scores for multi-label classification and a novel metric, 'Precision-Recall Entropy Score' (PRES), designed to evaluate balance between precision and recall in a hierarchical manner. Your team must develop a highly interpretable model that demonstrates exceptional performance in this niche domain.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"ecology\",\n        \"description\": \"Develop a machine learning model for the Rare Insect Behavior Identification Dataset to forecast BugType labels (5 categories) and rare behavioral traits 'Endangered Habitat Indicator' and 'Coexistence Strategy Score' with a ternary classification system. Focus on multi-label F1 scores and introduce a new metric, Precision-Recall Entropy Score (PRES). Aim for a highly interpretable model with exceptional performance in the niche domain of insect ecology.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null,\n                \"labels\": \"BugType\"\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null,\n                \"labels\": \"Endangered Habitat Indicator\"\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null,\n                \"labels\": \"Coexistence Strategy Score\"\n            },\n            {\n                \"name\": \"Precision-Recall Entropy Score (PRES)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model interpretability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Rare Insect Behavior Identification Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"BugType\", \"Endangered Habitat Indicator\", \"Coexistence Strategy Score\"],\n            \"specification\": {\n                \"split_distribution\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                },\n                \"feature_dimensions\": null,\n                \"sample_size\": null\n            },\n            \"description\": \"A tabular dataset with insect morphology and environmental factors for classification into BugType and rare behavioral traits. Consists of train, validation, and test partitions to maintain ecological integrity.\",\n            \"preprocessing\": [\n                \"Data normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"model_structure\": null\n            },\n            \"description\": \"A highly interpretable model for multi-label classification with precision-recall balance in a niche insect ecology domain.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hierarchical nature of the 'Coexistence Strategy Score' and 'Endangered Habitat Indicator' must be taken into account when designing the model.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Imputation for missing values\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Include a detailed performance report and model interpretation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response reflects the user's role as a data science project manager and the complexity of the given task. It covers the required tasks, dataset, performance metrics (including PRES), and model characteristics. However, some unspecified values such as the model name, structure, and exact performance numbers indicate that there is room for further clarification in the user's instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "For the upcoming Amazon co-purchase graph analysis, design a highly optimized ChebNet architecture (incorporating at least three levels of spectral convolution) that exhibits exceptional scalability. Target a minimum precision of 95% while ensuring real-time adaptability for frequent graph updates, with a focus on performance optimization in a Big Data context.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Design a highly optimized ChebNet architecture with at least three levels of spectral convolution for Amazon co-purchase graph analysis, targeting minimum precision of 95%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"real-time\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"exceptional\",\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"response time for frequent updates\",\n                \"value\": \"real-time\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Amazon co-purchase graph\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node labels\"],\n            \"specification\": {\n                \"graph_type\": \"co-purchase\",\n                \"node_size\": \"large\",\n                \"edge_volume\": \"big data\"\n            },\n            \"description\": \"A large-scale graph dataset for Amazon co-purchase analysis, where nodes represent products and edges indicate co-purchasing relationships.\",\n            \"preprocessing\": [\"node normalization\", \"edge pruning\"],\n            \"augmentation\": [],\n            \"visualization\": [\"large-scale graph visualization\"],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ChebNet\",\n            \"family\": \"Spectral Convolution Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"levels\": 3,\n                \"optimization\": \"performance\",\n                \"architecture\": \"highly-optimized\"\n            },\n            \"description\": \"A highly optimized ChebNet architecture with at least three levels of spectral convolutions designed for the Amazon co-purchase graph dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"In a Big Data context, use distributed computing and efficient algorithms for both training and inference.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"distributed feature extraction\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"AWS Lambda or similar for real-time updates\",\n        \"inference_engine\": [\"Apache Spark, DGraph, or GraphX for scalability\"],\n        \"demonstration\": \"Perform live graph updates and real-time performance metrics.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response parses the user's instruction into a build task, with a focus on precision, scalability, and real-time adaptability for frequent updates. It aligns with the graph machine learning area and Amazon co-purchase graph. Performance optimization in a Big Data context is also taken into account. Model specifics, preprocessing, and feature engineering steps are detailed based on the requirements. The confidence score is high since the details seem to cover the key aspects.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Develop a sophisticated visual question answering system using YouTube-8M, specifically targeting short-form documentaries discussing niche ecological topics. Employ a hybrid approach of 3D ResNet-Transformer architecture, integrating temporal and spatial features for precise frame-level understanding. Ensure the system differentiates between subtle visual cues and handles rapidly changing scenes with accurate, up-to-date responses within ecological event timeframes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"ecology and documentaries\",\n        \"description\": \"Develop a sophisticated visual question answering system using YouTube-8M dataset, focusing on short-form documentaries with niche ecological topics. Utilize a 3D ResNet-Transformer hybrid architecture that integrates temporal and spatial features for fine-grained frame-level understanding. Ensure the system can distinguish between subtle cues and handle fast-changing scenes with accurate and up-to-date responses within ecological event timeframes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube-8M\",\n            \"modality\": [\"video\"],\n            \"target_variables\": [\"frame-level annotations\", \"documentary metadata\"],\n            \"specification\": {\n                \"dataset_size\": \"large\",\n                \"video_length\": \"variable\",\n                \"ecological_focused_annotations\": true\n            },\n            \"description\": \"A video dataset with diverse ecological documentaries for training a visual question answering system.\",\n            \"preprocessing\": [\n                \"frame extraction\",\n                \"temporal-spatial feature extraction\"\n            ],\n            \"augmentation\": [\n                \"temporal shifting\",\n                \"spatial cropping\"\n            ],\n            \"visualization\": [\n                \"attention maps\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"3D ResNet-Transformer\",\n            \"family\": \"neural networks\",\n            \"type\": \"hybrid\",\n            \"specification\": {\n                \"architecture\": \"3D ResNet blocks followed by Transformer layers\",\n                \"temporal_depth\": \"Transformer attends to adjacent frames\",\n                \"spatial_resolution\": \"Fusion of spatial features using ResNet\",\n                \"fine-grained\": true\n            },\n            \"description\": \"A hybrid architecture combining 3D ResNet and Transformer for VQA in the ecological domain, focusing on temporal and spatial fusion.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Subtle visual cues often present challenges for deep learning models in ecological scenarios\",\n        \"Rapid scene changes require temporal context and handling to maintain accuracy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal encoding\",\n            \"spatial feature extraction\"\n        ],\n        \"target_device\": \"cloud-based GPU servers\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"TensorFlow serving\", \"GPU accelerated\"],\n        \"demonstration\": \"Interactive visualisation of question answering process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON accurately captures the user's intent to build a sophisticated VQA system for niche ecological topics. It reflects the user's high expertise level. Key areas, tasks, and application domains are clearly defined, along with performance metrics and complexity constraints. The model and dataset are specified as required, while the knowledge, service, and feature engineering components are shaped according to the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For the \"Green City Smart Traffic\" initiative, we mandate a state-of-the-art YOLOv5 model fine-tuned on a subsets of Cityscapes focused on detecting premium categories: high-visibility bicycles, distinct types of vehicles (e.g., SUVs, electric scooters), and unique traffic signs. Target a remarkable real-time performance of 50 FPS on hardware with AI acceleration, while ensuring a precision above 90% in challenging urban scenarios with diverse lighting and weather conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city, traffic management\",\n        \"description\": \"Mandate a state-of-the-art YOLOv5 model fine-tuned on a subset of Cityscapes, focusing on detecting high-visibility bicycles, specific vehicle types (SUVs, electric scooters), and unique traffic signs for the 'Green City Smart Traffic' initiative.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": 50,\n                \"unit\": \"FPS\"\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.9,\n                \"unit\": \"\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"AI acceleration\",\n                \"value\": true\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes subset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"high-visibility bicycles\", \"SUVs, electric scooters\", \"unique traffic signs\"],\n            \"specification\": {\n                \"subset_details\": \"Focused on urban scenarios with diverse lighting and weather conditions\"\n            },\n            \"description\": \"A dataset derived from Cityscapes, fine-tuning for the 'Green City Smart Traffic' initiative with specific object detection requirements.\",\n            \"preprocessing\": [\"fine-tuning\"],\n            \"augmentation\": [\"aiming for robustness in challenging urban scenarios\"],\n            \"visualization\": [\"model performance on diversified scenarios\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5\",\n            \"family\": \"state-of-the-art object detection\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy target\": \"precision above 90%\",\n                \"target_device_optimization\": \"real-time performance of 50 FPS\"\n            },\n            \"description\": \"A YOLOv5 model fine-tuned for detecting premium categories in the 'Green City Smart Traffic' initiative.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"optimized for hardware acceleration\"],\n        \"target_device\": \"AI-accelerated hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time performance\"],\n        \"demonstration\": \"challenging urban scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON includes the user's intent to build a state-of-the-art model with high-performance targets (90% precision, 50 FPS) and fine-tuning requirements. The model, dataset, and performance metrics clearly align with the specified task and domain. However, the user's level of expertise is assumed to be high due to the complexities and specific objectives. Deployment details and demonstration are based on the user's specifications.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "machine translation", "instruction": "Develop a state-of-the-art neural machine translation (NMT) system using the Transformer-XL architecture for English to Russian conversational translations from the diverse OPUS-XL corpus, considering domain-specific slang, idioms, and colloquial expressions. Target a BLEU score of 35 while maintaining real-time translation under 250 milliseconds per utterance to guarantee seamless user engagement in high-traffic chat applications.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"conversational AI\",\n        \"description\": \"Develop a state-of-the-art neural machine translation system using the Transformer-XL architecture, targeting English to Russian translations, particularly focusing on domain-specific slang, idioms, and colloquial expressions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 35\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 250,\n                \"unit\": \"milliseconds per utterance\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS-XL corpus (English to Russian)\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": null,\n            \"description\": \"A diverse corpus for English to Russian conversational translations that includes domain-specific slang, idioms, and colloquial expressions.\",\n            \"preprocessing\": [\n                \"tokenization\",\n                \"sentence segmentation\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"Transformer-XL\",\n                \"attention mechanism\": \"self-attention\",\n                \"sequence length\": null\n            },\n            \"description\": \"A state-of-the-art neural machine translation model built using the Transformer-XL architecture for English to Russian conversation translation.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware adaptation\",\n            \"domain adaptation techniques\"\n        ],\n        \"target_device\": \"real-time, high-traffic chat applications\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for latency\"\n        ],\n        \"demonstration\": \"seamless user experience in high-frequency chat interactions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's request for a state-of-the-art NMT system with emphasis on domain-specific language and real-time performance. It includes the Transformer-XL architecture, target BLEU score, and the constraint on inference time. Performance metrics, dataset properties, and deployment requirements are provided. The user's expertise level suggests a high level of understanding of the technical aspects.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the project manager for a machine learning team, your task is to develop an advanced time-series forecasting model for a complex solar energy project. We need to analyze hourly data with seasonality, trends, and irregularities, accurately predicting energy output for the next 7 days, while ensuring real-time adjustments for weather conditions and equipment maintenance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"solar energy production\",\n        \"description\": \"Develop an advanced time-series forecasting model for a solar energy project analyzing hourly data with seasonality, trends, and irregularities. The model should predict energy output for the next 7 days and account for real-time adjustments due to weather conditions and equipment maintenance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Absolute Error (MAE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Percentage Error (MAPE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time (real-time adjustments)\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Solar Energy hourly data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"energy output\"],\n            \"specification\": {\n                \"data_length\": null,\n                \"seasonality\": \"hourly\",\n                \"trend\": \"complex\",\n                \"irregularities\": true\n            },\n            \"description\": \"Hourly data with solar energy production and relevant weather and equipment data for the project.\",\n            \"preprocessing\": [\"seasonality extraction\", \"trend analysis\", \"outlier detection\"],\n            \"augmentation\": [],\n            \"visualization\": [\"data profiling\", \"time-series patterns\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Time-Series Forecasting Model\",\n            \"family\": \"Deep Learning (RNN, LSTM, or Transformer variants)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"complexity\": \"high\",\n                \"real-time adjustments\": true\n            },\n            \"description\": \"A sophisticated model that incorporates hourly data, seasonality, trends, irregularities, and real-time adjustments for weather and equipment conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"data assimilation\": \"Real-time integration of weather data and equipment performance monitoring\"\n        },\n        {\n            \"feature engineering techniques\": \"Effective use of rolling windows and historical patterns for forecasting\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"window-based feature creation\",\n            \"engineering for weather data fusion\"\n        ],\n        \"target_device\": \"cloud or scalable compute infrastructure\",\n        \"deployment_endpoint\": \"energy production control system API\",\n        \"inference_engine\": [\"streaming model inference\"],\n        \"demonstration\": \"Predicted energy output visualizations and real-time system updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response covers the user's role, task requirements for the model, target metrics, and specific constraints. The dataset and model specifications match the complexities of the solar energy project. Some fields are left open to accommodate real-time adjustments, uncertainty in metric targets, and potential missing details due to the project manager's role.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for the real estate price predictor, mandate the team to utilize the 'Housing Market Trends' dataset from the last five years. Mandate the implementation of a Bayesian Ridge Regression model, utilizing its advanced uncertainty quantification. The objective is to reach a 5% prediction error margin, while also generating posterior predictive intervals for each forecast, ensuring robustness and reliability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"real estate\",\n        \"description\": \"As a project manager, mandate the team to utilize the 'Housing Market Trends' dataset from the last five years and implement a Bayesian Ridge Regression model for real estate price prediction. The goal is to achieve a prediction error margin of 5% and generate posterior predictive intervals for forecasts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Prediction error margin\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"posterior predictive intervals\",\n                \"value\": null,\n                \"unit\": \"generated for each forecast\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Housing Market Trends\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"Real Estate Prices\"],\n            \"specification\": {\n                \"time_frame\": \"last five years\",\n                \"data_source\": \"publicly available\"\n            },\n            \"description\": \"A dataset containing housing market trends from the past five years for real estate price prediction.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Bayesian Ridge Regression\",\n            \"family\": \"statistical models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"uncertainty_quantification\": true\n            },\n            \"description\": \"A statistical model utilizing Bayesian Ridge Regression with advanced uncertainty quantification for the Housing Market Trends dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Ensuring robustness and reliability through posterior predictive intervals in forecasts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately reflects the user's project manager mandate, specifying the data source, model selection, performance goals, and inclusion of posterior predictive intervals. The user's high expertise level is also indicated. However, there is a lack of details about data preprocessing or specific deployment, as these were not detailed in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "Develop a real-time time-series anomaly detection model for our e-commerce platform, focusing on specific weekends, holidays, and peak promotional periods. The system must identify sudden spikes or unusual dips in user traffic, differentiating between genuine traffic surges and bot activity, and proactively flag them for immediate investigation by the security team.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Develop a real-time time-series anomaly detection model targeting weekends, holidays, and peak promotional periods. The system aims to detect sudden spikes or unusual dips in user traffic, distinguishing genuine surges from bot activity, and flag them for immediate investigation by the security team.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"True Positive Rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"False Positive Rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Average Precision (MAP)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time inference latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"E-commerce user traffic data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"user traffic\", \"bot activity\"],\n            \"specification\": {\n                \"time granularity\": \"hourly\",\n                \"length\": null,\n                \"feature_types\": [\"volume\", \"user behavior\"]\n            },\n            \"description\": \"A time-series dataset reflecting user traffic patterns on weekends, holidays, and peak promotional periods, including both genuine traffic and potential bot activity.\",\n            \"preprocessing\": [\"data cleaning\", \"seasonality adjustment\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"traffic patterns over time\",\n                \"bot activity analysis\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"time-series anomaly detection model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"RNN-based (LSTM, GRU)\",\n                \"training methodology\": \"deep learning\"\n            },\n            \"description\": \"A real-time, deep-learning model designed for detecting anomalies in e-commerce user traffic, targeting specific weekends, holidays, and promotional periods.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider incorporating machine learning for bot detection and incorporating contextual information about promotions and holidays\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"traffic smoothing\", \"bot behavior clustering\"],\n        \"target_device\": \"cloud server\",\n        \"deployment_endpoint\": \"real-time anomaly detection API\",\n        \"inference_engine\": [\n            \"TensorFlow Serving\",\n            \"Apache MXNet\"\n        ],\n        \"demonstration\": \"Weekly anomaly report and live traffic dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response parses the given instruction into a clear, structured format. It identifies the time-series analysis area and anomaly detection task, while also considering the e-commerce context. The system is built to handle real-time detection, with emphasis on weekends, holidays, and peak promotions. Metrics for identifying genuine spikes and bot activity are suggested, although specific values aren't provided. A neural network-based model with RNNs and user-defined feature preprocessing is proposed. The response covers additional steps like feature engineering, deployment, and a demonstration plan.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As the lead project manager for a seasoned ML development team, your objective is to design a sophisticated, adaptive time-series forecasting system. This model must handle multi-faceted solar energy data, which includes minute-by-minute readings with non-linear patterns, irregularities, and varying seasonality. The challenge lies in forecasting energy output for the subsequent 7 days with real-time adjustments for meteorological fluctuations and predictive maintenance of diverse equipment, while considering economic factors and long-term energy market trends.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy production\",\n        \"description\": \"Design a sophisticated, adaptive time-series forecasting system to handle multi-faceted solar energy data with minute-by-minute readings, non-linear patterns, irregularities, and varying seasonality. Forecast energy output for the next 7 days, accounting for real-time meteorological fluctuations, predictive maintenance of equipment, and economic factors, along with long-term energy market trends.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multi-faceted solar energy data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"energy output\"],\n            \"specification\": {\n                \"data_frequency\": \"minute-by-minute\",\n                \"data_complexity\": \"non-linear, irregular, varying seasonality\"\n            },\n            \"description\": \"A dataset with minute-level readings of solar energy production with various complexities and fluctuations.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"resampling\",\n                \"seasonality analysis\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"data patterns\",\n                \"anomalies detection\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural network-based (ideally LSTM or Transformer variants)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptability\": \"sophisticated, adaptive\",\n                \"feature_integration\": [\"meteorological data\", \"maintenance logs\", \"economic indicators\", \"market trends\"]\n            },\n            \"description\": \"A highly advanced time-series forecasting model, possibly incorporating deep learning techniques like LSTM or Transformer, for solar energy output prediction with real-time adjustments and diverse factors\"\n        }\n    ],\n    \"knowledge\": [\n        \"non-linear regression analysis, seasonality decomposition, anomaly detection, predictive maintenance strategies\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"dynamic feature incorporation\", \"ensembling\"],\n        \"target_device\": \"cloud or edge\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\", \"high-performance computing\"],\n        \"demonstration\": \"real-time performance and adaptability demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a sophisticated time-series forecasting system, considering their high expertise level. It reflects the complexities in the data and requirements for real-time adjustments and multi-factor analysis. Performance metrics are expected to highlight accuracy, while considering the model's time and memory footprint. The dataset and model descriptions are comprehensive, matching the user's objectives.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "In the realm of astrophysical network analysis, develop a novel GraphSAGE extension, specifically a hierarchical Dual-Scale Attention Graph Neural Network (DSAGNN) with four distinct sub-modules, each operating on different semantic layers. This architecture should achieve an unprecedented area-under-the-curve (AUC) of 98% for identifying exotic celestial object interactions, while accommodating for variable star formation patterns and integrating real-time gravitational wave data. Emphasize fault tolerance and zero-downtime learning to maintain data integrity across a distributed network of telescopes processing petabyte-scale data streams.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"astrophysics\",\n        \"description\": \"Develop a novel GraphSAGE extension - a hierarchical Dual-Scale Attention Graph Neural Network (DSAGNN) with four sub-modules for analyzing celestial object interactions. The target is to achieve an AUC of 98% for identifying exotic objects, considering variable star formation patterns and real-time gravitational wave data. Zero-downtime learning and fault tolerance are essential for maintaining data integrity in a distributed telescope network processing petabyte-scale data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"fault tolerance\",\n                \"value\": 1\n            },\n            {\n                \"name\": \"zero-downtime learning\",\n                \"value\": 1\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Astrophysical network data with celestial objects\",\n            \"modality\": [\"graph\", \"time series\", \"multimodal\"],\n            \"target_variables\": [\"interactions\", \"star formation patterns\", \"gravitational wave data\"],\n            \"specification\": null,\n            \"description\": \"Graph data containing celestial objects with their interactions, time-series data on star formation, and real-time gravitational wave data for training and testing the DSAGNN model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Dual-Scale Attention Graph Neural Network (DSAGNN)\",\n            \"family\": \"GraphSAGE\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"sub-modules\": \"4\",\n                \"semantic_layers\": \"distinct\",\n                \"astronomy_domain\": \"exotic celestial objects\",\n                \"data_processing\": \"real-time\"\n            },\n            \"description\": \"A GraphSAGE extension utilizing a hierarchical DSAGNN with four sub-modules, each handling different semantic layers, designed for astrophysical network analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Zero-downtime learning can be achieved using techniques such as model checkpointing and online learning.\",\n        \"Real-time Gravitational Wave data integration can be done through real-time data pipelines.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"handling variable node attributes\",\n            \"integrating time-series features\"\n        ],\n        \"target_device\": \"distributed network of telescopes\",\n        \"deployment_endpoint\": \"potentially a distributed processing or monitoring service\",\n        \"inference_engine\": [\"optimized for petabyte-scale data\"],\n        \"demonstration\": \"showcase of AUC results and robustness with variable star formation patterns\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's instruction about developing a novel DSAGNN model for astrophysical data analysis, focusing on the performance metric, architectural details, and the distributed environment requirements. Given the user's high expertise, missing information is limited and can be inferred or requested for more context. The confidence score is high as most key aspects are covered.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project director overseeing a leading-edge materials innovation lab, your mandate is to engineer a state-of-the-art tabular regression algorithm for the recently acquired extensive Mohs Hardness Dataset. Rigorously partition the data into training, validation, and testing sets with a 15% margin in accuracy over previously established industry standards, measured by the stringent root mean squared percentage error (RMSPE). Moreover, pursue a multi-faceted feature importance study, necessitating advanced model explainability techniques while maintaining exceptional predictive prowess for groundbreaking scientific insights.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"material science\",\n        \"description\": \"Engineer a state-of-the-art tabular regression algorithm for the Mohs Hardness Dataset, with rigorous data partitioning and advanced feature importance analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mohs Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"Hardness\"],\n            \"specification\": null,\n            \"description\": \"A large dataset for predicting the Mohs hardness of materials using advanced techniques and industry-standards for partitioning.\",\n            \"preprocessing\": [\"Handling missing values\", \"Normalization\", \"Encoding categorical variables\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Feature correlation analysis\",\n                \"Partial dependence plots\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"Advanced deep learning architecture\",\n                \"number_of_layers\": null,\n                \"optimizer\": \"Adam\",\n                \"loss_function\": \"Mean Squared Percentage Error\"\n            },\n            \"description\": \"A highly interpretable, deep learning tabular regression model for the Mohs Hardness Dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Industry standards suggest a 15% margin for accuracy in data partitioning.\",\n        \"Advanced explainability techniques required for feature importance study.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Principal Component Analysis\", \"Feature selection using permutation importance\"],\n        \"target_device\": \"High-performance computing cluster\",\n        \"deployment_endpoint\": \"Internal API for lab's data visualization platform\",\n        \"inference_engine\": [\"TensorFlow-Serving\", \"Horovod for distributed training\"],\n        \"demonstration\": \"Interactive dashboard showcasing predictive prowess and feature importance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essential aspects of the user's instruction for a material science project, including the requirement for a state-of-the-art algorithm, data partitioning, feature importance, model explainability, and performance measures. Given the user's high expertise level, the response covers advanced techniques and considerations. However, some metric values are left unspecified (RMSPE, inference time, memory usage), assuming they would be optimized during the development process.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the upcoming project, mandate the machine learning development team to design an advanced image segmentation model capable of real-time analysis for high-resolution aerial surveillance of properties. The model should not only distinguish between intricate architectural elements, lush vegetation, and intricate road patterns, but also detect minute changes, track temporal shifts with utmost precision, and incorporate a sophisticated deep learning framework with state-of-the-art algorithms. Additionally, the pipeline must adhere to stringent accuracy standards, efficiency requirements, and automated error detection, aiming to streamline the inspection process while minimizing the need for manual oversight.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"aerial surveillance\",\n        \"description\": \"Design an advanced image segmentation model for real-time analysis of high-resolution aerial surveillance data, focusing on distinguishing architectural elements, vegetation, and road patterns. The model should also detect minute changes, track temporal shifts precisely, and utilize state-of-the-art deep learning algorithms.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Segmentation Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Real-time Inference Speed\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory Footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Aerial Surveillance Datasets\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"Segments (architecture, vegetation, roads)\"],\n            \"specification\": {\n                \"image_resolution\": {\"min\": \"high\", \"max\": \"ultra-high\"},\n                \"image_size\": {\"min\": \"very large\", \"max\": \"very large\"}\n            },\n            \"description\": \"A dataset of high-resolution aerial images with varying complexities for training and testing the model.\",\n            \"preprocessing\": [\"Image normalization\", \"Data augmentation\"],\n            \"augmentation\": [\"Random cropping\", \"Color jittering\"],\n            \"visualization\": [\"Segmentation masks\", \"Temporal analysis\"],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Deep Learning Framework (e.g., TensorFlow or PyTorch)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"State-of-the-art image segmentation network\",\n                \"algorithm\": \"Semantic segmentation or instance segmentation\"\n            },\n            \"description\": \"A sophisticated deep learning model for real-time image segmentation using state-of-the-art algorithms\"\n        }\n    ],\n    \"knowledge\": [\n        \"Deploy on specialized hardware to enhance real-time performance\",\n        \"Integrate autoencoder for temporal shift prediction and anomaly detection\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Multi-scale feature extraction\", \"Context-aware feature fusion\"],\n        \"target_device\": \"Edge or Cloud-based server for real-time inference\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU or specialized hardware acceleration\"],\n        \"demonstration\": \"Live demonstration with sample results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's requirement for designing an advanced image segmentation model, incorporating state-of-the-art algorithms, real-time capabilities, and high accuracy standards. The performance metrics reflect the need for precision and efficiency, while the model details are broad enough to accommodate various advanced approaches. The dataset description and preprocessing steps imply the requirement for diverse and complex aerial images. Some fields, like model name and specific deployment details, are left unspecified as the instruction mainly focuses on the high-level requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "The project manager is requested to design an advanced GraphSAGE variant, incorporating EdgeConv layers with adaptive receptive field selection for the diverse DBLP citation network. The objective is to achieve state-of-the-art paper classification in multiple fine-grained academic domains, maintaining precision and recall above 0.95, and concurrently minimizing computational overhead within stringent resource constraints and real-time deployment requirements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Design an advanced GraphSAGE variant with EdgeConv layers, incorporating adaptive receptive field selection for the diverse DBLP citation network. The objective is to achieve state-of-the-art paper classification in multiple fine-grained academic domains, targeting precision and recall above 0.95 while minimizing computational overhead within strict resource constraints and real-time deployment requirements.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"real-time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"resource utilization\",\n                \"unit\": \"constraints\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DBLP citation network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"paper domains\"],\n            \"specification\": {\n                \"network complexity\": \"high\",\n                \"diversity\": \"fine-grained\"\n            },\n            \"description\": \"A complex citation network with diverse DBLP dataset, where nodes represent research papers and edges denote citations.\",\n            \"preprocessing\": [\"data cleaning\", \"domain mapping\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model interpretability\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced GraphSAGE with EdgeConv\",\n            \"family\": \"GraphSAGE\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptative receptive field\": \"enabled\",\n                \"resource optimization\": \"real-time\"\n            },\n            \"description\": \"An advanced GraphSAGE variant using EdgeConv layers with adaptive receptive field selection, optimized for real-time deployment in diverse academic domains on the DBLP citation network.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Best practices for GraphSAGE architectures, fine-grained domain adaptation, and efficient resource utilization.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific feature extraction\"],\n        \"target_device\": \"real-time deployment\",\n        \"deployment_endpoint\": \"unspecified\",\n        \"inference_engine\": [\"optimized for resource-constrained environments\"],\n        \"demonstration\": \"performance report on various fine-grained domains\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's request for a high-level design, emphasizing the advanced GraphSAGE variant, performance targets, and resource constraints. Performance metrics are set, and preprocessing and feature engineering steps are suggested based on the instruction. However, some specific values and constraints are missing, leaving room for interpretation, which could affect the confidence score.\",\n        \"score\": 0.8\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a cutting-edge sentiment-driven node classification project on the NASA Space Shuttle Communication Network using a Neuromorphic Graph Attention Network (NGAT). Categorize astronauts and ground control operators into specialized categories based on their real-time, mission-critical communications, while accounting for emotional cues and adaptability to unexpected scenarios. The system must function seamlessly in a dynamically evolving graph with billions of nodes and edges, maintaining high interpretability and achieving sub-second response times using federated learning without compromising data privacy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"space communication\",\n        \"description\": \"Develop a sentiment-driven node classification project on the NASA Space Shuttle Communication Network using Neuromorphic Graph Attention Network (NGAT). Categorize astronauts and ground control operators into specialized categories based on real-time mission-critical communications, incorporating emotional cues and adaptability for unexpected scenarios. The system should operate in a dynamically evolving graph with billions of nodes and edges, ensuring high interpretability and sub-second response times using federated learning with data privacy preservation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": {\"unit\": \"sub-seconds\"}\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NASA Space Shuttle Communication Network\",\n            \"modality\": [\"graph\", \"text\"],\n            \"target_variables\": [\"astronauts\", \"ground control operators\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"total\": null,\n                    \"edges_per_node\": null\n                },\n                \"text_data_size\": null,\n                \"dynamic\": true\n            },\n            \"description\": \"A large, real-time communication dataset containing astronaut and ground control communications with emotional cues, suitable for NGAT.\",\n            \"preprocessing\": [\"emotional cue extraction\", \"dynamic graph processing\"],\n            \"augmentation\": [],\n            \"visualization\": [\"interpretability visualization\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Neuromorphic Graph Attention Network (NGAT)\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"NGAT\",\n                \"FLOPs\": null,\n                \"federated learning\": true\n            },\n            \"description\": \"A state-of-the-art sentiment-driven model for node classification, leveraging NGAT and addressing the unique challenges of dynamic graphs with billion-scale data.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Federated learning can be applied to preserve data privacy while training on the edge devices of the network.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"emotional feature encoding\", \"node embeddings\"],\n        \"target_device\": \"edge device\",\n        \"deployment_endpoint\": \"federated learning infrastructure\",\n        \"inference_engine\": [\"distributed computing\"],\n        \"demonstration\": \"interactive and visually interpretable dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided information covers the user's requirements for a cutting-edge machine learning project. The user's high expertise indicates a deeper understanding of the project's complexity. The specification includes performance metrics (accuracy, interpretability, and response time) and space-time complexity constraints. The dataset is specified as dynamically evolving and large with emotional and text modality, aligning with the NGAT model choice. Federated learning, data privacy, and sub-second response are important aspects.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As the lead project manager for the elite financial data interpretation unit, your mandate is to enhance the proprietary \"Financial Insights Lexicon\" dataset by dividing news articles into a sophisticated taxonomy of 25 subdomains (e.g., microeconomics, mergers, and green finance). Mandate a highly-optimized BERT variant model for speed and precision, targeting at least 92% precision in pinpointing extreme market impact news, maintaining an overall F1 score of 88% with a focus on minimizing false positives in niche topics, while completing the project within a stringent 60-day deadline. In addition, devise a novel hybrid evaluation metric integrating F1, precision-at-recall levels, and cost-sensitive measures to balance accuracy in each sub-genre.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Enhance the 'Financial Insights Lexicon' dataset by dividing news articles into a taxonomy of 25 subdomains, targeting extreme market impact news with at least 92% precision, 88% F1 score, and minimizing false positives in niche topics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.92\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.88\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"evaluation_metrics\": {\n            \"name\": \"hybrid metric\",\n            \"description\": \"A combination of F1, precision-at-recall, and cost-sensitive measures to balance accuracy in each sub-genre.\"\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Financial Insights Lexicon\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"news article categories\"],\n            \"specification\": {\n                \"size\": \"to be determined\",\n                \"diversity\": \"25 subdomains\"\n            },\n            \"description\": \"Dataset of news articles to be categorized into 25 subdomains of the financial domain.\",\n            \"preprocessing\": [\n                \"BERT tokenization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"performance across subdomains\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT variant\",\n                       \"family\": \"pre-trained language models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization\": \"for speed and precision\",\n                \"specific_metric\": \"at least 92% precision on extreme market impact news\"\n            },\n            \"description\": \"A highly-optimized BERT variant designed for text classification in the financial domain.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Project deadline: 60 days\",\n        \"Cost sensitivity in sub-genre evaluation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"custom sub-domain modeling\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"CPU or GPU acceleration\"\n        ],\n        \"demonstration\": \"Interactive breakdown of sub-genre performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the user's mandate for a model that enhances the Financial Insights Lexicon dataset with a focus on speed, precision, and niche topic accuracy. The strict deadline, high expertise level, and hybrid evaluation metric are accurately captured. Some details are left to be filled (like the exact dataset size and inference engine specifics), assuming those will be determined further during implementation.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for the time-series classification team, your challenge is to create a state-of-the-art machine learning solution that accurately identifies and forecasts intricate kinesthetic patterns using the UWave Gesture Dataset v2.0. This dataset, composed of ultra-high resolution wearable data (945 samples, 8 specialized bio-sensors - INPUT_SEQ_LEN: 945, INPUT_DIM: 8, sampled at 500Hz), is divided into train (12,600 samples, 75%), validation (3,150 for iterative refinement, 15%), and test sets (3,150 sequences) with varying hand gesture complexities. Design an advanced fusion model integrating LSTMs, bidirectional encoders, and multi-head attention, targeting a minimum precision of 98% on unseen, real-time gestures, while ensuring model explainability and latency optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"human motion analysis\",\n        \"description\": \"Create a state-of-the-art machine learning solution for classifying and forecasting intricate kinesthetic patterns using the UWave Gesture Dataset v2.0. The dataset consists of 945 ultra-high resolution wearable samples from 8 bio-sensors (INPUT_SEQ_LEN: 945, INPUT_DIM: 8, 500Hz sampling), with train, validation, and test sets having a 75-15-10% split. The target is a precision of at least 98% on unseen, real-time gestures, prioritizing model explainability and latency optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Model Explainability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UWave Gesture Dataset v2.0\",\n            \"modality\": [\"time series\", \"graph\"],\n            \"target_variables\": [\"Gesture classifications\"],\n            \"specification\": {\n                \"sequence_length\": 945,\n                \"input_dim\": 8,\n                \"sampling_rate\": 500,\n                \"train_samples\": 12600,\n                \"validation_samples\": 3150,\n                \"test_samples\": 3150,\n                \"complexities\": \"in intricate kinesthetic patterns\"\n            },\n            \"description\": \"Ultra-high resolution wearable data from 8 bio-sensors for gesture analysis and classification.\",\n            \"preprocessing\": [\"Data cleaning, normalization\"],\n            \"augmentation\": [\"None needed for gesture data\"],\n            \"visualization\": [\"Data exploration plots\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Fusion Model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"components\": [\"LSTMs, bidirectional encoders, multi-head attention\"],\n                \"real-time_inference\": true\n            },\n            \"description\": \"A state-of-the-art model for time-series classification, combining LSTMs, bidirectional encoders, and multi-head attention, with focus on precision, model explainability, and latency optimization.\"\n        }\n    ],\n    \"knowledge\": [\"Sensor noise reduction techniques, explainability methods like SHAP or attention maps\"],\n    \"service\": {\n        \"feature_engineering\": [\"Sensor fusion, feature extraction\"],\n        \"target_device\": \"Edge device or embedded system\",\n        \"deployment_endpoint\": \"Cloud-based deployment with low-latency service\",\n        \"inference_engine\": [\"TensorFlow Lite, optimized for real-time inference\"],\n        \"demonstration\": \"Live demos showcasing gesture recognition and interpretability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the user's intent to build a complex machine learning solution, their high expertise level, and details about the time-series analysis problem and dataset. The model, performance metrics, and complexity metrics are carefully designed based on the user's requirements. The missing metric values imply optimization for those aspects. The confidence score acknowledges the completeness of provided information.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager, your challenge is to develop a cutting-edge text classification solution for the arXiv repository, specifically in high-frequency subfields of Physics, Math, and Computer Science. Implement an advanced, HAN+BERT hybrid architecture that demands exceptional performance, with both micro- (93%) and macro-averaged (92%) F1 scores. Ensure the system not only demonstrates adaptability in real-time to accommodate continuous data expansion but also maintains a stringent latency benchmark of less than 100 milliseconds for new article submissions, while maintaining inter-departmental collaboration and streamlining the research workflow.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"scientific research\",\n        \"description\": \"Develop a cutting-edge text classification solution for the arXiv repository, focusing on high-frequency subfields of Physics, Math, and Computer Science. Use HAN+BERT hybrid architecture that requires exceptional performance with both micro- (93%) and macro-averaged (92%) F1 scores. Ensure real-time adaptability, latency benchmark of less than 100 milliseconds for new submissions, and support for inter-departmental collaboration and efficient research workflow.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Micro-F1\",\n                \"value\": 0.93\n            },\n            {\n                \"name\": \"Macro-F1\",\n                \"value\": 0.92\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 100,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv repository\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"category\"],\n            \"specification\": {\n                \"domain\": [\"Physics\", \"Math\", \"Computer Science\"],\n                \"size\": {\"train\": \"\", \"eval\": \"\", \"test\": \"\"}\n            },\n            \"description\": \"A large-scale repository of scientific articles with text data, focusing on Physics, Math, and Computer Science.\",\n            \"preprocessing\": [\"cleaning, normalization\", \"tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"category distribution\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HAN+BERT hybrid architecture\",\n            \"family\": \"neural networks\",\n            \"type\": \"transformer-based\",\n            \"specification\": {\n                \"architecture\": \"HAN+BERT\",\n                \"parameters\": {\n                    \"model_size\": \"\",\n                    \"training time\": \"\"\n                }\n            },\n            \"description\": \"A state-of-the-art text classification model that combines Hierarchical Attention Networks (HAN) and BERT for arXiv text classification.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"contextual embeddings\", \"cross-lingual analysis\"],\n        \"target_device\": \"cloud or edge\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"collaborative, real-time prediction dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the user's project manager intent to build a high-performing text classification solution. It includes the HAN+BERT hybrid architecture, F1 score requirements, and latency constraints. The dataset details are specified for Physics, Math, and Computer Science. The performance metrics and complexity metrics are in line with the user's expectations. However, some fields like model specifications and deployment endpoint are missing specific details and need to be filled according to the actual implementation details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "Assemble a sophisticated, deep-learning hybrid model for multi-stage time-series tabular regression, requiring the team to handle an extensive dataset with nested dependencies. Incorporate diverse project attributes (like resource allocation, technology evolvingness, and regional trends), develop a robust outlier detection mechanism, and meticulously analyze long-term seasonal patterns to provide accurate and actionable budget forecasts across multiple future initiatives.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"business forecasting\",\n        \"description\": \"Assemble a sophisticated, deep-learning hybrid model for multi-stage time-series tabular regression on a large, nested dataset. The model should handle dependencies, incorporate diverse project attributes (resource allocation, technology evolution, regional trends), and include a robust outlier detection mechanism. Key is to analyze long-term seasonal patterns for accurate budget forecasts across multiple future initiatives.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Absolute Error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Squared Error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"R-squared\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Training time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"Memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extensive time-series tabular data\",\n            \"modality\": [\"time series\", \"tabular\"],\n            \"target_variables\": [\"Budget forecasts\"],\n            \"specification\": {\n                \"rows\": null,\n                \"columns\": null,\n                \"time_series_length\": null,\n                \"nested_structure\": true\n            },\n            \"description\": \"A large and complex dataset with nested dependencies, including various project attributes and historical financial data.\",\n            \"preprocessing\": [\"Handling missing values\", \"Normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Seasonality analysis\", \"Dependency visualization\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Time-Series Model\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Multi-stage\",\n                \"layers\": {\n                    \"RNN/LSTM\": {\n                        \"type\": \"sequence\",\n                        \"units\": null,\n                        \"layers_count\": null\n                    },\n                    \"Embeddings\": {\n                        \"dimension\": null\n                    },\n                    \"Classifier\": {\n                        \"type\": \"regression\"\n                    }\n                },\n                \"training_details\": {\n                    \"optimizer\": \"\",\n                    \"batch_size\": null,\n                    \"epochs\": null\n                }\n            },\n            \"description\": \"A deep-learning model designed for multi-stage time-series regression, capable of capturing nested dependencies, diverse attributes, and analyzing long-term patterns.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Seasonality adjustments for fiscal year\",\n        \"Resource allocation strategies\",\n        \"Technology adoption trends\",\n        \"Impact of regional economic conditions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Feature extraction from project attributes\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom API\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Apache.mxnet\"],\n        \"demonstration\": \"Interactive forecast dashboard with explanation of factors contributing to budget estimates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the given instruction, detailing a high-level project requirement for a deep learning hybrid model with emphasis on multi-stage time-series tabular regression, handling extensive data, and addressing diverse factors. Performance metrics are left unspecified as the instruction does not provide specific targets, and complexity metrics reflect the need for efficient handling of a large and complex dataset. Assumptions are made for feature engineering and deployment requirements based on the scope of the project.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "link prediction", "instruction": "For the project, enhance the link prediction model in Citeseer by integrating a feature importance analysis module, ensuring interpretability, achieving a minimum precision of 90% even with sparse data, and optimizing for scalability to manage terabytes of growing scholarly citations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"scholarly research\",\n        \"description\": \"Enhance the link prediction model in Citeseer by integrating a feature importance analysis module to improve interpretability. Target precision should reach at least 90% even with sparse data, and prioritize scalability for handling large datasets of growing scholarly citations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null,\n                \"unit\": \"terabytes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Citeseer dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"citations\"],\n            \"specification\": {\n                \"sparse_data\": {\n                    \"type\": \"sparse\"\n                }\n            },\n            \"description\": \"A citation network dataset with the focus on link prediction, where the challenge lies in sparse data and scalability.\",\n            \"preprocessing\": [\"feature importance analysis\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"link prediction model (enhanced with feature importance)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"sparse_data_performance\": \"optimized\"\n            },\n            \"description\": \"An advanced graph neural network model for link prediction in Citeseer, incorporating feature importance analysis and optimized for sparse data and scalability.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"The model will require effective techniques to handle large sparse data and maintain high precision even with limited information.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature importance analysis\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"showcase interpretability with feature importance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured to match the given user's instruction, considering enhancing a link prediction model in Citeseer. Expertise level is assumed high, implying detailed understanding. Metrics such as precision, interpretability, and scalability are included. The source for the dataset is provided as 'direct-search', and preprocessing steps like feature importance analysis are mentioned. Given the level of detail, the confidence score is high.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series anomaly detection", "instruction": "As the project manager, envision a cutting-edge time-series anomaly detection system for a real-time, high-frequency financial market surveillance in the cryptocurrency domain. The team must develop a highly scalable, fault-tolerant model using a novel fusion of deep learning and graph neural networks, operating on streaming data with 10-second resolution. The challenge lies in achieving an AUC-PR score of at least 95% while maintaining interpretability for identifying illicit trading activities and ensuring a latency of under 100 milliseconds for crucial decisions. Additionally, propose a visualization dashboard to present anomalies in real-time with detailed heatmaps and interactive visual explanations, all while adhering to GDPR regulations for data privacy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"financial market surveillance\",\n        \"description\": \"Design a cutting-edge time-series anomaly detection system for real-time, high-frequency cryptocurrency market surveillance. The model should combine deep learning and graph neural networks, operate on streaming data with 10-second resolution, target AUC-PR of at least 95%, and ensure interpretability for illicit trading activities detection with latency below 100 milliseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC-PR score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 100,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cryptocurrency market data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"transaction data\", \"price fluctuations\"],\n            \"specification\": {\n                \"resolution\": 10,\n                \"frequency\": \"second\"\n            },\n            \"description\": \"Real-time, high-resolution data from the cryptocurrency market for anomaly detection purposes.\",\n            \"preprocessing\": [\"data cleaning\", \"normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"heatmap\",\n                \"interactive visual explanations\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep Learning and Graph Neural Networks fusion\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"scalability\": \"high\",\n                \"fault tolerance\": \"yes\",\n                \"interpretability\": \"yes\"\n            },\n            \"description\": \"A real-time, high-frequency anomaly detection model that combines deep learning and graph neural networks for financial market surveillance.\"\n        }\n    ],\n    \"knowledge\": [\n        \"GDPR compliant data privacy measures for anomaly detection in cryptocurrency markets\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"streaming data processing\", \"real-time fusion\"],\n        \"target_device\": \"streaming environment\",\n        \"deployment_endpoint\": \"financial surveillance system\",\n        \"inference_engine\": [\"real-time\", \"GPU-optimized\"],\n        \"demonstration\": \"GDPR-compliant anomaly visualization dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key elements from the user's instruction, focusing on the development of a scalable and fault-tolerant anomaly detection system for the cryptocurrency market. The requirements for performance, latency, interpretability, and data privacy are included. The model and dataset details address the novel fusion of technologies and the real-time requirements. The service section outlines necessary steps for deployment and the dashboard expectations. The confidence score is high due to the comprehensive coverage of the instruction's details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "For the groundbreaking machine learning initiative, design a sophisticated, multilingual question-answering entity-retrieval system using HFT-BERT's advanced fusion and hierarchical architecture. The project must exclusively fine-tune the exclusive 'Esoteric Quandaries Oceanography' (EQOQ) dataset, featuring intricately complex historical and niche knowledge questions. The system must achieve a minimum F1 score of 97% while demonstrating real-time responsiveness within 120 milliseconds, catering to users who seek answers in five less-common tongues. Additionally, focus on optimizing cross-lingual understanding and maintaining uniform accuracy levels, targeting seamless navigation through diverse and obscure knowledge domains.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"scholarly research or knowledge base\",\n        \"description\": \"Design a sophisticated, multilingual question-answering entity-retrieval system using HFT-BERT's advanced fusion and hierarchical architecture, exclusively fine-tuning on the Esoteric Quandaries Oceanography (EQOQ) dataset. The system aims to handle complex historical and niche knowledge questions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.12,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Esoteric Quandaries Oceanography (EQOQ)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": \"...\",\n                    \"validation\": \"...\",\n                    \"test\": \"...\"\n                },\n                \"language\": [\"English\", \"Spanish\", \"French\", \"German\", \"Italian\"]\n            },\n            \"description\": \"A dataset containing complex historical and niche knowledge questions and answers, specifically for fine-tuning the system on five less-common languages.\",\n            \"preprocessing\": [\"data cleaning\", \"language identification\"],\n            \"augmentation\": [],\n            \"visualization\": [\"data distribution analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HFT-BERT\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Hierarchical Fusion and Advanced BERT\",\n                \"parameter_size\": \"...\",\n                \"FLOPs\": \"...\",\n                \"train_speed\": \"...\"\n            },\n            \"description\": \"A multilingual, deep-learning-based question-answering model with hierarchical fusion and HFT-BERT architecture.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"cross-lingual embeddings\",\n            \"domain adaptation\"\n        ],\n        \"target_device\": \"real-time application\",\n        \"deployment_endpoint\": \"web or API-based\",\n        \"inference_engine\": [\n            \"GPU-optimized\"\n        ],\n        \"demonstration\": \"Interactive interface with real-time response\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's request for a high-end machine learning project that includes advanced NLP and real-time performance. The model is specified as a sophisticated multilingual QA system, the EQOQ dataset is described as exclusive and tailored for the task, and the performance, complexity, and target languages are clearly defined. The potential shortcomings are mainly about missing specific values for model size and inference engine optimization, but these can be inferred based on the user's expected 'high' expertise level.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a hierarchical deep residual LSTM architecture with adaptive multi-seasonality modeling for the solar and wind energy time-series dataset. Request a 72-hour forecast that exhibits incremental accuracy, targeting 99.9% accuracy in real-time with quantifiable performance degradation. Optimize for ultra-low latency, not exceeding 10 microseconds per prediction while considering edge device constraints and memory footprint optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy production\",\n        \"description\": \"Develop a hierarchical deep learning model using a residual LSTM architecture with adaptive multi-seasonality modeling for solar and wind energy time-series data. Aim for a 72-hour forecast with incremental accuracy, targeting at least 99.9% accuracy for real-time predictions. Optimize for ultra-low latency, not exceeding 10 microseconds per prediction, while considering edge device constraints and memory footprint optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"forecast accuracy\",\n                \"value\": 0.999\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 10,\n                \"unit\": \"microseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency per prediction\",\n                \"value\": 10,\n                \"unit\": \"microseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"unknown (optimization goal)\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Solar and wind energy time-series dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"Power generation for solar and wind\"],\n            \"specification\": {\n                \"length\": {\"description\": \"Unknown, inferred from task\", \"value\": null},\n                \"dimensions\": {\"description\": \"Time-series related\", \"value\": null}\n            },\n            \"description\": \"A time-series dataset with solar and wind energy generation data for training and forecasting.\",\n            \"preprocessing\": [\"Adaptive seasonality modeling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Residual LSTM with Multi-seasonality\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"Hierarchical, residual, LSTM with adaptive multi-seasonality\"\n            },\n            \"description\": \"A deep learning model for time-series forecasting with a focus on solar and wind energy, targeting 72-hour forecasts with real-time 99.9% accuracy.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"time-series data preprocessing and feature extraction\"],\n        \"target_device\": \"edge device\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for edge latency\"],\n        \"demonstration\": \"Quantifiable degradation over time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's requirements for a building task, with a high level of expertise assumed. The problem area, task, and application domain are specified according to the solar and wind energy data. Performance metrics include accuracy and latency, with constraints on edge devices and memory optimization. The dataset is described for time-series and the model is defined with a hierarchical residual LSTM architecture. The service section covers feature engineering, target device, and latency-focused inference engines. Missing information, such as the complete dataset structure and model optimization details, may be required to fully address the edge constraint and memory footprint optimization.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Design a highly interpretable node classification model for the YouTube Algebric Connectivity dataset, employing Significance Graph Neural Networks (SGNNs) with a sophisticated feature extraction strategy for handling sparse data. Mandate a minimum accuracy of 85% and exhibit the ability to discriminate among various content creator genres through step-by-step feature transformation and multi-level reasoning. Additionally, compare the model's performance with traditional approaches and justify the choice of SGNNs.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"social media analysis\",\n        \"description\": \"Design a highly accurate node classification model using Significance Graph Neural Networks (SGNNs) for the YouTube Algebraic Connectivity dataset. The focus is on sparse data with a minimum accuracy of 85% and differentiating between various content creator genres via step-by-step feature transformation and multi-level reasoning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Training time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube Algebraic Connectivity\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"content creator genres\"],\n            \"specification\": {\n                \"sparse_data_handling\": true\n            },\n            \"description\": \"A graph dataset with content creators as nodes and relationships between them, designed for node classification using SGNNs.\",\n            \"preprocessing\": [\n                \"handling sparse data\",\n                \"feature normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"graph structure analysis\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Significance Graph Neural Networks (SGNN)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_extraction\": \"sophisticated for sparse data\",\n                \"reasoning_levels\": \"multi-level\"\n            },\n            \"description\": \"A node classification model using SGNNs for the YouTube Algebraic Connectivity dataset, with a focus on interpretability and addressing sparse data.\"\n        }\n    ],\n    \"knowledge\": [\n        \"SGNNs excel in sparse data by considering local neighborhood context without explicit feature interactions.\",\n        \"Multi-level reasoning improves model performance by capturing hierarchical relationships in the data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"preprocessing\",\n            \"feature selection\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"fast推理 on sparse data\"\n        ],\n        \"demonstration\": \"step-by-step process and performance comparison with traditional methods\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's intent for building a model, their expertise level, and the requirements of a highly interpretable node classification model for the YouTube Algebraic Connectivity dataset. The model uses Significance Graph Neural Networks, specifies a minimum accuracy, and incorporates feature extraction and multi-level reasoning for sparse data. The missing metric values are left open for optimization as per the instruction. However, it is assumed that some traditional approaches are to be used for comparison, and a demonstration of the model's performance and reasoning would be provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "For an elite planetary exploration team, design a deep learning model using a specialized ESA dataset like the Hubble Ultra-Deep Field for intricate classification of celestial objects, such as differentiating quasars from TDEs. Implement transfer learning with tailored image warping techniques, accounting for the scarcity of rare classes and architectural trade-offs to maximize precision. Enumerate potential obstacles and recommend a cutting-edge model architecture.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy/planetary science\",\n        \"description\": \"Design a deep learning model for intricate classification of celestial objects, specifically differentiating quasars from TDEs using a specialized ESA dataset like the Hubble Ultra-Deep Field. Implement transfer learning and tailored image warping techniques to account for rare class scarcity. Optimize for precision while considering architectural trade-offs.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hubble Ultra-Deep Field\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"quasars\", \"TDEs\"],\n            \"specification\": {\n                \"image_shape\": null,\n                \"number_of_classes\": 2,\n                \"dataset_size\": null\n            },\n            \"description\": \"A specialized ESA dataset for planetary science, containing intricate images of celestial objects for classification tasks.\",\n            \"preprocessing\": [\"transfer learning\", \"image warping\"],\n            \"augmentation\": [\"addressing rare class scarcity\"],\n            \"visualization\": null,\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep convolutional neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for celestial object classification\",\n                \"base_model\": \"pre-trained on a large dataset\",\n                \"architecture_trade-offs\": []\n            },\n            \"description\": \"A deep learning model with transfer learning and custom image warping techniques designed for classifying quasars and TDEs in the Hubble Ultra-Deep Field.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Potential obstacles: limited labeled data for rare classes, computational resource constraints, fine-tuning trade-offs between accuracy and complexity.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"custom feature extraction for celestial object analysis\"],\n        \"target_device\": \"high-performance computing cluster\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"state-of-the-art inference frameworks\"],\n        \"demonstration\": \"Provide visualizations and detailed results for model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure is aligned with the user's request for a machine learning model in the field of computer vision. The instruction covers the domain, task, specific techniques, performance metric, and potential challenges. Some details are left blank due to the nature of the task, but the overall structure is comprehensive. The user's high expertise level suggests a requirement for a more advanced model and approach.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image classification", "instruction": "For the complex cat breed classification task, design a hybrid model incorporating Inceptionv5 and ResNeXt, accounting for inter-breed similarities, local texture analysis, and transfer learning from an ImageNet pre-trained model. Specify data augmentation techniques, color normalization, and propose a custom loss function to handle imbalance classes in the diverse, high-resolution dataset with 50 unique breeds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal recognition\",\n        \"description\": \"Design a hybrid model using Inceptionv5 and ResNeXt for complex cat breed classification, considering inter-breed similarities, local texture analysis, and transfer learning from an ImageNet pre-trained model.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"complex cat breed dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\"],\n            \"specification\": {\n                \"size\": {\"width\": null, \"height\": null},\n                \"resolution\": \"high\",\n                \"number_of_classes\": 50\n            },\n            \"description\": \"A large, diverse dataset with 50 unique cat breeds for classification, requiring high-resolution images and handling of inter-breed similarities.\",\n            \"preprocessing\": [\n                \"color normalization\",\n                \"data augmentation (e.g., random cropping, flipping, rotation)\"\n            ],\n            \"augmentation\": [\n                \"random cropping\",\n                \"horizontal flipping\",\n                \"small rotation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Model\",\n            \"family\": \"Inceptionv5 + ResNeXt\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer learning\": \"ImageNet pre-trained\",\n                \"inter_breed_similarity\": \"incorporated\",\n                \"local_texture_analysis\": \"incorporated\"\n            },\n            \"description\": \"A deep learning model combining Inceptionv5 and ResNeXt for cat breed classification, leveraging transfer learning and designed to address breed similarities and texture analysis.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporate intra-breed and inter-breed similarities for better classification.\",\n        \"Employ local texture analysis for capturing fine details.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes the user's intent to build a model, leveraging their high expertise level in AI and ML. The problem description covers the task, the importance of transfer learning, texture analysis, and addressing class imbalance. Data augmentation and color normalization are specified. Custom loss function to handle class imbalance is also suggested. However, the exact performance metrics and complexity constraints are not provided, as they were not explicitly mentioned in the instruction.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "Construct a state-of-the-art translation model for multilingual communication, leveraging the extensive ParaCrawl corpus. Design a novel hybrid dual encoder that integrates transformer-based architecture with a cultural context module, focusing on English to Dutch translations. The system must excel in maintaining lexical accuracy, emotional authenticity, and cultural sensitivity while handling complex idiomatic expressions and historical nuances.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multilingual communication\",\n        \"description\": \"Create a state-of-the-art translation model, leveraging the ParaCrawl corpus, specifically for English to Dutch translations. The model should use a hybrid dual encoder combining transformer-based architecture with a cultural context module, prioritizing lexical accuracy, emotional authenticity, and cultural sensitivity, while handling complex idiomatic expressions and historical nuances.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"lexical accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"emotional authenticity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"cultural sensitivity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"English to Dutch translations\"],\n            \"specification\": {\n                \"corpus size\": null,\n                \"languages\": [\"English\", \"Dutch\"],\n                \"specificity\": \"multilingual communication\"\n            },\n            \"description\": \"A large multilingual corpus for training the translation model with a focus on English to Dutch translations.\",\n            \"preprocessing\": [\"lemmatization\", \"tokenization\"],\n            \"augmentation\": [\"back-translation\"],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Dual Encoder\",\n            \"family\": \"Transformer-based architecture with cultural context module\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"encoder architecture\": \"Transformer\",\n                \"cultural context module details\": null\n            },\n            \"description\": \"A state-of-the-art machine translation model integrating a transformer with a cultural context module for English to Dutch translations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Performance on preserving complex idiomatic expressions and historical nuances not mentioned explicitly in the dataset.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-aware feature extraction\"],\n        \"target_device\": \"cloud-based or high-performance computing\",\n        \"deployment_endpoint\": \"specific API or platform for multilingual translation\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"showcase of sample translations with improved lexical accuracy, emotional authenticity, and cultural sensitivity\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent for building a state-of-the-art translation model, with emphasis on their high expertise level. The problem area, task, and domain are clearly defined, as is the target dataset and model specifics. Performance metrics for lexical accuracy, emotional authenticity, and cultural sensitivity are mentioned with undefined values, indicating that these should be optimized during development. The model design integrates a cultural context module, and complexity metrics reflect the requirement for efficient inference. However, additional context may be needed to fine-tune the model's cultural context module and performance measurements.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image segmentation", "instruction": "The medical imaging project necessitates the development of an Enhanced V-Net++ model tailored for the challenging BraTS dataset. Require a sophisticated, iterative optimization pipeline that includes hyperparameter tuning, ensemble learning, and a minimum Dice coefficient benchmark of 0.95. Additionally, mandate real-time integrated Grad-CAM visualizations for in-depth feature inspection, and the generation of interactive, clinically interpretable heatmaps and comprehensive, user-friendly reports for seamless integration into diagnostic workflows.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"medical imaging\",\n        \"description\": \"Develop an Enhanced V-Net++ model for the BraTS dataset, addressing a challenging medical imaging task. The pipeline should include sophisticated optimization with hyperparameter tuning, ensemble learning, and aim for a minimum Dice coefficient benchmark of 0.95.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Dice coefficient\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BraTS dataset\",\n            \"modality\": [\"image\", \"multimodal\"],\n            \"target_variables\": [\"segmentation masks\"],\n            \"specification\": {\n                \"size\": \"unknown\",\n                \"modalities\": [\"T1, T1c, T2, FLAIR\"],\n                \"complexity\": \"challenging\"\n            },\n            \"description\": \"Dataset for brain tumor segmentation in medical imaging, with multimodal MRI scans and segmentation masks.\",\n            \"preprocessing\": [\n                \" skull stripping\",\n                \"intensity normalization\"\n            ],\n            \"augmentation\": [\n                \"GAN-based data augmentation\"\n            ],\n            \"visualization\": [\n                \"Grad-CAM visualizations\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced V-Net++\",\n            \"family\": \"V-Net architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"iterative optimization pipeline, ensemble learning\"\n            },\n            \"description\": \"A tailored deep learning model for brain tumor segmentation in the BraTS dataset, incorporating an Enhanced V-Net++ design.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Use of transfer learning from pre-trained models\",\n        \"Clinical context of segmentation performance\",\n        \"Impact of domain adaptation methods\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"gradient-based optimization\",\n            \"automated hyperparameter search\"\n        ],\n        \"target_device\": \"CPU-GPU hybrid for real-time performance\",\n        \"deployment_endpoint\": \"cloud-based storage for scalable sharing\",\n        \"inference_engine\": [\n            \"real-time Grad-CAM for visualizations\"\n        ],\n        \"demonstration\": \"interactive heatmaps and user-friendly reports for clinical workflow integration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the requirements for the medical imaging project, focusing on the Enhanced V-Net++ model, target metrics, dataset, optimization pipeline, and integration with diagnostic workflows. The user's high expertise level is represented in the structure. However, there may be room for clarification on dataset size and specifics of gradient-based optimization and domain adaptation.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "The task now requires not only the enhancement of the existing Inception-v3 model for car classification but also the integration of an adversarial learning component. The objective is to develop a robust system that recognizes not only rare vintage and modern supercars but also withstands occlusions and style transfer attacks, targeting a minimum precision and recall of 97% for each subcategory. Additionally, provide a comprehensive analysis of the model's resilience against adversarial examples and present a defendable accuracy metric. Utilize the Stanford Cars Dataset with advanced augmentation and low-light conditions for training.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"automotive\",\n        \"description\": \"Enhance the Inception-v3 model for car classification, including adversarial learning to handle rare vintage and modern supercars, with resilience to occlusions and style transfer attacks. Aim for precision and recall of 97% for each subcategory, and evaluate model's resilience against adversarial examples with a defendable accuracy metric. Utilize the Stanford Cars Dataset with advanced augmentation and low-light conditions for training.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resilience against adversarial examples\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Cars Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"car type\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": {\n                        \"number_of_images\": null,\n                        \"advanced_augmentation\": true,\n                        \"low_light_conditions\": true\n                    },\n                    \"test\": {\n                        \"number_of_images\": null\n                    }\n                }\n            },\n            \"description\": \"A dataset for car classification with vintage, modern, and supercars, including the need for robustness against occlusions and style transfer attacks.\",\n            \"preprocessing\": [\"advanced augmentation\", \"low-light condition handling\"],\n            \"augmentation\": [\"occlusion, style transfer\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Inception-v3 with Adversarial Learning\",\n            \"family\": \"Inception-v3\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy under adversarial attacks\": null,\n                \"targeted_at\": [\"rare vintage, modern supercars\"]\n            },\n            \"description\": \"An advanced car classification model that enhances Inception-v3, integrates adversarial learning, and is designed to withstand occlusions and style transfer.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Focus on evaluating model resilience through adversarial examples and propose a defendable accuracy metric.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Visual explanations for adversarial examples and model robustness under varying conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the given user's instruction, focusing on enhancing Inception-v3, integrating adversarial learning, and targeting performance metrics for rare car categories. The dataset is specified using the Stanford Cars Dataset with advanced augmentation and low-light conditions. Specific model details, such as adversarial resilience and defendable accuracy, are left unspecified for further collaboration with the user. The high expertise level indicates a deeper understanding of the requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "Utilize advanced, real-time wavelet denoising techniques for the ' hourly_polar Bears_co2_emissions_and_AtmosphericCO2Levels_1950-2022' dataset, specifically focusing on sub-seasonal patterns. The task is to detect sudden anomalies in Arctic regions' CO2 levels during specific polar bear migration seasons, ensuring accurate assessment of climate change impacts on wildlife.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"climate science\",\n        \"description\": \"Utilize advanced real-time wavelet denoising techniques for the 'hourly_polar_Bears_co2_emissions_and_AtmosphericCO2Levels_1950-2022' dataset, focusing on sub-seasonal patterns. The objective is to identify sudden anomalies in Arctic regions' CO2 levels during polar bear migration seasons.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"anomaly detection rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"hourly_polar_Bears_co2_emissions_and_AtmosphericCO2Levels_1950-2022\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"CO2 levels\"],\n            \"specification\": {\n                \"frequency\": \"hourly\",\n                \"time_period\": \"1950-2022\",\n                \"feature_dimensions\": {\n                    \"time\": \"timepoints\",\n                    \"variables\": [\"CO2 levels\"]\n                }\n            },\n            \"description\": \"A time-series dataset containing hourly CO2 emissions and levels from 1950 to 2022, focused on detecting anomalies during polar bear migration seasons in the Arctic regions.\",\n            \"preprocessing\": [\n                \"wavelet denoising\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"real-time wavelet-based anomaly detection\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"temporal convolutional network\"\n            },\n            \"description\": \"A deep learning model employing real-time wavelet denoising for detecting anomalies in sub-seasonal CO2 levels during polar bear migration seasons.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced understanding of wavelet analysis and its application to time-series data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"wavelet coefficients\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-based\"],\n        \"demonstration\": \"Generated anomaly detection reports and interactive visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON response covers the user's request for a machine learning project involving advanced wavelet denoising, anomaly detection in a specific time-series dataset, and the context of polar bear migration and climate change. It addresses the user's high expertise level and includes performance metrics that align with the task. The model family, specifications, and feature engineering techniques reflect the user's instruction. However, some specific metric values are left unspecified to accommodate potential optimization based on real-world performance.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "multimodal forgery detection", "instruction": "As a project manager for the machine learning team, design a multimodal forgery detection system that not only analyses visual and text-based alterations in academic records but also integrates real-time cross-referencing with global databases, ensuring traceability and stringent zero-tolerance for counterfeits in the digital era.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"academic records and security\",\n        \"description\": \"Design a multimodal forgery detection system for academic records, analyzing visual and text alterations, and integrating real-time cross-referencing with global databases for traceability and strict zero-tolerance against counterfeits.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\n                \"image\",\n                \"text\"\n            ],\n            \"target_variables\": [\"forged status\"],\n            \"specification\": {\n                \"data_shape\": null,\n                \"modalities_size\": {\n                    \"image\": null,\n                    \"text\": null\n                },\n                \"cross-referenced_db_size\": null\n            },\n            \"description\": \"A dataset for multimodal forgery detection, including visual and text data, with potential real-time cross-referencing against global databases.\",\n            \"preprocessing\": [\n                \"data normalization\",\n                \"feature extraction\"\n            ],\n            \"augmentation\": [\"real-time data synchronization\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks\",\n            \"type\": \"multimodal learning\",\n            \"specification\": {\n                \"model_architecture\": null,\n                \"modal融合技术\": null,\n                \"database_interoperability\": null\n            },\n            \"description\": \"A multimodal forgery detection system that integrates visual and text-based analysis with real-time cross-referencing\"\n        }\n    ],\n    \"knowledge\": [\n        \"Global databases are essential for cross-referencing and ensuring traceability\",\n        \"Zero-tolerance policy requires strict design against false positives\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\",\n            \"modal interaction learning\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"custom educational platform\",\n        \"inference_engine\": [\n            \"GPU-optimized\",\n            \"real-time data processing\"\n        ],\n        \"demonstration\": \"live and interactive demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's request for a machine learning project involving a multimodal forgery detection system. It reflects the user's high expertise level in understanding the requirements. While performance metrics are not explicitly specified, the request for real-time cross-referencing implies a focus on response time and memory usage. The dataset and model sections capture key aspects like modality fusion and database interoperability. However, certain details such as model architecture and exact specifications are left open as the project would require further discussion for clarification.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "visual question answering", "instruction": "For the upcoming community knowledge repository, devise a sophisticated visual question-answering system utilizing the TextVQA dataset. Demand a deep, multi-modal model capable of not only extracting information but also understanding complex relationships between text and images, ensuring high precision and adaptability across diverse educational contexts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Design a sophisticated visual question-answering system for a community knowledge repository using the TextVQA dataset. The system should employ a deep, multi-modal model that can extract and understand complex relationships between text and images, prioritizing high precision and adaptability across diverse educational contexts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model complexity\",\n                \"value\": null,\n                \"unit\": \"parameters\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TextVQA dataset\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [\"answer\"],\n            \"specification\": {\n                \"dataset_size\": null,\n                \"image_channels\": null,\n                \"text_lengths\": null,\n                \"question_types\": []\n            },\n            \"description\": \"A dataset for visual question answering, containing questions and associated images from the educational context.\",\n            \"preprocessing\": [\"pre-trained multi-modal embeddings\", \"data augmentation\"],\n            \"augmentation\": [\n                \"synthetic image generation\",\n                \"text augmentation\"\n            ],\n            \"visualization\": [\n                \"model performance analysis on examples\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"multi-modal deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep transformer-based model\",\n                \"modal fusion\": \"late fusion\",\n                \"depth\": null,\n                \"width\": null,\n                \"pre-training\": \"M4C or similar\"\n            },\n            \"description\": \"A deep, multi-modal model for visual question answering, specifically designed for TextVQA, with emphasis on understanding intricate relationships between text and images.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model should be adaptable to new question formats and educational domains\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"cloud-based deployment\",\n        \"inference_engine\": [\n            \"GPU-based inference\"\n        ],\n        \"demonstration\": \"demo videos showcasing question answering on various educational context examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided information accurately reflects the user's request for a complex visual question-answering system, targeting precision, adaptability, and multi-modal understanding. The dataset, TextVQA, aligns with the instruction, and the deep, multi-modal model suggests a sophisticated approach. However, specific model parameters and some metric values are not provided, allowing for optimization. The user's high expertise level suggests a need for detailed information, which is assumed to be part of the actual design.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "The machine learning project for image classification on the specialized Car Rarity and Aesthetics Dataset requires developing an advanced Inception-v3 model with fine-grained recognition for boutique car manufacturers, limited-edition models, and paint color variations. The model must demonstrate robustness under challenging conditions like low light, extreme angles, and diverse textures, ensuring precision and recall for individual subcategories, striving for a minimum average precision of 97% across all classes.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"automotive\",\n        \"description\": \"The task is to develop an advanced Inception-v3 model for image classification on the Car Rarity and Aesthetics Dataset, focusing on boutique car manufacturers, limited-edition models, and paint color variations. The model should exhibit robustness in low light, extreme angles, and diverse textures, with a minimum average precision of 97% for all subcategories.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Car Rarity and Aesthetics Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"boutique car manufacturers\", \"limited-edition models\", \"paint color variations\"],\n            \"specification\": {\n                \"image_shape\": \"299x299 pixels\",\n                \"image_size\": \"not specified\",\n                \"classes\": \"boutique car manufacturers, limited-edition models, paint color variations\"\n            },\n            \"description\": \"A dataset for fine-grained image classification of boutique car manufacturers, limited-edition models, and paint color variations, accounting for challenging conditions.\",\n            \"preprocessing\": [\"data augmentation for low light, extreme angles, and diverse textures\"],\n            \"augmentation\": [\"data augmentation techniques for robustness\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Inception-v3\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Inception-v3\",\n                \"specifics\": \"advanced model architecture for fine-grained image recognition\"\n            },\n            \"description\": \"Advanced Inception-v3 model for image classification on the Car Rarity and Aesthetics Dataset with a focus on boutique cars.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Challenging conditions may require data augmentation techniques to improve model robustness.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction from Inception-v3\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow serving or a dedicated deployment platform\"],\n        \"demonstration\": \"Illustrate class-wise precision and recall for the trained model.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a machine learning model and their high expertise level. It covers the problem description, performance goals, specific dataset requirements, model details, and the expected service requirements, including a focus on fine-grained recognition and robustness under specified conditions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "As a project manager for a premier media analytics firm, your team has been assigned to develop a state-of-the-art sequential video recommendation system using the 'Bilibili Danmu' dataset. Given the scale of user interactions and the need for real-time, context-sensitive suggestions, devise an advanced deep learning model that integrates sentiment analysis, time-series patterns, and collaborative filtering. Emphasize on enhancing efficiency while preserving the depth of information in massive text comments for seamless browsing experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"media analytics\",\n        \"description\": \"Develop a state-of-the-art sequential video recommendation system using the 'Bilibili Danmu' dataset. The model should integrate sentiment analysis, time-series patterns, and collaborative filtering for real-time, context-sensitive suggestions. Focus on efficiency while maintaining depth of information in text comments for a seamless browsing experience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"speed\",\n                \"value\": {\n                    \"unit\": \"real-time\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": {\n                    \"unit\": \"milliseconds\"\n                }\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Bilibili Danmu\",\n            \"modality\": [\"video\", \"text\"],\n            \"target_variables\": [\"user interactions\", \"video recommendations\"],\n            \"specification\": {\n                \"scale\": \"massive\",\n                \"data structure\": \"sequences\"\n            },\n            \"description\": \"Dataset of Bilibili video comments with user interactions for building a sequential recommendation system.\",\n            \"preprocessing\": [\"text normalization\", \"sequence alignment\"],\n            \"augmentation\": [\n                \"time-series pattern extraction\",\n                \"sentiment-based sampling\"\n            ],\n            \"visualization\": [\n                \"sequence patterns\",\n                \"user sentiment analysis\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced deep learning model combining LSTM, Transformer, and Attention mechanisms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": {\n                    \"time-series modeling\": true,\n                    \"sentiment analysis\": true,\n                    \"collaborative filtering\": true\n                },\n                \"depth of text information\": \"preserved\"\n            },\n            \"description\": \"Integrating sentiment analysis, time-series patterns, and collaborative filtering for real-time video recommendation\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider real-time data processing pipelines and efficient data storage strategies for handling large scale interactions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware feature extraction\"\n        ],\n        \"target_device\": \"cloud and edge devices\",\n        \"deployment_endpoint\": \"API for real-time recommendations\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Apache MXNet\"],\n        \"demonstration\": \"Interactive video recommendations with sentiment, time-series, and collaborative filtering context\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the essence of the project manager's requirements, focusing on a build task for a sequential recommendation system, integrating sentiment analysis, time-series patterns, and collaborative filtering using a Bilibili Danmu dataset. The expert-level expertise indicates a high understanding of the requirements. Performance and complexity metrics are set to emphasize efficiency while maintaining depth in the comments. Missing values reflect areas where more specific information could be filled based on industry best practices or user provided details.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image classification", "instruction": "As a project manager for a specialized deep-learning team, you are tasked with creating a highly-accurate deep learning model for astrozoology. The team must design an advanced Image Recognition Transformer (IRT), inheriting the Vision Transformer (ViT) architecture, specifically tailored for the challenging NGC Galaxy Image Database. Focus on optimizing for intricate details, like variable nebulae and extragalactic phenomena, ensuring exceptional generalization ability to classify elusive celestial objects for breakthrough astronomical discoveries.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Create a highly-accurate deep learning model using an advanced Image Recognition Transformer (IRT) based on the Vision Transformer (ViT) architecture for the challenging NGC Galaxy Image Database. Focus on optimizing for intricate details like variable nebulae and extragalactic phenomena, aiming for excellent generalization to classify elusive celestial objects for significant astronomical discoveries.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NGC Galaxy Image Database\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"astronomical object categories\"],\n            \"specification\": {\n                \"database_size\": null,\n                \"image_dimension\": null,\n                \"image_depth\": null,\n                \"label_count\": null\n            },\n            \"description\": \"A challenging database of galaxy images with intricate details, containing variable nebulae and extragalactic phenomena for classification.\",\n            \"preprocessing\": [\"data augmentation for enhancing diversity\"],\n            \"augmentation\": [\"image resizing\", \"random cropping\", \"color jittering\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"IRT (Vision Transformer)\",\n            \"family\": \"Image Recognition Transformer (derived from ViT)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"ViT architecture with adaptations for astrozoology\",\n                \"flop_count\": null,\n                \"parameter_count\": null\n            },\n            \"description\": \"A state-of-the-art deep learning model specifically designed for challenging astrozoological image recognition, tailored to optimize for variable nebulae and extragalactic features.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider attending to class imbalance between categories through oversampling or loss function adjustments.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"transfer learning for domain adaptation\", \"feature extraction for interpretability\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized inference framework\"],\n        \"demonstration\": \"Creating a pipeline for live demonstration of model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON represents a comprehensive response to the user's task as a project manager. The user's intent is to build, and their expertise level is high. The problem area and task are clearly defined, along with performance and complexity metrics, datasets (with some preprocessing steps), model specifications, and considerations for fine-tuning and deployment. However, specific performance metric values, memory size, and some other details are left open for optimization.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a seasoned project manager for the Media Campaign Cost Analysis, you have been assigned to develop a cutting-edge machine learning model for a complex, real-world regression problem. Using the Private Alpha Media Dataset, comprising intricate numerical features, split into non-overlapping train (70%), validation (15%), and test sets (15%), your objective is to minimize the root mean squared logarithmic error (RMSLE) in predicting the costs for various advertising initiatives. Emphasize model interpretability, feature engineering, and explainability, while adhering to strict performance benchmarks and ensuring compliance with data privacy regulations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"Develop a machine learning model for a complex, real-world regression problem focusing on Media Campaign Cost Analysis. Use the Private Alpha Media Dataset with a train, validation, and test split of 70%, 15%, and 15% respectively, targeting low RMSLE while emphasizing interpretability, feature engineering, and explainability. Data privacy regulations must be followed.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model interpretability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"feature engineering time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explainability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Private Alpha Media Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"costs for advertising initiatives\"],\n            \"specification\": {\n                \"split\": [\n                    {\n                        \"train\": 0.7\n                    },\n                    {\n                        \"validation\": 0.15\n                    },\n                    {\n                        \"test\": 0.15\n                    }\n                ],\n                \"data privacy\": \"strict\"\n            },\n            \"description\": \"A dataset with intricate numerical features for predicting advertising costs in a real-world campaign scenario.\",\n            \"preprocessing\": [\"required for interpretability\"],\n            \"augmentation\": [],\n            \"visualization\": [\"required for feature engineering\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"interpretable machine learning models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"explainability features\": null\n            },\n            \"description\": \"A model optimized for Media Campaign Cost Analysis with an emphasis on interpretability and feature interpretability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\",\n            \"feature scaling\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"planned for explanations and insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's instruction, focusing on a tabular regression task with emphasis on model interpretability and feature engineering. It adheres to performance benchmarks by specifying RMSLE and includes data privacy constraints. Some performance and complexity metrics are left unspecified for precision, reflecting the need for detailed information about the user's specific requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The task at hand involves enhancing a specialized V-Net deep learning architecture for precise medical image segmentation on the BraTS dataset, targeting a Dice similarity coefficient of 0.85 or above. However, the model must not only exhibit high accuracy but also integrate a novel interpretability module that generates visual heatmaps and feature attributions, demanding a multi-faceted approach to boost trust in diagnostic predictions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"medical imaging\",\n        \"description\": \"Enhance a V-Net deep learning architecture for precise medical image segmentation on the BraTS dataset. Aim for a Dice similarity coefficient of 0.85 or above, and integrate a novel interpretability module for visual heatmaps and feature attributions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Dice similarity coefficient\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability requirements\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BraTS dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"pixel-level segmentation\"],\n            \"specification\": null,\n            \"description\": \"A medical image dataset for brain tumor segmentation, used to fine-tune and evaluate the V-Net model.\",\n            \"preprocessing\": [\n                \"Rescaling\",\n                \"Normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Segmentation masks\",\n                \"Attention maps\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"V-Net for Medical Image Segmentation\",\n            \"family\": \"V-Net\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specialized V-Net\",\n                \"interpretability_module\": \"inbuilt\"\n            },\n            \"description\": \"A deep learning model using a specialized V-Net architecture with an integrated interpretability module, providing visual heatmaps and feature attributions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art medical image segmentation techniques\",\n        \"Integration of interpretability for diagnostic trust\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Domain-specific feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-optimized\"\n        ],\n        \"demonstration\": \"Interactive heatmaps and model explanation for predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's request for enhancing a V-Net model for medical image segmentation with a target Dice score of 0.85. The performance metric is set, and interpretability requirements are specified along with preprocessing steps and visualization. The expert level of expertise is reflected. However, the target device and deployment details are not specified and might require additional information to fully address the user's needs.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager for the satellite image segmentation task using Sentinel-2 data, you've been tasked with enhancing the current land classification system by not only identifying water, forests, and urban areas but also distinguishing between five unique vegetation types (primary forest, deciduous forest, grasslands, wetlands, and croplands) and three distinct urban development stages (early, mature, and post-industrial). The team should explore the feasibility of real-time processing on resource-constrained devices, while maintaining high accuracy and minimizing false positives. Additionally, the chosen model should be interpretable, allowing for the identification of key factors influencing segmentation results. Please research and propose an appropriate model architecture and workflow that fulfills these requirements.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"remote sensing and land use analysis\",\n        \"description\": \"Enhance the land classification system for Sentinel-2 satellite data by identifying water, forests (primary and deciduous), grasslands, wetlands, and croplands. Aim for real-time processing on resource-constrained devices with high accuracy and low false positives, while maintaining model interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Overall accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Specificity (reducing false positives)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Interpretability metrics (e.g., feature importance)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time on resource-constrained devices\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2 satellite data\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"water\", \"primary forest\", \"deciduous forest\", \"grasslands\", \"wetlands\", \"croplands\"],\n            \"specification\": {\n                \"image_size\": null,\n                \"bands\": null,\n                \"spatial_resolution\": null\n            },\n            \"description\": \"Satellite imagery with multiple spectral bands for Sentinel-2 data, capturing the land features needed for segmentation.\",\n            \"preprocessing\": [\n                \" Sentinel-2 data normalization\",\n                \"Data augmentation (potentially for resource-constrained devices)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Proposed Model Architecture (e.g., lightweight U-Net with interpretability techniques)\",\n            \"family\": \"Convolutional Neural Networks (specifically interpretable variant)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": {\n                    \"number_of_layers\": null,\n                    \"kernel_size\": null,\n                    \"number_of_filters\": null,\n                    \"use_of_interpretability_methods\": true\n                },\n                \"inference_speed\": null,\n                \"memory_footprint\": null\n            },\n            \"description\": \"A lightweight, interpretable model designed for real-time processing with reduced computational requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"title\": \"Real-time processing constraints and challenges\",\n            \"content\": \"Research on efficient model architectures and techniques for real-world scenarios with resource limitations.\"\n        },\n        {\n            \"title\": \"Interpretable segmentation models\",\n            \"content\": \"Investigation into methods that enhance model interpretability without compromising accuracy.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Band fusion or feature extraction techniques\"\n        ],\n        \"target_device\": \"resource-constrained devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Workflow for real-time processing and interpretation of segmentation results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intention to build a model for satellite image segmentation. The project manager's expertise level is assumed high, guiding the requirement gathering. The problem area, tasks, and metrics are specified according to the instruction. Model architecture and workflow are proposed based on the requirements of real-time processing, interpretability, and distinct land classifications. Some unspecified metrics and details are left open to account for the possibility of additional research or the user filling in more specific information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As a time-series anomaly detection expert for a high-frequency trading firm, design and fine-tune a state-of-the-art deep learning model, integrating explainable artificial intelligence techniques, such as LIME and SHAP, for real-time detection of subtle market manipulations in cryptocurrency trades. This model should handle large-scale data from multiple exchanges (including Binance, Coinbase, and Bitfinex) with microseconds precision and be optimized for resource efficiency on edge devices. Additionally, develop a custom visualization dashboard to interpret and present detected anomalies to the security team, ensuring regulatory compliance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"high-frequency trading, cryptocurrency\",\n        \"description\": \"Design a deep learning model for real-time detection of market manipulations in cryptocurrency trades, integrating explainable AI techniques like LIME and SHAP. The model should handle data from Binance, Coinbase, and Bitfinex exchanges, with a precision of microseconds and optimized for resource efficiency on edge devices. A custom visualization dashboard for security team interpretation and regulatory compliance is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Inference speed\",\n                \"value\": \"microseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"Inference time (on edge devices)\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Binance, Coinbase, and Bitfinex cryptocurrency trades\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"Market manipulations\"],\n            \"specification\": {\n                \"data_size\": null,\n                \"sample_rate\": null,\n                \"time_frequency\": \"microseconds\"\n            },\n            \"description\": \"Largely unstructured time-series data containing cryptocurrency trades from multiple exchanges, including Binance, Coinbase, and Bitfinex.\",\n            \"preprocessing\": [\n                \"Feature normalization\",\n                \"Windowing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Anomaly detection patterns\",\n                \"SHAP and LIME explanations\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep learning, state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Deep learning model for time-series anomaly detection\",\n                \"explainable AI components\": \"Integrated LIME and SHAP\",\n                \"optimization\": \"Resource efficient for edge devices\"\n            },\n            \"description\": \"A deep learning model optimized for real-time detection of market manipulations in cryptocurrency trades using explainable AI techniques.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"LIME and SHAP for feature understanding\",\n            \"Microsecond precision time handling\"\n        ],\n        \"target_device\": \"Edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Efficient model execution on edge\"],\n        \"demonstration\": \"Custom visualization dashboard for anomaly interpretation and regulatory compliance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the user's instructions, addressing the task of building a deep learning model for anomaly detection in cryptocurrency trades, integrating explainable AI and optimization for edge devices. The performance and complexity metrics are included, as well as the requirement for a custom visualization dashboard. Some specific metric values are not provided, but the intent and focus on resource efficiency and regulatory compliance are clearly captured.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager is tasked with developing a multi-faceted legal document analysis system, integrating an enhanced DeBERTa model fine-tuned exclusively for the LegalQA dataset. The model must not only demonstrate high precision and understanding of legal terminologies but also offer detailed traceability, handle sensitive information with GDPR-compliant practices, and undergo extensive explainability testing to ensure the reliability and ethical decision-making of its responses.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"legal\",\n        \"description\": \"Develop a multi-faceted legal document analysis system using an enhanced DeBERTa model fine-tuned for the LegalQA dataset. The model should have high precision, understanding of legal terminologies, detailed traceability, GDPR-compliant handling of sensitive information, and undergo explainability testing for reliable and ethical decision-making.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"understanding of legal terminologies\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \" GDPR compliance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"explainability testing\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LegalQA dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"fine-tuned DeBERTa\",\n                \"domain\": \"legal\"\n            },\n            \"description\": \"A dataset specifically designed for legal question answering, containing text of legal documents for the model's training and fine-tuning.\",\n            \"preprocessing\": [\" GDPR compliant data handling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced DeBERTa\",\n            \"family\": \"transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"specifics\": \"Fine-tuned for LegalQA with GDPR-compliant handling\",\n                \"explainability\": \"facilitating transparency\"\n            },\n            \"description\": \"A DeBERTa model enhanced for legal document analysis, demonstrating precision and understanding of legal terminologies while ensuring GDPR compliance and explainability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\" GDPR-compliant best practices\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \" traceability, response explainability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the user's instruction, detailing the requirement of a multi-faceted legal document analysis system. The DeBERTa model is specified to be fine-tuned for LegalQA, focusing on precision and legal terminology understanding. GDPR compliance and explainability are explicitly mentioned as critical factors, along with requirements for privacy and transparency. However, there is room for improvement with a more specific value or metric for GDPR compliance, precision, and explainability.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for the machine learning team, your objective is to enhance the Concrete Strength Dataset regression challenge. In addition to developing a model with excellent RMSE, ensure the model demonstrates interpretability by integrating feature importance analysis and must handle multicollinearity effectively using regularization techniques, all while maintaining a streamlined and efficient codebase.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"civil engineering\",\n        \"description\": \"Enhance the Concrete Strength Dataset regression challenge by developing a model with excellent RMSE, ensuring interpretability through feature importance analysis, and handling multicollinearity using regularization techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"code efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Concrete Strength Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"Concrete compressive strength\"],\n            \"specification\": {\n                \"dimensions\": {},\n                \"features\": {}\n            },\n            \"description\": \"A dataset for regression task on concrete strength prediction, including features to address multicollinearity.\",\n            \"preprocessing\": [\"handling multicollinearity\"],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretable\": true,\n                \"regularization\": true\n            },\n            \"description\": \"A regression model for the Concrete Strength Dataset with interpretability and regularization methods, to handle multicollinearity effectively.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"preprocessing\", \"regularization\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"feature interpretability and model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the project manager's goal to enhance a regression challenge with clear objectives of improving RMSE, focusing on interpretability, and tackling multicollinearity. The problem area is specified, target task is mentioned, dataset and model properties are derived from the instruction, and performance and efficiency metrics are set. However, specific model name and family, dimensions, and feature analysis details are not provided due to potential variations in dataset and model choices.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the upcoming multifaceted marketing initiative, we aim to enhance customer sentiment analysis by differentiating not only individuals but also their contextual surroundings in diverse promotional images. Develop a sophisticated image segmentation algorithm that can parse multiple layers of information and identify key interaction zones, ensuring a nuanced understanding of audience engagement.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"marketing\",\n        \"description\": \"Enhance customer sentiment analysis by developing a sophisticated image segmentation algorithm that differentiates individuals and their contextual surroundings in promotional images, focusing on identifying key interaction zones for a nuanced understanding of audience engagement.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"multifaceted marketing images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": \"[variable, variable, variable]\",\n                \"color_depth\": \"RGB\",\n                \"content_types\": [\"promotional\", \"contextual\"]\n            },\n            \"description\": \"A collection of promotional images containing diverse contextual surroundings for algorithm training and evaluation.\",\n            \"preprocessing\": [\n                \"resize to uniform dimensions\",\n                \"normalize color channels\"\n            ],\n            \"augmentation\": [\n                \"context-aware data augmentation\"\n            ],\n            \"visualization\": [\n                \"examples of successfully parsed interaction zones\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-layer convolutional neural network with deep supervision\",\n                \"methods\": [\"attention mechanism\", \"context-aware features\"]\n            },\n            \"description\": \"A deep learning algorithm specifically designed for image segmentation, capable of parsing multiple layers and extracting context from promotional images.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Include domain-specific datasets for improved performance in various promotional scenarios\",\n        \"Consider using transfer learning to leverage pre-trained models\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-based feature extraction\"],\n        \"target_device\": \"cloud-based server for efficient computation\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time image processing system\"],\n        \"demonstration\": \"generation of performance visualizations during the deployment phase\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's request to build an image segmentation algorithm for sentiment analysis in marketing. It includes details on the area, task, performance and complexity metrics, dataset requirements, and model specifications. The algorithm's sophistication and the context-awareness are explicitly mentioned. The user's high expertise level justifies the need for a complex solution. Some areas like deployment endpoint and specific performance values could be left out, as they are not provided in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a specialist in fine-grained feline image recognition, I need a machine learning model that can distinguish among 50 distinct cat breeds present in my comprehensive dataset of high-resolution photographs. Can you recommend an optimized model and training protocol for this task?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal recognition\",\n        \"description\": \"The user needs a machine learning model for fine-grained feline image recognition with a focus on distinguishing among 50 distinct cat breeds, based on a comprehensive dataset of high-resolution photographs.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score (micro-averaged)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Comprehensive cat breed dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\"],\n            \"specification\": {\n                \"resolution\": \"high\",\n                \"image_format\": [\"JPEG\", \"PNG\"]\n            },\n            \"description\": \"A large dataset containing high-resolution photographs of 50 distinct cat breeds for fine-grained classification.\",\n            \"preprocessing\": [\n                \"Data augmentation (e.g., rotation, resizing, flipping)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specific for fine-grained image recognition\",\n                \"depth\": null,\n                \"pretrained\": false\n            },\n            \"description\": \"A tailored CNN model for fine-grained feline image recognition, optimized to distinguish among 50 cat breeds.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Due to the complexity and number of classes, transfer learning from a similar large-scale dataset or a pre-trained model in the wild could enhance performance.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Interactive demonstration of breed classification on sample images\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essential elements from the instruction, with a focus on model development, including recommendations for a high-expertise user. Performance metrics are not specified since the user isn't giving specific values, and the lack of model specifications still allows for optimization. The knowledge piece suggests possible approaches to enhance performance. However, more concrete details like exact model architecture, feature engineering steps, and deployment specifics are not provided due to the nature of the 'recommendation' aspect of the user's request.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a cutting-edge time series forecasting model for our app's DAU by integrating a hybrid approach of deep ARIMA and LSTM neural networks. The model should adapt to real-time data anomalies, explicitly account for weekly and quarterly patterns, and evaluate performance using leave-one-month-out cross-validation to ensure robustness against seasonality.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"mobile app\",\n        \"description\": \"Develop a cutting-edge time series forecasting model for daily active users (DAU) using a hybrid approach of deep ARIMA and LSTM neural networks. The model should be adaptable to real-time data anomalies, explicitly consider weekly and quarterly patterns, and use a leave-one-month-out cross-validation for robustness against seasonality.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAU data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"DAU\"],\n            \"specification\": {\n                \"data_frequency\": \"daily\",\n                \"time_period\": \"historic\"\n            },\n            \"description\": \"Dataset containing historical daily active users data for the app, including anomalies and seasonal patterns.\",\n            \"preprocessing\": [\"data cleaning\", \"handling anomalies\"],\n            \"augmentation\": [],\n            \"visualization\": [\"anomaly detection\", \"seasonality analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep ARIMA-LSTM Hybrid\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"ARIMA components\": \"\",\n                    \"LSTM layers\": \"\"\n                },\n                \"hyperparameters\": {\n                    \"adaptivity\": \"real-time\",\n                    \"seasonality components\": [\"weekly\", \"quarterly\"]\n                }\n            },\n            \"description\": \"A hybrid model combining deep ARIMA and LSTM networks for time-series forecasting, targeting DAU with real-time anomaly handling and consideration of weekly and quarterly patterns.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time anomaly detection\",\n        \"Leave-one-month-out cross-validation strategy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"rolling window feature extraction\",\n            \"ensemble feature combinations\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"app-server API\",\n        \"inference_engine\": [\n            \"TensorFlow serving\",\n            \"Apache Kafka for streaming data\"\n        ],\n        \"demonstration\": \"Real-time DAU forecasts and visualization of model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's instruction to build a cutting-edge time series forecasting model for DAU, integrating ARIMA and LSTM with real-time data handling and leave-one-month-out cross-validation. Performance metrics and complexity metrics are included, and dataset and model specifications are detailed based on the requirements. The user's high expertise level suggests their familiarity with these complexities.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "community detection", "instruction": "As a project manager for our machine learning team, we need to design a scalable community detection algorithm for a diverse customer base. In addition to identifying clusters, require near real-time updates, and analyze the effect of socio-demographic factors on service preferences. Can you propose a method that balances computational efficiency with high accuracy?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"social network analysis\",\n        \"description\": \"Design a scalable community detection algorithm for a diverse customer base with near real-time updates. The algorithm should consider the impact of socio-demographic factors on service preferences and strive for a balance between computational efficiency and high accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time updates speed\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"customer network data\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"density\": null\n            },\n            \"description\": \"A graph representing customer relationships and socio-demographic data.\",\n            \"preprocessing\": [\n                \"normalize data\",\n                \"handle missing values\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Scalable Community Detection Algorithm\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"algorithm\": \"Heterogeneous Graph Neural Networks with adaptive updates\"\n            },\n            \"description\": \"A scalable algorithm that integrates GNNs with real-time update mechanisms and socio-demographic features for improved community detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporate community detection techniques such as GAE, GAT, and FastGCN to balance efficiency and accuracy\",\n        \"Use incremental learning or streaming graph algorithms for near real-time updates\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Socio-demographic feature extraction and fusion\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"custom deployment service\",\n        \"inference_engine\": [\"Apache Spark\", \"TensorFlow serving\"],\n        \"demonstration\": \"Visual dashboards displaying real-time cluster updates and analysis results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the user's request for a scalable community detection algorithm. We assumed the user has high expertise in AI and ML to propose a solution. The performance metrics (accuracy, real-time updates), complexity constraints (computational time and memory), and dataset requirements are included. Model family and features are proposed, along with relevant knowledge, service, and deployment aspects. However, values for some metrics and specific algorithms are left as null to indicate the need for further discussion or the user's preference.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for a specialized machine learning team, you're faced with a real-world challenge involving the ETTm2 time-series dataset. The train, validation, and test splits consist of intricate, non-linear patterns spanning 96 timesteps (INPUT_SEQ_LEN=96, INPUT_DIM=7). Your task is to develop a cutting-edge forecasting model that not only predicts the subsequent 96 steps (PRED_SEQ_LEN=96, PRED_DIM=7) but also accounts for seasonality and outliers. Strive for excellence in both mean squared error (MSE) and mean absolute error (MAE) metrics, while ensuring model interpretability and generalization across diverse scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"business or predictive analytics\",\n        \"description\": \"Develop a cutting-edge forecasting model using the ETTm2 time-series dataset with intricate, non-linear patterns over 96 timesteps. The model should predict the next 96 steps while handling seasonality and outliers. Focus on minimizing MSE and MAE, and ensure model interpretability and generalization across various scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Squared Error (MSE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Error (MAE)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model interpretability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"generalization across scenarios\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ETTm2\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 7\n            },\n            \"description\": \"A time-series dataset containing complex, non-linear patterns over 96 timesteps for forecasting the subsequent 96 steps.\",\n            \"preprocessing\": [\"account for seasonality\", \"outlier detection and handling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks (ideally deep learning or transformer-based for non-linearity and interpretability)\",\n            \"specification\": {\n                \"model_structure\": null,\n                \"explanation\": \"Model with interpretable components and designed for time-series forecasting\"\n            },\n            \"description\": \"A state-of-the-art, interpretable forecasting model for handling ETTm2 dataset with emphasis on seasonality, outliers, MSE, and MAE.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"visualizations and explanations of model's decision-making process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the given instruction, highlighting the task as a machine learning project to develop a model for time-series forecasting on the ETTm2 dataset with focus on interpretability and generalization. Performance metrics are mentioned as targets without specific values (null), as required. Preprocessing steps, such as handling seasonality and outliers, are included. Model selection indicates a preference for deep learning or transformer models while emphasizing interpretability.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "For a sophisticated text classification challenge, apply the DAIGT dataset to distinguish between AI-generated (label 1) and human-written (label 0) texts, requiring not only high accuracy but also robustness to adversarial examples and interpretability of model decisions. Utilize advanced techniques like ensemble learning and perform extensive hyperparameter tuning with explainable AI methods.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"artificial intelligence and cybersecurity\",\n        \"description\": \"Address a sophisticated text classification challenge using the DAIGT dataset to classify AI-generated texts (label 1) from human-written (label 0). Focus on achieving high accuracy, robustness to adversarial examples, and model interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness to adversarial examples\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DAIGT dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"AI-generated or human-written\"],\n            \"specification\": {\n                \"size\": null,\n                \"vocab_size\": null,\n                \"document_length\": null,\n                \"labels_distribution\": null\n            },\n            \"description\": \"A dataset designed for text classification, differentiating AI-generated and human-written texts.\",\n            \"preprocessing\": [\"tokenization\", \"text cleaning\"],\n            \"augmentation\": [\"synonym replacement\", \"back translation\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_models\": [\"random forest\", \"gradient boosting\", \"convolutional neural networks\"],\n                \"hyperparameters\": {\n                    \"exploration\": \"Bayesian optimization\"\n                }\n            },\n            \"description\": \"An ensemble model with advanced neural networks using explainable AI techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adapt ensemble models as they often demonstrate improved performance and robustness to adversarial examples.\",\n        \"Employ explainable AI methods like LIME or SHAP to interpret model decisions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow\", \"Scikit-learn\"],\n        \"demonstration\": \"visualize and interpretability reports of model performance and decision-making process\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's instruction, focusing on a text classification project with the DAIGT dataset. The user's high expertise level suggests they expect a more in-depth and complex solution. The ensemble learning and explainable AI requirements are captured in the model section. Performance metrics are identified, though specific values are not provided. The user's requirements for high accuracy, robustness, and interpretability are addressed. A few fields lack details (such as specific models and performance metrics) due to the open-ended nature of the ensemble and hyperparameter tuning process.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "For a fine-grained author classification project on the specialized Coauthorship Science dataset using a Graph Convolutional Hierarchical Network (GC-HGN), design an efficient model that differentiates between 50 niche research disciplines by leveraging unique paper-topology features and citation patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Design an efficient model for a fine-grained author classification project on the Coauthorship Science dataset using Graph Convolutional Hierarchical Network (GC-HGN), differentiating between 50 niche research disciplines based on paper-topology features and citation patterns.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coauthorship Science\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"research discipline\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"total\": null,\n                    \"size\": null\n                },\n                \"edges\": {\n                    \"total\": null,\n                    \"density\": null\n                }\n            },\n            \"description\": \"A dataset for fine-grained author classification with unique paper-topology features and citation patterns.\",\n            \"preprocessing\": [\"handling hierarchical structure\"],\n            \"augmentation\": [],\n            \"visualization\": [\"network visualization\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GC-HGN\",\n            \"family\": \"Graph Convolutional Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"层数\": null,\n                \"隐藏单元数\": null,\n                \"卷积操作\": \"Hierarchical Graph Convolution\"\n            },\n            \"description\": \"An efficient Graph Convolutional Hierarchical Network (GC-HGN) model designed for author classification across 50 niche research disciplines on the Coauthorship Science dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"data_availability\": \"Limited due to specialization and niche research disciplines\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"handling sparse data\", \"extracting topology features\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient inference on large graph structures\"],\n        \"demonstration\": \"model performance on representative validation samples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response parses the given instruction into a structured format. The user's high expertise level and the detailed requirements for the model, performance metrics, and complexity metrics demonstrate a clear understanding of the project. Some metrics are left unspecified (such as exact accuracy or inference time) to allow flexibility and cater to the project's requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "For the advanced urban resilience initiative, develop a state-of-the-art image classifier using the challenging and diverse Sentinel-2A+ dataset. The model should employ a fine-tuned ViT-Mixer hybrid, ensuring high accuracy (96%) in detecting not only traditional land use but also recognizing eco-sensitive zones and seasonal changes. Additionally, the model must be optimized for real-time edge deployment, with robustness guarantees and TPU integration, and include extensive, interactive visualizations in the performance report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"urban resilience\",\n        \"description\": \"Develop a state-of-the-art image classifier for an advanced urban resilience initiative using the Sentinel-2A+ dataset. The model should focus on fine-tuning a ViT-Mixer hybrid, targeting high accuracy (96%) in detecting traditional land use, eco-sensitive zones, and seasonal changes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.96\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"TPU integration\",\n                \"value\": true\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2A+\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"land use\", \"eco-sensitive zones\", \"seasonal changes\"],\n            \"specification\": null,\n            \"description\": \"A diverse and challenging dataset for urban resilience, capturing land use, eco-sensitive areas, and seasonal changes using Sentinel-2A+ satellite imagery.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"interactive performance report\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Fine-tuned ViT-Mixer hybrid\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"accuracy\": 0.96,\n                \"optimization\": \"real-time edge deployment\",\n                \"TPU integration\": true\n            },\n            \"description\": \"A state-of-the-art image classifier that combines ViT and Mixer architectures, fine-tuned for high accuracy and real-world urban resilience use case.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge device\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TPU\"],\n        \"demonstration\": \"interactive and detailed\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the user's advanced-level expertise and their specific requirements for an image classification model. The instruction highlights accuracy, real-time deployment, TPU integration, and interactive visualizations, which are reflected in the complexity metrics and model specifications. However, some placeholders for metric values (e.g., inference time) exist to indicate the need for further detail to fully address the user's instructions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As the project manager for our elite AI research team, we are tasked with developing a state-of-the-art BERT-based question-answering model using RoBERTa. The model must excel on SQuAD 2.0, targeting an impressive F1 score of 95% or higher. However, not only must it surpass benchmark accuracy, but also demonstrate efficient parallel processing, aiming to provide answers within a tight 1.5-second latency while maintaining neural network complexity at a minimum.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"AI research\",\n        \"description\": \"Develop a state-of-the-art BERT-based question-answering model using RoBERTa, targeting SQuAD 2.0 with an F1 score of 95% or higher.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 1.5,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"neural network complexity\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"answers\"],\n            \"specification\": null,\n            \"description\": \"A question-answering dataset for benchmarking the model on SQuAD 2.0.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"RoBERTa\",\n                \"accuracy_target\": \"SQuAD 2.0\",\n                \"complexity_tradeoff\": \"minimum\"\n            },\n            \"description\": \"A state-of-the-art BERT-based question-answering model using RoBERTa, optimized for high F1 score and parallel processing efficiency.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"parallel processing\"\n        ],\n        \"demonstration\": \"immediate and efficient\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the user's instruction, indicating a build project for an advanced NLP model, specifically targeting BERT-RoBERTa, with SQuAD 2.0 as the dataset and performance metric of F1 score. The task emphasizes on efficient parallel processing and minimizing neural network complexity. The model description includes target values specified in the task, and the confidence score reflects the high level of detail provided, considering the user's 'high' expertise level.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Sure, your project manager needs a detailed request for developing a state-of-the-art image segmentation model capable of distinguishing various breeds of cats in complex real-world scenarios, while adhering to strict latency and accuracy standards.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"image recognition\",\n        \"description\": \"Develop a state-of-the-art image segmentation model for distinguishing various breeds of cats in complex real-world scenarios. The model must meet high accuracy standards while adhering to strict latency requirements.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cat Breed dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed classification\"],\n            \"specification\": {\n                \"image_size\": \"variable\",\n                \"real-world complexity\": \"high\"\n            },\n            \"description\": \"A dataset consisting of complex, real-world images of cats for training and evaluation, covering multiple breeds.\",\n            \"preprocessing\": [\"data augmentation\", \"image normalization\"],\n            \"augmentation\": [\"random cropping\", \"Gaussian blur\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art (e.g., U-Net, Mask R-CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"latest advances\",\n                \"accuracy benchmark\": \"high\",\n                \"latency optimization techniques\": \"applied\"\n            },\n            \"description\": \"A highly-accurate, real-time image segmentation model specifically designed for recognizing various cat breeds in challenging conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adherence to real-world scenarios is crucial for model generalization.\",\n        \"Latency optimization is essential for timely processing in practical applications.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"contextual information\"],\n        \"target_device\": \"CPU-optimized for latency, GPU for high accuracy\",\n        \"deployment_endpoint\": \"potentially, a cloud or edge platform\",\n        \"inference_engine\": [\"real-time inference framework\"],\n        \"demonstration\": \"Presentation of model performance on sample images and benchmarking against latency constraints\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's requirements for a complex computer vision task, with a focus on accuracy, latency, and complex real-world scenarios. It covers the essential aspects of model development, dataset selection, and performance evaluation. However, it is assumed that the model architecture and specific accuracy values will be determined based on the state-of-the-art. Missing details might be added after conducting further research or consultation.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image segmentation", "instruction": "As a project manager for our avant-garde fashion tech team, we seek to create an innovative DeepFashion2-enhanced fashion item segmentation tool. Employ a cutting-edge EfficientNet-B7 variant tailored for an advanced FCN model, ensuring sub-pixel accuracy in differentiating intricate overlapping garments. The tool must meet a stringent 90%+ precision, maintain real-time performance on mobile devices, and comply with stringent energy efficiency guidelines.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"fashion tech\",\n        \"description\": \"As a project manager, aim to develop a DeepFashion2-enhanced fashion item segmentation tool using an advanced EfficientNet-B7 variant in a FCN model. The tool should have sub-pixel accuracy for distinguishing intricate overlapping garments with a precision target of 90%+, while maintaining real-time performance on mobile devices and adhering to strict energy efficiency guidelines.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"real-time on mobile devices\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepFashion2-enhanced data\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"garment segments\"],\n            \"specification\": null,\n            \"description\": \"A fashion dataset containing images with segmented garments, focusing on complex overlapping garments for the EfficientNet-B7 FCN model.\",\n            \"preprocessing\": [\"DeepFashion2 data preprocessing\"],\n            \"augmentation\": [\"Advanced image augmentation techniques\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet-B7 FCN\",\n            \"family\": \"EfficientNet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"sub-pixel\",\n                \"architecture\": \"FCN\"\n            },\n            \"description\": \"A cutting-edge image segmentation model using EfficientNet-B7 tailored for fashion item segmentation, ensuring high accuracy in differentiating complex garment overlap.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Experience with DeepFashion2 and efficient model design for mobile devices.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Image feature extraction for DeepFashion2\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Optimized for real-time mobile performance\"],\n        \"demonstration\": \"Demonstrate tool performance on varied mobile devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's request for developing a cutting-edge image segmentation tool with specific precision, real-time mobile performance, and energy efficiency requirements. The project manager's expertise level suggests a deep understanding of the constraints. The model and dataset details are provided based on the user's request, and the performance metrics and complexity requirements are set accordingly.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "In the context of the niche \"Aeroacoustics Anomaly Detection,\" design a cutting-edge time-series classification model that employs the short-time Fourier transform (STFT) and a novel hierarchical Temporal Convolutional Network (HTCN) architecture. The model should analyze aerodynamic noise from aircraft wing vibrations while accounting for turbulence and variable wind conditions. Performance should be evaluated using precision, recall, and DET curve analysis, with a focus on real-time processing efficiency and robustness under high-speed data fluctuations. Additionally, propose a metric to measure the model's ability to distinguish subtle differences between subsonic and supersonic turbulence patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"aeroacoustics\",\n        \"description\": \"Design a cutting-edge model for anomaly detection in aerodynamic noise from aircraft wing vibrations, considering turbulence and variable wind conditions. Employ STFT and a novel HTCN architecture, focusing on real-time processing efficiency and robustness.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"DET curve analysis\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time processing efficiency\",\n                \"value\": null,\n                \"unit\": \"fps\"\n            },\n            {\n                \"name\": \"robustness to high-speed data fluctuations\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Aeroacoustic noise data with wing vibrations\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"anomaly label\", \"subsonic vs. supersonic turbulence\"],\n            \"specification\": {\n                \"sample_rate\": null,\n                \"length\": null\n            },\n            \"description\": \"Dataset capturing aerodynamic noise data from aircraft wing vibrations, including turbulence and variable wind conditions.\",\n            \"preprocessing\": [\"STFT conversion\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"HTCN (Hierarchical Temporal Convolutional Network)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"STFT + HTCN\",\n                \"real-time inference\": null\n            },\n            \"description\": \"A novel time-series classification model using STFT and a hierarchical Temporal Convolutional Network designed for anomaly detection in aerodynamics, considering turbulence and variable wind conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"For subtle difference between subsonic and supersonic turbulence, a metric that measures the model's discriminatory power between those patterns could be devised by fine-tuning on subsonic-supersonic mixed data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time inference engine\"\n        ],\n        \"demonstration\": \"Real-time processing and anomaly visualization of wing vibration data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the user's intent to build a model with high expertise level, and the focus on aerodynamic noise, STFT, and HTCN architecture. Performance metrics include precision, recall, DET curve, and a custom metric for subtle difference detection. The dataset and model specifications are detailed based on the given requirements. Some metric values are left unspecified due to the lack of concrete numbers, but the focus on real-time processing and robustness is noted.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a machine translation project manager, request a highly optimized transformer-based model, specifically an M-BART, leveraging the updated Europarl v7.0 dataset with domain-specific fine-tuning for Spanish-English translations. Emphasize near-native accuracy and real-time, latency-sensitive performance, targeting sub-200 milliseconds response times in a low-latency, voice-controlled chat application for international business meetings.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"international business\",\n        \"description\": \"Request a highly optimized transformer-based model, specifically an M-BART, for Spanish-English translations. Focus on near-native accuracy and real-time, latency-sensitive performance for a low-latency voice-controlled chat application in international business meetings using the Europarl v7.0 dataset with domain-specific fine-tuning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"near-native accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean response time\",\n                \"value\": {\n                    \"constraint\": \"sub-200 milliseconds\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference latency\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"constraint\": \"less than 200\"\n                }\n            },\n            \"real-time performance\",\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Europarl v7.0\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translations from Spanish to English\"],\n            \"specification\": {\n                \"dataset size\": \"large\",\n                \"domain\": \"Spanish-English\"\n            },\n            \"description\": \"A multilingual dataset updated to v7.0 for Spanish-English translation tasks with a focus on business domain.\",\n            \"preprocessing\": [\"domain adaptation for business meetings\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M-BART\",\n            \"family\": \"transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine-tuning\": \"domain-specific for Spanish-English\",\n                \"latency requirements\": \"real-time\"\n            },\n            \"description\": \"An M-BART transformer model optimized for near-native accuracy, targeting a low-latency voice-controlled chat application with sub-200ms response times.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"low-latency environment\",\n        \"deployment_endpoint\": {\n            \"type\": \"voice-controlled chat application\"\n        },\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"sub-200ms response times for Spanish-English translations in voice-controlled chat\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's intent to build a highly optimized M-BART model with domain-specific fine-tuning, targeting near-native accuracy and real-time performance for a low-latency chat application. The dataset and performance metrics are specified based on the requirements of the Spanish-English translation task and the voice-controlled environment. Some open-ended metrics, such as exact BLEU score, are left to the service's discretion to ensure the model meets these real-world constraints.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As a project manager, your request for the time-series anomaly detection task involves evaluating 250 diverse Hexagon ML/UCR datasets using a resource-efficient, lightweight transformer model. The model must outperform with an average accuracy of 75%, and also demonstrate exceptional interpretability in identifying patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"data analysis, possibly in a manufacturing or IoT environment\",\n        \"description\": \"Evaluate 250 diverse Hexagon ML/UCR time-series datasets using a resource-efficient, lightweight transformer model. The model should achieve an average accuracy of 75% and demonstrate exceptional interpretability for pattern identification.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"average accuracy\",\n                \"value\": 0.75\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"minutes per dataset\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hexagon ML/UCR time-series datasets\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"dataset_count\": 250,\n                \"diversity\": \"diverse\"\n            },\n            \"description\": \"A large collection of diverse time-series datasets from Hexagon ML/UCR for anomaly detection evaluation.\",\n            \"preprocessing\": [\"resource-efficient\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                {\n                    \"type\": \"interpretability analysis\",\n                    \"description\": \"Visual interpretability for identifying patterns\"\n                }\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Lightweight Transformer\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"resource_efficiency\": \"true\",\n                \"interpretability\": \"high\"\n            },\n            \"description\": \"A resource-efficient and interpretable lightweight transformer model for time-series anomaly detection.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Visual explanations of identified patterns\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response reflects the user's request for a time-series anomaly detection project, targeting high accuracy, resource efficiency, and interpretability. It specifies an average accuracy goal, and accounts for dataset diversity, resource-efficient preprocessing, and interpretability in visualizations. The model and service sections are based on the requirements of evaluating multiple datasets, efficiency, and interpretability. However, specific values for complexity metrics and deployment details are not provided, as the instruction doesn't specify strict constraints.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a machine learning project manager, develop a sophisticated tabular regression model that not only forecasts budget and duration but also accounts for project complexity, resource allocation, and external market factors, ensuring 95% accuracy for high-impact projects.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"project management\",\n        \"description\": \"Develop a sophisticated tabular regression model to forecast budget, duration, and project complexity, considering resource allocation and external market factors. The model should achieve at least 95% accuracy for high-impact projects.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"project management dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"budget\", \"duration\", \"project_complexity\"],\n            \"specification\": {\n                \"variables\": [\"project complexity\", \"resource allocation\", \"external market factors\"],\n                \"quality\": \"high-impact\"\n            },\n            \"description\": \"A dataset containing project characteristics and historical data for budget, duration, and project impact to build the regression model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced tabular regression\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Accounts for project complexity, resource allocation, and external factors\",\n                \"target_metric\": \"95% accuracy\"\n            },\n            \"description\": \"A sophisticated tabular regression model that integrates complex features for predicting budget, duration, and project complexity in high-impact projects.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"cloud-based\"],\n        \"demonstration\": \"Performance and interpretability dashboard for high-impact project predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's intent to build a sophisticated tabular regression model, considering project management characteristics and external factors. The expert level indicates the user's familiarity with the advanced requirements. Performance metrics have been set to meet the accuracy goal, and some complexity metrics are mentioned for optimization. The source of the dataset and model deployment method are specified. Missing information, such as specific feature engineering techniques, are assumed to be part of the model development process.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager, you're tasked with developing an advanced AI-based sports analytics platform. The team must enhance the existing RetinaNet model, leveraging the 'Expanded Sports-1M' dataset with millions of diverse sport sequences. The customization should optimize object detection for real-time sports events, prioritize low latency (<100ms), and demonstrate exceptional performance in recognizing equipment and crucial player movements in high-resolution, high-speed broadcasts. Additionally, the project should include a thorough evaluation of model efficiency under various lighting and weather conditions, and the team must propose strategies for continuous learning to adapt to new sports or equipment types.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"sports analytics\",\n        \"description\": \"As a project manager, task is to develop an advanced AI-based sports analytics platform, focusing on enhancing the RetinaNet model with the 'Expanded Sports-1M' dataset. The goal is to optimize real-time object detection for sports events, prioritize low latency (<100ms), and excel in recognizing equipment and player movements in high-resolution, high-speed broadcasts. The platform must also evaluate model efficiency under varying lighting and weather conditions and propose strategies for continuous learning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 100\n            },\n            {\n                \"name\": \"Object Detection Accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Expanded Sports-1M\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"millions\",\n                \"diversity\": \"diverse sport sequences\",\n                \"resolution\": \"high-resolution\",\n                \"speed\": \"high-speed\"\n            },\n            \"description\": \"Dataset for enhancing the RetinaNet model, containing large-scale, diverse sport sequences to optimize object detection for real-time sports events\",\n            \"preprocessing\": [\"low-latency image preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [\"performance analysis under varying conditions\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced RetinaNet\",\n            \"family\": \"neural networks\",\n            \"type\": \"object detection\",\n            \"specification\": {\n                \"latency_goal\": \"100ms\",\n                \"new_features\": \"optimized for sports equipment and player movements\",\n                \"adaptability\": \"continuous learning for new sports or equipment types\"\n            },\n            \"description\": \"The customized RetinaNet model tailored for real-time sports analytics with low latency and improved equipment and player movement recognition\"\n        }\n    ],\n    \"knowledge\": [\n        \"The importance of adaptability and resilience to varying lighting and weather conditions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time sports events\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for low latency\"],\n        \"demonstration\": \"live event demonstration with model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the main aspects of the project, focusing on enhancing the RetinaNet model, data source, performance requirements (latency, detection accuracy), and the evaluation of efficiency in specific scenarios. It accounts for key areas like low latency, high-resolution inputs, and continuous learning. The missing information, such as specific model accuracy or the deployment endpoint, is due to the lack of such details in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Incorporate advanced neuro-symbolic AI to develop a real-time visual question answering system that can decipher ancient inscriptions on tablets and artifacts discovered at archeological sites, presenting historical context and details to researchers with sub-pixel precision, while adhering to ethical guidelines for cultural heritage preservation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"historical research\",\n        \"description\": \"Design a real-time visual question answering system using advanced neuro-symbolic AI for deciphering ancient inscriptions on archaeological tablets and artifacts with sub-pixel precision.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Speed (real-time)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Sub-pixel precision\",\n                \"value\": 1\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Archaeological Inscriptions\",\n            \"modality\": [\"image\", \"text\"],\n            \"target_variables\": [\"ancient inscriptions\", \"historical context\"],\n            \"specification\": {\n                \"image_size\": \"sub-pixel\",\n                \"text_length\": null\n            },\n            \"description\": \"A dataset containing images of artifacts and inscriptions with associated text for training the visual question answering system.\",\n            \"preprocessing\": [\"image segmentation\", \"text preprocessing\"],\n            \"augmentation\": [\"sub-pixel level image transformations\"],\n            \"visualization\": [\"training progress monitoring\"],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Neuro-Symbolic Visual Question Answering\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"advanced neuro-symbolic fusion\",\n                \"symbolic_reasoning\": true,\n                \"neural_encoder\": {\n                    \"type\": \"CNN\"\n                },\n                \"neural_reasoner\": {\n                    \"type\": \"transformer\"\n                }\n            },\n            \"description\": \"A system combining deep learning and symbolic reasoning for real-time VQA on ancient inscriptions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adherence to cultural heritage preservation ethics\",\n        \"Expert-level knowledge in neuro-symbolic AI and its application to archeological contexts\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"contextual feature extraction\", \"question representation\"],\n        \"target_device\": \"cloud-based for real-time processing\",\n        \"deployment_endpoint\": \"archeological research platform or specialized API\",\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"Interactive demo showcasing on real archaeological data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the core elements of the user's instruction, including the requirement for advanced neuro-symbolic AI, real-time performance, and ethical considerations. Performance metrics are set with placeholders for accuracy and speed, reflecting the need for further clarification. The dataset and model components are designed to align with the specific task and context of deciphering ancient inscriptions. Some details like exact model architecture or deployment details are yet to be specified, leaving room for the user to provide additional information.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a machine learning project manager, I need you to design a cutting-edge time-series forecasting system using the prophet forecast ensemble technique. The task involves analyzing the intricate patterns in the amalgamation of daily solar radiation, urban heat island intensity, and atmospheric CO2 levels for a global network of metropolises. The project should:\r\n\r\n1. Develop a hybrid model, integrating Prophet and a deep convolutional neural network (CNN), to account for short-term and long-term trends in the data.\r\n2. Implement an anomaly detection mechanism to identify and handle outliers, ensuring robustness against extreme weather events.\r\n3. Integrate a modular real-time learning framework that automatically updates forecasts as new meteorological data and satellite imagery become available.\r\n4. Design a user-friendly dashboard for non-technical stakeholders to visualize predicted temperatures, energy consumption, and public cooling infrastructure needs, with interactive explanations of the influencing factors.\r\n5. Perform extensive backtesting and cross-validation, comparing the ensemble model's performance against state-of-the-art baselines like LSTM and XGBoost on unseen data.\r\n6. Present a detailed report highlighting model interpretability, uncertainty quantification, and any biases introduced by the incorporation of external variables.\r\n\r\nYour goal is to deliver a high-accuracy, adaptable, and user-friendly solution that anticipates urban heat behavior under evolving environmental conditions. Ensure the project is scalable, robust, and able to deliver actionable insights for energy and urban planning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy and urban planning\",\n        \"description\": \"Design a cutting-edge time-series forecasting system using Prophet forecast ensemble with a deep convolutional neural network (CNN) for analyzing solar radiation, urban heat island intensity, and atmospheric CO2 levels in global metropolises. The project needs to handle anomalies, real-time learning, user-friendly dashboard, extensive backtesting, and model interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time learning\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory scalability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"GPU usage\",\n                \"value\": null,\n                \"unit\": \"%\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"daily solar radiation data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"solar radiation\"],\n            \"specification\": null,\n            \"description\": \"Daily data on solar radiation for global metropolises.\",\n            \"preprocessing\": [\"anomaly detection\"],\n            \"augmentation\": [],\n            \"visualization\": [\"visualizing trends\"],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"urban heat island intensity data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"urban heat island intensity\"],\n            \"specification\": null,\n            \"description\": \"Daily data on urban heat island intensity for global cities.\",\n            \"preprocessing\": [\"anomaly detection\"],\n            \"augmentation\": [],\n            \"visualization\": [\"visualizing patterns\"],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"CO2 levels data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"atmospheric CO2 levels\"],\n            \"specification\": null,\n            \"description\": \"Time series data on atmospheric CO2 levels for global cities.\",\n            \"preprocessing\": [\"anomaly detection\"],\n            \"augmentation\": [],\n            \"visualization\": [\"trend analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Prophet-CNN ensemble\",\n            \"family\": \"time-series models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"short-term horizon\": null,\n                \"long-term horizon\": null,\n                \"CNN layers\": null\n            },\n            \"description\": \"A hybrid model integrating Prophet and a deep CNN for time-series forecasting with short- and long-term trend considerations.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Incorporate external meteorological data and satellite imagery for real-time learning.\"\n        },\n        {\n            \"content\": \"Design an interpretable model with uncertainty quantification and handle biases introduced by external variables.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time data processing\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"dedicated data platform\",\n        \"inference_engine\": [\"TensorFlow Serving\"],\n        \"demonstration\": \"Interactive dashboard with visual explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's requirements for a time-series forecasting system integrating Prophet and CNN, addressing anomaly detection, real-time learning, dashboard design, and model evaluation. The user's high expertise level justifies a high level of detail. The structure corresponds to the given schema, ensuring the response is valid and actionable.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "Create a highly optimized DTW-based model for the Sign Language Time Series dataset, integrating real-time adaptive segmentation and noise filtering. The model must exhibit exceptional precision and recall even in recognizing nuanced hand movements, with a stringent requirement of <=5% classification error for hearing-impaired users in low-light conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"sign language recognition\",\n        \"description\": \"Create an optimized DTW-based model for the Sign Language Time Series dataset, incorporating real-time adaptive segmentation and noise filtering. Target is to achieve high precision and recall, with a classification error <=5% for hearing-impaired users in low-light conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"classification error\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"noise filtering efficiency\",\n                \"value\": null,\n                \"unit\": \"ratio\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sign Language Time Series dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hand movements\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing sign language sequences for real-time recognition, with focus on nuanced hand movements.\",\n            \"preprocessing\": [\"real-time adaptive segmentation\", \"noise filtering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"DTW-based model with adaptive segmentation and noise filtering\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"long short-term memory (LSTM) with Temporal Alignment using Dynamic Time Warping (DTW)\",\n                \"optimization\": \"real-time parameter adjustments\"\n            },\n            \"description\": \"A highly optimized model for recognizing sign language time series, incorporating real-time adaptive segmentation and noise filtering techniques to handle nuanced hand movements and low-light conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Domain adaptation techniques to enhance recognition in low-light conditions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"highlight low-light conditions and nuanced hand movements examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response captures the essence of the given instruction, with emphasis on building a DTW-based model, optimizing for low-light and nuanced hand movements. Performance metrics include precision, recall, and a classification error target of <=5%, with space for optimization in complexity metrics. The dataset is accurately represented, and domain-specific knowledge is mentioned. However, without more specific details on the model architecture and feature engineering, a full optimization is not possible, hence the 'null' values in some fields.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "object detection", "instruction": "For our next cutting-edge wildlife conservation project, we've decided to delve into the lesser-known \"Biodiversity Unseen\" dataset. The goal is to develop a real-time, energy-efficient object detection system using a novel fusion of the GhostNet and FoD-RCNN architectures. This hybrid model must detect not only the standard big five but also cryptic and endangered species with minimal latency, as they exhibit intricate camouflage patterns in challenging low-light environments. The system should be designed to run seamlessly on resource-constrained underwater ROVs, while maintaining a high precision rate above 95%. In addition, the project requires integration with a smart tracking algorithm that anticipates animal movements and predicts potential habitat disturbances. Please outline a comprehensive plan for designing, training, and deploying this groundbreaking model.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"Develop a real-time, energy-efficient object detection system using a GhostNet and FoD-RCNN fusion for the Biodiversity Unseen dataset. The system must detect big five, cryptic, and endangered species in low-light environments with intricate camouflage, achieving a precision rate of 95% or higher. The model should be designed for resource-constrained underwater ROVs, and integrate a smart tracking algorithm for predictive habitat monitoring.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference latency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Biodiversity Unseen dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"all species, cryptic and endangered\"],\n            \"specification\": {\n                \"data format\": \"multimodal, low-light images\",\n                \"environment\": \"underwater ROV\"\n            },\n            \"description\": \"A dataset containing challenging low-light, camouflage patterns for a wide range of wildlife, including big five and endangered species.\",\n            \"preprocessing\": [\n                \"data augmentation for low-light and camouflage\",\n                \"image resizing to accommodate fusion architecture\"\n            ],\n            \"augmentation\": [\n                \"image noise injection\",\n                \"lighting variations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GhostNet + FoD-RCNN\",\n            \"family\": \"fusion architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"GhostNet and FoD-RCNN fusion\",\n                \"training setup\": \"custom, low-light, camouflage adaptation\"\n            },\n            \"description\": \"An object detection model that combines GhostNet and FoD-RCNN for real-time and resource-efficient detection in challenging environments.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Underwater object detection best practices\",\n        \"Incorporating smart tracking algorithms\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction for low-light and camouflage\",\n            \"ROI selection for resource-constrained devices\"\n        ],\n        \"target_device\": \"underwater ROV\",\n        \"deployment_endpoint\": \"custom cloud platform or embedded system\",\n        \"inference_engine\": [\n            \"optimized for latency and energy consumption\"\n        ],\n        \"demonstration\": \"live demos during training progress and after deployment\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key aspects of the user's instruction, including the specific dataset, the novel fusion model, performance metrics, and the resource constraints. The requirement for a smart tracking algorithm and predictive habitat monitoring is also represented. The user's high expertise level suggests a deep understanding of the project requirements and the potential complexities involved.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "Our task is to design an advanced text classification system for deciphering and categorizing highly specialized patents in the fields of quantum computing and bio-nano technology, using a combination of convolutional neural networks (CNN) and transformers with a nested multi-task learning approach. The model should achieve a state-of-the-art precision above 95% in recognizing subfields, while maintaining real-time adaptability for incorporating novel patent data with minimal retraining. Furthermore, the system must prioritize interpretability, enabling domain experts to understand the model's reasoning behind each classification decision.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"scientific research\",\n        \"description\": \"Design an advanced text classification system for patent analysis, focusing on quantum computing and bio-nano technology. Use a combination of CNNs and transformers with nested multi-task learning, aiming for state-of-the-art precision above 95% in subfield recognition. Ensure real-time adaptability and interpretability for domain experts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision (subfields)\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"patent dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"quantum computing\", \"bio-nano technology\"],\n            \"specification\": {\n                \"type\": \"domain-specific\",\n                \"language\": \"technical patent language\",\n                \"size\": \"large\"\n            },\n            \"description\": \"A dataset of specialized patents with text data, covering quantum computing and bio-nano technology for model development and evaluation.\",\n            \"preprocessing\": [\n                \"text normalization\",\n                \"domain-specific tokenization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature importance graphs\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum-BioNano Text Classifier\",\n            \"family\": \"CNN-Transformers with Multi-Task Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"nested CNNs and transformers\",\n                \"interpretability techniques\": [\"attention mechanisms\", \"interpretability layers\"]\n            },\n            \"description\": \"A deep learning model combining CNNs and transformers for patent text classification, with nested multi-task learning for improved performance and interpretability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"prototype-based on domain knowledge\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"providing detailed explanations of classification decisions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured to reflect the user's requirements for a state-of-the-art text classification system, with emphasis on precision and interpretability. The dataset details, performance metrics, and model specifications have been adapted to match the patent data, quantum computing, and bio-nano technology domains. The need for real-time adaptability is addressed through the model's retraining minimization. However, actual inference time and specific deployment details are left as null for optimization.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "tabular regression", "instruction": "For a highly specialized tabular regression project, let's delve into predicting the success of a niche scientific research paper's citation count. Can a state-of-the-art machine learning model, utilizing a comprehensive fusion of 'Research Paper Metadata', 'Scientific Impact Factors', and 'Collaboration Network Dynamics', accurately forecast the lifetime impact score of a paper within a specific domain, considering its abstract length, author reputation, and conference prestige? Please outline the necessary preprocessing steps, feature engineering, and model selection to achieve high precision in this unusual prediction task.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"scientific research\",\n        \"description\": \"Predict the success of a niche scientific research paper's citation count using a state-of-the-art model with a fusion of Research Paper Metadata, Scientific Impact Factors, and Collaboration Network Dynamics. Consider abstract length, author reputation, and conference prestige as features, aiming for high precision.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Niche Scientific Research Papers\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"lifetime impact score\"],\n            \"specification\": {\n                \"rows\": \"\",\n                \"columns\": {\n                    \"Abstract Length\": \"\",\n                    \"Author Reputation\": \"\",\n                    \"Conference Prestige\": \"\",\n                    \"Research Paper Metadata\": \"\",\n                    \"Scientific Impact Factors\": \"\",\n                    \"Collaboration Network Dynamics\": \"\"\n                }\n            },\n            \"description\": \"A dataset combining metadata, impact factors, and collaboration data to predict the lifetime impact score of niche scientific research papers.\",\n            \"preprocessing\": [\n                \"Data cleaning and normalization\",\n                \"Imputation for missing values\",\n                \"Feature encoding\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Exploratory data analysis\",\n                \"Feature correlation analysis\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Comprehensive fusion model (e.g., transformer-based or ensemble)\",\n                \"parameters\": {}\n            },\n            \"description\": \"A cutting-edge machine learning model integrating various data sources for high-precision prediction of lifetime impact score.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider scaling and normalization of numerical features for better model performance\",\n        \"Use domain-specific feature engineering techniques\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Embedding for categorical features\",\n            \"Time-aware feature extraction for dynamic collaboration networks\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Distributed inference for scalability\"\n        ],\n        \"demonstration\": \"Report on model performance with detailed prediction examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response covers the user's instruction for a specialized tabular regression project. It includes the model type, data sources, performance metric, complexity, preprocessing, and feature engineering steps. Some unspecified fields, such as model name and exact architecture, can be determined later based on user's knowledge and available resources. The response should provide a solid foundation to address the unusual prediction task.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "Could you design an advanced, real-time object detection model for detecting diverse wildlife intruding into my private garden, ensuring high accuracy and minimal latency for seamless surveillance?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"home security or wildlife monitoring\",\n        \"description\": \"Design an advanced, real-time object detection model for detecting diverse wildlife intruding into a private garden, prioritizing high accuracy and low latency for efficient surveillance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mAP (mean average precision)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": {\n                    \"min_value\": 0.9,\n                    \"target\": \"high\"\n                }\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": {\n                    \"min_value\": \"real-time\",\n                    \"unit\": \"milliseconds\"\n                },\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Garden Wildlife Data Set\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"wildlife labels\"],\n            \"specification\": {\n                \"frame_rate\": 30,\n                \"resolution\": {\n                    \"width\": \"variable\",\n                    \"height\": \"variable\"\n                }\n            },\n            \"description\": \"A dataset containing images and potentially videos of wildlife in a private garden, with annotations for object detection training.\",\n            \"preprocessing\": [\n                \"resize to a consistent resolution\",\n                \"color normalization\"\n            ],\n            \"augmentation\": [\n                \"horizontal flip\",\n                \"random cropping\"\n            ],\n            \"visualization\": [\n                \"training/validation sample visualization\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Real-time Object Detection\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"e.g., YOLOv5, Faster R-CNN, or EfficientDet\",\n                \"detector_size\": \"optimized for resource-constrained devices\"\n            },\n            \"description\": \"A state-of-the-art, real-time object detection model specifically designed for wildlife detection in a private garden.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using transfer learning to leverage pre-trained models on similar datasets or larger datasets.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time feature extraction\"],\n        \"target_device\": \"embedded devices or low-power IoT devices\",\n        \"deployment_endpoint\": \"custom edge computing or IoT infrastructure\",\n        \"inference_engine\": [\n            \"TensorFlow Lite\"\n        ],\n        \"demonstration\": \"live object detection feed or anomaly notifications\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intention to build an advanced object detection model with high accuracy and low latency. The AI and ML expertise level is assumed to be high, given the request for a complex task. The performance metrics include mAP and specific accuracy and latency requirements. The model's design and dataset selection are consistent with the task. The response mentions preprocessing, augmentation, and feature engineering steps, which demonstrate attention to detail and domain-specific knowledge. Transfer learning, deployment targets, and inference engines are appropriately suggested for effective implementation.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image classification", "instruction": "As a project manager, you've been tasked with developing an advanced dog breed identification app that efficiently classifies canine breeds from a live smartphone camera feed. Utilizing the Stanford Dogs Dataset, evaluate the feasibility of utilizing a convolutional neural network (CNN) for real-time performance. Additionally, propose a multi-stage approach to enhance accuracy, considering feature extraction, model architecture comparison, and integration of transfer learning for optimal speed and complexity management.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"mobile applications\",\n        \"description\": \"Task is to develop an advanced dog breed identification app that classifies canine breeds from live smartphone camera feed using a convolutional neural network (CNN) on the Stanford Dogs Dataset. The focus is on real-time performance, feasibility, and a multi-stage approach for accuracy enhancement.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Dogs Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\"],\n            \"specification\": {\n                \"number_of_classes\": null,\n                \"image_size\": null,\n                \"number_of_images\": null\n            },\n            \"description\": \"A dataset containing images of various dog breeds for real-time breed classification.\",\n            \"preprocessing\": [\"feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Network (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"layers\": null,\n                    \"real-time\": true\n                },\n                \"transfer_learning\": true\n            },\n            \"description\": \"Proposed CNN model for real-time dog breed classification, considering feature extraction, model comparison, and transfer learning.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Feasibility of CNN on a smartphone with real-time constraints.\"\n        },\n        {\n            \"description\": \"Multi-stage approach for accuracy enhancement: feature extraction, comparing different CNN architectures, and using transfer learning.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction for real-time performance\"\n        ],\n        \"target_device\": \"smartphone\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"on-device\", \"server-assisted\"],\n        \"demonstration\": \"live demo showcasing real-time breed classification\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the project task, focusing on the CNN application, dataset, performance metrics, complexity considerations, and the multi-stage approach. The user's high expertise level suggests a deep understanding of the requirements and potential solutions. However, specific model architecture details and performance metrics are left open for further investigation based on the provided information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "For the enhanced Seismic Volcanic Alert System, we seek a robust time-series classifier using RNNs, employing a multi-stage predictive model. The model must not only distinguish between seismic activity and eruptions, but also assess their severity and issue granular alerts, prioritizing precision and latency within milliseconds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"geosciences, seismology\",\n        \"description\": \"Seek a robust time-series classifier using RNNs for an Enhanced Seismic-Volcanic Alert System. The model should differentiate between seismic activity and eruptions, assess severity, and issue granular alerts with high precision and minimal latency (milliseconds).\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enhanced Seismic-Volcanic Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"seismic activity\", \"eruption type\", \"severity\"],\n            \"specification\": {\n                \"data_size\": null,\n                \"sampling_rate\": null,\n                \"length\": null\n            },\n            \"description\": \"A time-series data set capturing seismic and volcanic events with granular information for classification and severity assessment.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RNN-based Multi-Stage Classifier\",\n            \"family\": \"Recurrent Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"RNN architecture, e.g., LSTM or GRU\",\n                \"hidden_units\": null,\n                \"training_details\": null\n            },\n            \"description\": \"A multi-stage predictive model using RNNs to classify seismic events, assess their severity, and issue granular alerts.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"low-latency devices (e.g., IoT devices)\",\n        \"deployment_endpoint\": \"dedicated alert system\",\n        \"inference_engine\": [\n            \"optimized for real-time inference\"\n        ],\n        \"demonstration\": \"interactive dashboard showcasing latency and alert prioritization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the key elements of the user's instruction, with a high expertise level assumed. The model and dataset are specific to time-series analysis with RNNs, focusing on precision and latency. The complexity metrics and dataset specifications are left open-ended to account for potential requirements in a real-world scenario. The services section highlights the emphasis on latency-sensitive deployment and optimization.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "For a large-scale enterprise project, develop a highly efficient question-answering system using the latest TriviaQA compilation. The system must excel in tackling intricate, multi-faceted factoid and literary comprehension questions, employing a state-of-the-art transformer architecture like GPT-4. Mandate a scalable design capable of managing massive query loads while maintaining an impeccable sub-second response time, even under heavy concurrency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"enterprise\",\n        \"description\": \"Develop a large-scale question-answering system for an enterprise project using the latest TriviaQA compilation and focusing on complex, multi-faceted factoid and literary comprehension questions. The system should employ a state-of-the-art transformer architecture, such as GPT-4.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Speed (response time)\",\n                \"value\": \"sub-second\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Scalability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Concurrency handling\",\n                \"value\": null,\n                \"unit\": \"requests/second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TriviaQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\"],\n            \"specification\": null,\n            \"description\": \"A large-scale question-answer dataset, latest compilation, for training and testing the question-answering system.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer (specifically GPT-4)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"architecture_details\": \"Transformer-based, potentially incorporating large model capacity\"\n            },\n            \"description\": \"A highly efficient, state-of-the-art question-answering system using GPT-4 architecture.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for high concurrency\"],\n        \"demonstration\": \"scalability test with heavy concurrency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured to address the user's request for a large-scale question-answering system with GPT-4 and TriviaQA. The expert level of the user indicates a deeper understanding of the requirements, and specific performance metrics like sub-second response time and scalability expectations are considered. Some performance metric values are left unspecified for optimization, while emphasizing the need to handle heavy concurrency. The system's design and knowledge about the project context are directly based on the provided instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "In the context of the Enron Email dataset, design a highly specialized node classification model using a state-of-the-art GraphSAGE variant for R-GCNs. Focus on differentiating employees by their intricate communication networks, accounting for departmental hierarchy and communication frequency. Optimize for parallel execution and distributed computing to handle immense graphs with up to 100k nodes, ensuring real-time performance on high-velocity datasets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"corporate communication\",\n        \"description\": \"Design a specialized node classification model using GraphSAGE variants for R-GCNs on the Enron Email dataset. Focus on differentiating employees based on communication networks, considering departmental hierarchy and communication frequency. Aim for real-time performance on high-velocity datasets with parallel execution and distributed computing for graphs with up to 100k nodes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"milliseconds\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory usage\",\n                \"unit\": \"GB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email Dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"employee roles\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"max_nodes\": 100000,\n                    \"node_features\": {\n                        \"communication_network\": {},\n                        \"department_hierarchy\": {}\n                    }\n                },\n                \"edges\": {\n                    \"max_edges\": null,\n                    \"edge_types\": [\"communication\", \"hierarchical\"]\n                }\n            },\n            \"description\": \"A dataset from the Enron email corpus, capturing employee communication networks, accounting for departmental hierarchy.\",\n            \"preprocessing\": [\"feature normalization\", \"node feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [\"community detection\", \"edge type distribution\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Specialized GraphSAGE for R-GCNs\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"variant\": \"GraphSAGE-R-GCN\",\n                    \"Layers\": {\n                        \"number\": 2,\n                        \"attention\": \"R-GCN-specific attention mechanism\"\n                    }\n                },\n                \"parallelism\": \"fully parallel\",\n                \"distributed_computing\": true\n            },\n            \"description\": \"A state-of-the-art model that classifies Enron employees based on their communication networks with R-GCNs and departmental hierarchy.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Consider recent advancements in GraphSAGE and R-GCNs for node classification, such as hierarchical aggregation and attention mechanisms.\"\n        },\n        {\n            \"description\": \"Explore graph partitioning algorithms to distribute the workload for efficient parallel and distributed execution.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"node embeddings\",\n            \"graph embeddings\"\n        ],\n        \"target_device\": \"cloud or high-performance computing cluster\",\n        \"deployment_endpoint\": \"optimized for real-time inference in high-velocity environments\",\n        \"inference_engine\": [\"Apache GraphX\", \"Dask-ML\"],\n        \"demonstration\": \"Visualize classification results and performance metrics over time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately reflects the given instruction, including the use of GraphSAGE-RGCNs for node classification, departmental hierarchy, and distributed computing requirements. Performance metrics and complexity constraints are accounted for, but specific numerical targets are not provided since the user did not specify them. The structure of the JSON adheres to the provided schema.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for a cutting-edge AI team, you are tasked with developing a state-of-the-art time-series classification model. The SCP1 dataset, a complex real-world collection with multi-faceted patterns (INPUT_SEQ_LEN=896, INPUT_DIM=6), has been expertly divided into train, validation, and test partitions. Your objective is to design a model that not only accurately predicts labels (labels {0, 1}) but also demonstrates robustness to seasonality and noise, with a focus on improving F1-score and handling long-term dependencies. Ensure the model's interpretability is maintained while optimizing for efficiency on resource-constrained hardware.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"AI and predictive analytics\",\n        \"description\": \"Develop a state-of-the-art time-series classification model for the SCP1 dataset with a complex real-world collection and multi-faceted patterns (INPUT_SEQ_LEN=896, INPUT_DIM=6). The model should accurately predict labels (0, 1), exhibit robustness to seasonality and noise, optimize for F1-score, and handle long-term dependencies while maintaining interpretability on resource-constrained hardware.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"milliseconds\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory usage\",\n                \"unit\": \"MB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SCP1\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"labels\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 896,\n                \"INPUT_DIM\": 6\n            },\n            \"description\": \"A complex, real-world time-series dataset for classification, with train, validation, and test partitions, designed to test robustness to seasonality and noise, and handle long-term dependencies.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                \"seasonality handling\",\n                \"noise reduction techniques\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\",\n            \"description_long\": \"Expertly divided into train, validation, and test partitions for model development and evaluation\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art time-series classification model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true,\n                \"hardware_optimizations\": [\"resource-constrained\"]\n            },\n            \"description\": \"A model designed for SCP1 dataset, featuring interpretability and efficiency on low-resource hardware while addressing the given challenges.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Use of recurrent or transformer-based architectures for handling long-term dependencies\",\n        \"Seasonality decomposition and Fourier analysis for handling seasonality\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"resource-constrained hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretable models\"],\n        \"demonstration\": \"generate visual explanations for key predictions and model insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately reflects the user's instruction, specifying the model development task for a complex time-series classification problem, the SCP1 dataset, and performance metrics related to F1-score, robustness, interpretability, and efficiency. The expert level of expertise indicates a deep understanding of the task. Some values in the complexity metrics are not provided as specific optimization goals are not stated; however, the model's interpretability is incorporated as a requirement.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, elicit from the ML team a highly advanced image classification system utilizing the Fashion-MNIST dataset. Emphasize the exclusive adoption of Capsule Networks, targeting a state-of-the-art accuracy of 95% with geometric invariance. Demmand a thorough analysis that not only outperforms contemporary CNNs but also evaluates computational efficiency and generalization across different deformation types. Include a comprehensive report on the innovations and nuanced advantages capsule networks bring over traditional architectures.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"Request for a highly advanced image classification system using the Fashion-MNIST dataset. Specifically, the system should be based on Capsule Networks, aiming for a state-of-the-art accuracy of 95% with a focus on geometric invariance. The project should provide a comprehensive analysis demonstrating superiority over contemporary CNNs in terms of computational efficiency and generalization across different deformation types.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"generalization across deformation types\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"image labels\"],\n            \"specification\": {\n                \"image_size\": 28,\n                \"number_of_classes\": 10,\n                \"training_samples\": null,\n                \"test_samples\": null\n            },\n            \"description\": \"Fashion-MNIST dataset for image classification, focusing on fashion items instead of handwritten digits.\",\n            \"preprocessing\": [\"normalized to [0, 1] range\"],\n            \"augmentation\": [\"geometric invariance techniques\"],\n            \"visualization\": [\"progress visualization during training\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Capsule Network\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_details\": \"Exclusive adoption, likely including dynamic routing and margin loss\"\n            },\n            \"description\": \"A highly advanced image classification model based on Capsule Networks, designed to achieve state-of-the-art performance with geometric invariance.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"innovations\": \"Capule Networks' benefits include better representation of object relationships and disentanglement of features.\",\n            \"nuanced_advantages\": \"Over CNNs, they offer improved geometric invariance and are more robust to deformation.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"A report on the system performance with visual explanations and comparative analysis with CNNs.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the user's request for a machine learning project, specifically focusing on a high-performance image classification system using Capsule Networks. It includes details such as accuracy, and emphasizes computational efficiency and generalization. However, since the user didn't specify a concrete performance benchmark for computational efficiency, that field remains null. The request for a comprehensive report on capsule network innovations and advantages is also captured.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for an advanced surveillance system, you require a machine learning model to precisely localize and classify car models in congested, low-resolution parking lot footage. Compare COCO and Pascal VOC's suitability based on their respective annotation formats and image diversity. Investigate and recommend a real-time, resource-efficient object detection model that demonstrates exceptional performance on such challenging scenarios while maintaining high precision and recall rates.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"surveillance\",\n        \"description\": \"Require a machine learning model for precise car model localization and classification in low-resolution parking lot footage, especially in congested scenes. Compare COCO and Pascal VOC's suitability based on their annotation formats and image diversity. Look for a real-time, resource-efficient model with high precision and recall.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"time\"\n            },\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": [\"COCO dataset\", \"Pascal VOC dataset\"],\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"car model annotations\"],\n            \"specification\": {\n                \"COCO\": {\n                    \"annotation_format\": \"COCO format\",\n                    \"image_diversity\": \"high\"\n                },\n                \"Pascal VOC\": {\n                    \"annotation_format\": \"VOC format\",\n                    \"image_diversity\": \"moderate\"\n                }\n            },\n            \"description\": [\n                \"COCO dataset potentially provides a broader range of car images in diverse scenes for model training.\",\n                \"Pascal VOC dataset could offer a more structured and potentially less challenging set of images for car detection.\"\n            ],\n            \"preprocessing\": [\n                \"adaptation to low-resolution\"\n            ],\n            \"augmentation\": [\n                \"geometric transformations for challenging scenarios\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art object detection model (e.g., YOLOv5, EfficientDet, or Mask R-CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"specific_model_performance\": {\n                    \"high precision on challenging scenarios\": true,\n                    \"high recall\": true\n                }\n            },\n            \"description\": \"A real-time, resource-efficient model optimized for object detection in low-resolution, congested parking lot scenes.\"\n        }\n    ],\n    \"knowledge\": [\n        \"COCO annotations support bounding boxes and instance segmentation, making it suitable for complex scenarios. Pascal VOC annotations are primarily bounding boxes, which might require additional data preprocessing for similar performance.\",\n        \"Consider factors like annotation overhead, model training time, and required compute resources when comparing COCO and Pascal VOC.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"data preprocessing for low-resolution images\"\n        ],\n        \"target_device\": \"low-power embedded systems or edge devices\",\n        \"deployment_endpoint\": \"real-time inference service\",\n        \"inference_engine\": [\"optimized for resource-constrained environments\"],\n        \"demonstration\": \"demonstrate model performance on representative parking lot footage\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's requirements for a machine learning model, comparison of COCO and Pascal VOC, and a recommendation for a real-time, resource-efficient model. The instruction hints at a high level of expertise, and the response reflects this by offering comprehensive analysis of the problem domain, datasets, and model selection. The performance and complexity metrics are not specified, as they would typically depend on experimentation and benchmarking. The user's task is assumed to be well-defined, and the confidence score is set at a high level, reflecting the confidence in interpreting the given instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for an AI team, your objective is to develop a multi-tiered, interactive question-answering platform specifically designed for pedagogical content targeted at elementary school children. Utilize the specialized Kid'sQA dataset and engineer a simplified, BERT-based model with educational tweaks for enhanced comprehension. Emphasize user experience, incorporating intuitive design elements to captivate and retain young users' interest throughout the learning process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Develop a multi-tiered, interactive question-answering platform targeting elementary school children. Use the Kid'sQA dataset and create a simplified, BERT-based model with educational adjustments for improved understanding. Focus on user experience with intuitive design elements.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"user engagement rate\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Kid'sQA dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"answers\", \"questions\"],\n            \"specification\": {\n                \"size\": \"not specified\",\n                \"complexity\": \"simplified for children\"\n            },\n            \"description\": \"A dataset tailored for pedagogical content with simplified questions and answers for elementary school children.\",\n            \"preprocessing\": [\"BERT tokenization\"],\n            \"augmentation\": [\"synthetic data generation for children's language\"],\n            \"visualization\": [\"interactive learning paths\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Simplified BERT-based Q&A model\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"number of layers\": \"reduced\",\n                \"training techniques\": \"educational tweaks for comprehension\"\n            },\n            \"description\": \"An adapted BERT model designed for efficient and age-appropriate question answering in the educational domain.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"age-appropriate features\",\n            \"learning curve simplification\"\n        ],\n        \"target_device\": \"mobile and tablet devices\",\n        \"deployment_endpoint\": \"dedicated learning platform\",\n        \"inference_engine\": [\"GPU-optimized for speed\"],\n        \"demonstration\": \"user-friendly interface with gamification elements\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response captures the essence of the project's requirements, targeting an educational platform for young children with a simplified BERT-based model. Key elements include the Kid'sQA dataset, user experience focus, and performance metrics not directly specified but acknowledging the importance of response time and user engagement. Some specifics such as the model's reduced layer count and educational tweaks are plausible in achieving the goal. The deployment endpoint suggests a dedicated platform, and the design considerations emphasize user engagement and simplification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the advanced machine learning team, your challenge is to enhance the existing text classification system. Utilizing the intricate BoolQ dataset with nested reasoning questions, design a model that not only differentiates factual inference but also discerns implicit relationships. Optimize for high precision and recall, ensuring a balanced F1-score, and present a detailed analysis of model robustness against adversarial examples.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"knowledge base systems\",\n        \"description\": \"Enhance the existing text classification system by designing a model for the BoolQ dataset with a focus on nested reasoning questions. The model should differentiate factual inference and discern implicit relationships. Optimize for high precision, recall, and balanced F1-score, while analyzing model robustness against adversarial examples.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"adversarial robustness\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BoolQ dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"nested reasoning\", \"factual inference\", \"implicit relationships\"],\n            \"specification\": {\n                \"questions\": {\n                    \"nested_depth\": \"unknown\",\n                    \"reasoning_complexity\": \"unknown\"\n                },\n                \"labels\": {\n                    \"factual_inference\": \"binary\",\n                    \"implicit_relations\": \"binary\"\n                }\n            },\n            \"description\": \"A dataset containing nested reasoning questions for improving an existing text classification system, focusing on BoolQ.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"architecture\": \"to be determined\",\n                \"hyperparameters\": {\n                    \"optimizer\": \"unknown\",\n                    \"learning_rate\": \"unknown\",\n                    \"layers\": {\n                        \"number\": \"unknown\",\n                        \"size\": \"unknown\"\n                    }\n                }\n            },\n            \"description\": \"A deep learning model to enhance the text classification system for BoolQ dataset with nested reasoning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adversarial training and robust model architectures may need to be considered.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual embeddings\",\n            \"reasoning-aware feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Presentation of model performance, interpretability, and adversarial example analysis.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes key elements from the user's instruction, with high expertise indicating a detailed understanding. The problem area and task are clear, along with metrics and dataset specifications. However, certain details like model architecture and adversarial robustness are left for determination based on the user's preferences or research. The model family and type are set to 'neural networks' and 'deep learning' as these are common for text classification tasks. The confidence score is likely to be high, but some information is incomplete due to the open-ended nature of the request.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "question answering", "instruction": "Elaborate a highly specialized medical question-answering model, employing the challenging MIMIC-III and PubMedBERT fusion, targeting real-time, precise diagnosis of rare diseases with complex symptoms. The system should not only generate answers but also propose differential diagnoses backed by peer-reviewed articles, ensuring bibliographic traceability and maintaining interpretability for medical professionals.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medical\",\n        \"description\": \"Develop a specialized medical question-answering model using MIMIC-III and PubMedBERT fusion for real-time and precise diagnosis of rare diseases with complex symptoms. The model should generate answers and propose differential diagnoses, backed by peer-reviewed articles, maintaining bibliographic traceability and interpretability for medical professionals.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision for rare diseases\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall for complex symptom detection\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time for real-time response\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"model size for deployment\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MIMIC-III\",\n            \"modality\": [\"text\", \"time series\"],\n            \"target_variables\": [\"patient health data\", \"diagnosis\"],\n            \"specification\": null,\n            \"description\": \"A large-scale clinical database with detailed medical records and time-series physiological signals for rare diseases and complex symptoms.\",\n            \"preprocessing\": [\"de-identification\", \"phenotyping\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"PubMedBERT fusion corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"article count\": null,\n                \"relevant article ratio\": null\n            },\n            \"description\": \"A fusion corpus of MIMIC-III and PubMed articles, with relevance to rare diseases and complex symptoms for bibliographic traceability\",\n            \"preprocessing\": [\"document alignment\", \"cross-referencing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Medical Q&A Model\",\n            \"family\": \"MIMIC-III & PubMedBERT Fusion\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-based\",\n                \"BERT-specific\": {\n                    \"model_size\": null,\n                    \"pretraining_steps\": null\n                }\n            },\n            \"description\": \"A state-of-the-art medical QA model combining MIMIC-III's patient data and PubMedBERT's literature understanding.\"\n        }\n    ],\n    \"knowledge\": {\n        \"requirements\": [\n            \"Interpretability techniques for medical professionals\",\n            \"SOTA fusion techniques for combining MIMIC-III and PubMedBERT\"\n        ]\n    },\n    \"service\": {\n        \"feature_engineering\": [\"context-aware feature extraction\", \"domain-specific feature engineering\"],\n        \"target_device\": \"real-time edge or cloud environment\",\n        \"deployment_endpoint\": \"secure, HIPAA-compliant platform\",\n        \"inference_engine\": [\"FasterR-CNN for symptom extraction\", \"DifferentialDiagnosis recommendation module\"],\n        \"demonstration\": \"Interactive platform with expert-level explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response is based on the user's instruction for a highly specialized medical QA model. The details cover the problem area, performance metrics, complexity considerations, datasets, model specifications, and service requirements. However, specific numerical values for performance and complexity metrics are left open-ended due to the lack of specific targets. The structure and rationale reflect the emphasis on rare diseases, complex symptoms, bibliographic traceability, and interpretability.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As a project manager, you're tasked with developing a comprehensive machine learning solution for the PlantVillage dataset. The objective is to design an advanced attention-guided DenseNet architecture that not only segments leaves into symptomatic and healthy regions but also requires multi-level feature extraction and in-depth understanding of disease patterns. In addition, enhance model explainability by integrating visualization techniques for interpretability, ensuring that the output can be easily interpreted by non-technical users for accurate plant health diagnostics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Design an advanced attention-guided DenseNet architecture for leaf segmentation in the PlantVillage dataset. The model should distinguish symptomatic and healthy regions, with multi-level feature extraction and an emphasis on disease pattern understanding. The solution must include model explainability through visualization techniques for non-technical users.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"model explanation complexity\",\n                \"value\": null,\n                \"unit\": \"user-friendly\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PlantVillage dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"symptomatic regions\", \"healthy regions\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"height\": null,\n                    \"width\": null\n                },\n                \"number_of_classes\": 2\n            },\n            \"description\": \"Dataset containing leaf images for plant health diagnostics, focusing on symptomatic and healthy regions with disease patterns.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"model interpretability visualizations\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention-guided DenseNet\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"architecture_details\": \"advanced attention mechanism, DenseNet layers, multi-level feature extraction\",\n                \"model_size\": null\n            },\n            \"description\": \"An advanced machine learning model designed for leaf segmentation with symptom detection and explainability in the PlantVillage dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"DenseNet's ability in capturing long-range dependencies and multi-scale features for image segmentation\",\n        \"Use of attention mechanisms to guide feature extraction for improved segmentation accuracy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Visualization of attention maps and model predictions for plant health diagnostics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's intent as a building project with high expertise. The area and downstream task align with the plant disease detection and segmentation. Model details are specified, and the user emphasizes on explainability, visualization, and interpretability. Although some metric values are not provided, it reflects the need for optimization.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a seasoned project manager, assist me in implementing an advanced object detection system that can not only identify and count fruits in diverse images, but also distinguish between rare and commonly occurring species. Require the model to perform in low-light and occluded conditions, integrating with a real-time video feed and providing species classification with 95% accuracy. Utilize the existing dataset while considering its regional variety and image quality inconsistencies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"agriculture and image analysis\",\n        \"description\": \"Implement an advanced object detection system for identifying and counting fruits in diverse images, differentiating between rare and common species. The model must work robustly in low-light and occluded conditions, and be integrated with real-time video feed, targeting 95% species classification accuracy. Consider regional variety and image quality inconsistencies in the existing dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Species classification accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Robustness to low light and occlusion\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fruit Detection Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"fruit species\", \"quantity\"],\n            \"specification\": {\n                \"regional variety\": true,\n                \"quality inconsistencies\": true\n            },\n            \"description\": \"A dataset containing diverse images of fruits with varying regional origins and image quality.\",\n            \"preprocessing\": [\"low-light enhancement\", \"image enhancement for occlusions\"],\n            \"augmentation\": [\"synthetic data generation for low-light, occlusion scenarios\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Fruit Detection Model\",\n            \"family\": \"deep learning (e.g., YOLOv5, Faster R-CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"real-time capability\": true\n            },\n            \"description\": \"An advanced object detection model designed for robust fruit detection in diverse conditions, with species classification and counting capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using transfer learning to handle the region-specific variations and image quality issues.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"fine-tuning for target species\", \"data post-processing for counting\"],\n        \"target_device\": \"embedded systems with sufficient compute for real-time processing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"OpenVINO\", \"TensorRT for optimized inference\"],\n        \"demonstration\": \"real-time video demonstration with species identification and count\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's intent for building an advanced object detection system with target accuracy, performance, and challenges. The dataset and model sections reflect the specific requirements like real-time video feed, regional variety, and image quality handling. Specific model specifications and inference optimizations are included, as well as potential steps for post-processing and deploying the model.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "Incorporate clickstream data from Bilibili's additional features like 'likes', 'views', and 'watch time', alongside the 'Bilibili Danmu' dataset. The enhanced model, still using Bert4Rec, must excel in capturing temporal user behavior patterns and perform collaborative filtering while considering comment sentiments for improved video recommendation in a high-traffic environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"video streaming\",\n        \"description\": \"The task is to enhance video recommendation by integrating clickstream data from Bilibili's features (likes, views, watch time) and the Bilibili Danmu dataset. The model must utilize Bert4Rec to capture temporal user behavior and incorporate comment sentiments for high-traffic recommendation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"NDCG\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Recall@N\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage during training\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enhanced Bilibili dataset (clickstream + Bilibili Danmu)\",\n            \"modality\": [\"time series\", \"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"time_series_length\": null,\n                \"text_vector_length\": null\n            },\n            \"description\": \"A dataset that combines Bilibili's clickstream data (likes, views, watch time) and Bilibili Danmu comments, aiming to model temporal user behavior and comment sentiments.\",\n            \"preprocessing\": [\n                \"Timestamp aggregation\",\n                \"Sentiment analysis on comments\"\n            ],\n            \"augmentation\": [\n                \"Sequential data augmentation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Bert4Rec\",\n            \"family\": \"neural networks\",\n            \"type\": \"recommendation models\",\n            \"specification\": {\n                \"model_architecture\": \"BERT4Rec\",\n                \"temporal_embedding\": true,\n                \"sentiment_embedding\": true\n            },\n            \"description\": \"A model that combines Bert4Rec with clickstream data and sentiment analysis to capture temporal user behavior patterns and improve collaborative filtering for video recommendation on Bilibili.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Context-aware filtering\",\n            \"Behavioral sequence modeling\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"Bilibili recommendation service\",\n        \"inference_engine\": [\n            \"GPU-based inference\"\n        ],\n        \"demonstration\": \"Real-time A/B testing with dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON parses the user's instruction to build a recommendation model that incorporates multiple Bilibili datasets, focuses on temporal user behavior with Bert4Rec, and considers comment sentiments. Performance metrics (NDCG and Recall@N) are left unspecified to account for the need to report on metrics relevant to the high-traffic environment. Complexity metrics, like inference time and memory usage, are included as expected for large-scale applications. Data preprocessing and feature engineering steps reflect the integration of clickstream and sentiment information. Deployment-related details (target device and endpoint) are suitable for the context, and the model's specifications align with the described enhancements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Design a high-precision, multi-attribute image recognition system using a cutting-edge fusion of Vision Transformers and Capsule Networks, with the objective to not only distinguish between male and female genders with remarkable accuracy, but also discern five discreet age categories (child, teenager, young adult, middle-aged, and senior) in facial images, ensuring the model can handle diverse ethnic features and expressions while maintaining real-time performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"biometrics, security\",\n        \"description\": \"Design a high-precision, multi-attribute image recognition system that fuses Vision Transformers and Capsule Networks for accurate gender (male/female) and age (child/teenager/young adult/middle-aged/senior) classification in facial images. The model must handle diverse ethnic features and expressions with real-time performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"Gender classification F1-score\",\n                \"value\": \"0.95+\"\n            },\n            {\n                \"name\": \"Age classification accuracy\",\n                \"value\": \"0.9+\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"real-time\",\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Diverse Facial Images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"gender\", \"age\"],\n            \"specification\": {\n                \"size\": \"large\",\n                \"diversity\": \"ethnicities, expressions\"\n            },\n            \"description\": \"A large, labeled dataset with a wide range of facial images featuring diverse ethnic features and expressions for gender and age classification.\",\n            \"preprocessing\": [\"data augmentation\"],\n            \"augmentation\": [\"random cropping, flipping, facial landmarks\"],\n            \"visualization\": [\"example image distribution\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Vision Transformers and Capsule Networks fusion\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-Capsule\",\n                \"parameters\": \"large, fine-tuned\"\n            },\n            \"description\": \"A state-of-the-art model combining Vision Transformers and Capsule Networks for robust gender and age classification in real-time conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cutting-edge fusion technique to enhance feature extraction and capsules for robustness\",\n        \"Using appropriate loss functions for multi-label classification\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"facial landmark detection\"],\n        \"target_device\": \"cloud or edge (real-time performance)\",\n        \"deployment_endpoint\": \"API-based or custom deployment\",\n        \"inference_engine\": [\"GPU-optimized for speed\"],\n        \"demonstration\": \"Interactive demo showcasing diverse scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response covers the user's request for a cutting-edge image recognition system with high accuracy, targeting gender and age classification in diverse facial images. Performance metrics, including F1-score and accuracy, are specified. The emphasis on real-time performance and handling ethnic features and expressions is captured in the model and dataset descriptions. However, the specific model names and deployment details are not provided, which may be required for complete clarity.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular classification", "instruction": "You must develop a tailored tabular deep learning model for the specific ZymoRegNet dataset, which contains intricate enzyme-substrate interactions. Train, validate, and test splits are provided. The challenge lies in forecasting binary labels EC1 and EC2 with high precision, assessed by the Macro F1 Score and Average Precision (AP) to demonstrate superior multi-label performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Develop a tailored tabular deep learning model for the ZymoRegNet dataset focusing on enzyme-substrate interactions. The goal is to forecast binary labels EC1 and EC2 with high precision, using Macro F1 Score and Average Precision (AP) to measure multi-label performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Macro F1 Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Average Precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ZymoRegNet\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"EC1\", \"EC2\"],\n            \"specification\": {\n                \"train_size\": null,\n                \"validation_size\": null,\n                \"test_size\": null,\n                \"features\": null,\n                \"labels\": null\n            },\n            \"description\": \"Dataset containing enzyme-substrate interactions with binary labels EC1 and EC2 for tabular deep learning.\",\n            \"preprocessing\": [\"data normalization\", \"feature scaling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"tabular deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for ZymoRegNet dataset\",\n                \"Layers\": null,\n                \"Optimizer\": null,\n                \"Training parameters\": {\n                    \"epochs\": null,\n                    \"batch_size\": null\n                }\n            },\n            \"description\": \"A custom deep learning model designed specifically for enzyme-substrate interactions in the ZymoRegNet dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider transfer learning from similar tabular datasets or ensemble methods for better performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"feature selection\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow, scikit-learn\"],\n        \"demonstration\": \"Include a separate evaluation of model performance on validation set and comparison with baselines\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intention to build a model for the ZymoRegNet dataset. The model's requirement for high precision and multi-label performance evaluation metrics are indicated. Performance metrics and data complexities are not specified in decimal values, as the exact values are not provided. However, the model's accuracy, given the user's medium expertise, can be inferred by incorporating known practices and techniques. Some details are left to be filled based on specific dataset characteristics and user preferences.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "node classification", "instruction": "In the context of a groundbreaking bioinformatics project, you've been tasked with developing a state-of-the-art model for protein-protein interaction (PPI) node classification using Hierarchical Attention Graph Neural Networks (HAGNNs). The data originates from the STRING database and comes with millions of high-confidence interactions, making it not only extremely large but also densely connected. The model should achieve a remarkable precision of 95%, maintaining interpretability, and be designed to adapt seamlessly to the discovery of novel protein complexes in real-time. Ensure that the architecture is optimized for both computational efficiency and biological relevance, with a focus on minimizing false positives.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Develop a state-of-the-art model for protein-protein interaction (PPI) node classification using Hierarchical Attention Graph Neural Networks (HAGNNs) on a STRING database with millions of high-confidence interactions, aiming for precision of 95% while maintaining interpretability. The model should adapt to novel protein complexes in real-time and optimize computational efficiency and biological relevance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"False Positive Rate\",\n                \"value\": null,\n                \"unit\": \"percentage\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"STRING Protein-Protein Interaction\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"Protein Interactions\"],\n            \"specification\": {\n                \"size\": \"large\",\n                \"density\": \"high\"\n            },\n            \"description\": \"A large and densely connected graph database of protein interactions from the STRING database for PPI node classification.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"feature scaling\"\n            ],\n            \"augmentation\": [\n                \"graph sampling\"\n            ],\n            \"visualization\": [\"interaction network visualization\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Attention Graph Neural Network (HAGNN)\",\n            \"family\": \"Hierarchical Attention\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"adapting_strategy\": \"real-time\"\n            },\n            \"description\": \"A state-of-the-art model using HAGNN for PPI node classification with emphasis on precision, interpretability, and real-time adaptability.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"protein_complex_discovery\": \"The model should consider the dynamic nature of protein complexes for real-time updates.\"\n        },\n        {\n            \"biological_relevance\": \"Fine-tuning on domain-specific datasets will enhance biological relevance.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\",\n            \"feature extraction\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"omics platform or dedicated server\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"interactive visualizations and interpretability insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the given instructions for a bioinformatics project, covering node classification with HAGNNs, dataset details, and model requirements. The user's high expertise level and the emphasis on precision, interpretability, and real-time adaptation are captured. However, some aspects such as specific metric values and computational efficiency optimization are left open-ended as they may require additional context or clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Create a specialized RNN-based machine translation model using the extensive OPUS corpus for translating colloquial and informal English to Russian. The model must not only target a BLEU score of 33 but also exhibit exceptional proficiency in deciphering slang and non-standard expressions, while maintaining a stringent real-time constraint of 250 milliseconds per sentence for seamless user interaction in a high-traffic chatbot scenario.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"chatbot technology\",\n        \"description\": \"Create a specialized RNN-based machine translation model from colloquial and informal English to Russian, targeting BLEU score of 33 and focusing on handling slang and non-standard expressions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 33\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.25,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS corpus (colloquial English)\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [\"translated Russian text\"],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"language variety\": \"colloquial and informal English\"\n            },\n            \"description\": \"An extensive corpus containing informal English text for training the translation model.\",\n            \"preprocessing\": [\"tokenization\", \"lemmatization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        },\n        {\n            \"name\": \"OPUS corpus (informal Russian)\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [\"translated English text\"],\n            \"specification\": {\n                \"language variety\": \"colloquial and informal Russian\"\n            },\n            \"description\": \"Corpus with informal Russian text for target language translation and evaluation.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Specialized RNN-based machine translation model\",\n            \"family\": \"seq2seq models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"RNN (potentially LSTM, GRU, or attention-based)\",\n                \"specialization\": \"sensitive to slang and non-standard expressions\"\n            },\n            \"description\": \"A deep learning model specifically designed for RNN-based machine translation with a focus on informal English to Russian conversion.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider incorporating domain-specific corpora or data augmentation techniques for handling slang and informal expressions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"slang and informal expressions handling\"],\n        \"target_device\": \"high-traffic chatbot environment\",\n        \"deployment_endpoint\": \"real-time translation API\",\n        \"inference_engine\": [\"optimized for high-performance\"],\n        \"demonstration\": \"High-traffic chatbot scenario with smooth real-time interactions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response meets the user's requirements, including the model type, performance metrics, and constraints related to slang handling and real-time performance. The expert level of expertise suggests a comprehensive understanding. However, specific model architecture details and certain preprocessing steps are not specified, leaving room for further clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "In a challenging scenario, we possess an extensive dataset containing microscopic images of various obscure and rare plant species from different ecosystems. The task is to develop a highly specialized object detection model that not only accurately identifies the specific plant species but also distinguishes between extremely similar genera with minute morphological differences. The model should also be able to handle occlusions and variable lighting conditions, while maintaining real-time performance for a remote biodiversity monitoring application.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"botany/remote biodiversity monitoring\",\n        \"description\": \"Develop a highly specialized object detection model for microscopic images of obscure and rare plant species. The model must accurately identify species, distinguish between closely related genera, handle occlusions and variable lighting conditions, and maintain real-time performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Microscopic plant species dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"plant species\", \"genus\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"variable_representation\": \"microscopic\"\n            },\n            \"description\": \"Extensive dataset with microscopic images of rare plant species, covering different ecosystems with varying occlusions and lighting conditions.\",\n            \"preprocessing\": [\n                \"image resizing\",\n                \"histogram equalization\"\n            ],\n            \"augmentation\": [\n                \"random occlusions\",\n                \"lighting variations\"\n            ],\n            \"visualization\": [\"image segmentation maps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced object detection model (e.g., YOLOv5, Mask R-CNN, or customized)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"real-time\",\n                \"inference framework\": null\n            },\n            \"description\": \"A deep learning-based object detection model designed for specialized plant species and genus recognition in challenging conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Closely related genera have distinct morphological features.\",\n        \"Fine-tuning on specific domain data improves performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"microscopic image preprocessing pipelines\", \"domain-specific embeddings\"],\n        \"target_device\": \"remote, possibly edge-computing\",\n        \"deployment_endpoint\": \"custom API or cloud-based\",\n        \"inference_engine\": [\"optimized for latency\"],\n        \"demonstration\": \"live object detection demo with real-time species identification\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's request, focusing on the task of developing a specialized object detection model for plant species identification. The model requirements, performance metrics (such as accuracy), and handling of occlusions and lighting conditions are addressed. The user's high expertise level implies a need for advanced techniques and robust performance under challenging conditions. The dataset is described as extensive and processed to handle the specific requirements. The missing metric values indicate they are needed for optimization, while the confidence score reflects the completeness and correctness of the information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "For our e-commerce website, we require an advanced, real-time time-series anomaly detection model that specifically isolates weekends, holiday effects, and non-seasonal patterns. The system must trigger instant alerts for critical drops or sudden increases in conversion rates, helping us analyze user behavior during peak hours and unusual traffic surges to ensure seamless user experience and protect against potential fraud.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Develop an advanced real-time time-series anomaly detection model to isolate weekends, holiday effects, and non-seasonal patterns. The model should trigger alerts for critical drops or sudden increases in conversion rates, supporting user behavior analysis and fraud prevention during peak hours and unusual traffic surges.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"alert accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": \"real-time\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"E-commerce website transaction data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"conversion rates\"],\n            \"specification\": {\n                \"time_frequency\": \"real-time\",\n                \"data_size\": \"streaming\",\n                \"time_range\": \"daily\"\n            },\n            \"description\": \"A time-series dataset capturing our website's transaction data, including conversion rates, to model and detect anomalies.\",\n            \"preprocessing\": [\"timestamp normalization\", \"resampling\"],\n            \"augmentation\": [],\n            \"visualization\": [\"line plots for trend analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Real-time Anomaly Detection Model\",\n            \"family\": \"Deep Learning (e.g., LSTM, Autoencoder)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ensemble of deep learning models\",\n                \"complexity\": \"state-of-the-art\",\n                \"feature Engineering\": [\"seasonality decomposition\", \"holiday effects\"]\n            },\n            \"description\": \"An advanced model using deep learning techniques to detect anomalies in real-time, factoring in weekends, holidays, and non-seasonal patterns.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Experience with handling streaming time-series data and real-time analysis\",\n        \" weekend/holiday specific models, such as incorporating holiday calendars\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonality decomposition\",\n            \"holiday effect adjustments\"\n        ],\n        \"target_device\": \"cloud for real-time processing\",\n        \"deployment_endpoint\": \"API for instant alerts\",\n        \"inference_engine\": [\"GPU-powered\", \"real-time stream processing\"],\n        \"demonstration\": \"Interactive dashboard showing live alerts and analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's requirements for a specialized time-series anomaly detection model in the e-commerce context. It addresses the need for real-time analysis, weekend and holiday detection, and non-seasonal patterns. Performance metrics are based on accuracy and real-time response, while complexity metrics consider time and memory constraints. The dataset is a real-time streaming dataset, and preprocessing steps address the specific patterns. The model choice and architecture meet the advanced nature of the requirement, and the knowledge and service sections ensure a comprehensive solution for monitoring and fraud prevention.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For our advanced agricultural precision farming initiative, design a state-of-the-art Mask R-CNN model enhanced with transfer learning from extra domain datasets. The model must identify not just diseases and nutrient deficiencies, but also distinguish between multiple stress factors. Achieve a minimum of 95% precision, ensuring real-time performance for processing high-resolution drone images taken from oblique and nadir angles.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Design a state-of-the-art Mask R-CNN model for advanced precision in identifying diseases, nutrient deficiencies, and multiple stress factors in plants for an agricultural precision farming initiative. The model must be capable of real-time performance for high-resolution drone images captured from oblique and nadir angles.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"real-time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"extra domain datasets\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"diseases\", \"nutrient deficiencies\", \"stress factors\"],\n            \"specification\": {\n                \"high-resolution\": true,\n                \"drone images\": {\n                    \"oblique angles\": true,\n                    \"nadir angles\": true\n                }\n            },\n            \"description\": \"Supplementary datasets for transfer learning with diverse agricultural imagery\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Mask R-CNN with transfer learning\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"state-of-the-art\": true,\n                \"transfer learning from\": \"extra domain datasets\"\n            },\n            \"description\": \"A real-time, high-precision Mask R-CNN model specifically tailored for precision farming with ability to handle diverse stress factors and image angles.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"showcase real-time processing of high-resolution images\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's requirements for a machine learning project in the agricultural precision farming domain. The model, performance metrics, and dataset properties are based on the given details. Transfer learning from extra domain datasets is included, and the emphasis on real-time performance and high-resolution images is captured. However, the user's target device and specific deployment details are not mentioned and require clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For an elite precision agriculture project, devise a highly-optimized Mask R-CNN model, integrating domain-adaptation techniques from a diverse range of specialized datasets. The model should detect and differentiate between five distinct stress factors, diseases, and nutrient deficiencies in high-resolution drone imagery captured at variable angles (oblique and nadir), demanding real-time efficiency with a strict benchmark of 98% accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"precision agriculture\",\n        \"description\": \"Design a highly-optimized Mask R-CNN model for precision agriculture, detecting and differentiating between five distinct stress factors, diseases, and nutrient deficiencies in high-resolution drone imagery captured at variable angles.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"real-time\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"various specialized datasets\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"stress factors\", \"diseases\", \"nutrient deficiencies\"],\n            \"specification\": null,\n            \"description\": \"A collection of diverse specialized datasets for domain adaptation with high-resolution oblique and nadir drone imagery.\",\n            \"preprocessing\": [\"domain adaptation techniques\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Mask R-CNN\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"optimized for real-time efficiency\"\n            },\n            \"description\": \"A highly-optimized model integrating Mask R-CNN for detecting multiple stress, disease, and nutrient deficiency indicators in precision agriculture imagery.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time inference\"],\n        \"demonstration\": \"real-time detection and differentiation of stress factors, diseases, and nutrient deficiencies\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response captures the essence of the user's elite-level expertise and their request for a precision agriculture project using a highly-optimized Mask R-CNN model. The performance metrics align with the strict benchmark, and the source of the data indicates the requirement for domain adaptation from diverse datasets. Real-time efficiency is explicitly mentioned, and some aspects like inference time and memory consumption are left unspecified for optimization.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As an experienced project manager for an e-commerce giant, your team is tasked with developing a sophisticated, multi-layered time-series anomaly detection system. The model must not only differentiate between weekends, holidays, and non-traditional trends, but also factor in regional variations, peak hour trends, and dynamically evolving user preferences. The system should deploy anomaly notifications with real-time precision, enabling swift investigation of unusual conversion spikes or dips, and supporting the team in devising targeted interventions for both customer experience optimization and fraud prevention, all while maintaining a high level of performance under extreme data influx scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Develop a sophisticated, multi-layered time-series anomaly detection system that accounts for weekends, holidays, regional variations, peak hour trends, and dynamically evolving user preferences. The system should have real-time precision for anomaly notifications, supporting swift investigation and targeted interventions for customer experience and fraud prevention, and maintain high performance under extreme data influx scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Response Time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory Consumption\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"Extreme Data Influx Handling\",\n                \"value\": \"yes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"user conversions\"],\n            \"specification\": {},\n            \"description\": \"A time-series dataset capturing e-commerce transaction data with various features, including regional, peak hour, and user-specific data.\",\n            \"preprocessing\": [\n                \"Resampling\",\n                \"Time-series normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Anomaly detection visualizations\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multi-layered Anomaly Detection\",\n            \"family\": \"neural networks\",\n            \"type\": \"time-series models\",\n            \"specification\": {\n                \"architecture\": \"deep learning\",\n                \"number_of_layers\": null,\n                \"feature_engineering\": [\"weekend/holiday detection\", \"regional patterns\", \"peak hour trends\", \"user preference adaptation\"]\n            },\n            \"description\": \"A deep learning model designed for anomaly detection in e-commerce time series, incorporating multi-layered features and adaptive modeling\"\n        }\n    ],\n    \"knowledge\": [\n        \"An understanding of various anomaly detection techniques and their application in real-time e-commerce.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Alert customization\",\n            \"Alarm escalation rules\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"real-time anomaly detection service\",\n        \"inference_engine\": [\n            \"TensorFlow Serving\",\n            \"Apache PredictionIO\"\n        ],\n        \"demonstration\": \"Real-time anomaly visualization dashboard and notifications\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's requirement for a complex time-series anomaly detection system, with a focus on multiple layers, real-time notifications, and extreme data handling. The user's expertise level suggests a deep understanding of the project requirements. Some metrics like values for performance and complexity are left as null, as the user did not provide specific numbers, but the model architecture is appropriately detailed based on the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop an advanced Transformer-based neural machine translation system specifically tailored for translating highly colloquial, regional, and temporally evolving English subcultures into contemporary, slang-infused Russian. The model should surpass a Chrf++ score of 45, not only capturing cultural nuances but also predicting local slang variations with a remarkable precision rate of 90%. Furthermore, ensure the model's compatibility with edge computing, enabling it to deliver translations in real-time with a latency of less than 100 milliseconds in a massive concurrent chatbot deployment, while accommodating for the dynamic expansion of the language dataset in the future.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"Develop an advanced Transformer-based neural machine translation system for translating colloquial, regional, and temporally evolving English subcultures into contemporary, slang-infused Russian.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Chrf++ score\",\n                \"value\": 45\n            },\n            {\n                \"name\": \"Precision rate for local slang prediction\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 100,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Real-time compatibility\",\n                \"value\": true\n            },\n            {\n                \"name\": \"Dynamic dataset expansion support\",\n                \"value\": true\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Russian translation\"],\n            \"specification\": {\n                \"source_language\": \"English\",\n                \"target_language\": \"Russian\",\n                \"data_variability\": \"colloquial, regional, evolving English subcultures\",\n                \"slang_dataset_prevalence\": \"high\"\n            },\n            \"description\": \"A dataset that captures the essence of colloquial, regional, and temporally evolving English subcultures for the Transformer-based model.\",\n            \"preprocessing\": [\"slang normalization\", \"temporal adaptation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based Neural Machine Translation System\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for NMT with colloquial and slang handling\",\n                \"runtime\": \"edge computing compatible\"\n            },\n            \"description\": \"A sophisticated model focusing on capturing cultural nuances and real-time slang prediction with low latency.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"contextual data enrichment\", \"slang adaptation\"],\n        \"target_device\": \"edge computing\",\n        \"deployment_endpoint\": \"massive concurrent chatbot\",\n        \"inference_engine\": [\"real-time translation engine\"],\n        \"demonstration\": \"Leveraging dynamic expansion to maintain translation quality\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's detailed requirements, including advanced performance targets and constraints. The areas of natural language processing, machine translation, and the inclusion of language adaptation techniques demonstrate a comprehensive understanding of the problem. Furthermore, the emphasis on real-time performance and compatibility with edge computing aligns with the user's expectations.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "node classification", "instruction": "For a premier bioinformatics venture, enhance the Hierarchical Attention Graph Neural Network (HAGNN) model for protein-protein interaction (PPI) node classification, leveraging STRING database's extensive, highly connected data. Mandate a stringent precision of 97% while preserving interpretability. The model must be scalable to identify novel protein complexes in real-time, exhibit computational efficiency, and minimize false positives. Also, require the model to incorporate novel edge feature extraction and incorporate a temporal component reflecting protein interaction dynamics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Enhance the Hierarchical Attention Graph Neural Network (HAGNN) for protein-protein interaction (PPI) node classification using STRING database. Aim for 97% precision with interpretability, real-time scalability, computational efficiency, and minimize false positives. Incorporate novel edge feature extraction and a temporal component for protein interaction dynamics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"real-time\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"STRING database\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"protein complexes\"],\n            \"specification\": {\n                \"size\": null,\n                \"density\": null,\n                \"dynamic properties\": {\n                    \"temporal\": true\n                }\n            },\n            \"description\": \"An extensive, highly connected protein-protein interaction dataset for enhancing the HAGNN model.\",\n            \"preprocessing\": [\n                \"novel edge feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"interpretability visualization\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HAGNN\",\n            \"family\": \"Hierarchical Attention Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true,\n                \"temporal_component\": true\n            },\n            \"description\": \"An enhanced HAGNN model specifically tailored for protein-protein interaction (PPI) node classification, leveraging STRING data and incorporating edge feature extraction and temporal dynamics.\"\n        }\n    ],\n    \"knowledge\": [\n        \"STRING database offers extensive data connectivity and temporal protein interaction information.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for computational efficiency\"\n        ],\n        \"demonstration\": \"Demonstration of real-time protein complex identification\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON reflects the provided user instruction, focusing on enhancing an HAGNN model for PPI, with high precision requirements and specific complexities. Novel edge features and temporal dynamics are incorporated. Some fields have defaults, such as inference time or target device, but assumptions are made based on the instructions (e.g., computational efficiency). The confidence score is set to 0.9, reflecting confidence in the interpretation of the user's requirements.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "question answering", "instruction": "Develop a sophisticated medical QA framework that integrates advanced MIMIC-III and PubMedBERT hybrid architecture, focusing on instant and precise identification of esoteric diseases with intricate presentations. The system should not only generate precise answers but require the model to conduct multi-step reasoning, rank differential diagnoses based on evidence from diverse, expert-validated articles, and generate detailedbibliographies for traceability. Additionally, ensure model explainability to facilitate collaboration between AI and human medical experts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medicine\",\n        \"description\": \"Design a sophisticated medical QA framework utilizing MIMIC-III and PubMedBERT hybrid architecture, targeting esoteric diseases with complex presentations. The system should achieve accurate, multi-step reasoning for disease identification, rank differential diagnoses based on expert-validated articles, and generate detailed bibliographies for traceability. Emphasis on model explainability to promote collaboration between AI and medical professionals.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"answer precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"multi-step reasoning accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MIMIC-III\",\n            \"modality\": [\"text\", \"time series\"],\n            \"target_variables\": [\"medical concepts\", \"differential diagnoses\"],\n            \"specification\": null,\n            \"description\": \"A dataset combining electronic health records and clinical notes from MIMIC-III for complex medical QA tasks.\",\n            \"preprocessing\": [\n                \"text normalization\",\n                \"time-series feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"PubMedBERT's Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"A diverse set of articles from PubMed for expert-validated content and ranking.\",\n            \"preprocessing\": [\"sentence tokenization\", \"BERT encoding\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MIMIC-III PubMedBERT Hybrid\",\n            \"family\": \"neural networks\",\n            \"type\": \"hybrid architecture\",\n            \"specification\": {\n                \"architecture\": \"MIMIC-III + PubMedBERT\",\n                \"parameters\": \"explanatory, multi-hop reasoning\"\n            },\n            \"description\": \"A QA model integrating MIMIC-III EHR data and PubMedBERT for esoteric disease identification and reasoning.\"\n        }\n    ],\n    \"knowledge\": [\n        \"emphasis on explainability via attention mechanisms or model interpretability techniques\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual encoding\",\n            \"evidence ranking\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"AI-assisted medical consultation platform\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"interactive, real-time consultations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response parses the user's requirements, specifying the hybrid MIMIC-III and PubMedBERT model, its focus on esoteric diseases, multi-step reasoning, and explainability. Metrics like precision and inference time are expected but not given specific values. The system is designed to integrate multiple datasets and perform preprocessing for a robust medical QA system. The user's high expertise level justifies including more specific design details for the model and deployment.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for a premier AI development team, you're tasked with architecting an advanced time-series forecasting system that pushes the boundaries of predictive analytics. The project necessitates the following intricate steps:\r\n\r\n1. Design a hybrid model, coupling Prophet and a state-of-the-art deep residual-CNN, to capture both transient and persistent trends, considering historical seasonal patterns and complex spatial dependencies across 100 global megacities.\r\n2. Implement a two-stage anomaly detection system: real-time monitoring and historical context-based analysis, safeguarding against extreme weather events and data anomalies with precision.\r\n3. Develop a modular learning pipeline that leverages distributed computing for continuous, streaming integration of live meteorological data, satellite imagery, and ground-level sensors, enabling dynamic and adaptive forecasts.\r\n4. Create an interactive, data-driven dashboard with an intuitive dashboard, providing non-technical stakeholders with easy-to-understand visualizations for temperature forecasts, energy consumption forecasts, and customized cooling infrastructure recommendations, featuring interactive, Jupyter Notebook-style explanations of key influencers.\r\n5. Rigorously test the ensemble model's forecasting prowess using nested cross-validation and holdout techniques, comparing its performance to bleeding-edge baselines such as stacked LSTM and hierarchical XGBoost on a diverse, temporally stratified dataset.\r\n6. Conduct an in-depth analysis, presenting a comprehensive report that includes model explainability through partial dependence plots, probabilistic uncertainty decomposition, and a thorough examination of potential biases introduced by integrating multiple heterogeneous data sources, along with recommendations for future data assimilation strategies.\r\n\r\nChallenge yourself to deliver a top-notch, adaptable, and user-friendly solution that accurately forecasts urban heat dynamics in the face of rapidly changing environmental factors, while ensuring scalability and tangible utility for large-scale energy and urban planning initiatives.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy planning, urban planning, and environmental management\",\n        \"description\": \"Design an advanced time-series forecasting system using a hybrid model combining Prophet and deep residual-CNN, addressing seasonal patterns and spatial dependencies across 100 global megacities. Implement a two-stage anomaly detection system, a distributed learning pipeline, an interactive dashboard, and conduct comprehensive testing and analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MAE (Mean Absolute Error)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MSE (Mean Squared Error)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSE (Root Mean Squared Error)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Scalability\",\n                \"value\": null,\n                \"unit\": \"Large Scale\"\n            },\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"Milliseconds\"\n            },\n            {\n                \"name\": \"Memory Efficiency\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Historical data from 100 global megacities\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"temperature\", \"energy consumption\", \"cooling infrastructure\"],\n            \"specification\": {\n                \"time_series_length\": null,\n                \"feature_dimensions\": null,\n                \"data_resolution\": null\n            },\n            \"description\": \"Time-series data with historical seasonal patterns from various sources such as meteorological data, satellite imagery, and ground-level sensors across 100 megacities.\",\n            \"preprocessing\": [\"seasonal decomposition\", \"feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [\"temporal patterns\", \"cross-city comparisons\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Model (Prophet + Residual-CNN)\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"Prophet + deep residual layers\",\n                \"integrative_capacity\": \"100 megacities\",\n                \"spatial_analysis\": \"true\",\n                \"temporal_complexity\": null\n            },\n            \"description\": \"A hybrid model that leverages Prophet for time-series forecasting and a deep residual-CNN for capturing complex spatial dependencies.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"meteorological data preprocessing\", \"sensor data fusion\"],\n        \"target_device\": \"distributed computing\",\n        \"deployment_endpoint\": \"cloud-based\",\n        \"inference_engine\": [\"real-time monitoring\", \"historical analysis\"],\n        \"demonstration\": \"Jupyter Notebook-style explanations\"\n    },\n    \"knowledge\": [\n        \"Model explainability techniques\",\n        \"Uncertainty decomposition\",\n        \"Potential biases from data integration\"\n    ],\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the high-level details of the time-series forecasting project based on the instruction, incorporating the requested hybrid model, anomaly detection, learning pipeline, interactive dashboard, and performance evaluation. The model, complexity metrics, and feature engineering are tailored to the project requirements. However, missing performance targets indicate the need for a more detailed description or expectation from the user for these metrics.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "Act as a project manager for a cutting-edge AI-driven finance firm, requiring the design of a state-of-the-art tabular regression model. This model must integrate advanced algorithms, handle multiple predictors (including project intricacy, resource optimization, and real-time market fluctuations), and meet a stringent 99% accuracy benchmark, exclusively for high-impact projects with stringent regulatory compliance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Design a state-of-the-art tabular regression model for an AI-driven finance firm, integrating advanced algorithms, handling multiple predictors (project intricacy, resource optimization, and real-time market fluctuations), and aiming for 99% accuracy, exclusively for high-impact projects with strict regulatory compliance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Model complexity\",\n                \"value\": \"optimized\"\n            },\n            {\n                \"name\": \"Real-time performance\",\n                \"unit\": \"milliseconds\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Regulatory compliance\",\n                \"value\": \"strict\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AI-driven finance firm data\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"impact\", \"project complexities\", \"resource utilization\", \"real-time market indicators\"],\n            \"specification\": {\n                \"dimensions\": {\n                    \"rows\": \"large\",\n                    \"columns\": \"variable\"\n                },\n                \"data_types\": \"structured\"\n            },\n            \"description\": \"A comprehensive dataset with multiple predictors for financial projects, including project intricacy, resource optimization, and real-time market fluctuations.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"feature scaling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Tabular Regression Model\",\n            \"family\": \"Advanced Deep Learning Algorithms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"optimizer\": \"custom\",\n                \"loss_function\": \"custom\"\n            },\n            \"description\": \"A high-performance model integrating deep learning algorithms designed for handling multi-dimensional financial data and meeting the 99% accuracy target.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Recent trends in financial data analysis and interpretability techniques to ensure regulatory compliance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Feature selection for high-impact features\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"firm's secure data center\",\n        \"inference_engine\": [\"GPU-powered, optimized for real-time predictions\"],\n        \"demonstration\": \"Interactive data-driven reports and predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the project manager's role, indicating a high expertise level. It addresses the tabular regression task, integrating advanced algorithms, and specifies a 99% accuracy goal. Performance metrics and complexity constraints are well-defined, while acknowledging the need for handling high-impact projects and strict regulatory requirements. Data sources, preprocessing, and model architecture align with the provided information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "Incorporate a novel adaptation of the Conformal Prediction Theory into the Aeroacoustics Anomaly Detection project, requiring the time-series classification model to not only employ STFT and HTCN for aerodynamic noise analysis, but also dynamically adjust prediction intervals based on real-time high-speed data fluctuations. The model must exhibit enhanced precision, recall, and DET curve analysis, while distinguishing subsonic and supersonic turbulence with improved accuracy. Additionally, emphasize the need for energy-efficient hardware optimization to ensure real-time processing is maintained without compromising robustness under extreme conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"aeroacoustics\",\n        \"description\": \"Incorporate a novel adaptation of Conformal Prediction Theory into an Aeroacoustics Anomaly Detection project. The model should use STFT and HTCN for aerodynamic noise analysis, dynamically adjust prediction intervals based on real-time data fluctuations, and exhibit enhanced precision, recall, and DET curve analysis for distinguishing subsonic and supersonic turbulence with improved accuracy. Emphasis is on energy-efficient hardware optimization for real-time processing under extreme conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"DET curve analysis\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"% improvement in energy consumption\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Aeroacoustics time-series data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"anomaly detection\", \"subsonic vs supersonic turbulence\"],\n            \"specification\": {\n                \"length\": null,\n                \"dimension\": null,\n                \"sampling_rate\": null\n            },\n            \"description\": \"Dataset for aerodynamic noise analysis, containing time-series data for STFT and HTCN processing.\",\n            \"preprocessing\": [\n                \"STFT\",\n                \"HTCN\"\n            ],\n            \"augmentation\": [\"dynamic data fluctuations\"],\n            \"visualization\": [\"anomaly detection patterns\", \"precision-recall curves\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Conformal Prediction Theory-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"algorithm\": \"novel adaptation of Conformal Prediction\",\n                \"model_structure\": \"ensemble with STFT and HTCN\",\n                \"energy_consumption\": \"optimized for real-time\"\n            },\n            \"description\": \"A time-series classification model using Conformal Prediction, tailored for aerodynamic anomaly detection and efficient hardware utilization.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"energy-efficient hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": \"real-time processing with improved accuracy and DET analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response reflects the user's intention to build a model using Conformal Prediction Theory with a focus on a real-time Aeroacoustics Anomaly Detection project. The request specifies specific requirements for the model's performance, such as precision, recall, DET curve analysis, and distinguishing subsonic and supersonic turbulence. It also emphasizes energy efficiency for maintaining real-time processing under extreme conditions. There are areas like model name and performance metric values which are not specified here as they would be dependent on the user's specific adaptations of the Conformal Prediction Theory.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "In the realm of interdisciplinary research, develop a cutting-edge Node Embedding and Multi-Task Learning (N-EMTL) model for a unique inter-collaborator network on the hybrid MixAuthorship-Field dataset. The model should identify and classify among 75 distinct sub-disciplines within the science, technology, engineering, and mathematics (STEM) spectrum, optimizing for interpretability while capturing temporal citation dynamics and co-authorship collaboration density. Additionally, incorporate ethical considerations for bias mitigation and data privacy in the model's design and evaluation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"interdisciplinary research\",\n        \"description\": \"Develop a cutting-edge Node Embedding and Multi-Task Learning (N-EMTL) model for a unique inter-collaborator network on the hybrid MixAuthorship-Field dataset, targeting classification into 75 distinct STEM sub-disciplines, prioritizing interpretability, temporal citation dynamics, and co-authorship collaboration density.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Interpretability Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Bias Mitigation Effectiveness\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"Data Privacy Preservation\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"MixAuthorship-Field dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"75 distinct sub-disciplines\"],\n            \"specification\": {\n                \"network_type\": \"hybrid\",\n                \"temporal\": true,\n                \"collaboration_density\": true\n            },\n            \"description\": \"A hybrid dataset representing an inter-collaborator network with temporal citation dynamics and co-authorship collaboration density.\",\n            \"preprocessing\": [\"bias mitigation\", \"data anonymization\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"interpretability visualization of model decisions\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"N-EMTL\",\n            \"family\": \"Node Embedding & Multi-Task Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"custom N-EMTL design\",\n                \"interpretability_focused\": true\n            },\n            \"description\": \"A cutting-edge model that incorporates Node Embedding and Multi-Task Learning for node classification on the MixAuthorship-Field dataset, addressing STEM sub-disciplines, temporal dynamics, and ethical considerations.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Consider incorporating domain-specific fairness algorithms and privacy-preserving techniques for data aggregation.\"\n        },\n        {\n            \"text\": \"Study and adaptively weight tasks to balance interpretability with task performance.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection for interpretability\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom AI platform or repository\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"interactive model explanation dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intent to build a high-level, advanced model. The problem definition includes specific details about the area, task, application domain, and requirements for interpretability, dynamics, and ethics. Performance metrics have been included, but specific values are not provided as they require model testing. Complexity metrics address the ethical and computational aspects. The dataset and model descriptions match the details provided in the instruction, and the confidence score is high given the comprehensive coverage of the user's request.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Design an innovative, real-time adaptive time series forecasting framework for the app's daily active users (DAU) by merging deep ARIMA and LSTM models. Incorporate a multi-step reasoning approach that explicitly discerns and handles weekly, quarterly, and transient data anomalies. Implement a rigorous 4-week moving block validation scheme to evaluate model resilience against seasonal fluctuations, ensuring high accuracy under varying market conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"user behavior analytics\",\n        \"description\": \"Design an innovative real-time adaptive time series forecasting framework for daily active users (DAU) using a merged deep ARIMA and LSTM model. Incorporate multi-step reasoning to handle weekly, quarterly, and transient anomalies. Implement a 4-week moving block validation for seasonal fluctuations and high accuracy under varying market conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"App Daily Active Users\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"DAU\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing historical DAU data for the app, with varying seasonal patterns and anomalies to be forecast.\",\n            \"preprocessing\": [\"data cleaning\", \"resampling\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"anomaly detection\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Hybrid (ARIMA + LSTM)\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"layers\": [\n                    {\n                        \"type\": \"LSTM\",\n                        \"units\": null,\n                        \"dropout\": null,\n                        \"return_sequences\": true\n                    },\n                    {\n                        \"type\": \"ARIMA\",\n                        \"p\": null,\n                        \"d\": null,\n                        \"q\": null\n                    }\n                ],\n                \"computation\": \"real-time\"\n            },\n            \"description\": \"A novel time series forecasting model that combines deep ARIMA and LSTM for DAU prediction with multi-step reasoning and adaptive anomaly handling.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating multi-step reasoning for handling various data patterns\",\n        \"Anomaly detection and handling strategies\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonality extraction\",\n            \"interpolation for missing data\"\n        ],\n        \"target_device\": \"real-time\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"4-week rolling validation results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's desire to build a forecasting framework, using high-level expertise. It specifies the time series analysis area, the details of the deep ARIMA and LSTM model, and the performance metrics under varying conditions. The rigorous validation and anomaly handling methods are captured, along with deployment requirements. Some elements like model specifics and performance values are left unspecified due to the lack of numerical targets.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for a multi-disciplinary team, you're tasked with developing a state-of-the-art image classification system capable of differentiating between 50 rare and diverse cat breeds in a large, high-resolution dataset. In addition to selecting a robust model, propose a transfer learning strategy and a novel data augmentation technique to enhance accuracy and efficiency in the face of limited labeled images.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal biology, image recognition\",\n        \"description\": \"Develop a state-of-the-art image classification system for differentiating between 50 rare and diverse cat breeds in a large, high-resolution dataset. Model selection should focus on robustness. The project should incorporate transfer learning and propose a novel data augmentation technique to improve accuracy and efficiency given limited labeled data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Rare Cat Breeds Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"cat breed\"],\n            \"specification\": {\n                \"size\": {\"total_images\": null, \"labeled_images\": null},\n                \"dimension\": {\"width\": null, \"height\": null},\n                \"high_resolution\": true\n            },\n            \"description\": \"Large dataset containing high-resolution images of 50 rare and diverse cat breeds, with limited labeled data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\n                {\"technique\": \"Proposed Novel Augmentation Technique for Limited Data\"}\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"model_depth\": null,\n                    \"transfer_learning\": true\n                },\n                \"performance\": {\n                    \"initial_accuracy\": null\n                }\n            },\n            \"description\": \"A robust image classification model using state-of-the-art techniques, incorporating transfer learning and a novel data augmentation strategy for improved performance on the rare cat breeds dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Strategies to leverage existing pre-trained models and fine-tune them for specific cat breed recognition, as well as the importance of novel data augmentation in compensating for limited labeled data.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Fine-tuning and domain adaptation of pre-trained model\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-accelerated inference\"],\n        \"demonstration\": \"Interactive dashboard showcasing model performance on unseen images\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's instruction for a multi-disciplinary team project, including the domain, task, and constraints (limited labeled data). It outlines specific performance metrics, complexity constraints, and details about model selection, data augmentation, and transfer learning. The model and dataset sections reflect the requirements for a complex image classification problem. Some open-ended fields like model name and depth are left unspecified for the user's flexibility.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "For the machine learning team, develop a cutting-edge multimodal forgery detection system tailored for academic certificates. Mandate the inclusion of advanced image-to-text and text-to-image comparisons, live cross-referencing with internationally verified data sources, and the implementation of blockchain technology for immutable provenance tracking in the face of sophisticated digital forgeries in the higher education sector.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"higher education\",\n        \"description\": \"Develop a cutting-edge multimodal forgery detection system for academic certificates, focusing on advanced image-to-text and text-to-image comparisons, live cross-referencing with internationally verified data sources, and incorporating blockchain technology for immutable provenance tracking.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Forgery detection accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Speed (live cross-referencing)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Blockchain transaction time\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time on complex cases\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Academic certificate multimodal data\",\n            \"modality\": [\"image\", \"text\"],\n            \"target_variables\": [\"Forgery status\"],\n            \"specification\": null,\n            \"description\": \"Dataset containing authentic and forged certificates for multimodal analysis, including image and text features.\",\n            \"preprocessing\": [\n                \"Image enhancement\",\n                \"Text extraction\"\n            ],\n            \"augmentation\": [\n                \"Adversarial examples\"\n            ],\n            \"visualization\": [\n                \"Cross-modal feature maps\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced multimodal detection\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A state-of-the-art multimodal forgery detection system for academic certificates, incorporating advanced image-to-text and text-to-image comparisons.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Current research trends in multimodal forgery detection\",\n        \"Best practices for blockchain integration in this context\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Custom feature embeddings\"\n        ],\n        \"target_device\": \"Cloud\",\n        \"deployment_endpoint\": \"Secure certification platform\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Blockchain API\"],\n        \"demonstration\": \"Custom-designed user interface\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes all aspects of the user's instruction, specifying the cutting-edge nature of the project, the requirements for advanced comparisons and data source verification, and the integration of blockchain. Performance metrics, complexity considerations, and specific techniques for feature engineering and deployment are included. The only missing information is the specific model metrics, which would typically depend on the chosen model architecture and training.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop an advanced, explainable node classification model for the YouTube Algebric Connectivity dataset using Enhanced Significance Graph Neural Networks (ESGNNs) with a novel, edge-weighted feature fusion mechanism. Achieve a benchmark accuracy of 90% while demonstrating fine-grained genre distinction among content creators through recursive feature engineering and hierarchical reasoning. Scrutinize the model's effectiveness by implementing state-of-the-art baselines, and provide a detailed explanation of why ESGNNs offer superior performance in sparse data analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"social media\",\n        \"description\": \"Develop an advanced node classification model using Enhanced Significance Graph Neural Networks (ESGNNs) with a novel edge-weighted feature fusion mechanism for the YouTube Algebric Connectivity dataset. Aim for benchmark accuracy of 90% and highlight fine-grained genre distinction among content creators. Include recursive feature engineering and hierarchical reasoning. Compare model's effectiveness against state-of-the-art baselines and provide an explanation for ESGNNs' superiority in sparse data analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube Algebric Connectivity dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"content creator genres\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"dimension\": \"variable\",\n                    \"edges\": \"variable\"\n                },\n                \"edge_weighted\": true\n            },\n            \"description\": \"A social network dataset for YouTube content creators, where nodes represent users and edges represent connections. The task involves classifying content creators by their genres.\",\n            \"preprocessing\": [\"Recursive feature engineering\", \"Hierarchical graph construction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ESGNN\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_fusion\": \"edge-weighted\",\n                \"novelty\": \"Enhanced Significance\"\n            },\n            \"description\": \"Advanced node classification model using ESGNNs, leveraging edge-weighted feature fusion and showing strong performance in sparse data.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"explanation\": \"ESGNNs superior in sparse data analysis due to their ability to emphasize significance and handle noisy connections more effectively.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Illustrate fine-grained genre distinction on a subset of content creators.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON reflects the user's request for a complex model development project. The model's specifications, dataset details, and metrics cater to the requirements specified, including accuracy, feature engineering, and ESGNN's superiority in sparse data analysis. The user's expertise level indicates a deep understanding of the project. The model comparison and performance explanation are also included.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "As a project manager, mandate the machine learning team to develop an enhanced link prediction model for Citeseer that not only incorporates a state-of-the-art feature attribution technique but also includes a dynamic interpretability dashboard. The model must demonstrate a consistent precision above 90% across various subdomains, handle sparse datasets efficiently, and showcase high efficiency in scaling to process terabytes of expanding, unstructured scholarly citations, ensuring real-time updates and maintaining data privacy regulations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"scholarly research\",\n        \"description\": \"Develop an enhanced link prediction model for Citeseer, incorporating a state-of-the-art feature attribution technique and a dynamic interpretability dashboard. The model must have consistent precision above 90% across subdomains, handle sparse datasets efficiently, and be scalable for terabytes of expanding, unstructured citations, ensuring real-time updates and adherence to data privacy regulations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"efficiency on sparse datasets\",\n                \"value\": null\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"teraBytes\",\n                \"unit\": \"data size\"\n            },\n            {\n                \"name\": \"real-time updates\",\n                \"value\": \"true\"\n            },\n            {\n                \"name\": \"data privacy\",\n                \"value\": \"yes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Citeseer graph dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"link presence\"],\n            \"specification\": null,\n            \"description\": \"A scholarly citations dataset for link prediction, requiring efficient handling of sparse data and scalability for terabytes of expanding data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Link Prediction Model\",\n            \"family\": \"State-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_attribution\": \"state-of-the-art technique\",\n                \"interpretability_dashboard\": \"dynamic\"\n            },\n            \"description\": \"A machine learning model for link prediction in Citeseer, incorporating a feature attribution technique and a dynamic interpretability dashboard, targeting precision above 90% and scalability for large, unstructured scholarly datasets.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time updates, interpretability dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's request for a high-end task, reflecting the expertise level of a project manager. It specifically outlines the model requirements, performance goals, and data handling aspects to align with the given instructions. The emphasis on precision, sparse datasets, scalability, and data privacy complies with the given information. Missing details, like exact feature attribution technique and metric values, can be filled with the corresponding state-of-the-art techniques available.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series classification", "instruction": "As a project manager, you must now lead the time-series classification team to develop a cutting-edge machine learning system that not only discerns and predicts complex kinesthetic patterns using the UWave Gesture Dataset v2.0 but also distinguishes subtle nuances among different users based on their unique sensor data profiles. Enhance the existing fusion model (LSTMs, bi-directional encoders, and multi-head attention) by incorporating long-term and short-term context dependencies, achieving a minimum precision of 98% on unseen, real-time gestures from diverse users, while maintaining model interpretability, latency below 10 milliseconds, and ensuring it adapts dynamically to new gesture variations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"human-kinetics or gesture recognition\",\n        \"description\": \"Develop a machine learning system for complex kinesthetic pattern recognition and user-specific differentiation using the UWave Gesture Dataset v2.0. Improve the fusion model (LSTMs, bi-directional encoders, and multi-head attention) for long-term and short-term context dependencies. Aim for a minimum precision of 98% on unseen real-time gestures, maintaining interpretability, latency below 10 milliseconds, and dynamic adaptation to new gesture variations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 10,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UWave Gesture Dataset v2.0\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"kinesthetic patterns\", \"user profiles\"],\n            \"specification\": {\n                \"data_format\": \"sensor readings\",\n                \"sample_rate\": \"user-defined\",\n                \"dimensions\": \"variable\"\n            },\n            \"description\": \"A dataset for gesture recognition containing complex kinesthetic patterns and unique sensor data profiles for individual users.\",\n            \"preprocessing\": [\"data normalization\", \"user profiling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Fusion Model\",\n            \"family\": \"LSTMs, bi-directional encoders, and multi-head attention\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"adaptability\": \"dynamic\",\n                \"context_dependencies\": \"long-term & short-term\"\n            },\n            \"description\": \"A cutting-edge machine learning system that incorporates advanced techniques for kinesthetic pattern recognition and user-specific differentiation.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Leverage previous research on interpreting time-series models and handling user-specific data for interpretability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"user-specific feature extraction\"],\n        \"target_device\": \"real-time, low-latency systems\",\n        \"deployment_endpoint\": \"custom API for dynamic gesture recognition\",\n        \"inference_engine\": [\"optimized for latency, e.g., TensorFlow Lite\"],\n        \"demonstration\": \"Create a live demo showcasing real-time gesture recognition with high precision and adaptability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the user's requirements, with a focus on time-series classification, performance metrics, and system design aspects, such as adaptability and interpretability. The user's high expertise level suggests a thorough understanding of the project requirements. However, some details like sample rate and exact model specifications are assumed based on typical requirements for such a task and would be provided by the user. The latency value is given in milliseconds, and performance metrics are set at a high level without a specific method for achieving the target precision.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As the project supervisor for the specialized high-frequency financial data analysis team, your directive is to refine the proprietary \"Quantitative Financial Trends Classification Corpus\" by segmenting financial news into a complex 50-tier taxonomy (e.g., nano-cap market trends, sustainable investment partnerships, and deep tech fintech disruptors). Task the cutting-edge FinBERTv3 model, targeting at least 95% precision for detecting disruptive market events with ultra-low latency, while maintaining an ensemble F1 score of 90% specifically for niche areas. Emphasize on minimizing false alarms in obscure sectors without compromising the overall accuracy, and ensure completion within a rigorous 45-day agile development cycle. Innovate a hybrid evaluation framework that combines F1, precision-recall trade-offs, and economic loss-based metrics to achieve optimal balance across each sub-domain while accounting for resource allocation in real-time.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"financial analysis\",\n        \"description\": \"Refine the 'Quantitative Financial Trends Classification Corpus' by segmenting financial news into a 50-tier taxonomy, using FinBERTv3 model. Aim for at least 95% precision in detecting disruptive market events with ultra-low latency, while maintaining an ensemble F1 score of 90% in niche areas, minimizing false alarms in obscure sectors, and staying within a 45-day agile development cycle.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"Ensemble F1 Score\",\n                \"value\": 0.90\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": null,\n                \"unit\": \"microseconds\"\n            },\n            {\n                \"name\": \"False Alarm Rate (Obscure Sectors)\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Quantitative Financial Trends Classification Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Market Events\"],\n            \"specification\": {\n                \"size\": \"custom\",\n                \"complexity\": \"complex\",\n                \"finance-specific\": true\n            },\n            \"description\": \"A large-scale financial news dataset to be refined for a high-frequency financial data analysis project.\",\n            \"preprocessing\": [\"FinBERTv3 fine-tuning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"FinBERTv3\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"FinBERTv3\",\n                \"specific_params\": \"for financial event detection\"\n            },\n            \"description\": \"A cutting-edge model for financial news text classification, designed for detecting disruptive market events with high precision and specialized F1 scores.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Custom taxonomy and target event mapping\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"proprietary platform\",\n        \"inference_engine\": [\"Real-time, low-latency inference\"],\n        \"demonstration\": \"Combines F1, precision-recall trade-offs, and economic loss-based evaluations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response captures the essence of the project's requirements, including a high-level build intent with a high level of expertise, specific needs for the area and task in NLP, and the task of refining a complex text classification corpus. Performance metrics align with the desired precision and F1 scores, while emphasizing on ultra-low latency and preventing false alarms. The 45-day development cycle is noted, as well as the hybrid evaluation framework. However, the absence of strict values for latency and false alarms implies room for input from the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a visionary project manager for a cutting-edge materials R&D team, design an advanced tabular regression model using the extensive Mohs Hardness Dataset. Implement a novel nested cross-validation approach with an extra 20% stratified split, ensuring optimal generalization and a minimum RMSPE of 0.05% compared to current industry benchmarks. Furthermore, develop a multifaceted model interpretability strategy, including SHAPley Additive Explanations and LIME, to uncover intricate relationships and facilitate actionable scientific revelations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"materials science\",\n        \"description\": \"Design an advanced tabular regression model using the Mohs Hardness Dataset. Implement a novel nested cross-validation approach with an extra 20% stratified split for optimal generalization. Aim for a minimum RMSPE of 0.05% compared to current industry benchmarks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSPE\",\n                \"value\": 0.0005\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mohs Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"hardness\"],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"dimensions\": {\n                    \"features\": {\"type\": \"integer\", \"value\": null},\n                    \"target\": {\"type\": \"integer\", \"value\": null}\n                },\n                \"data_structure\": \"structured\"\n            },\n            \"description\": \"Dataset for advanced tabular regression on Mohs Hardness values.\",\n            \"preprocessing\": [\"data cleaning, normalization\", \"handling missing values\"],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance plots\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Tabular Regression Model\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"novel, likely deep learning or ensemble\",\n                \"complexity\": null\n            },\n            \"description\": \"A tabular regression model utilizing a novel nested cross-validation approach and advanced interpretability techniques\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"explanation\": \"Exploration of SHAPley Additive Explanations (SHAP) for feature importance and LIME for local feature explanations.\"\n        },\n        {\n            \"goal\": \"Scientific insights and actionable recommendations based on intricate relationship discoveries in the data.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"novel feature creation\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-based\", \"optimized for interpretability\"],\n        \"demonstration\": \"Scientific presentations or reports highlighting model performance and insights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's expertise, the problem definition focusing on tabular regression with the Mohs Hardness Dataset. The performance metric, RMSPE of 0.05%, is clearly stated. The extra 20% stratified split is mentioned, as well as the requirements for model interpretability. The dataset details are aligned with the advanced nature of the project, and the model family is assumed to be neural networks due to the complexity of the task. The user's expected service outcomes, such as the development of a novel feature and the importance of interpretability, are included. The confidence score reflects the high level of detail and complexity of the task, with potential缺失 information due to the novel technique details.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "machine translation", "instruction": "Design an advanced NMT model, employing Transformer-XL with a focus on integrating context-aware techniques for handling rare and domain-specific slang, idioms, and colloquial expressions from the expansive and varied OPUS-XL dataset. Aim for a precision-oriented BLEU score of 38, simultaneously optimizing for ultra-fast real-time translation with a tight latency constraint of 200 milliseconds to ensure uninterrupted user experience in massive, latency-sensitive chat platforms operating at peak usage.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"communication\",\n        \"description\": \"Design an advanced NMT model using Transformer-XL, with a focus on context-aware techniques for handling rare and domain-specific slang, idioms, and colloquial expressions from the OPUS-XL dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 38\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 200,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS-XL dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"size\": \"extensive\",\n                \"diversity\": \"rare and domain-specific expressions\"\n            },\n            \"description\": \"A large and varied dataset for machine translation with an emphasis on slang, idioms, and colloquial expressions.\",\n            \"preprocessing\": [\"context-aware text cleaning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"context_length\": \"XL (extendable)\",\n                \"context-aware techniques\": \"for handling slang, idioms, and colloquial expressions\"\n            },\n            \"description\": \"An advanced NMT model based on Transformer-XL with a focus on ultra-fast and real-time translation with low latency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Context-awareness is crucial for handling specialized language in OPUS-XL.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"context-aware translation layer\"\n        ],\n        \"target_device\": \"ultra-fast, latency-sensitive\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for real-time performance\"\n        ],\n        \"demonstration\": \"Latency test on massive chat platforms during peak usage\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON includes the user's intent to build an advanced model. The expert level indicates a strong understanding of the requirements. The description and performance metrics focus on the specifics of the NMT model with Transformer-XL, precision-oriented BLEU score, and real-time translation with a tight latency constraint. The context-aware techniques for handling OPUS-XL's specialized language and the service section mentioning fast deployment and demonstration clearly cover the user's goal.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Design an advanced visual question-answering platform tailored to remote and isolated regions' extreme wildlife conservation documentaries captured through high-resolution underwater drone footage. Integrate a cutting-edge Quantum FusionNet architecture, seamlessly merging cross-modal quantum embeddings and dynamic attention mechanisms, to parse not only visual information but also interpret encrypted ecological data. The system should decode complex ecological challenges, recognize biomarkers in real-time, and anticipate potential ecological disruptions within micro-ecosystems, all while maintaining performance under low-bandwidth conditions and powering limited-resource devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"Design an advanced visual question-answering platform for remote and isolated regions' wildlife conservation documentaries captured via high-resolution underwater drone footage. The platform should incorporate Quantum FusionNet, fusing cross-modal quantum embeddings and dynamic attention mechanisms for both visual and encrypted ecological data interpretation. It should analyze ecological challenges, real-time biomarker recognition, and anticipate disruptions in micro-ecosystems. Performance should prioritize low-bandwidth and limited resource settings.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy under low-bandwidth conditions\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time biomarker recognition rate\",\n                \"value\": null\n            },\n            {\n                \"name\": \"micro-ecosystem disruption prediction accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time under low-bandwidth\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage for limited-resource devices\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Underwater wildlife纪录片 dataset\",\n            \"modality\": [\"image\", \"audio\", \"text\"],\n            \"target_variables\": [\"encrypted ecological data\", \"biomarkers\", \"micro-ecosystem characteristics\"],\n            \"specification\": null,\n            \"description\": \"Dataset capturing high-resolution underwater drone footage of remote wildlife, along with encrypted ecological data and associated metadata.\",\n            \"preprocessing\": [\n                \"quantum encoding\",\n                \"cross-modal data fusion\"\n            ],\n            \"augmentation\": [\n                \"low-bandwidth data compression\",\n                \"real-time encryption decryption\"\n            ],\n            \"visualization\": [\n                \"real-time biomarker detection visualization\",\n                \"micro-ecosystem state monitoring\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Quantum FusionNet\",\n            \"family\": \"Quantum multimodal neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Quantum FusionNet with cross-modal embeddings and dynamic attention mechanisms\",\n                \"quantum_embedding_depth\": null,\n                \"attention_heads\": null\n            },\n            \"description\": \"Advanced QNN architecture for parsing visual and ecological data from underwater drone footage.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Quantum FusionNet's advantages in processing encrypted ecological data and low-bandwidth requirements\",\n        \"Techniques for optimizing performance in limited resource environments\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"extracting relevant features from encrypted data\",\n            \"micro-ecosystem feature extraction\"\n        ],\n        \"target_device\": \"limited-resource devices\",\n        \"deployment_endpoint\": \"remote, low-bandwidth environments\",\n        \"inference_engine\": [\"Quantum-native inference\"],\n        \"demonstration\": \"real-time, low-latency interaction with conservation researchers\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the user's high-level expertise and the specific requirements for a visual question-answering platform. The task, description, and performance/complexity metrics address the given constraints on remote settings and low-resource devices. Metrics are not hard-coded, reflecting the need for flexibility under varying conditions. The dataset covers the necessary modalities and includes preprocessing and augmentation steps. The model is tailored for the task and considers the unique challenges (encrypted ecological data and low-bandwidth). The service and demonstration aspects are also aligned with the scenario.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a healthcare AI consultant, design a cutting-edge question-answering system integrating BioASQ for disease diagnosis, F1-score enhancement, and entity disambiguation tailored to medical specialties. Implement continuous learning from live PubMed feeds, while ensuring patient privacy with real-time privacy-preserving algorithms and strict HIPAA compliance, including data encryption at rest and in transit.", "resposne": "{\n    \"user\": {\n        \"intent\": \"consult-build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Design a cutting-edge question-answering system integrating BioASQ for disease diagnosis, focusing on F1-score enhancement and entity disambiguation in the medical domain.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time privacy-preserving algorithms\",\n                \"value\": null,\n                \"unit\": \"HIPAA compliant\"\n            },\n            {\n                \"name\": \"data encryption at rest and in transit\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"type\": \"medical literature\",\n                \"task\": \"disease diagnosis\"\n            },\n            \"description\": \"A dataset for medical question answering, incorporating disease diagnosis from live PubMed feeds.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"medical QA model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"continuous learning\": {\n                    \"data source\": \"PubMed live feeds\"\n                },\n                \"entity disambiguation\": true\n            },\n            \"description\": \"A model that integrates BioASQ, focuses on F1-score improvement, and handles entity disambiguation for medical question answering.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Strict adherence to HIPAA regulations for patient privacy.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            {\n                \"technique\": \"continuous learning\"\n            }\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"privacy-preserving algorithms\"\n        ],\n        \"demonstration\": \"HIPAA compliant\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the consult-build intent with a high level of expertise. It specifies the area of NLP, question answering, and healthcare application domain. Performance metrics include F1-score, and complexities involve real-time privacy and HIPAA compliance. The user's goal for continuous learning from live PubMed feeds and data security measures are reflected, while the lack of specific metrics or model details allows room for consultation. A key piece of knowledge is included to ensure privacy compliance.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "As the project director of a pioneering ML research group, devise a next-generation deepfake detection system utilizing novel fusion of spectro-temporal and spatial features in the MesoInception-Fusion architecture. Challenge the team to enhance transformer models by integrating multimodal inputs from facial expressions, audio frequencies, and scene text in theCeleb-DF+EXT extended dataset. The objective is not only to surpass state-of-the-art accuracy but also to design an interpretable framework that can robustly identify and localize forged elements in real-time, with a focus on cross-platform compatibility and practical applicability in cybersecurity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"cybersecurity\",\n        \"description\": \"Develop a next-generation deepfake detection system using MesoInception-Fusion architecture. Integrate novel fusion of spectro-temporal and spatial features, with emphasis on transformer models and multimodal inputs from facial expressions, audio frequencies, and scene text in the Celeb-DF+EXT dataset. Aim for state-of-the-art accuracy, interpretability, real-time performance, and cross-platform compatibility.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"cross-platform compatibility\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF+EXT\",\n            \"modality\": [\"image\", \"audio\", \"text\"],\n            \"target_variables\": [\"forgery detection\"],\n            \"specification\": null,\n            \"description\": \"An extended dataset containing multimodal inputs for facial expressions, audio frequencies, and scene text for deepfake detection.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"multimodal\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MesoInception-Fusion\",\n            \"family\": \"deep neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"MesoInception-Fusion\",\n                \"fusion_method\": \"spectro-temporal and spatial\"\n            },\n            \"description\": \"A deepfake detection system employing MesoInception-Fusion, with transformer integration for multimodal inputs.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advances in transformer models for multimodal learning\",\n        \"Real-time inference techniques for speed-up\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"multimodal feature extraction\",\n            \"interpretability techniques\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"cross-platform compatible\",\n        \"inference_engine\": [\n            \"real-time compatible framework\"\n        ],\n        \"demonstration\": \"robust cross-platform forgery detection\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON reflects the user's desire for a high-end deepfake detection system. It specifies the area of multimodal learning, downstream task, and the MesoInception-Fusion architecture. The integration with transformers and multimodal inputs is highlighted, along with performance goals and the need for interpretability and real-time capabilities. The dataset details are included, and the project's scope covers advancements in accuracy, interpretability, and cross-platform compatibility.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for the \"Smart Grid AI Optimization Project,\" direct a specialized time-series forecasting team to develop a cutting-edge deep neural network model. Utilize the disaggregated U-Time-Grid dataset, segmented into chronologically distinct train, validation, and test sequences with complex seasonal patterns (Seasonality=12, Decomposition_Level=4). The model must process lengthy historical data inputs (96 hourly readings, 750 unique energy indicators, INPUT_SEQ_LEN=96, INPUT_DIM=750) to forecast 192-hour future scenarios (2x PRED_SEQ_LEN=192, PRED_DIM=750), improving upon advanced performance metrics like root mean squared logarithmic error (RMSLE) and quantile-based logcosh loss. Emphasize on interpretability through feature importance analysis and scalable deployment methods for real-time electricity grid monitoring and anomalies detection, while incorporating a step-by-step prototype refinement plan for every iteration.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart grid\",\n        \"description\": \"As a project manager for the 'Smart Grid AI Optimization Project', direct a team to develop a cutting-edge deep neural network model using the disaggregated U-Time-Grid dataset. The dataset should be segmented into chronological train, validation, and test sequences with complex seasonal patterns (Seasonality=12, Decomposition_Level=4). The model must process lengthy historical data (96 hourly readings, 750 unique energy indicators) and forecast 192-hour future scenarios (2x PRED_SEQ_LEN=192, PRED_DIM=750) while improving performance metrics such as RMSLE and logcosh loss. Emphasis should be on interpretability, feature importance analysis, real-time electricity grid monitoring, and anomaly detection. Include a prototype refinement plan for each iteration.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSLE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Quantile-based Logcosh Loss\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"disaggregated U-Time-Grid\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"sequence_length\": 96,\n                \"input_dimensions\": 750,\n                \"seasonality\": 12,\n                \"decomposition_level\": 4\n            },\n            \"description\": \"A dataset for time-series forecasting in the context of a smart grid, with complex seasonal patterns and 750 unique energy indicators.\",\n            \"preprocessing\": [\"segmentation into train, validation, and test sequences\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature importance analysis\",\n                \"anomaly detection\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"sequence_length_in\": 96,\n                \"input_size\": 750,\n                \"sequence_output_length\": 192,\n                \"output_size\": 750,\n                \"interpretability_methods\": [\"feature importance analysis\"],\n                \"real_time_capability\": true\n            },\n            \"description\": \"A cutting-edge DNN for smart grid time-series forecasting with emphasis on interpretability and real-time monitoring.\"\n        }\n    ],\n    \"knowledge\": [\n        \"prototype refinement plan\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Step-by-step refinement for every iteration\"],\n        \"target_device\": \"Real-time monitoring infrastructure\",\n        \"deployment_endpoint\": \"Smart Grid monitoring and anomaly detection system\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Iterative prototype demonstrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction clearly outlines a specific task with clear expectations, including dataset details, model requirements, and a focus on real-world implementation. The level of expertise indicates a thorough understanding of the project's requirements. The missing metric values would be filled in during the model development process. The JSON is structured according to the given schema.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "The project necessitates the innovative design of a lightweight GraphSAGE architecture, integrating EdgeConv layers with advanced reservoir-based receptive field sampling for the complex DBLP citation network. The model should not only surpass a precision and recall benchmark of 0.95, but also exhibit dynamic scalability, adapting to frequent data updates and ensuring energy efficiency in a highly distributed and resource-constrained computing infrastructure.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"The project requires an innovative lightweight GraphSAGE architecture that integrates EdgeConv layers with advanced reservoir-based receptive field sampling for the complex DBLP citation network. Key performance criteria include precision and recall of at least 0.95, dynamic scalability to handle frequent data updates, and energy efficiency in a distributed and resource-constrained computing environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"energy consumption per inference\"\n            },\n            {\n                \"name\": \"dynamic scalability\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DBLP citation network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"citations\"],\n            \"specification\": {\n                \"size\": null,\n                \"complexity\": \"complex\"\n            },\n            \"description\": \"A complex graph data with nodes representing research papers and edges representing citations. The model needs to handle scalability and frequent updates.\",\n            \"preprocessing\": [\"resistant to data updates\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Lightweight GraphSAGE with EdgeConv and reservoir-based sampling\",\n            \"family\": \"GraphSAGE\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": {\n                    \"model_size\": \"lightweight\",\n                    \"layer_type\": \"EdgeConv\",\n                    \"receptive_field_sampling\": \"reservoir-based\"\n                },\n                \"computational_resources\": {\n                    \"efficiency\": \"optimized for distributed and resource-constrained settings\"\n                }\n            },\n            \"description\": \"A graph neural network model utilizing GraphSAGE with EdgeConv and advanced reservoir-based receptive field sampling for the DBLP citation network.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"distributed and resource-constrained\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"showcase of scalability and energy efficiency with data updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a machine learning model with a high-level of expertise. The problem domain and task are clearly defined, along with performance targets of precision and recall. The model specification and dataset requirements aligns with the user's described complexities and resource constraints. The user's goal of energy efficiency and scalability are included in the complexity metrics. Missing information, such as specific efficiency metrics and model size, is expected as they may depend on additional design considerations.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning project manager, request the team to fine-tune a state-of-the-art CapsuleNet for the Fashion-MNIST dataset, targeting 97% accuracy with robustness against scaling, rotation, and translation. Incorporate a comparative study with advanced InceptionCapsule architecture, highlighting their energy efficiency and interpretability through Grad-CAM visualization of feature attribution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"Fine-tune a state-of-the-art CapsuleNet for the Fashion-MNIST dataset, aiming for 97% accuracy and robustness against scaling, rotation, and translation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"relative\"\n            }\n        ],\n        \"knowledge\": [\n            \"Comparative study with advanced InceptionCapsule architecture\",\n            \"Incorporation of Grad-CAM for feature attribution visualization\"\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"class labels\"],\n            \"specification\": {\n                \"dimension\": {\n                    \"height\": 28,\n                    \"width\": 28,\n                    \"channels\": 1\n                },\n                \"sample_size\": null,\n                \"dataset_size\": 60000,\n                \"split\": {\n                    \"train\": 55000,\n                    \"validation\": 5000,\n                    \"test\": 10000\n                }\n            },\n            \"description\": \"A dataset of 60,000 grayscale images of 10 fashion categories.\",\n            \"preprocessing\": [\n                \"Scaling, rotation, and translation augmentation\"\n            ],\n            \"augmentation\": [\n                \"image transformations\"\n            ],\n            \"visualization\": [\n                \"Grad-CAM for feature attribution\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"CapsuleNet\",\n            \"family\": \"Capsule Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"target accuracy\": 0.97\n            },\n            \"description\": \"Fine-tuned CapsuleNet optimized for Fashion-MNIST dataset\"\n        },\n        {\n            \"name\": \"InceptionCapsule\",\n            \"comparison\": \"advanced\",\n            \"description\": \"For comparative study with CapsuleNet\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"Advanced InceptionCapsule results and Grad-CAM visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the user's instructions, focusing on fine-tuning a CapsuleNet for Fashion-MNIST with the given accuracy target, robustness, and additional elements such as a comparative study and interpretability. The user's high level of expertise indicates a need for a more detailed analysis, which is not explicitly included in the response. Missing information may include the specific tuning process, hyperparameters, or a detailed dataset specification. However, the overall structure and provided metrics are aligned with the task at hand.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Elaborate the Enron Email dataset challenge for complex node classification using a sophisticated, multi-level Hierarchical Recurrent Graph Convolutional Network (HRGCN). Demand not only clustering of employees based on encrypted email interactions, but also unravel hidden patterns over time, predicting future connections while considering real-world network dynamics. Mandate concurrent implementation for real-time adaptability, strict adherence to low-latency requirements, and optimal performance optimization for managing a massive enterprise graph with billions of ever-evolving edges across a distributed, fault-tolerant database infrastructure.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"business and communication\",\n        \"description\": \"The challenge involves complex node classification using a sophisticated HRGCN on the Enron Email dataset. The objective is to cluster employees based on encrypted email interactions, detect hidden patterns over time, and predict future connections, considering real-world network dynamics. The system must support concurrent implementation, low-latency, and optimal performance for a large enterprise graph with billions of evolving edges across a distributed, fault-tolerant database.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"clustering accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"prediction accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time processing\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"distributed compute scalability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email dataset\",\n            \"modality\": [\"text\", \"graph\"],\n            \"target_variables\": [\"employee clusters\", \"future connections\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": {\n                    \"nodes\": null,\n                    \"edges\": null\n                },\n                \"time_steps\": null\n            },\n            \"description\": \"A large, dynamic enterprise dataset of encrypted email interactions between employees, representing real-world network dynamics.\",\n            \"preprocessing\": [\n                \"data encryption\",\n                \"graph construction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"temporal patterns\",\n                \"network topology\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Recurrent Graph Convolutional Network (HRGCN)\",\n            \"family\": \"neural networks\",\n            \"type\": \"graph neural networks\",\n            \"specification\": {\n                \"hidden_layers\": null,\n                \"time_steps\": null,\n                \"complexity\": null\n            },\n            \"description\": \"A state-of-the-art model designed for complex node classification on temporal, hierarchical, and large-scale graphs.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Real-world network dynamics, fault tolerance, and distributed systems\"\n        },\n        {\n            \"description\": \"Advanced graph processing algorithms for massive graphs\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"time-series based feature extraction\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"real-time processing service\",\n        \"inference_engine\": [\n            \"Distributed graph processing framework\"\n        ],\n        \"demonstration\": \"interactive visualizations with real-time updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response reflects a machine learning project based on the given instruction, covering graph machine learning, temporal patterns, and real-time adaptability. Specific performance metrics, such as clustering and prediction accuracy, and strict latency requirements are noted, but the exact values are not provided due to the lack of detailed data. The user's high expertise level suggests a deeper understanding of these complexities.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Enhance the SegNet for a sophisticated city planning initiative, using the intricate Vaihingen dataset. The task necessitates not just precise segmentation of various vegetation classes but also the identification of distinct plant species, estimation of their age, and detection of stress levels. The model must exhibit exceptional robustness against illumination variances, seasonal shifts, and unpredictable weather, incorporating strategies for climate adaptation. Provide a comprehensive, data-driven analysis for optimizing green spaces with adaptive management strategies, ensuring near real-time performance and interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"city planning\",\n        \"description\": \"Enhance the SegNet model for a city planning project using the Vaihingen dataset. The task requires precise segmentation of vegetation classes, identification of plant species, estimation of their age, and detection of stress levels. The model must be robust to illumination variations, seasonal changes, and unpredictable weather, with climate adaptation strategies. Provide a data-driven analysis for adaptive green space management with near real-time performance and interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"species classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"age estimation error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"stress detection recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vaihingen dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vegetation classes\", \"plant species\", \"age\", \"stress levels\"],\n            \"specification\": null,\n            \"description\": \"A high-resolution dataset for urban segmentation tasks, particularly focusing on vegetation with fine-grained classes and environmental challenges.\",\n            \"preprocessing\": [\n                \"data augmentation for illumination and weather variations\"\n            ],\n            \"augmentation\": [\n                \"Gaussian blur, random rotation, and weather simulation\"\n            ],\n            \"visualization\": [\n                \"Visualize class distribution and seasonal changes\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"SegNet\",\n            \"family\": \"Convolutional Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"Enhanced SegNet for multi-task learning\",\n                \"adaptation techniques\": \"Climate-invariant and interpretable features\"\n            },\n            \"description\": \"A SegNet model with advanced multi-task capabilities for precise vegetation segmentation, species identification, age estimation, and stress detection in urban environments.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Implement climate-adaptive loss functions and metrics for robustness\",\n        \"Use transfer learning from pre-trained models on larger datasets for better generalization\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature importance analysis\",\n            \"Adaptive management strategies based on real-time performance\"\n        ],\n        \"target_device\": \"edge or IoT for near real-time performance\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Lightweight deep learning frameworks\"\n        ],\n        \"demonstration\": \"Interactive visualizations for interpretability and decision-making\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has captured the user's intent for building a sophisticated city planning model, focusing on SegNet with adaptations for the Vaihingen dataset. The project details, including segmentation and multi-task requirements, have been incorporated. Performance metrics indicate the need for a comprehensive evaluation. The user's expertise level justifies the complexity of the provided specifications. However, specific deployment endpoint and model details are left open-ended, suggesting the need for further clarification.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "object detection", "instruction": "As a project manager for an AI wildlife monitoring team, I seek a state-of-the-art object detection model that not only recognizes diverse wild animals crossing front yards in real-time but also classifies them by species, tracks individual animals over time, and accounts for seasonal variations. The model must excel in low-light scenarios while maintaining energy efficiency within strict environmental impact goals and constant adaptability to occlusion and challenging weather conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife monitoring\",\n        \"description\": \"Develop a state-of-the-art object detection model for real-time monitoring of diverse wild animals crossing front yards, classifying species, tracking individuals, and accounting for seasonal variations. Model must perform well in low-light, be energy-efficient, meet environmental impact goals, and handle occlusion and challenging weather conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Species classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Tracking precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time (low-light conditions)\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"Energy consumption\",\n                \"value\": null,\n                \"unit\": \"Watts\"\n            },\n            {\n                \"name\": \"Environmental impact (power consumption limitations)\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wildlife front yard dataset\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"animal species\"],\n            \"specification\": null,\n            \"description\": \"A dataset capturing real-world front yard scenarios with varying lighting, occlusions, and weather conditions for animal detection and tracking.\",\n            \"preprocessing\": [\n                \"Low-light enhancement\",\n                \"Image augmentation\"\n            ],\n            \"augmentation\": [\n                \"Occlusion simulation\",\n                \"Weather variation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced object detection architecture (e.g., YOLOv5, EfficientDet)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"State-of-the-art\",\n                \"inference efficiency in low-light\": true\n            },\n            \"description\": \"A highly efficient and adaptive object detection model designed specifically for wildlife monitoring in challenging environments.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Recent advancements in infrared and night vision sensors\",\n        \"Energy-efficient hardware and model pruning techniques\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Feature extraction for low-light conditions\"],\n        \"target_device\": \"Edge devices with low power consumption\",\n        \"deployment_endpoint\": \"Remote wildlife monitoring platform\",\n        \"inference_engine\": [\"Energy-efficient AI inference engines\"],\n        \"demonstration\": \"Live demo showcasing real-time monitoring and species classification\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response parses the user's requirements for a project with emphasis on a high-performance object detection model, energy efficiency, and environmental impact. It captures the diverse metrics needed for evaluation, the handling of low-light conditions, and the adaptability to environmental constraints. The model and dataset sections detail the expected capabilities and the data, respectively. The model family name is left unspecified to accommodate various top-performing models. However, some metrics (e.g., specific accuracy values) are missing as they require further research or more precise user guidance.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager, your challenge is to develop a highly-specialized text classifier for arXiv's Physics, Math, and CompSci domains, using a tailored HAN+BERT fusion model. The system must surpass 95% F1 scores on both fine-grained and broad evaluations, while maintaining real-time adaptability with sub-millisecond latency for incremental dataset growth, without compromising on efficiency for newly submitted articles.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"academic research (Physics, Math, CompSci)\",\n        \"description\": \"Develop a highly-specialized text classifier for arXiv's Physics, Math, and CompSci domains using a HAN+BERT fusion model. The system should achieve at least 95% F1 scores on both fine-grained and broad evaluations and maintain real-time adaptability with sub-millisecond latency for incremental dataset growth.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score (fine-grained)\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"F1 score (broad)\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": \"sub-milliseconds\",\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"incremental_learning\": true\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv Physics, Math, and CompSci papers\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"classification categories\"],\n            \"specification\": {\n                \"source_diversity\": \"Physics, Math, and CompSci domains\"\n            },\n            \"description\": \"A dataset containing papers from arXiv's selected domains for text classification.\",\n            \"preprocessing\": [\n                \"HAN+BERT fusion\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HAN+BERT fusion model\",\n            \"family\": \"neural networks\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"real-time adaptability\": true\n            },\n            \"description\": \"A specialized model combining Hierarchical Attention Networks (HAN) and BERT for high accuracy in Physics, Math, and CompSci text classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Efficiency required for newly submitted articles\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time, latency-sensitive environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for efficiency\"],\n        \"demonstration\": \"Performance on recently submitted articles\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the project manager's requirements, including the need for a high F1 score, real-time adaptability, and efficiency. The model type, HAN+BERT fusion, is tailored to the task, as is the emphasis on the arXiv domains and incremental dataset growth. The potential need for additional preprocessing steps, such as HAN+Bert fusion, and deployment optimization are also accounted for.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "Design a real-time, location-sensitive hotel recommendation system leveraging the 'Extended Trivago Global Tourism' dataset with fine-grained user behavior analysis, implicit feedback, and spatiotemporal dynamics. Enhance the Graph Attention Network (GAT) with a hierarchical clustering algorithm to identify latent preferences, while enforcing strict mobile/device compatibility constraints to guarantee near-zero latency and a smooth user interface experience across diverse seasonal trends.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"travel\",\n        \"description\": \"Design a real-time, location-sensitive hotel recommendation system using the 'Extended Trivago Global Tourism' dataset. Include fine-grained user behavior analysis, implicit feedback, and spatiotemporal dynamics. Enhance Graph Attention Network (GAT) with hierarchical clustering for latent preferences and enforce mobile/device compatibility for near-zero latency and smooth user experience across seasonal trends.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mobile/device compatibility\",\n                \"value\": \"strict\",\n                \"unit\": \"compatibility\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"near-zero\",\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extended Trivago Global Tourism\",\n            \"modality\": [\"multimodal\", \"time series\"],\n            \"target_variables\": [\"hotel preferences\", \"user behavior\", \"implicit feedback\"],\n            \"specification\": {\n                \"time_span\": \"real-time\",\n                \"geographical_scope\": \"location-sensitive\"\n            },\n            \"description\": \"A dataset for hotel recommendation including user behavior, location, and fine-grained data across diverse seasonal trends.\",\n            \"preprocessing\": [\"fine-grained analysis\"],\n            \"augmentation\": [\"spatiotemporal data enrichment\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Graph Attention Network (GAT)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"enhancements\": \"hierarchical clustering algorithm for latent preferences\"\n            },\n            \"description\": \"A real-time recommendation model using GAT enhanced with hierarchical clustering for hotel recommendations, incorporating fine-grained user behavior, implicit feedback, and spatiotemporal dynamics.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Leverage spatiotemporal dynamics and mobile/device compatibility for improved user experience across seasonal trends.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"mobile/device compatibility optimization\"\n        ],\n        \"target_device\": \"mobile, IoT devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for low-latency\"],\n        \"demonstration\": \"user-friendly, near-zero latency across diverse seasonal trends\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is derived from the user's instruction, which outlines a real-time hotel recommendation system. Performance metrics are not specified but the complexity metrics address the user's emphasis on mobile/device compatibility, latency, and the system's effectiveness in capturing fine-grained user behavior and temporal dynamics. The dataset includes fine-grained analysis and spatiotemporal enrichment. The model integrates Graph Attention Network with hierarchical clustering. While some information may be open-ended, like performance metrics and specific model details, the response captures the core requirements of the system.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "For the upcoming machine learning project, the development team is expected to design a novel, explainable model that not only predicts links in the intricate Citeseer citation network with an accuracy exceeding 95%, but must also uncover latent feature interactions and handle voluminous, highly sparse data with optimized computational efficiency, ensuring real-time performance and insights.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Design a novel, explainable model to predict links in the Citeseer citation network with accuracy > 95%. Focus on uncovering latent feature interactions and optimizing for computational efficiency to achieve real-time performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"real-time\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Citeseer citation network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"link prediction\"],\n            \"specification\": {\n                \"data sparsity\": \"high\",\n                \"voluminous\": true\n            },\n            \"description\": \"A large, sparse graph dataset for link prediction in the Citeseer citation network.\",\n            \"preprocessing\": [\"handling sparsity\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": true,\n                \"latent feature interaction detection\": true\n            },\n            \"description\": \"A novel, explainable model for link prediction in the Citeseer citation network focusing on large, sparse data and real-time performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"optimized for large, sparse data\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"real-time performance and insights demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the core aspects of the user's instruction, specifying the model development task, target dataset, performance requirements, and computational efficiency aspects. The accuracy of 95% is set, and the complexity metrics target real-time performance and handling sparse data. The user's high expertise level indicates a need for a novel and optimized solution. Some details, such as model name and specific feature engineering techniques, are left unspecified for the user to provide more granular information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "For the avant-garde AI initiative, design a pinnacle question-answering architecture incorporating HFT-BERT+, an advanced fusion model with enriched entity disambiguation. The system, fine-tuned exclusively on the esoteric 'Mystery Quandary Panoply' (MQP) dataset, must excel in resolving questions rich with esoteric history, achieving an F1 score of over 98% for complex intricacies. In addition to this, ensure sub-millisecond (<50 ms) response times for ultra-reactionary user experience, even when navigating through arcane information. The system must also navigate through five niche languages concurrently, maintaining an accuracy threshold of 93% for each, while seamlessly integrating cross-lingual understanding.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"artificial intelligence\",\n        \"description\": \"Design a question-answering architecture for the avant-garde AI initiative using HFT-BERT+ with advanced entity disambiguation. Fine-tune on the esoteric 'Mystery Quandary Panoply' (MQP) dataset and target esoteric history questions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.98,\n                \"for\": \"complex intricacies\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 0.005,\n                \"unit\": \"seconds\",\n                \"constraint\": \"sub-millisecond (<50ms)\"\n            }\n        ],\n        \"multilingual\": {\n            \"languages\": [\n                {\n                    \"language\": \"language1\",\n                    \"accuracy_threshold\": 0.93\n                },\n                {\n                    \"language\": \"language2\",\n                    \"accuracy_threshold\": 0.93\n                },\n                {\n                    \"language\": \"language3\",\n                    \"accuracy_threshold\": 0.93\n                },\n                {\n                    \"language\": \"language4\",\n                    \"accuracy_threshold\": 0.93\n                },\n                {\n                    \"language\": \"language5\",\n                    \"accuracy_threshold\": 0.93\n                }\n            ]\n        },\n        \"integration\": \"cross-lingual understanding\"\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mystery Quandary Panoply (MQP)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \" answers\", \"esoteric history\"],\n            \"specification\": null,\n            \"description\": \"Esoteric dataset for fine-tuning question answering model with a focus on intricate historical queries.\",\n            \"preprocessing\": [\"fine-tuning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HFT-BERT+\",\n            \"family\": \"transformer-based\",\n            \"type\": \"neural networks\",\n            \"description\": \"Advanced fusion model with enriched entity disambiguation for question answering on esoteric history questions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Fine-tuning on the esoteric nature of the dataset\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"ultra-reactive environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for sub-millisecond response time\"],\n        \"demonstration\": \"cross-lingual seamless navigation and handling arcane information\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's request for a model development task, specifying the architecture, HFT-BERT+ model, fine-tuning dataset, performance expectations, response time constraints, and multilingual requirements. The model's purpose and complexity metrics are aligned with the instruction. However, some details like the model's exact specifications, deployment endpoint, and demonstration procedures are not explicitly mentioned, indicating that they could be further specified based on the user's input.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Utilize a high-performance, scalable deep residual LSTM architecture with adaptive multi-seasonality modeling, integrating external weather data and accounting for complex temporal dependencies. Demand forecast for the subsequent 72 hours must surpass 99.9% accuracy in real-time, while maintaining sub-millisecond latency (<1ms) for immediate edge deployments, ensuring model effectiveness amidst resource constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"forecasting\",\n        \"description\": \"Develop a high-performance deep residual LSTM model with adaptive multi-seasonality modeling, integrating external weather data for real-time demand forecasting over the next 72 hours. Target accuracy must exceed 99.9% and latency must be below 1 millisecond for edge deployments.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.999\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 1,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"weather data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"weather conditions\", \"seasonality\"],\n            \"specification\": {\n                \"data_size\": \"large\",\n                \"frequency\": \"real-time\"\n            },\n            \"description\": \"External weather data to be integrated with the LSTM model\",\n            \"preprocessing\": [\n                \"data alignment\",\n                \"feature engineering\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Residual LSTM w/ Adaptive Multi-Seasonality\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"residual LSTM\",\n                \"multi-seasonality\": \"adaptive\",\n                \"integration\": \"external weather data\",\n                \"resource_consumption\": \"highly optimized\"\n            },\n            \"description\": \"A deep residual LSTM model tailored for real-time demand forecasting with high accuracy and sub-millisecond latency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Dynamic adaptation of multi-seasonality for weather variability\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonality extraction\",\n            \"weather feature encoding\"\n        ],\n        \"target_device\": \"edge device\",\n        \"deployment_endpoint\": \"real-time demand forecasting API\",\n        \"inference_engine\": [\"optimized inference algorithms\"],\n        \"demonstration\": \"real-time forecasts and latency measurement\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure adheres to the user's requirements for a build task, with the emphasis on a high-performance LSTM model, multi-seasonality, and real-time integration with external weather data. The performance metric of 99.9% accuracy and sub-millisecond latency are specified, along with the necessary preprocessing and deployment steps for edge deployment. The expert level of the user suggests they understand these complexities, and a high confidence level is given.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Request the machine learning development team to design a sophisticated node classification model using the YouTube Algebric Connectivity dataset with enhanced Significance Graph Neural Networks (incorporating edge weight aggregation and meta-path based feature learning). Mandate a minimum 85% accuracy, while also requiring a comprehensive interpretability study to profile and explain the model's decision-making in classifying creators across various content categories, particularly in cases of sparse data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"social media content analysis\",\n        \"description\": \"Design a sophisticated node classification model using the YouTube Algebric Connectivity dataset with enhanced Significance Graph Neural Networks (incorporating edge weight aggregation and meta-path based feature learning). Target a minimum accuracy of 85% and mandate a comprehensive interpretability study for the model's decision-making in classifying creators across content categories, particularly in sparse data scenarios.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"training time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube Algebric Connectivity dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"content categories\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"size\": \"unspecified\",\n                    \"sparse_data_handling\": \"yes\"\n                },\n                \"edges\": {\n                    \"weighted\": true,\n                    \"meta_paths\": \"yes\"\n                }\n            },\n            \"description\": \"A social network dataset reflecting the connections between YouTube creators and their content, for node classification with enhanced Significance Graph Neural Networks.\",\n            \"preprocessing\": [\n                \"handle sparse data\",\n                \"edge weight aggregation\",\n                \"meta-path based feature engineering\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Significance Graph Neural Network\",\n            \"family\": \"SGNN with edge weight aggregation and meta-path based feature learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"not specified\",\n                \"explainability techniques\": \"required\"\n            },\n            \"description\": \"A cutting-edge GNN model for node classification, tailored for the YouTube dataset with a focus on interpretability in sparse data scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Interpretability in sparse data is a critical challenge for accurate model performance.\",\n        \"Inclusion of meta-path based features and edge weight aggregation improves model expressiveness\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"edge weight aggregation\", \"meta-path based feature engineering\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"requirement for explanations on classification outcomes\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON accurately captures the user's request for a state-of-the-art model, performance targets, and specific preprocessing steps. The expert-level expertise indicates the user's understanding of the requirements for a complex model. However, there is room for improvement in specifying the model architecture and interpretability techniques, as well as detailing the inference process and demonstration requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a high-level project manager for an elite Martian terrain analysis team, your mandate is to select an esoteric NASA Mars Reconnaissance Orbiter dataset (e.g., HiRISE ice layer or Valles Marineris terrain classification), while integrating advanced multi-scale feature extraction with domain-adapted image interpolation techniques. The project necessitates the design of a novel hybrid model, fusing Faster R-CNN and Inception-ResNet-v2, tackling the issues of anisotropic resolution variations and sparse annotations. Can you outline the intricate challenges and anticipated milestones to push the current performance benchmark in classifying rock formations and geological features?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"space exploration\",\n        \"description\": \"Select a Martian dataset (e.g., HiRISE ice layer or Valles Marineris) from NASA Mars Reconnaissance Orbiter and integrate advanced multi-scale feature extraction with domain-adapted image interpolation techniques. Develop a novel hybrid model fusing Faster R-CNN and Inception-ResNet-v2 to address anisotropic resolution variations and sparse annotations in classifying rock formations and geological features.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mAP\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HiRISE ice layer data / Valles Marineris terrain data\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"geological features, rock formations\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"min\": null,\n                    \"max\": null,\n                    \"variance\": \"significant\"\n                },\n                \"resolution\": {\n                    \"anisotropy\": true\n                },\n                \"annotations\": {\n                    \"density\": \"sparse\"\n                }\n            },\n            \"description\": \"NASA Mars Reconnaissance Orbiter dataset for terrain analysis, with specific focus on ice layers and Valles Marineris terrain.\",\n            \"preprocessing\": [\"multi-scale feature extraction\", \"image interpolation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid model (Faster R-CNN + Inception-ResNet-v2)\",\n            \"family\": \"neural networks\",\n            \"type\": \"computer vision\",\n            \"specification\": {\n                \"architecture\": [\"faster rcnn\", \"inception_resnet_v2\"],\n                \"addressing_resolution\": \"anisotropic\",\n                \"input_adaptation\": \"domain-specific\"\n            },\n            \"description\": \"Combines Faster R-CNN for object detection and Inception-ResNet-v2 for feature extraction, designed to handle varying image resolution and sparse annotations.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"In Martian terrain analysis, challenges include dealing with anisotropic resolution across different data sources, limited availability of labeled data due to sparse annotations, and the need for domain adaptation techniques to account for unique Martian geological features.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Create interactive visualizations showcasing improved classification of rock formations and geological features\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response outlines the desired project based on the user's expertise, the selection of specific NASA dataset, the advanced model architecture, and the consideration of challenges such as anisotropic resolution and sparse annotations. However, performance benchmark scores, specific target device, and inference engine details are left open for the user's input or based on their further requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design an advanced cross-lingual transformer-based model, utilizing the extensive ParaCrawl dataset, for precise Dutch translations from English. Emphasize on incorporating a dual encoder fusion technique that discernibly differentiates between lexical equivalence and subtle emotional nuances, while ensuring cultural sensitivity across a broad range of historical and contemporary contexts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Design an advanced cross-lingual transformer-based model for precise Dutch translations from English, utilizing the ParaCrawl dataset. The model should incorporate a dual encoder fusion technique to differentiate between lexical equivalence and emotional nuances, with cultural sensitivity across historical and contemporary contexts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 0.85\n            },\n            {\n                \"name\": \"Semantic similarity (average)\", \n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Model training time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"Inference speed\",\n                \"value\": null,\n                \"unit\": \"sentences/second\"\n            },\n            {\n                \"name\": \"Memory footprint during training\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl dataset (Dutch-English)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Dutch translation\"],\n            \"specification\": {\n                \"size\": null,\n                \"language_pair\": \"Dutch-English\",\n                \"raw_data_usage\": \"extensive\"\n            },\n            \"description\": \"A large-scale parallel corpus for training a cross-lingual transformer model, focusing on Dutch translations from English with a dual encoder fusion approach.\",\n            \"preprocessing\": [\n                \"Tokenization\",\n                \"Sentence splitting\",\n                \"Cultural context handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"Embedding space exploration\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Transformer-based (e.g., mT5, XLM-RoBERTa)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"dual encoder fusion\",\n                \"emotion_detection_layer\": \"incorporated\"\n            },\n            \"description\": \"A state-of-the-art cross-lingual transformer model with dual encoder fusion for Dutch translations, utilizing emotional nuances and cultural sensitivity.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"topic\": \"Subtle cultural nuances\",\n            \"description\": \"Incorporating broad cultural contexts for Dutch translations in contemporary historical scenarios.\"\n        },\n        {\n            \"topic\": \"Emotion recognition\",\n            \"description\": \"Differences in emotional expressions across languages and sensitivity to those in the model design.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Emotion and cultural tagging\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"Customizable API for end-users\",\n        \"inference_engine\": [\"GPU-based inference\"],\n        \"demonstration\": \"Translation examples showcasing nuanced differences and cultural adaptability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON captures the user's intent for model development, focusing on a cross-lingual transformer model for Dutch translations. The dual encoder fusion technique and cultural sensitivity are well-defined, along with performance metrics and complexity constraints. Key aspects of ParaCrawl dataset usage, preprocessing, feature engineering, and service requirements are included. The high expertise level indicates a deep understanding of the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "In the realm of remote wildlife conservation, develop a highly specialized convolutional neural network (CNN) architecture, integrating Hierarchical Attention U-Net (HANet) for intricate segmentation of occluded and camouflageouflaged species in high-resolution satellite images. The project necessitates real-time anomaly detection of poaching activities, along with a novel Uncertainty-guided Attention Mechanism (UMA) for precise instance segmentation. Each prediction should be validated with an Interoperable Habitat Suitability Index (IHSI) report, accompanied by a time-efficient, web-based interactive tool for ecologists to interpret and map animal populations in near real-time. Target a minimum Intersection over Union (IoU) of 0.95 and a processing speed capable of handling at least 1000 images per minute for large-scale monitoring operations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"Develop a specialized CNN architecture with Hierarchical Attention U-Net (HANet) for segmentation of occluded and camouflageouflaged species in high-resolution satellite images. Also includes real-time anomaly detection of poaching activities and an Uncertainty-guided Attention Mechanism (UMA) for precise instance segmentation. Predictions must be validated with IHSI reports and a web-based tool for ecologists to interpret and map populations in near real-time. Target IoU of 0.95 and a processing speed of at least 1000 images per minute.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Intersection over Union (IoU)\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"processing speed\",\n                \"value\": 1000,\n                \"unit\": \"images per minute\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High-resolution satellite images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species segmentation masks\", \"anomaly detection masks\"],\n            \"specification\": {\n                \"image_size\": {\"width\": \"variable\", \"height\": \"variable\"},\n                \"bands\": \"variable\"\n            },\n            \"description\": \"Satellite images with varying degrees of occlusion and camouflage for wildlife species segmentation and anomaly detection.\",\n            \"preprocessing\": [\"image resizing\", \"data augmentation\"],\n            \"augmentation\": [\"geometric transformations\", \"random noise\"],\n            \"visualization\": [\"instance segmentation maps\", \"anomaly detection maps\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Attention U-Net (HANet) for Wildlife Conservation\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"HANet\",\n                \"depth\": \"variable\",\n                \"attention_mechanism\": \"UMA\"\n            },\n            \"description\": \"Advanced CNN with HANet and UMA for wildlife segmentation and anomaly detection in high-resolution satellite images.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"IHSI integration\", \"uncertainty quantification\"],\n        \"target_device\": \"cloud or high-performance server\",\n        \"deployment_endpoint\": \"web-based platform\",\n        \"inference_engine\": [\"real-time\", \"high-performance\"],\n        \"demonstration\": \"interactive tool for ecologists\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's requirements for a wildlife conservation project, with a high-level CNN architecture, HANet, UMA, performance metrics, and real-time processing capacity. Specific details such as the number of image bands and exact image dimensions were not provided, which may be required to optimize the solution. The user's high expertise level indicates that they might have more specific technical knowledge to contribute if needed.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "As a project manager for a cutting-edge node classification challenge, guide the team to utilize the intricate CiteSeer graph dataset with multi-layered feature extraction. Implement Hierarchical Graph Attention Networks (HGAT) for enhanced discriminating power. Demand not only precision and recall parity in computer science theory and information retrieval but also ensure cross-validation robustness within a stringent time frame. Establish milestones for each epoch's F1 score progression and a minimum 2% improvement in macro-F1 over baseline.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Guide the team to use CiteSeer graph dataset for node classification, focusing on multi-layered feature extraction with Hierarchical Graph Attention Networks (HGAT) for a sophisticated model. Aim for precision and recall parity in computer science theory and information retrieval categories. Ensure cross-validation robustness, and set a goal for 2% improvement in macro-F1 over the baseline within a strict time frame, tracking progress by milestones per epoch's F1 score.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Macro-F1\",\n                \"value\": {\"type\": \"increase\", \"target\": \"2%\"}\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"robustness\",\n                \"value\": \"cross-validation\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CiteSeer\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node labels\"],\n            \"specification\": {\n                \"size\": null,\n                \"node_features\": \"multi-layered\",\n                \"edges\": null\n            },\n            \"description\": \"An intricate graph dataset for node classification with multi-layered features.\",\n            \"preprocessing\": [\"CiteSeer graph data preparation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Graph Attention Networks (HGAT)\",\n            \"family\": \"neural networks\",\n            \"type\": \"graph neural networks\",\n            \"specification\": {\n                \"Layers\": {\n                    \"type\": \"HGAT layers with multi-layered feature extraction\"\n                },\n                \"performance\": {\n                    \"baseline_macro_F1\": null\n                }\n            },\n            \"description\": \"A GNN model utilizing HGAT for enhanced node classification in computer science theory and information retrieval domains.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"multi-layered feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"HGAT-specific\"],\n        \"demonstration\": \"Tracking F1 score improvements per epoch\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON reflects the user's high expertise level and their project manager role. The task description is parsed into various aspects, including the requirement for multi-layered feature extraction and cross-validation robustness. Performance metrics include precision, recall, and a specified target for macro-F1 improvement. The time frame and monitoring milestones are also captured. The schema accurately mirrors the given information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular classification", "instruction": "As a project lead for a cutting-edge tabular classification team, design an ensemble model that forecasts intricate student performance across various disciplines, considering personalized learning styles, historical milestones, adaptive time-varying extracurricular involvement, and sparse labeled data, while implementing a hybrid feature extraction technique and achieving at least 95% accuracy with interpretability constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"education\",\n        \"description\": \"Design an ensemble model for predicting intricate student performance across various disciplines, considering personalized learning styles, historical milestones, adaptive time-varying extracurricular involvement, and sparse labeled data. Incorporate a hybrid feature extraction technique and aim for at least 95% accuracy with interpretability constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Interpretability constraints\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Student performance dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"performance across disciplines\"],\n            \"specification\": null,\n            \"description\": \"A dataset including student data with features related to personalized learning styles, historical milestones, extracurricular activities, and sparse labeled performance data.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Ensemble Model\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"feature_extraction_technique\": \"hybrid\"\n            },\n            \"description\": \"An ensemble model for student performance prediction that combines multiple models, incorporates a hybrid feature extraction method, and prioritizes interpretability.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the given instruction, focusing on building a tabular classification model for student performance prediction. It specifies a high accuracy target, hybrid feature extraction, and interpretability constraints. Some details are left open-ended as the instruction doesn't provide specific details on model name or types of preprocessing, augmentation, and deployment. The provided JSON captures the essence of the user's intent and requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "As a machine learning project manager for a node classification task, our goal is to utilize the CiteSeer dataset, focusing on research papers in computer science. Develop a semi-supervised approach, combining inductive learning with distant supervision, to classify academic papers into subfields. Estimate precision and F1-score for citation-based networks, and discuss the impact of label scarcity on model performance.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"As a machine learning project manager, we aim to develop a semi-supervised model for node classification in computer science research papers using the CiteSeer dataset. The approach combines inductive learning with distant supervision to classify academic papers into subfields, focusing on citation-based networks. Performance metrics include precision and F1-score, and we'll address the impact of label scarcity on model performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"label scarcity impact\",\n                \"value\": null,\n                \"unit\": \"percentage of labeled data\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CiteSeer\",\n            \"modality\": [\"graph\", \"text\"],\n            \"target_variables\": [\"subfields of research papers\"],\n            \"specification\": null,\n            \"description\": \"A citation network dataset with focus on computer science research papers, containing inductive learning and distant supervision context.\",\n            \"preprocessing\": [\"graph representation learning\", \"text preprocessing\"],\n            \"augmentation\": [],\n            \"visualization\": [\"network analysis\", \"data distribution\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Semi-supervised with inductive learning and distant supervision\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A model designed for node classification in academic papers, leveraging a combination of inductive learning and distant supervision on the CiteSeer dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Label scarcity may lead to reduced model performance due to limited data availability for training. Techniques like active learning and data augmentation could be explored to mitigate this.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"graph feature extraction\", \"text feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Impact of label scarcity on model performance with visualization of result analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON represents a semi-supervised node classification project in computer science using CiteSeer dataset. Performance metrics (precision and F1-score) are included, along with the impact of label scarcity. The expert level is set to medium, and the description gives a clear overview of the task and dataset pre-processing steps. A potential demonstration of label scarcity impact on model performance is also suggested.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a highly specialized machine learning project manager for an exclusive automotive enthusiasts' challenge, you've been tasked with developing a novel image classification system using the scarce and rare \"Vintage Motorabilia Dataset.\" The objective is to design an innovative model, utilizing the ResNeXt-101 architecture, that not only discerns between various vintage car models, makes, and their distinct paint schemes, but also can identify the era, restoration status, and unique customization features. The model must operate flawlessly under extreme weather conditions, different materials, and rearview mirror distortions, showcasing an extraordinary level of detail in close-up shots. Performance measures should include a per-class F1-score for niche categories, with a stringent overall precision of 98%, and a recall rate of 97% for the rarest vehicles. Moreover, the project should incorporate a user-friendly interactive interface that allows auto collectors to upload their own images for real-time classification and receive historical context and market value estimations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"automotive enthusiasts\",\n        \"description\": \"Develop an image classification system using the Vintage Motorabilia Dataset. The model should be based on the ResNeXt-101 architecture, classifying vintage car models, makes, paint schemes, era, restoration status, and unique customization features. The model must perform well under extreme weather, material variations, and rearview mirror distortions. Performance metrics include per-class F1-score and an overall precision of 98% with a recall rate of 97% for rare vehicles. Additionally, the project should include a user-friendly interactive interface for real-time classification and historical context/market value estimations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"per-class F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.98\n            },\n            {\n                \"name\": \"recall rate\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"model accuracy under extreme conditions\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Vintage Motorabilia Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"car models\", \"makers\", \"paint schemes\", \"era\", \"restoration status\", \"customization features\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing vintage car images with detailed annotations for model, make, paint scheme, era, restoration status, and customization features.\",\n            \"preprocessing\": [\n                \"augmentation (extreme weather conditions, material variations, rearview mirror distortions)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Vintage Motorabilia Classifier\",\n            \"family\": \"ResNeXt-101\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ResNeXt-101\",\n                \"customization\": \"invariant to extreme conditions and rearview mirror distortions\"\n            },\n            \"description\": \"A deep learning model based on ResNeXt-101 for vintage car classification and recognition of details.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"user_interface\": \"interactive, real-time classification and market value estimations\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent, focusing on building an image classification system with specific requirements for architecture, performance metrics, and user interface. The model's capabilities, dataset, and metrics are well-defined based on the given instruction. However, certain aspects like the model's precision and recall are still placeholders, as they require specific numerical values.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for a specialized machine learning team, you're tasked with optimizing yield prediction for the \"Western Highland Rainforest Blueberry Cultivar Diversity Dataset.\" Utilizing a unique subset of meteorological, soil, and genotypic features, split into train, validation, and test partitions, the team must minimize mean absolute error (MAE) by developing a novel regression model tailored to the specific microclimatic conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Optimize yield prediction for the Western Highland Rainforest Blueberry Cultivar Diversity Dataset using meteorological, soil, and genotypic features. Develop a novel regression model with train, validation, and test partitions targeting minimal mean absolute error (MAE) under microclimatic conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": 0.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Western Highland Rainforest Blueberry Cultivar Diversity Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"yield prediction\"],\n            \"specification\": {\n                \"train_size\": null,\n                \"validation_size\": null,\n                \"test_size\": null,\n                \"feature_dimensions\": null\n            },\n            \"description\": \"Dataset containing a unique subset of meteorological, soil, and genotypic features for predicting blueberry yield in Western Highland Rainforest microclimates.\",\n            \"preprocessing\": [\"feature scaling\", \"data normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"feature importance analysis\",\n                \"correlation matrix\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"novel regression model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"\",\n                \"model_depth\": null,\n                \"optimizer\": \"\",\n                \"loss_function\": \"MAE\"\n            },\n            \"description\": \"A tailored regression model designed to minimize MAE for predicting blueberry yields under microclimatic conditions using meteorological, soil, and genotypic features.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider fine-tuning the model based on the microclimatic gradient in the data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"account for seasonal patterns\",\n            \"handle missing data\"\n        ],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"custom API server\",\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"Interactive yield prediction dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's request for a machine learning project, focusing on optimizing yield prediction for a specific dataset with a unique set of features. The model development is aimed at minimizing MAE in a tailored regression model, and the expertise level is set at high, indicating deep understanding of the task. The dataset source, model type, and final deployment environment are detailed according to typical industry practices. Some unspecified details can be completed based on a team's specific requirements and knowledge.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Utilize the OpenSubtitles dataset to develop a sophisticated, multilingual conversational translation system. Specify BERT as the foundation for an English-to-Mandarin model, mandating not only accurate translation of standard language but also adept handling of colloquialisms, dialects, and cultural idiomatic expressions, with a focus on maintaining context and nuance in translations. Additionally, the model must undergo rigorous evaluation through cross-lingual perplexity tests and user-experience studies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual conversational systems\",\n        \"description\": \"Use the OpenSubtitles dataset to develop a multilingual conversational translation system, with a focus on English-to-Mandarin translation. The model should be based on BERT, ensuring accuracy in standard language, handling colloquialisms, dialects, and cultural idiomatic expressions, while maintaining context and nuance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"cross-lingual perplexity\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OpenSubtitles\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"English subtitles\", \"Mandarin subtitles\"],\n            \"specification\": {\n                \"languages\": [\"English\", \"Mandarin\"],\n                \"domains\": [\"conversational context\"],\n                \"corpus size\": null\n            },\n            \"description\": \"A multilingual dataset containing subtitles for English and Mandarin dialogue, for developing a conversational translation model.\",\n            \"preprocessing\": [\n                \"BERT text preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-based English-to-Mandarin conversational translator\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer\",\n                \"tokenizer type\": \"BERT\",\n                \"handling of colloquialisms\": \"incorporated\"\n            },\n            \"description\": \"A sophisticated BERT-based model designed for English-to-Mandarin conversational translation, considering colloquialisms, dialects, and cultural idiomatic expressions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Focus on maintaining context and nuance in translations, especially in colloquial and idiomatic expressions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"BERT contextual embeddings\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"plan for user-experience study and evaluation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's intent to build a machine learning project, focusing on a BERT-based conversational translation system. It reflects a high level of expertise and includes requirements for handling specific nuances and context. Performance and complexity metrics have been adapted for rigorous evaluation. Some fields remain open-ended, reflecting the need for additional specifics like model performance values and user-experience study details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "For a comprehensive real estate predictive analytics project, analyze a diverse and extensive dataset containing house prices. Develop an intricate feature set using multiple statistical and spatial dimensions. Implement advanced data cleaning techniques, handle missing values, and normalize categorical data. Utilize state-of-the-art machine learning models with feature interaction and ensemble methods. Conduct a thorough hyperparameter optimization, and present the prediction performance in the form of root mean squared error (RMSE) for logarithm-transformed sale prices, evaluated on a held-out test set, emphasizing interpretability and robustness.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"real estate predictive analytics\",\n        \"description\": \"Conduct a comprehensive real estate predictive analytics project using a large, diverse dataset for house prices. Focus on feature engineering with statistical and spatial dimensions. Apply advanced data cleaning and normalization to handle missing values and categorical data. Utilize state-of-the-art machine learning models with feature interactions and ensemble methods. Perform hyperparameter optimization and evaluate prediction performance with RMSE for logarithm-transformed sale prices on a held-out test set, prioritizing interpretability and robustness.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE (logarithm-transformed sale prices)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"training time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Extensive real estate dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"SalePrice\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensionality\": null,\n                \"sample_rate\": null\n            },\n            \"description\": \"A large and diverse dataset containing house prices with multi-dimensional statistical and spatial features.\",\n            \"preprocessing\": [\n                \"advanced data cleaning\",\n                \"missing value imputation\",\n                \"categorical data normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble models, feature interaction models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": null,\n                \"number_of_models\": null,\n                \"interaction_depth\": null\n            },\n            \"description\": \"A machine learning model suite combining state-of-the-art techniques with feature interactions and ensemble methods.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"statistical and spatial dimensions\", \"feature interactions\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for ensemble and interpretability\"],\n        \"demonstration\": \"focus on interpretability and robustness\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's instruction for a real estate predictive analytics project, focusing on the diverse dataset, feature engineering, data cleaning, model types, and performance metrics. The provided confidence score assumes a high expertise level, as the user is expected to understand the complexities and implications of the given task. However, specific details like model architecture and hyperparameter optimization results are left unspecified as they require more details from the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project requires a specialized 3D U-Net architecture for segmenting glioblastoma tumors in the challenging BraTS-2022 dataset, aiming for a Dice similarity coefficient above 0.9. Incorporate a grad-CAM explanation module to pinpoint crucial feature attributions in real-time, enabling interpretable and reliable diagnostic support for neurosurgeons.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"medical imaging\",\n        \"description\": \"Develop a specialized 3D U-Net architecture for segmenting glioblastoma tumors in the BraTS-2022 dataset, targeting a Dice similarity coefficient of at least 0.9.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Dice similarity coefficient\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"GPU memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BraTS-2022\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"glioblastoma segmentation\"],\n            \"specification\": {\n                \"image modalities\": \"3D MRI\",\n                \"size\": null,\n                \"format\": null,\n                \"classification_labels\": [\"tumor\", \"non-tumor\"]\n            },\n            \"description\": \"Dataset for glioblastoma tumor segmentation using 3D MRI scans\",\n            \"preprocessing\": [\"3D image normalization\", \"resampling\"],\n            \"augmentation\": [\"Gan-based image augmentation\"],\n            \"visualization\": [\"qualitative tumor segmentation results\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"3D U-Net for glioblastoma segmentation\",\n            \"family\": \"U-Net\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specialized 3D U-Net\",\n                \"number_of_layers\": null,\n                \"number_of_folds\": null,\n                \"specific_layer_details\": \"incorporates grad-CAM explanation module\"\n            },\n            \"description\": \"A 3D U-Net equipped with a grad-CAM module for real-time feature attribution in glioblastoma segmentation\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time interpretability is important for medical diagnostics.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adapt for BraTS-2022 dataset\"],\n        \"target_device\": \"GPU for fast processing\",\n        \"deployment_endpoint\": \"medical imaging platform\",\n        \"inference_engine\": [\"real-time visualization using grad-CAM\"],\n        \"demonstration\": \"patient-specific tumor segmentation with explanation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON representation captures the user's intention to build a model with high expertise level, focusing on a 3D U-Net architecture for BraTS-2022 dataset. It incorporates the Dice similarity coefficient target, grad-CAM for interpretability, and specifies the dataset modality and source. The response is complete given the information provided, but some unspecified values indicate the need for further clarification on architecture details and exact performance targets.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Develop a sophisticated translation framework utilizing the ParaCrawl dataset, employing a multi-stage dual encoder architecture for simultaneous translation from English to Dutch. Ensure the system not only provides precise word-for-word conversions but also accurately captures and conveys nuanced emotional nuances in the source text, requiring deep linguistic understanding and cultural context.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"Develop a sophisticated translation framework using the ParaCrawl dataset, specifically targeting simultaneous translation from English to Dutch. The framework should employ a multi-stage dual encoder architecture, aiming for precise word-for-word conversions and nuanced emotional understanding, requiring deep linguistic knowledge and cultural context.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Semantic Similarity Index\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"megabytes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ParaCrawl dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Translated Dutch text\"],\n            \"specification\": {\n                \"language pairs\": [\"English to Dutch\"],\n                \"size\": null,\n                \"contextual data\": true\n            },\n            \"description\": \"A large-scale parallel corpus for training a multi-stage translation model with focus on English to Dutch and capturing emotional nuances.\",\n            \"preprocessing\": [\"sentence segmentation\", \"language detection\"],\n            \"augmentation\": [\"back-translation\", \"contextual data augmentation\"],\n            \"visualization\": [\"data distribution analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Multi-stage Dual Encoder\",\n            \"family\": \"neural networks\",\n            \"type\": \"sequence-to-sequence\",\n            \"specification\": {\n                \"architecture\": \"dual encoder\",\n                \"stages\": \"multi-stage\",\n                \"emotional understanding\": true\n            },\n            \"description\": \"A deep learning model that uses a multi-stage dual encoder architecture specifically designed for simultaneous translation from English to Dutch, emphasizing precise translations and emotional nuances.\"\n        }\n    ],\n    \"knowledge\": [\n        \"ParaCrawl's large-scale corpus for cross-lingual understanding\",\n        \"Importance of cultural context in preserving nuanced translations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-aware feature extraction\", \"attention mechanisms\"],\n        \"target_device\": \"cloud or powerful GPU cluster\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow.js for web-based deployment\"],\n        \"demonstration\": \"Emphasize interactive demonstrations showcasing emotional nuances and real-world translation examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent for building a machine translation model using a multi-stage dual encoder architecture. The expert level of expertise indicates the need for sophisticated techniques. The provided data and metrics align with the goal of precision and nuanced translation. The dataset information reflects the ParaCrawl corpus, and the performance metrics allow for evaluating the system's language and emotional understanding. The user's requirements for feature engineering and deployment options suggest a focus on accuracy and accessibility. Some metric values are unspecified, as they require additional data or analysis.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager needs to integrate image segmentation for high-resolution Sentinel-2 data into the satellite land type classification system. Select a fast, efficient model tailored for land (water, forest, urban) detection, ensuring at least 90% accuracy within 60 minutes of processing time, and demonstrate its adaptability across varying weather conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"remote sensing\",\n        \"description\": \"Integrate image segmentation for high-resolution Sentinel-2 data into a satellite land type classification system, focusing on land (water, forest, urban) detection.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"processing time\",\n                \"value\": 3600, \n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2 data (land type)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"land classification labels\"],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"bands\": [\"RGB, NIR, SWIR\"]\n            },\n            \"description\": \"High-resolution Sentinel-2 satellite imagery for land type classification tasks, including water, forest, and urban detection.\",\n            \"preprocessing\": [\"noise reduction\", \"image enhancement\"],\n            \"augmentation\": [],\n            \"visualization\": [\"data exploration, distribution\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"efficient, tailored for land detection\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_speed\": \"fast\",\n                \"memory footprint\": \"low\",\n                \"weather_invariance\": \"demonstrated\"\n            },\n            \"description\": \"A lightweight, optimized model designed for land detection in Sentinel-2 imagery, ensuring at least 90% accuracy within 60-minute processing time and adaptable across varying weather conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Experience with Sentinel-2 data\",\n        \"Adaptation techniques for weather variations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"image preprocessing steps\"],\n        \"target_device\": \"satellite or cloud-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time, satellite data\"],\n        \"demonstration\": \"example segmentation results on diverse weather conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The information is parsed to match the user's requirement for an image segmentation model for Sentinel-2 data. Performance metrics have been included for accuracy (90%) and processing time (60 minutes). The model's efficiency is highlighted, and adaptation to weather conditions is addressed. Knowledge points and the deployment phase indicate adaptation to satellite-based platforms. Some areas like model names and fine-grained complexity details are left blank as they may be filled with specific suggestions in a consulting scenario. Overall, the information is comprehensive and aligned with the user's expertise.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "For the legal team's sophisticated access to case law and regulations, design an advanced question-answering model derived from DeBERTa, specifically tailored for legal terminologies. The adaptation should involve fine-tuning on the LegalQA corpus while maintaining GDPR compliance, and necessitate multi-level reasoning and clear, auditable responses for high confidence in decision-making.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"legal\",\n        \"description\": \"Design an advanced question-answering model based on DeBERTa, specialized for legal terminologies. The model should be fine-tuned on the LegalQA corpus while ensuring GDPR compliance and support multi-level reasoning for high-confidence decision-making.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LegalQA corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"corpus_size\": \"large\",\n                \"legal_domain_focus\": \"true\"\n            },\n            \"description\": \"A legal-specific question-answer dataset for fine-tuning the DeBERTa model.\",\n            \"preprocessing\": [\" GDPR-compliant data processing\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Legal DeBERTa\",\n            \"family\": \"DeBERTa\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine-tuning\": \"LegalQA corpus\",\n                \"GDPR compliance\": \"ensured\",\n                \"multi-level reasoning\": \"enabled\"\n            },\n            \"description\": \"A specialized DeBERTa model fine-tuned on LegalQA corpus, adapted for legal terminologies and meeting GDPR requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"legal terminologies\": \"leveraged in model adaptation\"\n        },\n        {\n            \"GDPR guidelines\": \"followed during model development\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"custom\"],\n        \"demonstration\": \"auditable, clear responses for decision-making\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent, which is to build a specialized QA model for legal terminology. The high expertise level suggests a need for detailed technical requirements. Performance metrics are left unspecified as the user emphasizes clear, auditable responses and GDPR compliance. The adaptation of DeBERTa on LegalQA corpus and GDPR compliance are core aspects. Missing details are placeholder indicators like model-specific F1 scores or inference times, which are best specified based on detailed technical constraints and the model's performance.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a machine learning project manager, your team has been tasked with developing an advanced image classification system for a smartphone app. The client provides a diverse set of photos from various sources, but there's a new constraint: classify not only objects but also assess the sentiment and context within the pictures. Optimize for real-time processing while ensuring at least 95% accuracy and a user-friendly interface for mobile users.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"smartphone app\",\n        \"description\": \"Develop an advanced image classification system that includes sentiment analysis and context recognition for a smartphone app, using a diverse set of photos from various sources. Prioritize real-time processing and achieve at least 95% accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Diverse set of photos from various sources\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"various\",\n                \"diversity\": \"high\"\n            },\n            \"description\": \"A large and diverse collection of photographs covering various contexts for image classification and sentiment analysis.\",\n            \"preprocessing\": [\"real-time resizing\", \"image normalization\"],\n            \"augmentation\": [\"lighting variations\", \"cropping\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"multi-task learning\",\n                \"real-time capabilities\": \"yes\"\n            },\n            \"description\": \"An advanced CNN model capable of simultaneous object classification, sentiment, and context assessment for real-time smartphone app use.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for mobile environment\", \"context-aware feature selection\"],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"cloud-based server\",\n        \"inference_engine\": [\"GPU accelerated\", \"MobileNets for efficient inference\"],\n        \"demonstration\": \"A mobile-friendly user interface with instant feedback on classification results and sentiment assessment.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the core requirements of the project, including developing an advanced model for image classification, sentiment analysis, and context recognition, and optimizing for real-time processing. The specified performance metrics and constraints are incorporated. The team's expertise level is assumed to be high, implying a deep understanding of the complexities. The chosen model and service components align with the task. The missing values in complexity metrics are left to allow for optimization and are based on the real-time processing priority.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "The task is to design a GraphSAGE model, optimizing for node classification on the PubMed dataset while maintaining high accuracy. Emphasize transferability to novel, domain-specific datasets, and aim for a minimal model architecture to minimize resource consumption. In addition, mandate a stringent real-time constraint of 25ms end-to-end inference time per node, ensuring seamless deployment in a high-traffic environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"biomedical research\",\n        \"description\": \"Design a GraphSAGE model for node classification on the PubMed dataset with a focus on transferability to novel datasets and minimal model size for resource efficiency. Aim for high accuracy and a strict real-time constraint of 25ms end-to-end inference time per node.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 25,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PubMed dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"nodes' classes\"],\n            \"specification\": null,\n            \"description\": \"A biomedical dataset for node classification in the graph machine learning context.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GraphSAGE\",\n            \"family\": \"GraphSAGE\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"optimized for transferability and minimal size\",\n                \"resource_consumption\": \"minimal\"\n            },\n            \"description\": \"A GraphSAGE model designed for high accuracy, transferability to new datasets, and real-time inference with a 25ms constraint.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning techniques and optimal model design for high-accuracy and resource-efficient graph models\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for high-speed inference\"],\n        \"demonstration\": \"including a requirement for seamless deployment in high-traffic conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response addresses the user's requirements for designing a GraphSAGE model, emphasizing accuracy, transferability, and resource efficiency. Performance metrics of accuracy and real-time inference constraint (with a specific value) are included. The model design, dataset, and service requirements are in line with the user's instruction. The missing parts may be filled based on a complete understanding of the user's expertise and the specific model optimization process.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "For an upcoming breakthrough project, we aim to refine a cutting-edge transformer model using the extensive Common Crawl dataset. Our focus lies in creating a highly accurate, colloquial translation system, specifically for English-Japanese dialects. Mandate a minimum precision of 97%, while preserving cultural nuances and subtext, necessitating intricate understanding of slang and context.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"Refine a cutting-edge transformer model using the Common Crawl dataset to create an accurate, colloquial English-Japanese translation system, focusing on precision >= 97% and preserving cultural nuances and subtext.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Common Crawl dataset (English-Japanese)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": null,\n            \"description\": \"An extensive text dataset for refining a transformer model, focusing on English-Japanese dialect translation.\",\n            \"preprocessing\": [\n                \"text cleaning\",\n                \"tokenizer customization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"data distribution analysis\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Transformer Model\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"attention_layers\": \"Transformer architecture with advanced adaptation for colloquial translation\",\n                \"contextual_embedding\": \"Incorporates cultural nuances and subtext\"\n            },\n            \"description\": \"A state-of-the-art transformer model tailored for accurate English-Japanese dialect translation with minimum precision of 97%.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Relevant literature on transformer models for language translation, cultural nuances, and colloquial understanding.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual embeddings for slang and dialect adaptation\"\n        ],\n        \"target_device\": \"high-performance compute environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"Example translations showcasing colloquial accuracy and cultural preservation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure covers the user's intention to build a model, the transformer model refinement targeting English-Japanese dialect translation, and the performance expectations with specific metrics like precision and cultural understanding. The user's high expertise suggests the level of detail provided. The dataset and model sections cater to the requirements. However, specific metric values or deployment details are not specified, leaving room for further inquiry.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "community detection", "instruction": "For the CiteSeer citation network, apply a stepwise, unsupervised Label Propagation method to identify evolving research communities over time. The algorithm's robustness must be demonstrated by its ability to seamlessly integrate new researchers and citations without major adjustments, reflecting real-world academia dynamics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Apply a stepwise, unsupervised Label Propagation method to identify evolving research communities over time in the CiteSeer citation network, ensuring algorithmic robustness for integrating new researchers and citations without major adjustments.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"algorithm stability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"adaptability to new researchers\",\n                \"value\": null\n            },\n            {\n                \"name\": \"integration of new citations\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CiteSeer citation network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": null,\n                \"temporal_scope\": {\n                    \"time_steps\": null,\n                    \"time_granularity\": null\n                }\n            },\n            \"description\": \"A time-evolving graph dataset where nodes represent research papers and edges represent citations in the CiteSeer domain.\",\n            \"preprocessing\": [\"unsupervised learning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Label Propagation\",\n            \"family\": \"unsupervised\",\n            \"type\": \"graph neural networks\",\n            \"specification\": {\n                \"algorithm\": \"stepwise\",\n                \"method\": \"unsupervised\"\n            },\n            \"description\": \"A graph-based method using stepwise, unsupervised Label Propagation for community detection in the CiteSeer citation network.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The algorithm should account for academia's dynamics where researchers join and new citations arise, and it must maintain stability and seamless integration.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Include a demonstration showcasing the algorithm's robustness and integration of new data points without significant adjustments.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is crafted based on the user's specified task of applying Label Propagation on the CiteSeer network, emphasizing the algorithm's unsupervised nature and ability to handle evolving communities. Performance metrics are not specified as the user focuses on the algorithm's robustness, while complexity metrics address the algorithm's adaptability. The given level of expertise suggests a mid-level understanding of the ML process. The confidence score may not be full due to lack of concrete performance values but covers the key aspects of the user's request.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "question answering", "instruction": "For a specialized vertical AI platform serving the dynamic and niche market of international space exploration, we require a next-generation question-answering system. This system should harness the prowess of the recently unveiled Federated Attention Transformer (FAT) model, trained specifically on the obscure and exclusive 'Celestial Chronicles' dataset. The model must not only provide real-time insights into celestial phenomena, satellite technology, and space diplomacy, but also exhibit exceptional Latex integration for complex scientific queries, while maintaining stringent latency constraints of under 500 milliseconds. Additionally, the platform must demonstrate adaptability to the ever-evolving space industry news, ensuring the model's comprehension and response capabilities remain up-to-date and unparalleled.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"space exploration\",\n        \"description\": \"Develop a next-generation question-answering system for a specialized vertical AI platform, targeting the international space exploration market. The system must utilize the Federated Attention Transformer (FAT) model trained on the 'Celestial Chronicles' dataset, ensuring real-time insights on celestial phenomena, satellite technology, and space diplomacy. It should also include Latex integration for complex scientific queries and maintain a latency of under 500 milliseconds. The platform should adapt to the evolving space industry news.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 0.5\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency (ms)\",\n                \"value\": 500,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celestial Chronicles\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"exclusive\",\n                \"domain\": \"space exploration\"\n            },\n            \"description\": \"A dataset specific to international space exploration, featuring obscure and niche content on celestial phenomena, satellite technology, and space diplomacy.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Federated Attention Transformer (FAT)\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architeture\": \"Federated Attention mechanism\",\n                \"dataset\": \"Celestial Chronicles\"\n            },\n            \"description\": \"A next-generation question-answering model specifically trained on the 'Celestial Chronicles' dataset, capable of handling real-time inquiries in the context of space exploration.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptability to space industry news and real-time insights\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Latex integration for complex scientific queries\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Real-time, low-latency processing\"],\n        \"demonstration\": \"Unparalleled comprehension and response to evolving space industry news\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's request for a high-end NLP model tailored to the space exploration domain. It specifically details the target model, dataset, performance requirements, and knowledge integration. The system's features and latency constraints are clearly defined. The given score of 0.99 assumes a high confidence due to the clear scope and specific technical requirements provided by the user.\",\n        \"score\": 0.99\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Could you develop an advanced, real-time deep learning model with state-of-the-art algorithms that accurately segments not only cats but also distinguishes between species, age groups, and distinctive markings in various lighting conditions, while maintaining efficient computational resources?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"computer vision and animal identification\",\n        \"description\": \"Develop an advanced real-time deep learning model using state-of-the-art algorithms that accurately segments cats and differentiates among species, age groups, and distinctive markings under various lighting conditions. Consider computational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"segmentation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"species classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"age group classification accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"lighting condition robustness\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"(various labeled) cat images with species, age, and markings annotations\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"species\", \"age groups\", \"distinctive markings\"],\n            \"specification\": null,\n            \"description\": \"A comprehensive dataset with labeled images for different cat species, age groups, and markings, covering diverse lighting conditions.\",\n            \"preprocessing\": [\"image normalization\", \"data augmentation for various lighting conditions\"],\n            \"augmentation\": [\"image rotation\", \"contrast adjustment\", \"lighting simulation\"],\n            \"visualization\": [\"image segmentation results, annotations for target classes\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art deep learning architecture (e.g., yolov5 for object detection, U-Net for segmentation)\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A real-time, deep learning model designed for advanced cat segmentation with species, age, and marking differentiation\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"extracting relevant features for species, age, and markings\"],\n        \"target_device\": \"efficient hardware or cloud deployment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow, PyTorch, or other compatible real-time frameworks\"],\n        \"demonstration\": \"live examples and video showcasing model performance in real-time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's request for an advanced, real-time model with state-of-the-art algorithms. The model's specifications are left open-ended to accommodate the user's preference for the most efficient deep learning architecture. The performance metrics are highlighted as requiring accuracy for segmentation, species, age, and lighting conditions, while the complexity metrics focus on inference time and memory usage. The dataset specification is detailed to cover the various annotations and lighting conditions, and preprocessing and augmentation steps are listed. The service section includes the implementation details and a plan for demonstrating the model's performance.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series classification", "instruction": "In the novel \"Ecophonic Metropolis: Soundscapes of Urban Resilience,\" your team is tasked with developing a state-of-the-art time-series classification model for a real-time biodiversity monitoring system. The model should employ a fusion of acoustic-environmental indices (AEI) derived from multifarious environmental sensors and a deep learning architecture integrating Temporal Convolutional Networks (TCN) and Long Short-Term Memory (LSTM) modules. The system must not only identify and classify native and invasive species but also predict their behavior patterns influenced by seasonal changes and urban noise pollution. Performance metrics must include weighted F1-score, precision, and recall, with a focus on interpretability and the ability to adapt dynamically to rapidly evolving urban ecosystems. Additionally, the model must have a power efficiency score, as it will run on low-resource edge devices embedded within smart green spaces.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"ecology and urban planning\",\n        \"description\": \"Develop a state-of-the-art time-series classification model for a real-time biodiversity monitoring system using acoustic-environmental indices (AEI) from various environmental sensors. The model should employ a fusion of Temporal Convolutional Networks (TCN) and Long Short-Term Memory (LSTM) modules. Target tasks include identifying native and invasive species, and predicting behavior patterns influenced by seasonal changes and urban noise pollution.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"weighted F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"power efficiency\",\n                \"value\": null,\n                \"unit\": \"energy consumption per inference\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ecophonic Metropolis dataset\",\n            \"modality\": [\"time series\", \"multimodal\"],\n            \"target_variables\": [\"native/invasive species\", \"behavior patterns\"],\n            \"specification\": {\n                \"data_size\": null,\n                \"sampling_rate\": null,\n                \"sensor types\": [\"environmental\"]\n            },\n            \"description\": \"Dataset containing acoustic-environmental indices from multifarious sensors in urban environments, capturing the dynamics of native and invasive species behavior influenced by seasonality and noise.\",\n            \"preprocessing\": [\n                \"AEI extraction\",\n                \"seasonality normalization\"\n            ],\n            \"augmentation\": [\n                \"data augmentation for time-series\"\n            ],\n            \"visualization\": [\n                \"performance visualization w.r.t. seasonal changes\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Fusion of TCN and LSTM\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"TCN+LSTM\",\n                \"model_size\": null,\n                \"FLOPs\": null,\n                \"inference_speed\": null,\n                \"memory_consumption\": null\n            },\n            \"description\": \"A deep learning model combining Temporal Convolutional Networks and Long Short-Term Memory units specifically designed for real-time biodiversity monitoring in an urban environment.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Relevant research on urban resilience, interpretability, and dynamic adaptation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonality-aware feature engineering\",\n            \"interpretable feature extraction\"\n        ],\n        \"target_device\": \"low-resource edge devices\",\n        \"deployment_endpoint\": \"smart green spaces\",\n        \"inference_engine\": [\"energy-efficient\", \"real-time\"],\n        \"demonstration\": \"A live demo showcasing real-time species classification and behavior prediction\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the user's requirement for a specific model architecture, performance metrics, and the need for interpretability and adaptability. The dataset and preprocessing steps are aligned with the target task and domain, accounting for urban noise and seasonal variations. The user's high expertise level is reflected, and the model requirements consider the deployment on edge devices. However, exact metric values or specifications are missing, which might be inferred based on the user's context or provided in further details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Utilize the Srats2018 Mouse Brain Graph Dataset, particularly theamygdala subregion, for node classification with a specialized Graph Wavelet Neural Network (GWNN). Target the distinct subtypes of pyramidal and inhibitory neurons with exceptional F1-score and local efficiency, while accounting for fine-grained edge weights reflecting synapse strength and temporal dynamics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Utilize the Srats2018 Mouse Brain Graph Dataset, specifically the amygdala subregion, for node classification with a specialized Graph Wavelet Neural Network (GWNN). Target the classification of pyramidal and inhibitory neurons with a high F1-score, while considering fine-grained edge weights reflecting synapse strength and temporal dynamics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"local efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Srats2018 Mouse Brain Graph\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"pyramidal neurons\", \"inhibitory neurons\"],\n            \"specification\": {\n                \"edge_weights\": {\n                    \"synapse_strength\": true,\n                    \"temporal_dynamics\": true\n                }\n            },\n            \"description\": \"A brain graph dataset focusing on the amygdala region, containing data for the classification of pyramidal and inhibitory neurons with edge weights reflecting synapse strength and temporal dynamics.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Wavelet Neural Network (GWNN)\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"specialized\": true\n            },\n            \"description\": \"A specialized Graph Neural Network designed for node classification on the Srats2018 Mouse Brain Graph Dataset, targeting pyramidal and inhibitory neuron classification with exceptional F1-score and taking edge weights into account.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction provides a clear task for building a model using a specific dataset and technique. The user's high expertise indicates a clear understanding of the requirements, particularly for the niche graph wavelet neural network. The performance metrics, F1-score and local efficiency, have been accounted for, but no specific values are provided. The structure and content of the JSON response accurately reflect the task's context and expected deliverables.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "For a cutting-edge research project at a global biotechnology firm, you've been commissioned to develop an AI-driven system capable of detecting and localizing rare biological agents, such as exotic viruses or genetically modified organisms, in real-time within a crowded laboratory environment. The system should utilize advanced computer vision, work seamlessly in high-resolution microscopes and macroscopic imaging, and discern subtle variations with minimal human intervention while ensuring strict privacy and data security.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"biotechnology\",\n        \"description\": \"Develop an AI-driven system to detect and localize rare biological agents like exotic viruses or genetically modified organisms in real-time in crowded laboratory environments. The system should work with high-resolution microscopes and macroscopic imaging, discern subtle variations with minimal human intervention, and ensure strict privacy and data security.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"detection speed\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"data privacy and security\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"biological agent dataset\",\n            \"modality\": [\n                \"image\",\n                \"video\"\n            ],\n            \"target_variables\": [\"location\", \"agent type\"],\n            \"specification\": null,\n            \"description\": \"Crowd-sourced dataset with high-resolution micrographs and macroscopic images containing rare biological agents, emphasizing diversity and variation.\",\n            \"preprocessing\": [\"image enhancement\", \"data augmentation\"],\n            \"augmentation\": [\"low-level visual transformations\", \"GAN-generated samples\"],\n            \"visualization\": [\n                \"false positive/negative analysis\",\n                \"confusion matrix\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning (e.g., YOLOv5, Mask R-CNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"An advanced computer vision model designed for real-time detection and localization of rare biological agents in both high-resolution microscopic and macroscopic images.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Advancements in deep learning architectures tailored for low-light and high-resolution images.\"\n        },\n        {\n            \"description\": \"Integration of privacy-preserving techniques in AI systems for handling sensitive biological data.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"multi-scale feature extraction\", \"contextual feature representation\"],\n        \"target_device\": \"cloud-based, for real-time processing\",\n        \"deployment_endpoint\": \"secure, encrypted infrastructure\",\n        \"inference_engine\": [\"GPU-optimized inference\"],\n        \"demonstration\": \"demonstration of end-to-end workflow and privacy measures\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's request for a cutting-edge computer vision project targeting biological agent detection. High expertise level, the specified application domain, and the performance metrics (accuracy and speed) align with the requirements. The system's limitations and need for human intervention are acknowledged. The datasets, model, and service sections are tailored based on the instruction. However, some performance metric values and service-specific details are left unspecified due to the absence of exact requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a seasoned project manager, refine the parking space detection objective for our cutting-edge smart city solution. mandate the integration of semantic segmentation for precise obstacle identification, an additional constraint to handle occlusions, and a requirement for transfer learning from Cityscapes dataset to improve generalization under diverse urban landscapes. Aim for at least 95% detection accuracy under challenging scenarios and sub-150ms latency. Ensure the M2Det architecture is enhanced for low-light and adverse weather resilience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city\",\n        \"description\": \"Refine the parking space detection objective for a smart city solution. Mandate integration of semantic segmentation for precise obstacle identification, handle occlusions, and leverage transfer learning from Cityscapes dataset for improved generalization across diverse urban landscapes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"detection accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 150,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"parking spaces\", \"obstacles\"],\n            \"specification\": {\n                \"low-light and adverse weather adaptation\": true\n            },\n            \"description\": \"A dataset for semantic segmentation and transfer learning, focusing on urban scenes like Cityscapes for improved parking space detection and obstacle identification.\",\n            \"preprocessing\": [\"transfer learning from Cityscapes\"],\n            \"augmentation\": [\"data augmentation for occlusion handling\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M2Det\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"enhancements\": [\"low-light and adverse weather resilience\"]\n            },\n            \"description\": \"An enhanced M2Det architecture that integrates semantic segmentation, handles occlusions, and leverages transfer learning from Cityscapes for parking space detection in smart city scenarios.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"smart city infrastructure\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"demonstrate improved detection in low-light and adverse weather conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the refined parking space detection objective, specifying semantic segmentation, occlusion handling, and transfer learning from Cityscapes. The user's high expertise level is reflected, and the target detection accuracy and latency requirements are clearly defined. The M2Det model is tailored for low-light and adverse weather resilience. Some fields may need further clarification, such as the M2Det family name or explicit device deployment information, but the existing structure covers the essential aspects.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "Can a zero-shot, multimodal transformer model effectively leverage contextualized representations from arXiv abstracts and paper titles to predict not only direct citations but also latent collaborative relationships between authors in a highly specialized interdisciplinary dataset like HEPPhy- Citations, while maintaining high accuracy and outperforming state-of-the-art link prediction methods?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"scientific research\",\n        \"description\": \"Investigate the effectiveness of a zero-shot, multimodal transformer model in predicting both direct citations and latent collaborative relationships between authors in the HEPPhy- Citations dataset, focusing on highly interdisciplinary papers and maintaining high accuracy compared to state-of-the-art methods.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"outperforming\",\n                \"value\": \"state-of-the-art\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HEPPhy- Citations\",\n            \"modality\": [\"text\", \"multimodal\"],\n            \"target_variables\": [\"collaborative relationships\", \"direct citations\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensionality\": null,\n                \"specialization\": \"highly interdisciplinary\"\n            },\n            \"description\": \"A dataset containing arXiv abstracts, paper titles, and citation information for a specialized interdisciplinary domain of physics (High Energy Physics)\",\n            \"preprocessing\": [\"contextualized representation extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Zero-shot Multimodal Transformer\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"zero-shot learning\",\n                \"modality\": \"multimodal\"\n            },\n            \"description\": \"A model designed to leverage contextualized representations from arXiv abstracts and paper titles for predicting collaborative relationships and direct citations in HEPPhy- Citations\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"contextual embeddings\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response corresponds to the user's query, explaining the need for a multimodal transformer model, context-specific performance metrics (accuracy, outperforming state-of-the-art), and considerations for inference time. The model, dataset, and preprocessing steps are tailored based on the task, and the inference endpoint is not specified, indicating the need for further information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As the Chief Data Analyst for a rapidly expanding technology firm, you've been tasked with enhancing the predictive capabilities of our machine learning platform. Develop a state-of-the-art tabular regression model that not only forecasts the combined budget and duration for upcoming software development projects, but also factors in external variables such as market trends, team size fluctuations, and the introduction of disruptive technologies. The model should achieve at least 90% accuracy and present interpretable insights for strategic decision-making, while considering the multicollinearity among the input variables and addressing potential outliers in the historical dataset.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"technology company\",\n        \"description\": \"Enhance the machine learning platform's predictive capabilities by building a state-of-the-art tabular regression model that forecasts combined budget and duration for software development projects, factoring in external variables like market trends, team size fluctuations, and disruptive technologies. Aim for at least 90% accuracy, ensure interpretability for strategic decisions, and handle multicollinearity and outliers.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Interpretability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Outliers detection\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Historical software development projects\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"combined budget\", \"duration\", \"market trends\", \"team size\", \"disruptive technologies\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"length\": null,\n                \"outliers prevalence\": null\n            },\n            \"description\": \"A dataset containing historical data on software development projects with various input variables and combined budget and duration outcomes.\",\n            \"preprocessing\": [\"Multicollinearity check\", \"Outlier detection\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"State-of-the-art tabular regression\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"num_layers\": null,\n                \"architecture\": null,\n                \"interpretability techniques\": null\n            },\n            \"description\": \"A high-performance model for forecasting combined budget and duration, incorporating external variables and delivering interpretable insights.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using advanced feature selection methods to handle multicollinearity.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature importance analysis\",\n            \"Variable selection\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Present model insights through interactive dashboards\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a state-of-the-art tabular regression model, addresses the high-level requirement of 90% accuracy, and acknowledges the need for interpretability and handling external variables. The dataset, preprocessing steps, and model type align with the instruction. The presence of possible multicollinearity and outliers in the dataset is acknowledged, and the confidence score reflects the completeness of the provided information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager must commission a specialized TinyBERT-based QA model for mobiles, optimized for NewsQA dataset. Guarantee at least 85% accuracy on diverse news articles, while ensuring a stringent 50 milliseconds response time on devices with limited processing power, and compliance with energy-efficient design guidelines.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"news and information retrieval\",\n        \"description\": \"Commission a specialized TinyBERT-based QA model for mobile devices, optimized for NewsQA dataset with at least 85% accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 50,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NewsQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"device_compatibility\": \"mobiles\",\n                \"data_format\": \"diverse news articles\"\n            },\n            \"description\": \"A dataset of news articles for training a TinyBERT QA model optimized for mobile devices.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT-based QA\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"optimization\": \"mobile-specific\",\n                \"architecture\": \"TinyBERT\"\n            },\n            \"description\": \"A lightweight TinyBERT QA model optimized for mobile devices on the NewsQA dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobiles with limited processing power\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"energy-efficient\"],\n        \"demonstration\": \"mobile performance demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's request to build a TinyBERT-based QA model for mobiles, optimized for NewsQA with specified performance metrics and complexity constraints. The user's high expertise level indicates a requirement for a specialized solution. Missing information, if any, can be filled upon clarification, such as the deployment endpoint or the specific energy efficiency standard.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a specialized project manager for the AI and NLP division, your mandate is to develop a state-of-the-art text classifier for the Snopes Veracity Corpus, which contains a diverse set of claim-evidence pairs. Your team must discern if the evidence supports, refutes, or is unrelated to the claims with a high precision and recall. Utilize advanced techniques like BERT-based fine-tuning and implement multi-task learning to enhance generalization. Aim for an F1 score of at least 90% and conduct rigorous experiments with different model architectures for comparison.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI and NLP\",\n        \"description\": \"Develop a state-of-the-art text classifier for the Snopes Veracity Corpus, differentiating between claim-evidence pairs as supporting, refuting, or unrelated. Focus on high precision and recall, using BERT-based fine-tuning and multi-task learning with the goal of achieving at least an F1 score of 90%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Model inference time\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Snopes Veracity Corpus\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [\"Label (supporting, refuting, unrelated)\"],\n            \"specification\": null,\n            \"description\": \"A dataset of claim-evidence pairs from Snopes to train a text classifier for verifying the veracity of claims.\",\n            \"preprocessing\": [\"BERT-based tokenization\", \"Claim-Evidence pair formatting\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"BERT with fine-tuning for multi-task learning\",\n                \"model_details\": \"Advanced BERT architecture\"\n            },\n            \"description\": \"A state-of-the-art text classifier using BERT for the Snopes Veracity Corpus, leveraging fine-tuning and multi-task learning.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"BERT embeddings\", \"Custom feature engineering for multi-task learning\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Experimental results and model comparison\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intent to build a text classifier with high precision and recall, using advanced BERT-based techniques and multi-task learning. The specified F1 score goal and missing performance metric values allow room for optimization. Some complexities are not detailed, but the emphasis on experimentations implies a focus on efficiency. The user's high expertise indicates an understanding of the requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "The project now aims at developing a high-precision HAN model for arXiv paper classification, requiring not just multi-level field and subfield differentiation but also semantic disambiguation. Target an impressive 92% F1 score with a real-time learning mechanism, capable of adapting to evolving arXiv trends with daily incremental updates. Emphasize efficiency in processing large volumes and ensuring interpretability of attention weights.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"academic research (arXiv)\",\n        \"description\": \"The project aims to develop a high-precision HAN model for arXiv paper classification. It should differentiate papers into multiple fields and subfields with semantic disambiguation, targeting an impressive 92% F1 score. The model must have a real-time learning mechanism and be able to adapt to arXiv trends with daily incremental updates. Efficiency in processing large volumes and interpretability of attention weights are essential.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.92\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"real-time\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"efficient\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv paper dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"paper fields and subfields\"],\n            \"specification\": {\n                \"size\": null,\n                \"average_length\": null\n            },\n            \"description\": \"A large text dataset of arXiv papers for multi-level classification and semantic disambiguation.\",\n            \"preprocessing\": [\"text normalization\", \"tokenization\"],\n            \"augmentation\": [\"synonym replacement\", \"sentence permutation\"],\n            \"visualization\": [\"attention weight distribution\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Attention Network (HAN)\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_type\": \"sequence-based\",\n                \"architecture_details\": \"Hierarchical, interpretable, and with real-time learning capabilities\"\n            },\n            \"description\": \"A high-precision HAN model for arXiv paper classification with multi-level field and subfield differentiation, semantic disambiguation, and daily incremental updates.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Semantic disambiguation techniques, such as word embeddings and contextualized word representations\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"incremental feature extraction\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"AI platform\",\n        \"inference_engine\": [\"real-time serving\"],\n        \"demonstration\": \"explainable decision-making with attention weights\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a highly-accurate HAN model with real-time learning and interpretability. The problem details, metrics, and dataset specifications are derived from the user's requirements. However, specific values for performance, memory usage, and certain feature engineering steps are left as null or unspecified due to potential variations or lack of explicit values.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the machine learning development team, you must enhance the text classification task on the WebMD Review dataset. The challenge now involves not only achieving high accuracy but also implementing a model that accounts for sentiment nuances and handles sarcasm, ensuring a sophisticated evaluation using F1-score.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"healthcare (WebMD Review dataset)\",\n        \"description\": \"Enhance the text classification task on the WebMD Review dataset to handle sentiment nuances and sarcasm, focusing on achieving high accuracy with a sophisticated evaluation metric, specifically F1-score.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Model complexity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WebMD Review dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Sentiment\"],\n            \"specification\": {\n                \"size\": null,\n                \"sentiment_classes\": [\"positive\", \"negative\", \"neutral\", \"sarcastic\"]\n            },\n            \"description\": \"A text-based dataset from WebMD reviews with the objective to handle sentiment nuances and sarcasm.\",\n            \"preprocessing\": [\"sentiment analysis, sarcasm detection\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model performance per sentiment type\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"WebMD Review Classifier\",\n            \"family\": \"Deep Learning (e.g., LSTM, BERT)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": null,\n                \"handling_of_sarcasm\": \"incorporated\"\n            },\n            \"description\": \"An advanced text classification model using deep learning techniques, specifically designed to account for sentiment nuances and sarcasm in the WebMD Review dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"challenges\",\n            \"description\": \"Accounting for sarcasm, a non-traditional aspect in sentiment analysis, may require specialized NLP techniques or contextual embeddings to identify subtle nuances.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"sentiment and sarcasm feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Cloud-based inference service\"],\n        \"demonstration\": \"Performance comparison with a baseline model on a nuanced evaluation set\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the given instructions, considering the user's expertise level and the specific requirements for enhancing the text classification task on the WebMD Review dataset. The problem domain and downstream task have been correctly identified. Performance metrics include F1-score, accuracy, and complexity considerations. However, some specific metric values are left unspecified to accommodate potential optimization goals or experimental setup. The provided knowledge section indicates the challenges and strategies for the task.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for a cutting-edge luxury retail analytics company, we've embarked on a mission to revolutionize store management with an exclusive object detection algorithm. Our focus shifts to the rare and niche 'Premium Luxury Product Dataset', containing high-resolution images of luxury items like haute couture, rare art, and exclusive watches. The model must be based on the advanced Mask R-CNN architecture, ensuring not just detection but also segmentation of these unique items to preserve their distinct attributes and positioning. The model must maintain a perfect balance between extremely high precision (less than 1% false positives) to avoid misidentifying luxury goods and recall, especially for elusive designer collaborations. Additionally, request a comprehensive analysis of the algorithm's impact on identifying genuine antiques and collectibles in crowded display cases, complete with a breakdown by geographical provenance. Don't forget to include a visually striking and interactive performance dashboard to showcase the results.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury retail\",\n        \"description\": \"As a project manager, we aim to develop an advanced object detection model using Mask R-CNN for the 'Premium Luxury Product Dataset' to detect and segment luxury items like haute couture, rare art, and exclusive watches. The model should have high precision (< 1% false positives) and maintain strong recall, especially for designer collaborations. Additionally, analyze impact on antique and collectible identification in crowded display cases, with a breakdown by geographical provenance. A visually striking and interactive dashboard is required.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.99,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Premium Luxury Product Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image resolution\": \"high\",\n                \"data type\": \"luxury items\"\n            },\n            \"description\": \"A dataset of high-resolution images of luxury items for object detection and segmentation using Mask R-CNN.\",\n            \"preprocessing\": [\n                \"resize images to a consistent resolution\",\n                \"image normalization\"\n            ],\n            \"augmentation\": [\n                \"data augmentation for increased robustness\"\n            ],\n            \"visualization\": [\n                \"showcase of performance on examples\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Exclusive Luxury Object Detection\",\n            \"family\": \"Mask R-CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"advanced\",\n                \"masking capabilities\": \"true\"\n            },\n            \"description\": \"A Mask R-CNN model designed for detecting and segmenting luxury products with extremely high precision and good recall.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"artifact extraction for unique item attributes\",\n            \"crowded scene analysis\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom-built platform\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"interactive dashboard presenting results by geographical provenance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The information provided accurately captures the user's high-level objective (expertise), the specific area and task, along with the model and dataset requirements. Performance metrics reflect the precision and recall targets, and complexity metrics suggest the need for optimization, likely due to the large and high-resolution images. The service section covers post-model development activities like analysis, dashboard, and device deployment. However, as this is a complex project, some information is subjective (precision and recall expectations) and requires domain knowledge to fine-tune.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for our machine learning team, can you develop a sophisticated time-series forecasting model that not only identifies peak demand periods but also accounts for seasonality, trends, and external factors? The system must provide accurate short-term and long-term forecasts to enhance staff scheduling and operation efficiency across various geographical locations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"business operations and scheduling\",\n        \"description\": \"Develop a sophisticated time-series forecasting model that accounts for seasonality, trends, and external factors to predict peak demand periods for staff scheduling across various geographical locations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Short-term Forecast Error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Long-term Forecast Error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory Size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multivariate Time-series Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"peak demand\"],\n            \"specification\": null,\n            \"description\": \"Data containing historical time-series data with multiple variables including geographical locations, seasonality, trends, and external factors for staff scheduling.\",\n            \"preprocessing\": [\n                \"Handling missing values\",\n                \"Normalization\"\n            ],\n            \"augmentation\": [\n                \"Resampling for different time granularities\"\n            ],\n            \"visualization\": [\n                \"Anomaly detection\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Sophisticated Time-series Model\",\n            \"family\": \"State-of-the-art forecasting model (e.g., ARIMA, LSTM, Prophet, or advanced hybrid models)\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A machine learning model tailored for time-series forecasting that incorporates seasonality, trends, and external factors.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Consider external factors like economic indicators, holidays, and weather patterns in the model\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Geographical-based feature extraction\",\n            \"Feature engineering for external factors\"\n        ],\n        \"target_device\": \"Cloud\",\n        \"deployment_endpoint\": \"AI Operations Platform\",\n        \"inference_engine\": [\n            \"Real-time or offline (batch) processing\"\n        ],\n        \"demonstration\": \"Interactive dashboard with visualizations and forecast results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the essence of the user's request for a time-series forecasting model with emphasis on accuracy and adaptability to various locations. Performance metrics are specified without values, assuming they would be set based on user-defined criteria. The model description and dataset properties align with the task requirements. Certain steps for data preprocessing and feature engineering reflect the need for accounting for peak demand, seasonality, trends, and external factors. The service and deployment specifications cover the practical implementation in a business setting. However, there might be a need for more specific model or dataset details, as the user's expertise level is high.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "Can you assist me in developing a state-of-the-art object detection model that specializes in identifying not only domestic pets but also exotic wildlife entering my private suburban property, ensuring real-time surveillance with minimal latency for swift intervention?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"security and surveillance\",\n        \"description\": \"The user wants to develop a state-of-the-art object detection model capable of identifying both domestic pets and exotic wildlife in real-time for their private suburban property with minimal latency for intervention.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time detection\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Suburban Wildlife and Pet Data\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"domestic pets\", \"exotic wildlife\"],\n            \"specification\": {\n                \"data distribution\": \"balanced for both domestic pets and exotic wildlife\",\n                \"capture environment\": \"private suburban property\"\n            },\n            \"description\": \"A dataset containing images and videos capturing various wildlife and pets for real-time object detection.\",\n            \"preprocessing\": [\"image resizing\", \"label annotation\"],\n            \"augmentation\": [\"real-time object insertions\", \"weather and lighting variations\"],\n            \"visualization\": [\"dataset annotations and label distributions\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art (e.g., YOLOv5, Mask R-CNN, or EfficientDet)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"detection algorithm\": null\n                },\n                \"accuracy\": {\n                    \"mAP\": null\n                },\n                \"real-time performance\": {\n                    \"latency\": null\n                }\n            },\n            \"description\": \"A highly accurate and real-time object detection model for identifying domestic pets and exotic wildlife.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using transfer learning to improve training efficiency and domain adaptation for exotic wildlife recognition.\",\n        \"Integrate efficient algorithms to minimize latency during real-time inference.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"extracting features relevant to wildlife and pet detection\"],\n        \"target_device\": \"real-time embedded system with low latency requirements\",\n        \"deployment_endpoint\": \"edge computing or on-premise server\",\n        \"inference_engine\": [\"Caffe2, TensorFlow, or TensorRT for optimized performance\"],\n        \"demonstration\": \"Live video feed with bounding boxes for detected pets and wildlife\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's request for a cutting-edge object detection model with an emphasis on real-time and minimal latency. Performance metrics reflect the need for real-time detection and latency, while complexity metrics focus on inference time. The dataset, model, and service specifications cater to the task of identifying pets and wildlife in a private suburban setting. The lack of specific values is due to the need for more technical details to be provided by the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "The client has proposed an ambitious project for an AI-powered mobile application, leveraging the Stanford Dogs Varied Shapes and Colors Dataset. They require a real-time, high-accuracy deep learning model (preferably a convolutional neural network) for rapid breed recognition. Can you propose a lightweight yet efficient architecture, ensuring minimal latency for users?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"mobile application\",\n        \"description\": \"The client wants an AI-powered mobile app for real-time breed recognition using the Stanford Dogs Varied Shapes and Colors Dataset, targeting high accuracy and minimal latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Dogs Varied Shapes and Colors Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\"],\n            \"specification\": {\n                \"size\": null,\n                \"image_size\": null,\n                \"number_of_classes\": null\n            },\n            \"description\": \"Dataset for real-time breed recognition of dogs with varied shapes and colors.\",\n            \"preprocessing\": [\"image resizing\", \"normalization\"],\n            \"augmentation\": [\"data augmentation\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"convolutional neural network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"lightweight\",\n                \"flops\": null,\n                \"parameters\": null,\n                \"latency\": null\n            },\n            \"description\": \"A lightweight and efficient CNN architecture designed for real-time, high-accuracy breed recognition on mobile devices.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"insight\",\n            \"value\": \"Due to the mobile application context, we should prioritize a model with few parameters and low inference time. MobileNet or SqueezeNet could be suitable candidates.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"on-device inference\"],\n        \"demonstration\": \"Live demo with latency measurements for user experience feedback\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON captures the essence of the client's requirements for a high-accuracy real-time breed recognition application. Performance metrics include accuracy and inference speed, while emphasizing on a lightweight model. The dataset refers to the Stanford Dogs dataset, and the recommendation for a CNN architecture includes a focus on efficiency. The expert level of the user indicates familiarity with complex tasks and the suitability of the provided information. However, the specific values for model performance and complexity metrics are not provided, leaving room for further inquiry.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image segmentation", "instruction": "Create a high-resolution image segmentation project using the lesser-known Paris StreetTrees dataset, focusing on the preservation and restoration of 18th-century ornamental trees. Develop a state-of-the-art U-Net architecture incorporating temporal-spatial fusion, to discern not only species but also microclimatic impacts on foliage and bark health. The model should be robust against occlusions and weather anomalies, with a key emphasis on forecasting potential stress due to climate change. Output visual heatmaps for early intervention planning and a detailed report quantifying tree health for heritage preservation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"heritage preservation and urban planning\",\n        \"description\": \"Create a high-resolution image segmentation project using the Paris StreetTrees dataset, focusing on 18th-century ornamental trees. Develop a state-of-the-art U-Net architecture with temporal-spatial fusion to identify species and assess microclimatic impacts on tree health. Model must handle occlusions and weather anomalies, emphasizing climate change-induced stress forecasting. Provide visual heatmaps for early intervention and a detailed health report for heritage preservation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mIoU (mean Intersection over Union)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Paris StreetTrees dataset\",\n            \"modality\": [\"image\", \"time series\"],\n            \"target_variables\": [\"tree species\", \"microclimatic impacts\", \"tree health\"],\n            \"specification\": {\n                \"image resolution\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"temporal resolution\": {\n                    \"frequency\": null,\n                    \"duration\": null\n                }\n            },\n            \"description\": \"Dataset of high-resolution images capturing 18th-century ornamental trees in Paris, including temporal data to analyze microclimatic conditions.\",\n            \"preprocessing\": [\n                \"temporal data augmentation\"\n            ],\n            \"augmentation\": [\n                \"occlusion simulation\",\n                \"weather anomaly generation\"\n            ],\n            \"visualization\": [\n                \"heatmaps for stress prediction\",\n                \"tree health visualizations\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Temporal-Spatial Fusion U-Net\",\n            \"family\": \"U-Net\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"incorporating temporal-spatial fusion modules\"\n            },\n            \"description\": \"State-of-the-art U-Net for image segmentation on the Paris StreetTrees dataset, capable of discerning tree species and health indicators under challenging conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Lesser-known Paris StreetTrees dataset offers unique data for studying climate change impacts on tree health\",\n        \"Microclimatic conditions affect tree health and resilience\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"temporal feature extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time or offline\"],\n        \"demonstration\": \"Interactive visual heatmaps and health reports\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been constructed based on the user's detailed instruction, including developing a U-Net for image segmentation with specific performance metrics. The model's robustness against occlusions and weather anomalies is highlighted, as well as the focus on forecasting tree stress due to climate change. The dataset and preprocessing steps align with the requirements. However, some metric values and device-specific details are not provided, indicating room for additional clarification based on the user's expertise level.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "For a large-scale enterprise project, develop a real-time, high-throughput question-answering system using the TriviaQA dataset. Mandate the integration of a state-of-the-art transformer model, like GPT-4, showcasing exceptional performance in tackling factoids and intricate queries. Additionally, design it to meet stringent latency requirements of under 50 milliseconds, ensuring seamless scalability and efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"enterprise\",\n        \"description\": \"Develop a real-time, high-throughput question-answering system for a large-scale enterprise project using the TriviaQA dataset. The system should incorporate a state-of-the-art transformer model, specifically GPT-4, for superior performance on factoids and intricate queries. It must meet a latency requirement of 50 milliseconds or less and ensure seamless scalability and efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Factoid accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Intricate query handling performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 50,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Throughput\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Memory efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Scalability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TriviaQA\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\"],\n            \"specification\": {\n                \"size\": null,\n                \"context_length\": null,\n                \"question_length\": null\n            },\n            \"description\": \"A large-scale dataset for question answering tasks, containing diverse factoids and intricate queries.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GPT-4\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A state-of-the-art transformer model for question-answering, targeting high performance on both factoids and intricate queries.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transformer architecture and recent developments in GPT series\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time, high throughput environment\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"optimized for latency\"],\n        \"demonstration\": \"Real-time, low-latency responses to various queries\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately reflects the user's request for a real-time, enterprise-grade QA system using the TriviaQA dataset and the GPT-4 model. Performance metrics for factoid and intricate queries are included, along with specific latency requirements (50ms) for scalability and efficiency. The model's state-of-the-art nature is highlighted, and it is assumed that the user has a high level of expertise. However, details on model specifications, throughput, and exact deployment details are lacking, requiring additional clarification from the user.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "matrix factorization", "instruction": "As a project manager for the AI-driven retail recommendation system, enhance the matrix factorization task by incorporating seasonality and geographical data, aiming to discover not only product co-purchasing patterns but also dynamically adjust recommendations based on real-time trends and customer location.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"matrix factorization\",\n        \"application_domain\": \"retail\",\n        \"description\": \"Enhance the matrix factorization task for an AI-driven retail recommendation system by incorporating seasonality and geographical data, targeting to discover product co-purchasing patterns and dynamically adjust recommendations based on real-time trends and customer location.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time adaptation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"geospatial inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Retail transaction data\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"product IDs\", \"customer locations\", \"purchase dates\"],\n            \"specification\": {\n                \"seasonality_data_length\": null,\n                \"geographical_data_resolution\": null,\n                \"data_size\": null\n            },\n            \"description\": \"Contains product sales data, including co-purchasing patterns, customer locations, and purchase dates to factor in seasonality and geography.\",\n            \"preprocessing\": [\"seasonality normalization\", \"geographical clustering\"],\n            \"augmentation\": [],\n            \"visualization\": [\"spatiotemporal patterns\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Matrix Factorization\",\n            \"family\": \"Collaborative Filtering\",\n            \"type\": \"statistical models\",\n            \"specification\": {\n                \"inclusion_of_factors\": \"seasonality and geography\",\n                \"real-time update mechanism\": null,\n                \"geolocation-aware recommendations\": true\n            },\n            \"description\": \"A matrix factorization model that integrates seasonality and geographical data to enhance recommendation accuracy and adaptability in a retail environment.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"geographical user profiles\",\n            \"seasonal product clusters\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"retail recommendation API\",\n        \"inference_engine\": [\"real-time analytics\"],\n        \"demonstration\": \"live A/B testing results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately reflects the user's request to enhance an AI-driven retail recommendation system with matrix factorization. The task incorporates seasonality and geographical data, as well as real-time trends and customer location for recommendations. The expert-level expertise indicates a thorough understanding of the project requirements. However, specific performance metrics and some complexities' value are left open as the instruction does not provide specific targets for them.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "In the field of forensic entomology, investigators require a high-resolution image segmentation algorithm that can not only differentiate between various types of insect species found on human remains, but also accurately identify their life stages, feeding patterns, and postmortem colonization stages. This is crucial for estimating time of death and determining the post-mortem interval in criminal investigations. Can you develop a machine learning model that can process macro-photographs and produce comprehensive segmentation maps for forensic entomologists to streamline their analysis?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"forensic science\",\n        \"description\": \"Develop a high-resolution image segmentation algorithm for forensic entomology, which must accurately differentiate insect species, identify their life stages, feeding patterns, and postmortem colonization stages for estimating time of death and post-mortem interval in criminal investigations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"species-specific F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"life stage identification accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Forensic Entomology Images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\n                \"insect species\",\n                \"life stages\",\n                \"feeding patterns\",\n                \"postmortem colonization stages\"\n            ],\n            \"specification\": null,\n            \"description\": \"A collection of macro-photographs of insects found on human remains for training and evaluation of the segmentation algorithm.\",\n            \"preprocessing\": [\n                \"image resizing\",\n                \"data augmentation (if required)\"\n            ],\n            \"augmentation\": [\n                \"geometric transformations\",\n                \"image flipping\"\n            ],\n            \"visualization\": [\"segmentation output for each image\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Convolutional Neural Networks (CNN) with U-Net or similar architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural details\": \"High-resolution input handling, multiple output channels for each identified category\",\n                \"hyperparameters\": \"Fine-tuned for forensic entomology task\"\n            },\n            \"description\": \"A deep learning model for image segmentation, designed to handle macro-photographs and produce high-resolution segmentation maps for forensic entomological analysis.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud or high-end computing environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU accelerated\", \"optimized for forensic workflow\"],\n        \"demonstration\": \"Workflow demonstrations showcasing improvements in time of death estimation and post-mortem interval determination\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction outlines a specific task for developing a machine learning model for forensic entomology, emphasizing the need for accuracy in various aspects. The user's expertise level is assumed to be high given the complexity of the problem. The JSON response includes performance metrics, complexity metrics, and specific data requirements to accommodate the task. However, specific metric values and some architectural details are left open for optimization based on further research or discussion.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Revise the task for the Enron Email dataset analysis using an RGCN model: Develop a scalable, high-performance node classifier that not only discerns between employee roles and communication subgroups, but also detects implicit collaboration networks. The model should handle a diverse set of attributes, large graph sizes (up to 100,000 nodes), and demonstrate adaptability to evolving communication dynamics over time.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"business, email communication\",\n        \"description\": \"Revise the Enron Email dataset analysis task using an RGCN model. The goal is to develop a scalable, high-performance node classifier that can distinguish employee roles, identify communication subgroups, and detect implicit collaboration networks. The model should handle large graphs with up to 100,000 nodes and be adaptable to changing communication dynamics over time.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"adaptability\",\n                \"value\": \"dynamic\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Enron Email dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"employee roles\", \"communication subgroups\", \"collaboration networks\"],\n            \"specification\": {\n                \"max_nodes\": 100000,\n                \"node_attributes\": \"diverse\"\n            },\n            \"description\": \"A large graph dataset from the Enron Email corpus, including attributes and connections to analyze employee roles, communication patterns, and collaboration networks.\",\n            \"preprocessing\": [\"handling large graph sizes\"],\n            \"augmentation\": [],\n            \"visualization\": [\"display evolving dynamics\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"RGCN\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"custom RGCN designed for large graphs and dynamic adaptation\",\n                \"node_embedding_size\": null,\n                \"hidden_layer_sizes\": null\n            },\n            \"description\": \"A scalable, high-performance RGCN model that addresses employee role and collaboration detection in the evolving Enron Email dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"name\": \"Large graph handling techniques\",\n            \"description\": \"Leverage efficient graph algorithms and parallel processing to manage large graph sizes\"\n        },\n        {\n            \"name\": \"Dynamic GNN adaptation\",\n            \"description\": \"Design the model to adapt to changing patterns in communication dynamics over time\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"extracting relevant features for employee roles and communication\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for cloud or distributed computing\"],\n        \"demonstration\": \"graphs and visual representations of evolving dynamics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's instruction by specifying the desired model, task, and dataset. Performance metrics emphasize scalability, high-performance, and adaptability, while complexities are managed through large graph handling techniques and efficient inference. However, specific metric values are not given as they would depend on the performance optimization during implementation.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager, you demand the machine learning team to design an advanced image classification system using Fashion-MNIST, focusing on Capsule Networks. The system must not only surpass a benchmark of 90% accuracy but also exhibit exceptional resilience to affine transformations. Moreover, mandate a thorough analysis, comparing capsule network performance with cutting-edge CNNs, emphasizing not only accuracy but also computational efficiency and interpretability, to justify the architectural choice.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"fashion\",\n        \"description\": \"Design an advanced image classification system using Fashion-MNIST with Capsule Networks, targeting at least 90% accuracy and resistance to affine transformations. Perform a comprehensive analysis comparing capsule network performance with state-of-the-art CNNs, focusing on accuracy, computational efficiency, and interpretability to justify architectural choices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashion-MNIST\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"image classes\"],\n            \"specification\": {\n                \"image_shape\": [28, 28, 1],\n                \"number_of_classes\": 10\n            },\n            \"description\": \"A dataset of fashion clothing items used for advanced image classification tasks.\",\n            \"preprocessing\": [\"image normalization\", \"affine transformation-resistant preprocessing\"],\n            \"augmentation\": [\"affine transformations\"],\n            \"visualization\": [\"comparison of capsule and CNN performance\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Capsule Network\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Capsule Networks\",\n                \"resistance_to_transformations\": \"affine\"\n            },\n            \"description\": \"An advanced image classification model based on Capsule Networks for Fashion-MNIST.\"\n        },\n        {\n            \"name\": \"Cutting-edge CNN\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art CNN\",\n                \"accuracy\": null,\n                \"computational_efficiency\": null,\n                \"interpretability\": null\n            },\n            \"description\": \"A reference comparison model demonstrating the latest CNN performance on Fashion-MNIST.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"comparison\",\n            \"content\": \"Capsule Networks exhibit enhanced resilience to affine transformations and better interpretability.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"network architecture optimization\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"systematic comparison of capsule network and CNN performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response accurately captures the project manager's demand for a machine learning team to build a high-performing image classification system with a focus on capsule networks and benchmark accuracy, while emphasizing efficiency and interpretability. Specific metric values, such as accuracy and inference time, are left open to indicate they would be determined during the project development process. The response reflects the user's expertise level and the depth of the required analysis.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for our advanced public safety system, you are required to develop a state-of-the-art object detection algorithm that not only recognizes and alerts incidents in real-time across diverse public transportation settings but also differentiates between various types of incidents (health codes, suspicious packages, unattended objects) with high accuracy. The system must integrate seamlessly with existing infrastructure and provide location-based prioritization for emergency responses.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"public safety\",\n        \"description\": \"Develop a state-of-the-art object detection algorithm for real-time incident recognition in public transportation, differentiating among health codes, suspicious packages, and unattended objects with high accuracy. The system should integrate with existing infrastructure and prioritize emergency responses based on location.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"frames per second\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Public Transportation Incident Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"health codes\", \"suspicious packages\", \"unattended objects\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"data format\": null\n            },\n            \"description\": \"A diverse dataset containing images of incidents across public transportation with labels for accurate object detection.\",\n            \"preprocessing\": [\"image normalization\", \"data augmentation\"],\n            \"augmentation\": [\"random cropping\", \"color jittering\"],\n            \"visualization\": [\n                \"confidence heatmaps\",\n                \"class activation maps\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Object Detection Model\",\n            \"family\": \"neural networks (CNN)\",\n            \"type\": \"deep learning\",\n            \"specification\": null,\n            \"description\": \"A state-of-the-art object detection architecture designed for real-time incident recognition in diverse public transportation scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using transfer learning from a pre-trained model to boost performance on a smaller dataset.\",\n        \"Exploit domain-specific features to differentiate between different incident types.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for incident detection\"],\n        \"target_device\": \"edge computing\",\n        \"deployment_endpoint\": \"real-time public safety system\",\n        \"inference_engine\": [\"GPU-optimized libraries\"],\n        \"demonstration\": \"live video stream with incident alerts and prioritized responses\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the given instruction, targeting a high-accuracy object detection algorithm for public safety. The user's expertise is assumed to be high given the project management role. Performance metrics, complexity metrics, and dataset details are expected but not explicitly provided in the given text. The model is assumed to fall under deep learning using CNNs, and the system is designed to integrate with existing infrastructure and prioritize based on location. The 'confidence' section reflects the need for further specification of the exact performance metrics and system deployment details.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "text classification", "instruction": "For the SMS Spam Detection project, develop a real-time, resource-efficient system using a hybrid architecture of stacked bidirectional LSTM and GRU cells, with attention mechanism. Optimize for precision, recall, and F1-score above 97%, ensuring the model runs smoothly on resource-constrained mobile devices with a stringent latency requirement of less than 50 milliseconds per message for seamless user experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"spam filtering or cybersecurity\",\n        \"description\": \"Develop a real-time SMS Spam Detection system using a hybrid architecture of stacked bidirectional LSTM and GRU cells with attention mechanism.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 50,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SMS Spam Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"spam or not spam\"],\n            \"specification\": {\n                \"size\": \"enough for training\",\n                \"dimension\": \"text-based features\",\n                \"length\": \"variable\"\n            },\n            \"description\": \"A dataset containing SMS messages labeled as spam or non-spam for real-time detection.\",\n            \"preprocessing\": [\n                \"tokenization\",\n                \"lowercasing\"\n            ],\n            \"augmentation\": [\n                \"synthetic data generation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid LSTM-GRU with Attention\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"stacked bidirectional LSTM and GRU cells\",\n                \"attention_mechanism\": \"true\"\n            },\n            \"description\": \"A resource-efficient, real-time spam detection model combining LSTM and GRU with attention mechanism.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilize transfer learning if available to improve resource efficiency\",\n        \"Fine-tune hyperparameters for resource constraints\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"lightweight feature extraction\"\n        ],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for mobile deployment\"],\n        \"demonstration\": \"real-time message classification demo\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response includes all the relevant details based on the instruction. The task is clear, specifying a spam detection system with high precision, recall, and F1-score targets. The metrics address resource efficiency and latency for mobile devices. The dataset is labeled and described, with specific preprocessing and augmentation steps. The model and its hybrid architecture are specified, acknowledging the need for resource efficiency. The knowledge section mentions possible strategies for optimizing the model under resource constraints, and the service section includes steps for deployment on mobile devices.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the high-resolution car rental platform, we need a specialized image segmentation model that can identify not just individual cars but also their make, model, and license plate within complex urban scenes. The system should analyze lighting conditions, overlapping vehicles, and markings to validate if the parking spot meets specific accessibility and safety requirements. Can you design and integrate this advanced algorithm into our existing architecture?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"transportation and logistics\",\n        \"description\": \"Design and integrate a specialized image segmentation model for recognizing cars, their makes, models, and license plates in complex urban scenes, accounting for lighting conditions, overlapping vehicles, and safety/ accessibility requirements for parking spots.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"high-resolution car rental platform images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"car make\", \"car model\", \"license plate\"],\n            \"specification\": {\n                \"image_size\": null,\n                \"complexity\": \"complex urban scenes\"\n            },\n            \"description\": \"A dataset containing images from the high-resolution car rental platform with diverse urban scenarios for model training and evaluation.\",\n            \"preprocessing\": [\"image resizing\", \"lighting normalization\"],\n            \"augmentation\": [\"overlapping vehicles synthesis\", \"variable lighting conditions\"],\n            \"visualization\": [\"model input-output examples\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Image Segmentation Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"convolutional neural networks\",\n            \"specification\": {\n                \"architecture\": \"e.g., U-Net, Mask R-CNN, or DeepLabV3+\",\n                \"accuracy metric\": null,\n                \"specific model constraints\": \"Real-time performance and high accuracy\"\n            },\n            \"description\": \"A deep learning model designed to handle complex urban scenes, achieving accurate segmentation of cars, their make, model, and license plates.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using transfer learning with pre-trained models on large-scale datasets like COCO or Cityscapes to boost performance on complex scenes.\",\n        \"Account for variations in lighting and overlapping vehicles through data augmentation techniques and model architectural improvements.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"extracting relevant features from raw images\"],\n        \"target_device\": \"cloud or edge\",\n        \"deployment_endpoint\": \"existing platform API integration\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"example use cases showcasing accuracy and system responsiveness\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers the user's intent to build a specialized image segmentation model for a specific problem, accounting for the user's high expertise level. The schema has been populated with details based on the instruction, like application domain, data modality, target task, and requirements for performance and complexity. Some fields left unspecified require further clarification, like performance metrics and specific model architecture. However, the main aspects are adequately addressed.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "question answering", "instruction": "The project manager must develop a sophisticated, interactive question-answering platform designed specifically for early childhood education, utilizing the exclusive KidsQA dataset. The system necessitates a BERT-based model with pedagogical optimization and a tailored, visually appealing interface that incorporates gamification elements to maximize learning engagement and comprehension for young users.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"early childhood education\",\n        \"description\": \"Develop a sophisticated, interactive question-answering platform specifically for early childhood education using the KidsQA dataset. The system must utilize a BERT-based model with pedagogical optimization and a visually appealing, gamified interface.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time for low-end devices\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"KidsQA dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\"],\n            \"specification\": null,\n            \"description\": \"A dataset designed for early childhood education, containing questions and their corresponding answers for a BERT-based question-answering platform.\",\n            \"preprocessing\": [\"BERT tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT-based Question Answering Model\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"num_layers\": null,\n                \"hidden_size\": null,\n                \"attention_heads\": null,\n                \"trainable\": true\n            },\n            \"description\": \"A BERT model tailored for question-answering in an educational context, with pedagogical optimization techniques applied.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Pedagogical optimization techniques: Explainability, uncertainty estimation, and personalized learning paths.\"\n        },\n        {\n            \"content\": \"Visually appealing interface: Minimalistic design, colorful elements, and animated responses.\"\n        },\n        {\n            \"content\": \"Gamification elements: Point system, badges, and progress tracking.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"personalized content recommendations\",\n            \"dynamic difficulty adjustment\"\n        ],\n        \"target_device\": \"mobile, tablet, and low-end devices\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\n            \"GPU-accelerated\"\n        ],\n        \"demonstration\": \"Include a mock-up or a brief video showcasing the platform's functionality and design.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON represents a clear and detailed request for a BERT-based QA platform for early childhood education. The instruction specifies a BERT-based model, pedagogical optimization, visual design, and gamification components. Performance metrics and complexity considerations are given with placeholders, as no specific values are provided in the instruction. The platform's service and deployment aspects are also covered according to the user's needs.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "For a machine learning development team in the competitive real estate market, design an advanced image classification system that not only differentiates between various property features (like pools, gardens, and kitchens) but also discerns subtle nuances like room orientations, architectural styles, and eco-friendly elements, ensuring a comprehensive and competitive listing presentation for high-end clients.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"real estate\",\n        \"description\": \"Design an advanced image classification system for a machine learning development team in the real estate market. The system must differentiate between various property features (pools, gardens, kitchens), identify subtle nuances like room orientations, architectural styles, and eco-friendly elements, aimed at providing a comprehensive and competitive listing presentation for high-end clients.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"seconds per image\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\n                \"property features\",\n                \"room orientations\",\n                \"architectural styles\",\n                \"eco-friendly elements\"\n            ],\n            \"specification\": null,\n            \"description\": \"A dataset containing high-quality images of properties, each with annotations for various features, nuances, and eco-friendly elements.\",\n            \"preprocessing\": [\n                \"image resizing\",\n                \"image normalization\",\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\"GAN-based synthetic data generation\"],\n            \"visualization\": [\n                \"example image heatmaps for feature detection\",\n                \"confusion matrix\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep convolutional neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A state-of-the-art deep learning model for image classification in real estate, capable of identifying a wide range of property features and nuances.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider transfer learning for faster convergence and better performance.\",\n        \"Utilize object detection techniques to enhance feature extraction.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"extracting ROI features for ecological elements\",\n            \"fusion of multiple CNN outputs for nuanced feature representation\"\n        ],\n        \"target_device\": \"cloud-based for efficient GPU processing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"real-time prediction API\"],\n        \"demonstration\": \"Augmented reality property tours showcasing detected features\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's instruction by detailing an advanced image classification system for a real estate setting, accounting for both explicit and nuanced property features. Expertise level is set to high, assuming a deep understanding of the requirements. Performance metrics are left unspecified for the user to define the desired level. Complexity metrics are included to address the focus on speed and resource efficiency. The dataset and model specifications are provided, and knowledge and service requirements reflect the competitive nature of the real estate market.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager for a cutting-edge machine learning team, your assignment is to tackle a high-dimensional, multivariate time series forecasting challenge. The \"Traffic\" dataset, now extended to include diverse variables and seasonality patterns, has been divided into train, validation, and test sets with non-consecutive timestamp blocks (INPUT_SEQ_LEN=96, INPUT_DIM=862). The objective is to develop a state-of-the-art model that not only predicts the subsequent future sequences (PRED_SEQ_LEN=96) but also incorporates interpretability and long-term dependency. Emphasize on reducing mean squared error (MSE) and mean absolute error (MAE), while ensuring robustness to outliers and temporal shifts in real-world conditions. Expectations include optimizing hyperparameters for ensemble models and presenting a detailed analysis of error propagation for effective forecasting.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"traffic management\",\n        \"description\": \"A high-dimensional, multivariate time series forecasting challenge with the Traffic dataset. The data is divided into train, validation, and test sets with non-consecutive timestamp blocks (INPUT_SEQ_LEN=96, INPUT_DIM=862). The objective is to develop a state-of-the-art model predicting future sequences (PRED_SEQ_LEN=96) with interpretability and long-term dependency. Focus on minimizing MSE and MAE, ensuring robustness to outliers and temporal shifts, and optimizing hyperparameters for ensemble models.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"temporal shifts robustness\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Traffic\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"future sequences\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 862\n            },\n            \"description\": \"A dataset extended to incorporate diverse variables and seasonality patterns for high-dimensional time series forecasting. Split into train, validation, and test sets with non-consecutive timestamp blocks.\",\n            \"preprocessing\": [\"data cleaning, normalization, and feature engineering for high-dimensional data\"],\n            \"augmentation\": [],\n            \"visualization\": [\"intermittent visualizations for interpretability\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": \"built-in or incorporate techniques\",\n                \"long-term dependency handling\": \"LSTM, Transformer, or other suitable architecture\",\n                \"ensemble models\": \"ensemble methods for improved robustness\"\n            },\n            \"description\": \"A state-of-the-art model designed for high-dimensional time series forecasting with interpretability and long-term dependency handling.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider techniques for handling high-dimensionality, such as dimensionality reduction and feature selection\",\n        \"Investigate methods to tackle seasonality and temporal shifts\",\n        \"Implement robust loss functions and outlier detection\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"optimizing for ensemble models\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"error propagation analysis and visualization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure covers the essential components as per the instruction, with a high-expertise level reflecting the project manager's role. The problem description and requirements are clear, including ensemble models, interpretability, long-term dependency, and error analysis. Specific model and performance metrics values are not provided as per the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "multimodal forgery detection", "instruction": "As a project manager for the machine learning development team, you've been tasked with exploring the applicability of advanced transformer models, specifically considering their capacity to handle multimodal analysis for forgery detection in the high-resolution 'Celeb-DF' dataset. The objective is to design a model that not only outperforms simple heuristics but also exhibits state-of-the-art performance in distinguishing genuine from manipulated video sequences.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"media forensics\",\n        \"description\": \"Explore the applicability of advanced transformer models for forgery detection in high-resolution Celeb-DF dataset, with a focus on outperforming heuristics and achieving state-of-the-art performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score (specific to forgery detection)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Inference Speed\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Model Size\",\n                \"value\": null,\n                \"unit\": \"parameters\"\n            },\n            {\n                \"name\": \"Memory Usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"Training Time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celeb-DF\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"forged vs genuine\"],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"video_resolution\": \"high\",\n                \"data_size\": null,\n                \"data_split\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                }\n            },\n            \"description\": \"Celeb-DF dataset for multimodal forgery detection, containing high-resolution images and videos for training and evaluation.\",\n            \"preprocessing\": [\n                \"data normalization\",\n                \"formatting for transformer model\"\n            ],\n            \"augmentation\": [\n                \"multimodal data augmentation techniques\"\n            ],\n            \"visualization\": [\"model performance on sample sequences\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-based Forgery Detection Model\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specific architecture for multimodal analysis\",\n                \"pretraining\": \"potentially pre-trained on a large multimodal dataset\"\n            },\n            \"description\": \"A state-of-the-art transformer model designed for detecting forgery in high-resolution Celeb-DF dataset, surpassing simple heuristics.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art approaches in multimodal forgery detection, best practices for training transformer models, and fine-tuning strategies.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"custom feature extraction for multimodal analysis\"\n        ],\n        \"target_device\": \"cloud or GPU-equipped devices\",\n        \"deployment_endpoint\": \"potentially a secure or private cloud deployment\",\n        \"inference_engine\": [\"high-performance inference libraries\"],\n        \"demonstration\": \"demo videos, qualitative results, and performance metrics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the project task, specifying the model development in multimodal learning and focus on Celeb-DF dataset. Metrics are mentioned as performance indicators, with emphasis on accuracy, F1 score, and inference speed. Model specifications and preprocessing are covered, acknowledging the need for fine-tuning and performance optimizations. The domain, complexities, and future steps are addressed based on the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for the AI-Med initiative, devise a specialized Question-Answering model tailored to dermatological symptom analysis. Utilize the extensive 'SkinDisorders-Comorbidity' dataset, fine-tune a domain-adapted MergedBERT model, ensuring interpretability and integrating citations from PubMed for each diagnostic inference.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"dermatology\",\n        \"description\": \"Design a specialized QA model for dermatological symptom analysis, using the 'SkinDisorders-Comorbidity' dataset. Fine-tune a domain-adapted MergedBERT model and ensure interpretability, integrating citations from PubMed for diagnostic inference.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"interpretability\",\n                \"value\": null,\n                \"unit\": \"score\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SkinDisorders-Comorbidity\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"dermatological symptoms\"],\n            \"specification\": null,\n            \"description\": \"A dataset for dermatological symptom analysis, containing questions and corresponding answers for fine-tuning the model.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Domain-adapted MergedBERT\",\n            \"family\": \"neural networks\",\n            \"type\": \"question answering models\",\n            \"specification\": {\n                \"interpretability_score\": null\n            },\n            \"description\": \"A fine-tuned MergedBERT model specifically adapted for dermatological symptom analysis, including interpretability features.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Integration of PubMed citations for diagnostic accuracy.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Illustrate diagnostic inferences with PubMed citations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's instruction, including building a QA model for dermatological analysis, fine-tuning a MergedBERT model, and ensuring interpretability. The dataset is specified, and integration with PubMed citations is mentioned. However, performance metrics are not provided as the instruction focuses more on the model design and content adaptation. The level of expertise implies a higher understanding of the requirements, and the structure seems comprehensive.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for a leading edge machine learning team, you've been tasked with optimizing the performance of a high-precision tabular regression project. The Mohs Hardness Dataset, now with additional domain-specific variables and non-linear interactions, has been stratified into train, validation, and test sets. The objective is to develop a state-of-the-art model that predicts the precise mineral strength with a focus on minimizing the Median Absolute Error (MedAE). In addition to model training, incorporate feature engineering techniques and propose a novel regularization method for improved generalization across all splits. Document your thought process and experimental setup in a clear, reproducible research protocol.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"material science\",\n        \"description\": \"Optimize the performance of a high-precision tabular regression project with the Mohs Hardness Dataset, focusing on minimizing Median Absolute Error (MedAE). Add domain-specific variables and non-linear interactions, and apply feature engineering techniques. Include a novel regularization method for improved generalization across train, validation, and test splits.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Median Absolute Error\",\n                \"value\": 0.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"generalization performance\",\n                \"value\": null,\n                \"unit\": \"percent reduction in MedAE\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Mohs Hardness Dataset (with additional variables)\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"mineral strength\"],\n            \"specification\": {\n                \"stratification_splits\": [\"train\", \"validation\", \"test\"]\n            },\n            \"description\": \"A stratified dataset with domain-specific variables and non-linear interactions for predicting mineral strength.\",\n            \"preprocessing\": [\n                \"stratified splitting\",\n                \"handling domain-specific variables\",\n                \"handling non-linear interactions\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art tabular regression\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": [\"interpretable model (e.g., tree-based)\", \"novel regularization method\"],\n                \"implementation\": \"clear and reproducible research protocol\"\n            },\n            \"description\": \"A high-precision model for mineral strength prediction with enhanced feature engineering and novel regularization to minimize MedAE.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Thought process: Analyzing domain-specific correlations, selecting relevant feature interactions, and balancing model complexity to optimize generalization.\"\n        },\n        {\n            \"content\": \"Experimental setup: A systematic evaluation of various feature engineering techniques and regularization methods, followed by detailed documentation of the winning strategy.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\",\n            \"handling missing values\",\n            \"encoding categorical variables\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"high-performance inference\"\n        ],\n        \"demonstration\": \"Research protocol and results summary report\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's project manager role and the requirements for an optimized regression model. It includes a high level of expertise, specific metrics, and acknowledges the need for feature engineering and novel regularization. The clear and reproducible research protocol aspect highlights the need for detailed documentation. Some fields such as target device and deployment endpoint are left unspecified as they are not mentioned in the instruction.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image segmentation", "instruction": "Our ambitious task for an underwater coral reef biodiversity study demands the development of a novel, deep learning-driven generative model. Employ a hybrid adversarial U-Net and transformer architecture to segment and isolate not only hard-to-detect species but also track temporal changes in reef health. Crucially, the model must provide pixel-level attribution maps for understanding how different coral forms contribute to overall ecosystem dynamics, while catering to the needs of marine biologists with limited computational expertise.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"marine biology\",\n        \"description\": \"Task is an underwater coral reef biodiversity study, requiring a deep learning-driven generative model using a hybrid adversarial U-Net and transformer architecture. The model should segment species, track temporal changes in reef health, and provide pixel-level attribution maps for coral form contributions to ecosystem dynamics. It needs to cater to marine biologists with limited computational expertise.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Segmentation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Temporal change detection accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Underwater coral reef imagery\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"Species labels\", \"Coral health status\"],\n            \"specification\": {\n                \"image_size\": \"variable\",\n                \"color_channels\": 3,\n                \"image_depth\": null,\n                \"samples_per_class\": null\n            },\n            \"description\": \"A dataset of underwater coral reef images containing diverse species and health status, for training and evaluation.\",\n            \"preprocessing\": [\"Image normalization\", \"Data augmentation (limited)\"],\n            \"augmentation\": [\"Small geometric transformations\", \"Limited color jittering\"],\n            \"visualization\": [\"Pixel-level attribution maps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid adversarial U-Net and Transformer Model\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"U-Net with transformer blocks\",\n                \"loss_function\": \"custom (generative adversarial and segmentation)\",\n                \"optimizer\": \"Adam with learning rate scheduling\"\n            },\n            \"description\": \"A deep learning model combining U-Net for image segmentation and transformers for long-range dependencies in coral reef data\"\n        }\n    ],\n    \"knowledge\": [\n        \"Limited computational expertise should be catered to by providing lightweight model optimization, training on low-end hardware, or developing explanations for non-experts.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Feature extraction for transformer blocks\", \"Coral feature representation\"],\n        \"target_device\": \"cloud with accessible GPUs for training\",\n        \"deployment_endpoint\": \"user-friendly web application for data submission and analysis\",\n        \"inference_engine\": [\"TensorFlow, compatible with lightweight deployment\"],\n        \"demonstration\": \"Interactive visualizations and model explainability tools\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's intent to build a complex model for coral reef biodiversity study. The user's high expertise indicates a need for a challenging and technically advanced solution. The specified hybrid model architecture, performance metrics, and service requirements align with the instruction's details. However, specific performance targets and service features (such as lightweight models and visualization) are placeholders without specific values or criteria, leaving room for completion based on actual requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a cutting-edge machine learning project manager for a specialized team, your task is to develop a state-of-the-art time-series forecasting model for an extraordinary meteorological dataset. The dataset, named 'Atmospheric Phenomena Predictor (APP)', contains highly granular data on global weather patterns and atmospheric anomalies spanning over a century, with irregular sampling rates. The dataset has been meticulously divided into four segments: an extended training set (80% historical data), a validation set for hyperparameter tuning (10%), an early warning set (5%), and a confidential extreme event prediction zone (5%). Each input sequence consists of a mix of long-term trends (INPUT_SEQ_LEN=500), multidimensional variables (INPUT_DIM=25), and extreme event indicators. The challenge is to forecast the subsequent 72-hour weather events (PRED_SEQ_LEN=72) with an emphasis on forecasting rare and catastrophic atmospheric conditions, such as hurricanes, tornadoes, and extreme temperature fluctuations, accurately.\n\nYour primary objective is to design, implement, and fine-tune a deep learning architecture that not only optimizes the Mean Squared Error (MSE) and Mean Absolute Error (MAE) but also incorporates novel performance indicators like the Probability of Detection ( POD) and False Alarm Ratio (FAR) to evaluate the model's ability to both identify and mitigate potential disasters. Additionally, the model should exhibit robustness against concept drift and seasonality, ensuring reliable forecasting even in the face of changing weather patterns.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"meteorology\",\n        \"description\": \"Develop a state-of-the-art time-series forecasting model for the Atmospheric Phenomena Predictor (APP) dataset. The model should handle irregular sampling rates, long-term trends, multidimensional data, and forecast rare/extreme weather events like hurricanes, tornadoes, and extreme temperature fluctuations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Mean Squared Error (MSE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Error (MAE)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Probability of Detection (POD)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"False Alarm Ratio (FAR)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Atmospheric Phenomena Predictor (APP)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"72-hour weather events\"],\n            \"specification\": {\n                \"history_length\": {\"INPUT_SEQ_LEN\": 500},\n                \"features\": {\"INPUT_DIM\": 25},\n                \"sampling_rate\": \"irregular\"\n            },\n            \"description\": \"A global weather dataset with 80% for training, 10% for validation, 5% for early warning, and 5% for confidential event prediction. Focus on extreme weather and irregular sampling.\",\n            \"preprocessing\": [\"handling irregular sampling rates\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"deep learning (e.g., LSTM, Transformer, or hybrid)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"potentially including layers for long-term trends, dimensionality reduction, and specialized event indicators\",\n                \"drift_handling\": \"robust against concept drift\",\n                \"seasonality\": \"addresses seasonality\"\n            },\n            \"description\": \"A state-of-the-art time-series forecasting model designed for rare event prediction with a focus on accuracy and disaster mitigation\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"incorporating extreme event indicators\"],\n        \"target_device\": \"\", \n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"potentially using specialized engines for time-series forecasting\"],\n        \"demonstration\": \"generation of forecasts with associated POD and FAR scores\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's task, specifying the problem area, dataset characteristics, performance metrics, and incorporating requirements for deep learning architecture, metric optimization, and resilience against concept drift and seasonality. However, specific model details are left vague to account for the user's preference for a state-of-the-art design. The confidence score indicates the level of detail that can be confidently included based on the provided instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "For a time-sensitive, real-world application, enhance the time-series classification task by incorporating energy-efficient feature selection and accounting for seasonal patterns in sensor data. Suggest a model capable of both interpretability and high accuracy within a strict latency constraint of 10 milliseconds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"real-world, time-sensitive\",\n        \"description\": \"Enhance time-series classification for a real-world application with energy-efficient feature selection and accounting for seasonal patterns in sensor data. The model should be interpretable and achieve high accuracy within a latency constraint of 10 milliseconds.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 10\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"classification labels\"],\n            \"specification\": null,\n            \"description\": \"Sensor data with time-series features, containing seasonal patterns, for real-time classification.\",\n            \"preprocessing\": [\n                \"energy-efficient feature selection\",\n                \"seasonality handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"interpretable time-series models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"interpretability\": true,\n                \"latency-compatible architecture\": true\n            },\n            \"description\": \"A time-series classification model designed for energy efficiency and interpretability, meeting the 10ms latency requirement.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"energy-efficient\",\n            \"real-time\"\n        ],\n        \"target_device\": \"time-sensitive environment (e.g., IoT device, cloud)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for latency\"\n        ],\n        \"demonstration\": \"Fast response for time-sensitive tasks\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been structured to capture the user's requirements for a time-series classification task in a real-world application. The model's performance metrics focus on accuracy and a strict latency constraint. Energy-efficient feature selection and handling seasonal patterns are incorporated as preprocessing steps. The model type (neural networks) is chosen for interpretability. However, the specific model name and some metric values are not provided since they are not directly mentioned in the user's instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Develop a state-of-the-art video-based visual question answering system using the extensive YouTube-8M dataset. Incorporate advanced 3D residual CNNs and multi-modal Transformer fusion for enhanced content analysis, while emphasizing on intricate concept understanding, temporal reasoning, and the ability to tackle diverse topics with real-time adaptation. Strive for exceptional accuracy and promptness in answering questions that require complex spatio-temporal and semantic comprehension.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"media and entertainment\",\n        \"description\": \"Develop a state-of-the-art video-based visual question answering system using the YouTube-8M dataset. The system should employ advanced 3D residual CNNs and multi-modal Transformer fusion for enhanced content analysis. Key focuses should be on intricate concept understanding, temporal reasoning, ability to handle diverse topics, and real-time adaptation. The system must achieve high accuracy and promptness in answering complex spatio-temporal and semantic questions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube-8M\",\n            \"modality\": [\"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_size\": \"extensive\",\n                \"feature_types\": \"3D residual CNN features and multi-modal data\",\n                \"topic_diversity\": \"high\"\n            },\n            \"description\": \"A large-scale video dataset containing diverse content for training the visual question answering system.\",\n            \"preprocessing\": [\"3D feature extraction\", \"frame-level feature extraction\"],\n            \"augmentation\": [\"temporal\", \"spatial\", \"multi-modal\"],\n            \"visualization\": [\n                \"qualitative analysis of answer accuracy across different question types\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"3D Residual CNNs\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"type_of_fusion\": \"multi-modal Transformer fusion\"\n            },\n            \"description\": \"An advanced video-based model employing 3D residual CNNs and Transformer fusion for visual question answering\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"technique\",\n            \"content\": \"Advanced concepts of 3D residual learning and multi-modal Transformer fusion\"\n        },\n        {\n            \"type\": \"area\",\n            \"content\": \"Real-time adaptation to handle diverse topics and complex spatio-temporal analysis\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"temporal reasoning module\", \"multi-modal fusion\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time inference accelerator\"],\n        \"demonstration\": \"Interactive demo showcasing the system's performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been structured based on the user's instruction, which specifies a state-of-the-art video-based visual question answering system using YouTube-8M. The areas of expertise, dataset, model, and performance metrics are included to accommodate the advanced techniques and requirements mentioned. Some values are left unspecified for further clarification from the user, reflecting a realistic scenario where these details are not yet finalized.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project demands a specialized e-commerce product description translation system, utilizing the Taobao dataset. Mandate a fine-tuned U-Net model tailored for the retail and fashion sectors, ensuring not just a BLEU score of 37+ but also cultural adaptability and the capacity to live-integrate with diverse APIs for instantaneous, context-aware translation of dynamic product catalogues, fostering seamless international consumer experience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"e-commerce, retail, and fashion\",\n        \"description\": \"A specialized e-commerce product description translation system is required, using the Taobao dataset. The system should employ a fine-tuned U-Net model, focusing on cultural adaptability and real-time integration with APIs for dynamic product catalog translation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 37.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"integration with diverse APIs\",\n                \"value\": null,\n                \"unit\": \"API compatibility\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Taobao dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"product descriptions\"],\n            \"specification\": {\n                \"domain\": \"e-commerce, retail, and fashion\"\n            },\n            \"description\": \"A dataset containing product descriptions for the retail and fashion sectors to fine-tune the U-Net model.\",\n            \"preprocessing\": [\"cultural adaptation preprocessing\"],\n            \"augmentation\": [\"context-aware data augmentation\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Fine-tuned U-Net\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"domain_specificity\": \"retail and fashion\",\n                \"cultural_adaptability\": true\n            },\n            \"description\": \"A U-Net model optimized for product description translation in the retail and fashion sectors, with cultural adaptability and real-time integration capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporate domain-specific terminology and cultural nuances to enhance translation quality.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"API integration, context-awareness\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"dynamic and scalable translation service\",\n        \"inference_engine\": [\"API-based real-time translation\"],\n        \"demonstration\": \"Live integration demo showcasing seamless international product catalog translations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON is designed based on the user's clear instructions, focusing on building a specialized translation system with fine-tuned U-Net, BLEU score target, cultural adaptability, and API integration. The Taobao dataset is specified, along with pre- and post-processing steps. Missing values, such as API speed, are left unspecified as the instruction doesn't require specific numbers or thresholds.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a machine learning project for the science fair, you need to analyze not only daily solar energy production but also incorporate weather forecasts and seasonality. Develop a robust time-series forecasting model that predicts the energy output for the upcoming week, accounting for real-time weather patterns and historical trends, with a maximum error tolerance of 5%.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"science fair\",\n        \"description\": \"Analyze daily solar energy production, incorporating weather forecasts and seasonality to create a robust model for forecasting energy output for the upcoming week. Aim for a maximum error tolerance of 5%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MAPE (Mean Absolute Percentage Error)\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Solar Energy Production Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"daily solar energy production\"],\n            \"specification\": {\n                \"length\": null,\n                \"time_interval\": null\n            },\n            \"description\": \"Daily solar energy production data with historical trends and associated weather data.\",\n            \"preprocessing\": [\n                \"resampling to match weather forecasts\",\n                \"handling missing values\"\n            ],\n            \"augmentation\": [\n                \"seasonality-aware data augmentation\"\n            ],\n            \"visualization\": [\n                \"time-series analysis of historical trends\"\n            ],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"Weather Forecasts\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"length\": null,\n                \"time_interval\": null\n            },\n            \"description\": \"Weather forecast data covering the solar energy production period with real-time conditions.\",\n            \"preprocessing\": [\n                \"aligning with energy production data\"\n            ],\n            \"augmentation\": [\n                \"weather-related data augmentation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"RNN (or LSTM/GRU) with exogenous inputs\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"model_size\": null,\n                    \"dropout_rate\": null\n                },\n                \"training\": {\n                    \"epochs\": null,\n                    \"batch_size\": null\n                }\n            },\n            \"description\": \"A time-series forecasting model that incorporates real-time weather patterns and historical solar energy data for week-long predictions.\"\n        }\n    ],\n    \"confidence\": {\n        \"rationale\": \"The given instruction has been parsed into a JSON object that outlines a machine learning project. It involves time-series forecasting for solar energy, incorporating weather forecasts and seasonality. Performance metric is set to a maximum error tolerance of 5% as specified. However, some fields such as model architecture and hyperparameters are left unspecified, as the exact details would depend on the dataset and user's expertise. This JSON is structured to guide further action and accommodate varying levels of input detail.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "tabular clustering", "instruction": "Perform advanced tabular clustering on the 'Fashionista Trends and Engagement' dataset using a hybrid approach of DBSCAN and Hierarchical clustering, integrating deep learning techniques for enhanced feature extraction. The objective is to detect sub-communities of fashion enthusiasts based on their unique buying behaviors, social media activity, and seasonal preferences. Design an algorithm that not only maintains a processing time of under 1 second but also offers a visually interpretable dendrogram for strategic product recommendations and influencer partnerships. Additionally, incorporate a sentiment analysis component to gauge emotional responses to product clusters, ensuring marketing messaging resonates with each segment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular clustering\",\n        \"application_domain\": \"e-commerce, fashion\",\n        \"description\": \"Perform advanced tabular clustering on the 'Fashionista Trends and Engagement' dataset using a hybrid approach of DBSCAN and Hierarchical clustering with deep learning feature extraction. The goal is to identify sub-communities based on buying behaviors, social media activity, and seasonal preferences. The algorithm should process in under 1 second and provide a visually interpretable dendrogram for product recommendations and influencer partnerships. Incorporate sentiment analysis for emotional response analysis of product clusters.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"processing time\",\n                \"value\": 1.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Fashionista Trends and Engagement\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"buying behaviors\", \"social media activity\", \"seasonal preferences\"],\n            \"specification\": {\n                \"data_size\": \"not specified\",\n                \"feature_dimensions\": \"not specified\"\n            },\n            \"description\": \"A dataset containing fashion-related data for clustering and analysis of fashion enthusiasts' behavior.\",\n            \"preprocessing\": [\"deep learning feature extraction\", \"data cleaning\", \"standardization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"interpretable dendrogram\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"hybrid (DBSCAN + Hierarchical clustering)\",\n            \"type\": \"ensemble models, statistical models, neural networks\",\n            \"specification\": {\n                \"deep learning component\": \"not specified\"\n            },\n            \"description\": \"An advanced clustering model using a combination of DBSCAN and Hierarchical clustering with deep learning for enhanced feature extraction on the Fashionista Trends and Engagement dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider applying sentiment analysis techniques like sentiment140 or VADER to gauge emotional responses.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"data normalization, dimensionality reduction\"],\n        \"target_device\": \"optimized for real-time performance, likely cloud-based\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized distributed computation, GPU acceleration\"],\n        \"demonstration\": \"Design a visual analysis tool to present dendrogram and sentiment insights to marketing team.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is crafted based on the user's advanced tabular clustering and integration of sentiment analysis. The user's high expertise level suggests they expect intricate and optimized methods. Processing time and the requirement for a visually interpretable dendrogram are detailed. However, exact metrics for sentiment analysis or model performance are not provided, leaving room for the user to specify further.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "Develop a real-time, time-series anomaly detection system for our website traffic, incorporating hourly geolocation data and user behavior patterns. The system must not only flag unusual spikes or dips but also differentiate between normal seasonal fluctuations and genuine anomalies, providing actionable insights for security audits.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"web security and analytics\",\n        \"description\": \"Build a real-time, time-series anomaly detection system for website traffic, incorporating hourly geolocation data and user behavior patterns. The system should distinguish between unusual spikes or dips, normal seasonal fluctuations, and genuine anomalies, providing actionable insights for security audits.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Detection accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"False Positive Rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Website traffic data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"anomaly flag\", \"geolocation\", \"user behavior\"],\n            \"specification\": {\n                \"time_resolution\": \"hourly\",\n                \"dimensions\": {\n                    \"geolocation\": {\n                        \"level\": \"hourly\"\n                    },\n                    \"user_behavior\": {\n                        \"level\": \"hourly\"\n                    }\n                }\n            },\n            \"description\": \"Contains website traffic data with hourly resolution, including geolocation and user behavior patterns for anomaly detection and differentiation.\",\n            \"preprocessing\": [\"hourly aggregation\", \"feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"time series visualization\",\n                \"geographical distribution visualization\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"time-series anomaly detection model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep learning\",\n                \"model_components\": [\"autoencoder\", \"recurrent layers\"]\n            },\n            \"description\": \"A real-time, deep learning-based model for time-series anomaly detection, incorporating hourly geolocation and user behavior patterns.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider incorporating moving averages and seasonal decomposition to separate normal fluctuations.\",\n        \"Use Bayesian techniques to handle uncertainty in data for improved anomaly classification.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time updates\", \"streaming data processing\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"API for security audits\",\n        \"inference_engine\": [\"Apache Kafka\", \"TensorFlow Serving\"],\n        \"demonstration\": \"Interactive dashboard with visualizations and alerts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a real-time anomaly detection system for website traffic. It addresses the requirements for hourly data, geolocation, and user behavior, along with performance metrics for accuracy and low-latency. The model is specified as a neural network within the time-series analysis domain. Missing metric values are denoted as null for placeholders, and additional information about handling fluctuations and deploying the system are provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "Develop a tailored link prediction model for Tencent Weibo using an Attention-enhanced Graph Convolutional Network (AGCN) with heterogeneous information integration, focusing on sub-graph level analysis and temporal dynamics to capture intricate affinity within the social network.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"social media\",\n        \"description\": \"Develop a tailored link prediction model for Tencent Weibo using an Attention-enhanced Graph Convolutional Network (AGCN) with heterogeneous information integration, focusing on sub-graph level analysis and temporal dynamics to capture intricate affinity within the social network.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds per query\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tencent Weibo data\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"potential connections\"],\n            \"specification\": {\n                \"node_types\": null,\n                \"edge_types\": null,\n                \"temporal_intervals\": null\n            },\n            \"description\": \"A social network dataset containing heterogeneous information for Tencent Weibo, focusing on sub-graph analysis and temporal dynamics.\",\n            \"preprocessing\": [\n                \"heterogeneous information integration\",\n                \"temporal feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention-enhanced Graph Convolutional Network (AGCN)\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"network_architecture\": null,\n                \"number_of_layers\": null,\n                \"heterogeneous_integration\": true\n            },\n            \"description\": \"A model that combines AGCN and heterogeneous information for link prediction in Tencent Weibo, accounting for sub-graph level and temporal dynamics.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Heterogeneous information, such as user profiles, text content, and timestamps, will be crucial for modeling in the Weibo dataset.\"\n        },\n        {\n            \"description\": \"Temporal graph convolutions help capture evolving relationships over time.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"time-aware feature engineering\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's intent to build a link prediction model using AGCN, integrating heterogeneous information and temporal dynamics. The model's performance metrics like precision, recall, and AUC-ROC are mentioned, leaving room for specific target values. The dataset description includes the pre-processing steps as suggested. However, the model's architecture and exact complexity metrics could benefit from further clarification based on the user's expertise level and possible constraints.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for the enhanced precision agriculture initiative, you are required to refine the existing drone-based disease and nutrient deficiency detection system. Instruct the machine learning team to integrate a state-of-the-art Mask R-CNN model on the 'Advanced PlantVillage' dataset, considering variations in illumination, weather conditions, and multiple plant species. The model must demonstrate superior performance with a minimum 95% mean Average Precision (mAP) on rectified aerial images, while optimizing for real-time processing and scalable deployment. Additionally, include a detailed plan for transfer learning from related object detection domains and validate the model with cross-validation techniques.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Refine the existing drone-based disease and nutrient deficiency detection system by integrating a Mask R-CNN model on the 'Advanced PlantVillage' dataset. Account for variations in illumination, weather conditions, and multiple plant species. Aim for a minimum mAP of 95% on rectified aerial images, prioritize real-time processing, and enable scalable deployment. Include a transfer learning plan and cross-validation for model validation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean Average Precision (mAP)\",\n                \"value\": 0.95,\n                \"unit\": \"%\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": \"optimized\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Advanced PlantVillage\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"diseases/deficiencies, plant species\"],\n            \"specification\": {\n                \"illumination variation\": true,\n                \"weather conditions\": true,\n                \"number of plant species\": \"multiple\"\n            },\n            \"description\": \"A high-resolution aerial dataset with variations in illumination, weather, and multiple plant species for drone-based disease and nutrient deficiency detection.\",\n            \"preprocessing\": [\n                \"Image rectification\"\n            ],\n            \"augmentation\": [\n                \"Adapt to varying weather and lighting conditions\"\n            ],\n            \"visualization\": [\n                \"Plant species and anomaly detection visualizations\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Mask R-CNN\",\n            \"family\": \"deep convolutional neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"transfer_learning\": true,\n                \"cross_validation\": true\n            },\n            \"description\": \"A state-of-the-art object detection model that targets disease and nutrient deficiency detection in diverse agricultural scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"subject\": \"Transfer learning\",\n            \"details\": \"From related object detection domains for better performance and faster adaptation.\"\n        },\n        {\n            \"subject\": \"Cross-validation\",\n            \"details\": \"To validate the model's robustness and generalizability across diverse conditions.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Adapt to real-time processing\"\n        ],\n        \"target_device\": \"edge, drone-mounted system\",\n        \"deployment_endpoint\": \"IoT platform, farm management system\",\n        \"inference_engine\": [\"optimized for real-time\"],\n        \"demonstration\": \"Deployed model performance with videos showcasing detection and analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately captures the user's instructions for the machine learning project, focusing on refining an object detection system, optimizing performance metrics, and ensuring real-time capabilities. Transfer learning and cross-validation are explicitly mentioned, as are the necessary dataset and deployment considerations. Given the high complexity of the task, the user's expertise is assumed to be high.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "As a project manager, devise a cutting-edge visual question answering platform utilizing the immense YouTube-8M corpus, mandating the integration of advanced 3D residual CNNs with cutting-edge recurrent cross-attention Transformers. Prioritize the system's capacity for deep intricate concept dissection, precise temporal analysis, and topic adaptability within real-time constraints. Guarantee exceptional performance in answering intricate spatio-temporal queries while maintaining ultra-fast response times, reflecting multi-modal, nuanced understanding of diverse visual content.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"media and entertainment\",\n        \"description\": \"Design a visual question answering platform using YouTube-8M corpus, incorporating advanced 3D residual CNNs with recurrent cross-attention Transformers. Focus on deep concept dissection, precise temporal analysis, and topic adaptability under real-time constraints, ensuring high performance in answering complex spatio-temporal queries with ultra-fast response times.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"temporal precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"topic adaptation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean response time\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube-8M\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"videos_per_category\": null,\n                \"duration_range\": null,\n                \"video_features\": null\n            },\n            \"description\": \"A large-scale video dataset for visual understanding and analysis, focusing on multimodal content from YouTube.\",\n            \"preprocessing\": [\n                \"3D residual CNN feature extraction\",\n                \"Temporal slicing for recurrent cross-attention\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced 3D Residual CNNs and Recurrent Cross-Attention Transformers\",\n            \"family\": \"multimodal deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"An advanced model combining 3D CNNs for spatial understanding and recurrent Transformers for temporal and cross-modal reasoning in visual question answering.\"\n        }\n    ],\n    \"knowledge\": [\n        \"3D feature extraction enhances spatio-temporal understanding\",\n        \"Recurrent cross-attention promotes adaptability and temporal precision\",\n        \"Efficient implementation is crucial for real-time performance and ultra-fast response times\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Efficient video representation\"\n        ],\n        \"target_device\": \"cloud-based servers\",\n        \"deployment_endpoint\": \"API-based for real-time query processing\",\n        \"inference_engine\": [\"GPU-optimized inference\"],\n        \"demonstration\": \"Interactive platform showcasing nuanced understanding of diverse visual content\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the essence of the project manager's requirements, focusing on a cutting-edge VQA platform using YouTube-8M with specific modeling techniques. The metrics and constraints reflect the need for deep concept understanding, temporal analysis, and real-time response. The system includes feature engineering and deployment considerations, though some performance metrics are left undefined for optimization based on user's target values.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for the elite AI Research and Optimization Team, you've been assigned to enhance a cutting-edge tabular regression project using the specialized_combined Minerals Strength Data Set. This expanded dataset integrates domain expertise with complex, non-linear interactions and domain-specific variables. The train, validation, and test partitions have been meticulously stratified for enhanced performance evaluation. Your mandate is to develop a cutting-edge model that predicts mineral hardness with sub-microhardness unit accuracy, optimizing for the Reduced Median Absolute Deviation (RMAD) metric. Implement advanced feature extraction methods, such as deep interaction networks, and propose a proprietary regularization technique tailored for high-dimensional data. Ensure clear documentation of a reproducible, iterative research pipeline that highlights your creative problem-solving and model fine-tuning strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"mineral strength prediction\",\n        \"description\": \"Enhance a cutting-edge tabular regression project using the specialized_combined Minerals Strength Data Set. Focus on predicting mineral hardness with sub-microhardness unit accuracy, optimizing for Reduced Median Absolute Deviation (RMAD). Use advanced feature extraction like deep interaction networks and propose a proprietary regularization technique for high-dimensional data. Document a reproducible, iterative research pipeline with model fine-tuning strategies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMAD\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"specialized_combined Minerals Strength Data Set\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"mineral hardness\"],\n            \"specification\": {\n                \"stratification\": true,\n                \"partition_splits\": [\"train\", \"validation\", \"test\"],\n                \"dimensions\": {\n                    \"complex_interactions\": true,\n                    \"domain_specific_variables\": true\n                }\n            },\n            \"description\": \"An expanded dataset with complex, non-linear interactions, domain-specific variables, and stratified partitions for improved performance evaluation.\",\n            \"preprocessing\": [\"stratified sampling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep interaction networks\",\n            \"specification\": {\n                \"feature_extraction\": \"deep interaction networks\",\n                \"regularization_technique\": \"proprietary\"\n            },\n            \"description\": \"A cutting-edge model for mineral hardness prediction with sub-microhardness unit accuracy, focusing on high-dimensional data and advanced feature extraction.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Proposed proprietary regularization technique for high-dimensional data and creative problem-solving strategies in fine-tuning the model.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"deep interaction networks\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"iterative and reproducible research pipeline\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's role as a project manager for an AI research team. The focus on enhancing a tabular regression project, specific dataset details, and the demand for advanced techniques align with the instruction. Performance metrics include RMAD, and the user's high expertise suggests understanding of the task's requirements. The model details are based on the given constraints. Some details like specific model name and deployment specifics are missing, but the scope of the project and the creative problem-solving strategies are covered.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "For the AI-Med project, design a precise dermatology-focused Q&A system. Employ the 'SkinDiseases-SyndromeInteractions' dataset, enhance a domain-specific MergedBERT++ model, and implement real-time literature review from PubMed highlights, ensuring feature attributions and retaining audit trails for each expert-level diagnosis recommendation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medicine, dermatology\",\n        \"description\": \"Design a precise dermatology-focused Q&A system for the AI-Med project. Use the SkinDiseases-SyndromeInteractions dataset and enhance the MergedBERT++ model, incorporating real-time literature review from PubMed highlights. Ensure feature attributions and maintain audit trails for expert-level diagnosis recommendations.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SkinDiseases-SyndromeInteractions\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"disorder and syndrome interactions\"],\n            \"specification\": {\n                \"size\": \"specified in the dataset documentation\",\n                \"dimension\": \"textual data dimensions\"\n            },\n            \"description\": \"A dataset for dermatology, containing syndrome-disease interactions for the Q&A system.\",\n            \"preprocessing\": [\n                \"domain-specific preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MergedBERT++\",\n            \"family\": \"transformers (BERT-based)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"customized MergedBERT++ for dermatology domain\",\n                \"parameters\": \"enhanced with real-time PubMed integration\"\n            },\n            \"description\": \"An enhanced domain-specific MergedBERT++ model for dermatology Q&A, leveraging real-world medical literature.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Include PubMed highlights for real-time literature review and improving model performance.\"\n        },\n        {\n            \"description\": \"Implement feature attributions for transparency and model explainability.\"\n        },\n        {\n            \"description\": \"Maintain audit trails for expert-level diagnosis recommendations for accountability.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain adaptation\", \"contextual embeddings\"],\n        \"target_device\": \"cloud-based for real-time PubMed lookup\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"Interactive user interface showcasing expert-level Q&A results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response covers the user's request for a dermatology-focused Q&A system, utilizing the SkinDiseases-SyndromeInteractions dataset and enhancing the MergedBERT++ model. Real-time literature review from PubMed, feature attributions, and audit trails for expert-level recommendations have been incorporated as per the instruction. The JSON structure adheres to the schema and represents the main components of the requested project. However, performance and complexity metrics are not specified due to the lack of quantitative goals in the user's instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "For the telemarketing sentiment analysis competition, design a cutting-edge mobile application that employs a tailored fusion of stacked bidirectional LSTM and GRU neural networks with a dynamic hierarchical attention mechanism. Target a minimum F1-score of 98.5% for financial phishing detection, while maintaining real-time classification with sub-20 milliseconds latency for unobtrusive user interactions on low-end Android phones with minimal processing power.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"security and marketing\",\n        \"description\": \"Design a mobile application for telemarketing sentiment analysis with a focus on financial phishing detection using stacked bidirectional LSTM and GRU neural networks with a dynamic hierarchical attention mechanism.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.985\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 20,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"latency for real-time classification\",\n                \"value\": 20,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"processing constraints\",\n                \"value\": \"sub-20 milliseconds for low-end Android phones with minimal processing power\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Telemarketing Sentiment Analysis - Financial Phishing dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"financial phishing detection\"],\n            \"specification\": null,\n            \"description\": \"A dataset for sentiment analysis in telemarketing, specifically targeting financial phishing detection.\",\n            \"preprocessing\": [\"tokenizer, cleaning, and data balancing\"],\n            \"augmentation\": [\"lightweight for low-end devices\"],\n            \"visualization\": [\"performance insights\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Stacked BiLSTM-GRU with Hierarchical Attention\",\n            \"family\": \"Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_details\": {\n                    \"bidirectional_LSTM\": {},\n                    \"GRU\": {},\n                    \"dynamic_attention\": {}\n                },\n                \"parameters\": {\n                    \"Layers\": \"Optimal for mobile performance\",\n                    \"Units per layer\": \"Sufficient to achieve 98.5% F1-score\"\n                }\n            },\n            \"description\": \"Advanced NLP model utilizing stacked bidirectional LSTM and GRU with a hierarchical attention mechanism for financial phishing detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Use lightweight libraries, like TensorFlow Lite, for efficient mobile deployment\",\n        \"Utilize transfer learning and model compression techniques for low-end devices\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"targeted for mobile app\"],\n        \"target_device\": \"Android phones\",\n        \"deployment_endpoint\": \"mobile app integration\",\n        \"inference_engine\": [\"TensorFlow Lite\"],\n        \"demonstration\": \"Interactive, unobtrusive phishing warnings during calls\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON represents the user's request for a machine learning project focusing on designing a mobile app for telemarketing sentiment analysis with financial phishing detection. The F1-score target, real-time classification latency, and device-specific constraints are clearly outlined. The chosen model, preprocessing steps, and knowledge pieces reflect the user's expertise and the requirements. However, more details about the data and model selection are left as potential areas for further inquiry.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Sure, I'll adapt the prompt for a more complex time-series forecasting task:\r\n\r\nAs the project manager, request a state-of-the-art machine learning model specializing in geographically diverse time-series analysis. It should demand seasonality decomposition, advanced trend analysis, and real-time integration of socio-economic indicators. The model should deliver highly precise forecasts for peak hours and demand anomalies across different locations, balancing both short-term operational planning and long-term resource allocation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"business and operations management\",\n        \"description\": \"Request a state-of-the-art machine learning model for geographically diverse time-series analysis. The model should incorporate seasonality decomposition, advanced trend analysis, and real-time integration of socio-economic indicators. Aim for highly precise forecasts of peak hours and demand anomalies in different locations, optimizing for both short-term operational planning and long-term resource allocation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"forecast accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSE for peak hour forecasting\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE for demand anomaly detection\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time integration latency\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            },\n            {\n                \"name\": \"resource consumption (CPU or memory)\",\n                \"value\": null,\n                \"unit\": \"CPU cycles or MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"geographically diverse time-series data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"demand\", \"peak hours\", \"socio-economic indicators\"],\n            \"specification\": {\n                \"time_frequency\": \"hourly\",\n                \"locations\": null,\n                \"feature_data_types\": [\"numeric\", \"categorical\"]\n            },\n            \"description\": \"Data containing time-series patterns across diverse locations with varying socio-economic factors to train a sophisticated forecasting model.\",\n            \"preprocessing\": [\"seasonality decomposition\", \"feature engineering for socio-economic indicators\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time series pattern identification\", \"anomaly detection\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"deep learning (e.g., LSTM, Transformer-based, or hybrid)\",\n                \"inclusion of external features\": true\n            },\n            \"description\": \"A complex model adept at handling geographically diverse time-series data, incorporating seasonality decomposition and advanced trend analysis for improved forecasting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider incorporating domain-specific knowledge graphs or external data sources for enhancing forecast accuracy.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"socio-economic indicator integration\",\n            \"adaptation for real-time forecasting\"\n        ],\n        \"target_device\": \"cloud-based or edge computing infrastructure\",\n        \"deployment_endpoint\": \"high-availability API\",\n        \"inference_engine\": [\"real-time\", \"batch processing\"],\n        \"demonstration\": \"detailed performance reports and live forecasting demos\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers the user's request for a complex time-series forecasting model, factors such as seasonality decomposition and real-time integration, and the necessary requirements for both short-term and long-term planning. Performance metrics include forecast accuracy, specifically RMSE and MAE, as well as complexity metrics like latency and resource consumption. The model family is left unspecified due to the complexity of such state-of-the-art models, but it's implied they will be highly complex.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "The project's primary objective is to create a sophisticated Hierarchical Attention Network (HAN) model for arXiv paper classification, demanding pinpoint accuracy in differentiating intricate fields and subfields, resolving nuanced semantics, and maintaining a lofty benchmark of 95% F1 score. The model must feature live, real-time learning, dynamically adjusting to the fluctuating patterns in arXiv domain every 4 hours, while ensuring near-instantaneous handling of extensive datasets and incorporating Explainable AI principles for the transparency of attention weights. Efficiency in power consumption and memory optimization is equally crucial for large-scale deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Create a sophisticated Hierarchical Attention Network (HAN) model for arXiv paper classification, focusing on high accuracy, nuanced semantics, and achieving a minimum F1 score of 95%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"hours\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"power consumption\",\n                \"value\": null,\n                \"unit\": \"Watts\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"paper field and subfields\"],\n            \"specification\": {\n                \"size\": null,\n                \"average_length\": null,\n                \"document types\": null\n            },\n            \"description\": \"Dataset consisting of arXiv papers for text classification.\",\n            \"preprocessing\": [\n                \"Live, real-time learning (4-hour intervals)\",\n                \"Explainable AI for attention weights transparency\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Attention Network (HAN)\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"Hierarchical structure, possibly including transformers or LSTM layers\",\n                \"requirements\": \"Real-time learning, dynamic adaptation, explainability\"\n            },\n            \"description\": \"A state-of-the-art model for arXiv paper classification with a focus on hierarchical attention, dynamic learning, and explainability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Relevant literature on HAN models, arXiv domain adaptation, and Explainable AI techniques.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Drift detection and adaption techniques\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"potentially a cloud-based or edge service\",\n        \"inference_engine\": [\"optimized for real-time and scalable performance\"],\n        \"demonstration\": \"demonstrating the model's performance with updated attention weights and dynamic learning curves\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the complexity of the project, the requirements for high precision, real-time learning, and explainability, as well as the focus on large-scale deployment. The specified model family, performance metrics, and learning methods align with the task. Some uncertainty remains due to missing exact dataset size or specific model architecture details, but overall, the provided information is comprehensive based on the given instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The project manager is tasked with developing a highly specialized TinyBERT-based question-answering system, tailored for mobile devices. Target an impressive 87%+ accuracy on a comprehensive and diverse news corpus, maintaining a rigorous sub-50ms response time on low-end hardware, and concurrently optimizing for Energy Star certification, under strict efficiency constraints.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"news and mobile technology\",\n        \"description\": \"Develop a highly specialized TinyBERT-based question-answering system tailored for mobile devices, aiming for 87%+ accuracy on a comprehensive and diverse news corpus. The system must maintain a strict sub-50ms response time on low-end hardware and optimize for Energy Star certification while under strict efficiency constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null,\n                \"expected_value\": \"87%+\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": 50,\n                \"unit\": \"ms\",\n                \"constraint\": \"sub-50ms\"\n            },\n            {\n                \"name\": \"Energy efficiency\",\n                \"value\": \"Energy Star certification\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Comprehensive and diverse news corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"question and answer pairs\"],\n            \"specification\": null,\n            \"description\": \"A large and varied collection of news articles for question-answering, catering to the system's specific requirements.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TinyBERT-based Question-Answering Model\",\n            \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"TinyBERT\",\n                \"platform\": \"mobile devices\"\n            },\n            \"description\": \"A highly specialized TinyBERT model designed for mobile question-answering, tailored for energy efficiency and low-end hardware performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Energy efficient\"],\n        \"demonstration\": \"Performance demonstration on low-end hardware\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure accurately reflects the user's task, which is to build a highly specialized TinyBERT-based QA system. The requested performance metrics, strict response time constraint, and Energy Star optimization are captured. The dataset is specified as comprehensive and diverse news corpus, matching the question-answering domain. However, specific model metrics and missing details on energy efficiency methods are left for the developer's discretion.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "\"Develop a cutting-edge, explainable AI-driven Node2Vec-based model for the anisotropic srats2018 Drosophila Connectome dataset, focusing on the olfactory cortex. Differentiate between primary Kenyon cells and mushroom body Output neurons with high accuracy, maintaining interpretability and considering the impact of directed information flow and synaptic plasticity dynamics in the learning process. The model should also exhibit enhanced multi-scale performance and resilience to sparse connectivity, with a benchmark F1-score of at least 95%.\"", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"neuroscience\",\n        \"description\": \"Develop an explainable AI-driven Node2Vec model for the anisotropic srats2018 Drosophila Connectome dataset, focusing on the olfactory cortex. The model should differentiate between primary Kenyon cells and mushroom body Output neurons with high accuracy, maintaining interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"interpretability requirements\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"multi-scale performance\",\n                \"value\": \"enhanced\"\n            },\n            {\n                \"name\": \"resilience to sparse connectivity\",\n                \"value\": \"addressed\"\n            },\n            {\n                \"name\": \"directed information flow consideration\",\n                \"value\": \"included\"\n            },\n            {\n                \"name\": \"synaptic plasticity dynamics\",\n                \"value\": \"incorporated\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"anisotropic srats2018 Drosophila Connectome\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"primary Kenyon cells\", \"mushroom body Output neurons\"],\n            \"specification\": {\n                \"data_format\": \"connectome\",\n                \"sparse_fraction\": \"unknown\",\n                \"dimensions\": {\"nodes\": \"unknown\", \"edges\": \"unknown\"}\n            },\n            \"description\": \"A brain graph dataset focusing on the olfactory cortex of Drosophila, specifically for node classification between primary Kenyon cells and mushroom body Output neurons.\",\n            \"preprocessing\": [\n                \"anisotropic scaling\",\n                \"feature selection (based on neuroscientific relevance)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"network layout showcasing connectivity patterns\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Node2Vec-based\",\n            \"family\": \"explainable AI\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"Node2Vec\",\n                \"interpretability_techniques\": [\"LIME\", \"SHAP\"]\n            },\n            \"description\": \"A deep learning model utilizing Node2Vec for graph representation learning with high interpretability, designed for node classification of primary Kenyon cells and mushroom body Output neurons in the anisotropic srats2018 Drosophila Connectome dataset.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"embedding generation\",\n            \"directed information flow mapping\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-accelerated\"\n        ],\n        \"demonstration\": \"Explanation of model decisions on key examples\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a model, their level of expertise, and the requirements for the anisotropic srats2018 Drosophila Connectome dataset. The model details align with the instruction, specifying the Node2Vec-based approach, interpretability, and specific performance metrics. The model's feature engineering and inference settings take into account the learning process and requirements for directed information flow and synaptic plasticity. However, certain dataset dimensions and sparse connectivity fraction are unknown, and these should be inferred based on a thorough understanding of the dataset. Overall, the provided information is comprehensive for the given task.\",\n        \"score\": 0.97\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The project manager is required to develop an advanced, deep learning-based semantic segmentation model for Sentinel-2 imagery, optimizing for fine-grained land classification (precisely differentiating waterbodies, coniferous vs deciduous forests, and densely urban areas) with a stringent minimum accuracy of 95% within a limited 45-second processing window. Emphasize the model's ability to handle seasonal changes and adapt to swiftly changing weather conditions, ensuring consistent performance in real-time satellite data analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"remote sensing\",\n        \"description\": \"The project aims to develop an advanced, deep learning semantic segmentation model for Sentinel-2 imagery, focusing on fine-grained land classification (waterbodies, coniferous vs deciduous forests, and densely urban areas). The model must achieve a minimum accuracy of 95% and process within a limited 45-second processing window.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 45,\n                \"unit\": \"seconds\"\n            }\n        ],\n        \"additional_requirements\": {\n            \"seasonal_adaptation\": \"true\",\n            \"real_time_performance\": \"true\",\n            \"weather_resilience\": \"true\"\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sentinel-2 Imagery\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"waterbodies\", \"coniferous vs deciduous forests\", \"densely urban areas\"],\n            \"specification\": {\n                \"image_size\": \"variable (optimized for fast processing)\",\n                \"spectral_bands\": \"multispectral data suitable for deep learning\"\n            },\n            \"description\": \"High-resolution satellite imagery with a focus on Sentinel-2 for land classification tasks.\",\n            \"preprocessing\": [\n                \"data normalization\",\n                \"seasonal adjustment\"\n            ],\n            \"augmentation\": [\n                \"weather condition simulation\"\n            ],\n            \"visualization\": [\"image segmentation masks\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Semantic Segmentation Model\",\n            \"family\": \"deep learning (specifically, convolutional neural networks)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep, computationally efficient for 45-second window\",\n                \"layer_type\": \"e.g., U-Net or encoder-decoder\",\n                \"specific_metrics\": {\n                    \"IoU for each class\": {\n                        \"waterbodies\": null,\n                        \"coniferous forests\": null,\n                        \"deciduous forests\": null,\n                        \"densely urban areas\": null\n                    }\n                }\n            },\n            \"description\": \"A state-of-the-art, deep learning-based model optimized for fine-grained land classification and fast processing under varying weather conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Handling seasonal changes is crucial in Sentinel-2 data due to its revisit frequency and Earth's rotation.\",\n        \"Adapting to real-time weather conditions requires robust training data augmentation and model architecture design.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"satellite data preprocessing\",\n            \"feature extraction from Sentinel-2 data\"\n        ],\n        \"target_device\": \"Cloud-based or Edge computing for real-time performance\",\n        \"deployment_endpoint\": \"Cloud service or satellite data processing pipeline\",\n        \"inference_engine\": [\"GPU accelerated, optimized for real-time\"],\n        \"demonstration\": \"Live demonstration showcasing model performance on real-time Sentinel-2 data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's request for an advanced deep learning model with a specific accuracy target and processing time. It also addresses the requirements for seasonal adaptation, real-time performance, and weather resilience. The details on data preprocessing, augmentation, and model architecture cater to the fine-grained land classification task.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "For a high-precision urban housing prediction project, analyze an expansive dataset of regional property values, including historical trends, architectural details, and land use zoning. Apply innovative data fusion techniques from geospatial statistics, handle complex nested missing data with advanced imputation methods, and perform dimensionality reduction on a mix of ordinal and categorical variables. Employ advanced deep learning models, such as transformers fused with XGBoost ensembles, and employ Bayesian hyperparameter tuning. Present the RMSE for normalized log-transformed price estimates, focusing on permutation-based feature importance and model stability analysis on a strictly segregated evaluation dataset.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"real estate\",\n        \"description\": \"A high-precision urban housing prediction project involving analysis of an expansive dataset of regional property values, historical trends, architectural details, and land use zoning. Requires innovative data fusion techniques, handling complex nested missing data with advanced imputation methods, dimensionality reduction for ordinal and categorical variables, and advanced deep learning models (transformers fused with XGBoost ensembles). Performance metric includes RMSE for normalized log-transformed price estimates, with emphasis on permutation-based feature importance and model stability analysis on a segregated evaluation dataset.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE (normalized log-transformed price estimates)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Urban Housing Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"price\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": null,\n                \"time_series_frequency\": null\n            },\n            \"description\": \"A comprehensive dataset of regional property values with historical trends, architectural details, and land use zoning information.\",\n            \"preprocessing\": [\n                \"geospatial statistics\",\n                \"data fusion\",\n                \"missing data imputation\",\n                \"dimensionality reduction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XGBoost Ensemble\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"transformer + XGBoost\",\n                \"hyperparameter_tuning\": \"Bayesian\",\n                \"feature_engineering\": [\"fusion techniques\", \"nested data handling\", \"variable transformation\"]\n            },\n            \"description\": \"Advanced deep learning model that combines transformers for handling diverse data and XGBoost for ensemble learning, incorporating Bayesian hyperparameter tuning.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"permutation-based feature importance and model stability analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been constructed based on the user's instruction, addressing the high-precision housing prediction project, the specific datasets involved, and the technical requirements like advanced data processing and modeling techniques. Performance metrics, complexity, and model specifications have been tailored accordingly. The missing values in the metrics reflect the need for actual performance numbers. The high expertise level suggests that the given information is expected to be in-depth and detailed.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "In the realm of cutting-edge academic research, as a project manager for a high-impact node classification project, your team is tasked with utilizing the recently released EURXIV-ASTRO dataset, exclusively focusing on astrophysics arXiv papers. Design an innovative fusion of multi-view graph learning and active learning strategies, leveraging unsupervised techniques to identify subfields within the vastness of extragalactic astronomy. Not only should you evaluate the model's performance with the Graph Attention Mechanism, but also conduct a comparative analysis of the results with traditional clustering methods. Discuss the influence of temporal citation patterns and the scarcity of expert-verified labels on the state-of-the-art model's accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Design an innovative fusion of multi-view graph learning and active learning strategies for node classification in astrophysics arXiv papers using the EURXIV-ASTRO dataset. Focus on identifying subfields within extragalactic astronomy, utilizing the Graph Attention Mechanism, and comparing with traditional clustering methods. Address the impact of temporal citation patterns and the scarcity of expert-verified labels on accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Graph Attention Mechanism performance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Accuracy compared to traditional clustering methods\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time under multi-view and active learning scenarios\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"EURXIV-ASTRO\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"subfields in extragalactic astronomy\"],\n            \"specification\": null,\n            \"description\": \"A recently released dataset containing astrophysics arXiv papers for multi-view graph learning and active learning, focusing on extragalactic astronomy.\",\n            \"preprocessing\": [\"Extracting multi-view features\"],\n            \"augmentation\": [],\n            \"visualization\": [\"Visualizing subfield patterns in the graph\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Multi-view Graph Learning + Active Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"mention_of_fusion\": \"innovative\",\n                \"method_used\": \"Graph Attention Mechanism\",\n                \"unsupervised\": true\n            },\n            \"description\": \"A model that combines multi-view graph learning and active learning techniques for node classification in astrophysics papers, leveraging Graph Attention Mechanism and adapting to the scarcity of labels.\"\n        },\n        {\n            \"name\": \"\",\n            \"family\": \"Traditional Clustering\",\n            \"type\": \"comparison reference\",\n            \"specification\": null,\n            \"description\": \"For comparative analysis, this would be a traditional clustering method to gauge the effectiveness of the proposed model.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Temporal citation patterns can provide valuable context for identifying subfields\",\n        \"Scarcity of expert-verified labels can affect model performance, potentially inspiring more efficient labeling strategies or semi-supervised approaches\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Multi-view feature extraction\", \"Subfield extraction\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Graph Attention-based inference\"],\n        \"demonstration\": \"Comparison of model's progress over time and its performance improvement through active learning\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's high expertise level, the domain of astrophysics research, and the specific challenge of node classification with a focus on multi-view graph learning and active learning. The performance metrics are set to null, indicating a need to evaluate the model's performance with the Graph Attention Mechanism and clustering comparison. The scarcity of expert-verified labels and temporal citation patterns are addressed in the model description and performance metrics. However, some values and specific features, such as deployment target, are yet to be filled as they are not directly specified in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for a specialized AI-driven wildlife surveillance team, I require a high-resolution, species-specific, and time-sensitive deep learning model designed for nocturnal and precision object detection. The model should identify, track unique caracal individuals across multi-seasonal landscapes, accounting for camouflage, weather patterns, and urban-suburban boundaries, while adhering to stringent energy constraints and minimizing environmental footprint through innovative algorithm optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife surveillance\",\n        \"description\": \"Develop a high-resolution, species-specific deep learning model for nocturnal and precision object detection, focused on caracals in multi-seasonal landscapes. The model should account for camouflage, weather patterns, and urban-suburban boundaries, while maintaining energy efficiency and minimizing environmental impact through algorithm optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mAP (mean average precision)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy for caracal detection\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time per frame\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"energy consumption during operation\",\n                \"value\": null,\n                \"unit\": \"Watts\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Wildlife Surveillance Dataset (caracals)\",\n            \"modality\": [\"image\", \"video\"],\n            \"target_variables\": [\"caracal instances\", \"multi-seasonal landscapes\"],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"weather conditions\": [\"day/night\", \"weather variations\"],\n                \"scene_complexity\": \"urban-suburban\",\n                \"data_distribution\": \"multi-seasonal\"\n            },\n            \"description\": \"A dataset of caracal imagery and videos captured in diverse nocturnal and multi-seasonal environments, including various landscapes, camouflage, and weather patterns.\",\n            \"preprocessing\": [\"high-resolution image resizing\", \"data augmentation for camouflage\"],\n            \"augmentation\": [\"night vision adaptation\", \"weather simulation\"],\n            \"visualization\": [\"object localization\", \"performance analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Species-Specific Nocturnal Object Detection Model (Caracal)\",\n            \"family\": \"Deep Neural Network (specifically, Region Proposal Networks or YOLOv5)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"specialized for caracals and camouflage\",\n                \"algorithm_optimization\": \"energy-efficient\"\n            },\n            \"description\": \"A deep learning model tailored for detecting and tracking caracals during night, optimizing for precision and camouflage detection, while adhering to low energy consumption.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Use lightweight models or pruning techniques to minimize energy footprint\",\n        \"Incorporate context-awareness for more accurate tracking in complex environments\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Fine-tuning for specific seasonal variations\",\n            \"Integration of real-time environmental data\"\n        ],\n        \"target_device\": \"Edge devices with limited compute power and energy constraints\",\n        \"deployment_endpoint\": \"Cloud-based with edge computing support for real-time analysis\",\n        \"inference_engine\": [\"TensorFlow-Lite\", \"ONNX-Runtime\"],\n        \"demonstration\": \"Interactive visualization and alerts for effective monitoring\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON accurately captures the user's request for a high-resolution, species-specific deep learning model with energy efficiency and environmental footprint considerations. The expert-level expertise indicates a clear understanding of the project requirements. Performance metrics like mAP and accuracy are appropriate for object detection. Dataset specification includes necessary challenges and pre-processing steps. Knowledge and service sections reflect practical suggestions for addressing the challenges.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a novel, multi-disciplinary challenge that combines the ProPublica's Secure Direct Access (SDA) healthcare dataset and the Mendeley research paper co-authorship network for a high-impact node classification task. Utilize a state-of-the-art, differentially private Graph Neural Network (DP-GNN) to discern not only the expertise of individual researchers based on their publication history, but also predict future collaborations, while maintaining patient privacy. The model must seamlessly integrate with a federated learning architecture, ensuring data stays within institution's silos, and can dynamically adapt to changing citation patterns across multiple domains. Strive for near real-time updates, while ensuring data security breaches are thwarted by robust cryptographic techniques and achieving optimal resource efficiency on a large, dynamically scalable graph storing millions of encrypted research papers and citations in a secure, decentralized quantum-resistant database.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"healthcare and academic research\",\n        \"description\": \"Create a multi-disciplinary challenge combining ProPublica's SDA healthcare dataset and Mendeley's co-authorship network for node classification, using DP-GNN to analyze publication history and predict future collaborations while maintaining patient privacy. Incorporate federated learning, cryptographic techniques, and a decentralized, quantum-resistant database for large-scale, secure data storage and real-time adaptation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Accuracy in predicting future collaborations\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time on large-scale encrypted graph\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Resource efficiency\",\n                \"value\": null,\n                \"unit\": \"% of available system resources\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ProPublica SDA healthcare dataset\",\n            \"modality\": [\"tabular\", \"multimodal\"],\n            \"target_variables\": [\"patient information\", \"health outcomes\"],\n            \"specification\": {\n                \"size\": \"millions of records\",\n                \"data type\": \"encrypted\"\n            },\n            \"description\": \"Contains patient data for discerning researcher expertise and ensuring privacy.\",\n            \"preprocessing\": [\"data anonymization\", \"data encryption\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        },\n        {\n            \"name\": \"Mendeley co-authorship network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"co-authorship relations\"],\n            \"specification\": {\n                \"nodes\": \"millions of researchers\",\n                \"edges\": \"citations and collaborations\"\n            },\n            \"description\": \"A network for analyzing publication history and predicting future collaborations.\",\n            \"preprocessing\": [\"node embedding\", \"graph anonymization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DP-GNN (Differentially Private Graph Neural Network)\",\n            \"family\": \"neural networks\",\n            \"type\": \"graph-based\",\n            \"specification\": {\n                \"architecture\": \"secure multi-party computation and federated learning\",\n                \"privacy mechanism\": \"differential privacy\",\n                \"scalability\": \"dynamically adaptable to changing citation patterns\"\n            },\n            \"description\": \"A state-of-the-art GNN designed to handle encrypted data, predict collaborations, and adhere to privacy regulations.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"technique\",\n            \"description\": \"Utilization of a quantum-resistant database to protect against data breaches and secure large-scale data storage.\"\n        },\n        {\n            \"type\": \"method\",\n            \"description\": \"Federated learning and seamless integration with institution silos to maintain data within their secure environments.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"privacy-preserving feature extraction\"],\n        \"target_device\": \"federated nodes, edge devices\",\n        \"deployment_endpoint\": \"secure, decentralized platform\",\n        \"inference_engine\": [\"cryptographic computations\"],\n        \"demonstration\": \"near real-time updates and demonstrable privacy-preserving collaboration predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's desire for a novel and multi-disciplinary project, combining healthcare and research data. It specifies the challenge of using DP-GNN with federated learning, preserving privacy, and addressing scalability. Performance metrics are left unspecified to reflect the need for near real-time updates, while emphasizing data security measures. The high expertise level of the user implies a thorough understanding of the requirements.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series forecasting", "instruction": "Instruct the \"Advanced Smart Grid Prognostics Team\" to design a deep LSTM-CuDNN hybrid model for the \"Hierarchically Decomposed Ultra-Temporal Power Network Forecasting Challenge.\" Utilize the PDTC-U-Time-Grid v3.0 dataset, featuring 5-year hourly data with 1,500 distinct meteorological and grid dynamics indicators (INPUT_SEQ_LEN=384, INPUT_DIM=1,500). Aim to forecast 720-hour ahead scenarios (3x PRED_SEQ_LEN=720) with enhanced accuracy, optimizing metrics like adaptive quantile-based alpha-divergence and hour-specific asymmetric mean absolute percentage error. Require a real-time model with real-world explanation through partial least squares path modeling and a dynamic deployment strategy, incorporating a six-phase prototype refinement roadmap for iterative performance enhancements, considering intermittent renewable integration and extreme events analysis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart grid\",\n        \"description\": \"Design a deep LSTM-CuDNN hybrid model for the Hierarchically Decomposed Ultra-Temporal Power Network Forecasting Challenge using the PDTC-U-Time-Grid v3.0 dataset. The data includes 5-year hourly data with 1,500 meteorological and grid dynamics indicators (INPUT_SEQ_LEN=384, INPUT_DIM=1,500). Forecast 720-hour ahead scenarios (3x PRED_SEQ_LEN=720) with focus on improved accuracy using adaptive quantile-based alpha-divergence and hour-specific asymmetric mean absolute percentage error metrics. The model should be real-time, provide real-world explanations through partial least squares path modeling, and have a dynamic deployment strategy with a six-phase prototype refinement roadmap for iterative enhancements, considering intermittent renewable integration and extreme events analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"adaptive quantile-based alpha-divergence\",\n                \"value\": null\n            },\n            {\n                \"name\": \"hour-specific asymmetric mean absolute percentage error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time model\",\n                \"value\": true\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PDTC-U-Time-Grid v3.0\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"power network indicators\"],\n            \"specifications\": {\n                \"INPUT_SEQ_LEN\": 384,\n                \"INPUT_DIM\": 1500\n            },\n            \"description\": \"A dataset for the Hierarchically Decomposed Ultra-Temporal Power Network with 5-year hourly data and 1,500 meteorological and grid dynamics indicators.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"LSTM-CuDNN hybrid\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy-oriented\": true,\n                \"real-world_explanation\": \"partial least squares path modeling\"\n            },\n            \"description\": \"A deep LSTM-CuDNN hybrid model designed for real-time power network forecasting with enhanced accuracy and explainability\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"intermittent renewable integration analysis\", \"extreme events analysis\"],\n        \"target_device\": \"real-time environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engines\": [\"dynamic\"],\n        \"demonstration\": \"six-phase prototype refinement roadmap\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's request for a machine learning project in the smart grid domain, specifying the time-series forecasting problem, dataset, LSTM-CuDNN hybrid model, and performance metrics. Expertise level is assumed to be high due to the technical nature of the challenge. The model specifications align with the requested focus on accuracy and explainability. Some metrics and model details are left unspecified based on the complexity of the specified requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a healthcare AI consultant, develop a highly specialized Q&A system that combines advanced BioASQ algorithms for multifaceted disease diagnosis, optimized for pediatric and geriatric populations. Enhance F1-score through deep learning and active learning from diverse PubMed feeds. Implement entity disambiguation with medical ontologies, ensuring patient privacy with on-the-fly differential privacy, continuous HIPAA compliance, and secure real-time data encryption with quantum-safe protocols.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Develop a highly specialized Q&A system that combines BioASQ algorithms for multifaceted disease diagnosis in pediatric and geriatric populations. Improve F1-score through deep learning and active learning from diverse PubMed feeds, while incorporating entity disambiguation with medical ontologies. Ensure patient privacy with on-the-fly differential privacy, continuous HIPAA compliance, and secure real-time data encryption using quantum-safe protocols.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time data encryption\",\n                \"value\": null,\n                \"unit\": \"Quantum-safe protocols\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ data and PubMed feeds\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"medical information\"],\n            \"specification\": {\n                \"PubMed feeds\": {\n                    \"sources\": \"Diverse pediatric and geriatric populations\"\n                }\n            },\n            \"description\": \"A collection of biomedical question-answer pairs and PubMed articles for fine-tuning and data augmentation, targeting pediatric and geriatric disease diagnosis.\",\n            \"preprocessing\": [\n                \"Entity extraction\",\n                \"Disease disambiguation\"\n            ],\n            \"augmentation\": [\n                \"Active learning from PubMed feeds\"\n            ],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Q&A System\",\n            \"family\": \"Combination of BioASQ algorithms and deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Deep learning (e.g., transformers, CNNs, LSTM) with BioASQ algorithms\",\n                \"target population\": \"Pediatric and geriatric\"\n            },\n            \"description\": \"A specialized Q&A system for multifaceted disease diagnosis with emphasis on pediatric and geriatric populations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Specialized for pediatric and geriatric needs\",\n        \"HIPAA compliance, differential privacy, quantum-safe encryption\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Entity disambiguation using medical ontologies\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Quantum-safe real-time data encryption\"],\n        \"demonstration\": \"Privacy-preserving and real-time response for patient queries\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's expert-level request for a specialized Q&A system. The system combines BioASQ algorithms, deep learning, and medical ontologies, targeting pediatric and geriatric populations. Performance metrics like F1-score are open-ended, allowing for continuous optimization. The system addresses privacy and security concerns following HIPAA and quantum-safe encryption standards. However, specific values for performance and complexity metrics are missing due to the general nature of the enhanced F1-score and the focus on security measures.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Revamp the NMT model using state-of-the-art M6 transformer, particularly focusing on the implementation of a context-dependent recurrent mechanism within the Transformer-XL architecture. Target a specialized BLEU score of 42 in low-resource translation scenarios, while maintaining exceptional fluency for handling esoteric domain jargon, dialectal slang from the extensive OPUS-XL corpus. Ensure seamless performance with a challenging real-time constraint of 150 milliseconds, prioritizing energy-efficient hardware optimization for uninterrupted translation in massive, high-traffic, low-latency chat applications during extreme load peaks.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"specialized domain translation\",\n        \"description\": \"Revamp the NMT model using state-of-the-art M6 transformer, focusing on context-dependent recurrent mechanism in Transformer-XL, targeting a BLEU score of 42 in low-resource scenarios and fluent handling of esoteric domain jargon and dialectal slang from the OPUS-XL corpus.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 42\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time constraint\",\n                \"value\": 150,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"OPUS-XL corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Translation\"],\n            \"specification\": {\n                \"source_language\": \"unknown\",\n                \"target_language\": \"unknown\",\n                \"size\": \"low-resource\",\n                \"domain\": \"specialized esoteric domain\"\n            },\n            \"description\": \"A specialized corpus containing low-resource domain jargon and dialectal slang for training and evaluation.\",\n            \"preprocessing\": [\"context-dependent preprocessing\"],\n            \"augmentation\": [\"minimal augmentation for domain-specific content\"],\n            \"visualization\": [\"model performance w.r.t. domain slang\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M6 Transformer\",\n            \"family\": \"Transformer-XL with context-dependent recurrent mechanism\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Transformer-XL\",\n                \"recurrent_mechanism\": \"context-dependent\",\n                \"target_specialization\": \"low-resource translation\"\n            },\n            \"description\": \"A state-of-the-art transformer model that combines M6 and Transformer-XL, aiming for high fluency and low-resource adaptation.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Challenges in low-resource translation, handling esoteric jargon, and dialects within OPUS-XL corpus\"\n        },\n        {\n            \"description\": \"Necessity of energy-efficient hardware for real-time operation in high-traffic chat applications\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain adaptation techniques\"],\n        \"target_device\": \"energy-efficient hardware\",\n        \"deployment_endpoint\": \"low-latency chat applications\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"translated, fluent, and context-aware responses under low-resource conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"This JSON response covers the user's requirement to revamp the NMT model with a context-dependent mechanism, emphasizing the M6 Transformer and Transformer-XL, and targeting the specified BLEU score and real-time constraint. It acknowledges the specialized domain, low-resource nature, and hardware efficiency. The missing performance metric for energy efficiency and the model's exact BLEU score are placeholders for optimization, as indicated by the user's lack of specific values.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "Instruct the ML team to design a cutting-edge link prediction system for Citeseer, integrating a novel explainability module. Demand a model with a minimum precision of 95% in specialized subfields, resilience to ultra-sparse data, and the capacity to manage exabytes of expanding citation data in real-time. Mandate efficient scaling, guaranteed compliance with privacy laws, and a dynamic interpretability dashboard that updates seamlessly, all while preserving anonymization norms.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Design a cutting-edge link prediction system for Citeseer, incorporating a novel explainability module. The model must achieve a minimum precision of 95% in specialized subfields, handle ultra-sparse data, and manage exabytes of real-time expanding citation data. The system must scale efficiently, comply with privacy laws, and include a dynamic interpretability dashboard that updates seamlessly while preserving anonymization norms.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.95,\n                \"subfield\": \"specialized\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resilience to ultra-sparse data\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"real-time data management\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"efficiency for scaling\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"privacy compliance\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Citeseer\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"citation relationships\"],\n            \"specification\": {\n                \"density\": \"ultra-sparse\"\n            },\n            \"description\": \"A citation network dataset for link prediction, focusing on Citeseer with sparse connectivity.\",\n            \"preprocessing\": [\"anonymization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"novel (explainable model)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"min_precision\": 0.95,\n                \"anonymization_capacity\": \"exabytes\",\n                \"real_time_processing\": true\n            },\n            \"description\": \"A cutting-edge link prediction model for Citeseer with a novel explainability module, designed for scalability, privacy, and dynamic interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Efficient handling of sparse data and real-time big data management\",\n        \"Privacy law compliance\",\n        \"Dynamic interpretability dashboard design\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"novel explainability module\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"custom\",\n        \"inference_engine\": [\"AI-friendly, real-time\"],\n        \"demonstration\": \"seamless updates while preserving anonymization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON captures the user's instruction, specifying the required cutting-edge link prediction system for Citeseer with a strong emphasis on performance, data handling, and privacy. The model requirements, performance metrics, and complexity metrics address the user's expertise level. The dataset is specified as a sparse graph and anonymization is mentioned. The novel explainability module, real-time processing, and legal compliance aspects are well captured in the model development and service components.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "multimodal forgery detection", "instruction": "Revise the multimodal forgery detection project for academic certificates by integrating real-time anomaly detection using deep learning, holographic watermarking, and voice biometrics. Require seamless integration with multiple international accreditation databases while ensuring GDPR-compliant data handling and a user-friendly interface for cross-validation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal forgery detection\",\n        \"application_domain\": \"academic validation\",\n        \"description\": \"Integrate real-time anomaly detection using deep learning, holographic watermarking, and voice biometrics into a multimodal forgery detection project for academic certificates. Seamless integration with multiple international accreditation databases while ensuring GDPR-compliant data handling.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time detection speed\",\n                \"value\": null,\n                \"unit\": \"frames per second\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds per detection\"\n            },\n            {\n                \"name\": \"database query response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"GDPR compliance\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\n                \"image\",\n                \"audio\"\n            ],\n            \"target_variables\": [\n                \"forgery detection\"\n            ],\n            \"specification\": {\n                \"image_dimensions\": null,\n                \"audio_samples\": null,\n                \"data_format\": \"\"\n            },\n            \"description\": \"Data for academic certificate images and voice samples for multimodal forgery detection\",\n            \"preprocessing\": [\n                \"holographic watermarking extraction\",\n                \"voice biometric feature extraction\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Real-time Anomaly Detection\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep learning model for multimodal anomaly detection\",\n                \"detection_method\": \"combination of holographic watermarking, voice biometrics, and DL\"\n            },\n            \"description\": \"A deep learning model that combines holographic watermarking, voice biometrics, and anomaly detection for real-time forgery detection\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Integration with international accreditation databases\"\n        },\n        {\n            \"description\": \"GDPR compliance measures for data handling\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"real-time\",\n            \"holistic\"\n        ],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"custom-designed for real-time performance\"\n        ],\n        \"demonstration\": \"cross-validation interface\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure is built based on the user's specific requirements for a machine learning project. It includes the integration of real-time anomaly detection, GDPR compliance, and a user-friendly interface. However, missing values in performance metrics and complexity metrics are placeholders, as the instruction lacks specific numerical targets or units for these requirements. The user's high expertise level implies they understand the need for technical details in these areas.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project lead for an elite team, your objective is to design an advanced deep learning classifier specifically tailored for a specialized task of differentiating 100 rare sub-species of endangered felines in a massive, high-contrast dataset. Mandate the selection of a tailored architecture for fine-grained classification, combining a domain-adapted pre-trained model with a hybrid data augmentation technique that cleverly fuses physical manipulation and Generative Adversarial Network (GAN) generated images, ensuring top-notch performance with minimal labeled resources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"Design an advanced deep learning classifier for differentiating 100 rare sub-species of endangered felines using a specialized task-oriented approach with a high-contrast dataset. The classifier should involve a domain-adapted pre-trained model, a hybrid data augmentation technique combining physical manipulation and GAN-generated images, targeting top-notch performance with minimal labeled data.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Endangered Feline Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"Subspecies\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": {\n                    \"height\": null,\n                    \"width\": null\n                },\n                \"contrast\": \"high\"\n            },\n            \"description\": \"A high-contrast dataset of endangered felines, consisting of distinct images for differentiating 100 rare sub-species.\",\n            \"preprocessing\": [\"data normalization\"],\n            \"augmentation\": [\n                {\n                    \"type\": \"Physical Manipulation\"\n                },\n                {\n                    \"type\": \"GAN-generated images\"\n                }\n            ],\n            \"visualization\": [\"class activation maps\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Fine-grained Image Classifier\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"pre-trained\": \"domain-adapted\",\n                    \"custom_layers\": \"hybrid\"\n                },\n                \"initial_labels\": \"limited\",\n                \"transfer_learning\": true\n            },\n            \"description\": \"A tailored deep learning classifier for fine-grained classification of endangered feline sub-species, using a domain-adapted pre-trained model with a hybrid data augmentation strategy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilizing a hybrid approach to boost performance with minimal labeled data\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction from pre-trained model\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"CUDA for optimized performance\"],\n        \"demonstration\": \"interactive visualization of classification results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's high-level objective and the specific requirements, such as using a domain-adapted pre-trained model and hybrid data augmentation. Performance metrics like accuracy, precision, and recall are included with placeholder values to be defined later. Some details might be incomplete, like the model's exact architecture and specific deployment details, but the overall structure is consistent with the given task.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Construct a cutting-edge, dynamic time series forecasting model for the app's DAU, fusing advanced deep ARIMA and LSTM architectures. Demand a multi-layered strategy that not only differentiates between diverse patterns (weekly, quarterly, and irregular spikes), but also forecasts and adapts to them. Mandate a sophisticated 60-day rolling validation process with a focus on identifying and mitigating long-term seasonality, while ensuring the model's robustness and precision across diverse economic scenarios and unexpected shifts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"app usage\",\n        \"description\": \"Construct a cutting-edge time series forecasting model for app DAU, combining advanced deep ARIMA and LSTM architectures. The model should handle diverse patterns (weekly, quarterly, and irregular spikes), forecast and adapt to them, and implement a 60-day rolling validation process with emphasis on long-term seasonality detection and mitigation, ensuring robustness and precision across various economic scenarios and sudden shifts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MASE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory consumption\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"App DAU Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"DAU\"],\n            \"specification\": {\n                \"length\": null,\n                \"granularity\": null,\n                \"data_format\": \"daily/weekly/quarterly\"\n            },\n            \"description\": \"A time series dataset capturing daily, weekly, and quarterly app Daily Active Users (DAU) for the fusion of deep ARIMA and LSTM models.\",\n            \"preprocessing\": [\" seasonality analysis\", \"data normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time series pattern identification\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Fusion (ARIMA+LSTM)\",\n            \"family\": \"deep time series\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\n                    {\n                        \"name\": \"Deep ARIMA\",\n                        \"hyperparameters\": {\n                            \"p\": 0, \n                            \"d\": 1, \n                            \"q\": 0\n                        }\n                    },\n                    {\n                        \"name\": \"LSTM Layer\",\n                        \"hyperparameters\": {\n                            \"units\": 128,\n                            \"return_sequences\": true\n                        }\n                    }\n                ],\n                \"adaptability\": \"diverse patterns\"\n            },\n            \"description\": \"A multi-layered model fusing deep ARIMA and LSTM for app DAU forecasting, capable of differentiating and adapting to various patterns.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"data cleaning\", \"feature engineering for seasonality\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"API for real-time forecasting\",\n        \"inference_engine\": [\"TensorFlow\", \"Keras\"],\n        \"demonstration\": \"rolling validation results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately represents the user's instruction, highlighting the need for a cutting-edge, deep ARIMA and LSTM model, a multi-layered strategy, and a robust validation process. Performance metrics are left unspecified for calculation. The model and dataset properties are coherent with the given requirements. Assuming high expertise, technical details are included. A potential missing piece could be more specific model hyperparameters or deployment details, given the complexity of the task.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design a cutting-edge Transformer-embedded neural machine translation model that specializes in converting highly informal, regional, and constantly evolving English subcultural expressions into modern, colloquial Russian idioms. The system must excel in Chrf++ evaluation, achieving a benchmark of 47+, with a remarkable 92% precision for capturing slang and cultural shifts. Emphasize on the model's compatibility with resource-constrained edge devices, ensuring real-time translations with a stringent latency requirement of 50 milliseconds for seamless operation in a gigantic, scalable multi-lingual chatbot infrastructure, while preserving flexibility for seamless addition of new linguistic data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"language translation\",\n        \"description\": \"Design a Transformer-based neural machine translation model for converting informal English subcultural expressions to modern Russian idioms, focusing on regional and evolving language. The system should surpass a benchmark of 47+ in Chrf++ evaluation and maintain a precision of 92% for slang and cultural shifts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Chrf++\",\n                \"value\": 47.0\n            },\n            {\n                \"name\": \"Precision (Slang and Cultural Shifts)\",\n                \"value\": 0.92\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Latency\",\n                \"value\": 50,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Compatibility\",\n                \"value\": \"resource-constrained edge devices\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Informal English-Russian Subcultural Expressions\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Modern Russian Idioms\"],\n            \"specification\": {\n                \"informal_data\": true,\n                \"regional_variations\": true,\n                \"evolving_language\": true\n            },\n            \"description\": \"A dataset with informal English subcultural expressions and their corresponding modern, colloquial Russian idioms.\",\n            \"preprocessing\": [\"Tokenization\", \"Normalization\"],\n            \"augmentation\": [\"Backtranslation\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-Embedded NMT\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_details\": {\n                    \"dependency_on_edge_devices\": true,\n                    \"real-time_translations\": true\n                },\n                \"resource_consumption\": {\n                    \"latency\": {\n                        \"prediction\": \"50ms\",\n                        \"deployment\": \"50ms\"\n                    }\n                },\n                \"customization\": \"flexible for adding new linguistic data\"\n            },\n            \"description\": \"A highly specialized model for informal-to-formal translation, optimized for resource-constrained edge devices and designed for real-time, low-latency operation in a large-scale chatbot system.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"multi-lingual chatbot infrastructure\",\n        \"inference_engine\": [\"Real-time Inference\"],\n        \"demonstration\": \"Seamless operation with new linguistic data integration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes all the details specified in the instruction: model architecture, target performance metrics, data requirements, resource constraints, and compatibility with edge devices. Some assumptions had to be made about the model's architectural details, and precision score is not explicitly stated for informal expressions but refers to slang and cultural shifts as per the instruction. The structure remains compliant with the JSON schema.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "Develop a cutting-edge Mask R-CNN model for our precision agriculture project, incorporating advanced transfer learning from diverse domain-specific repositories. The model must not only detect diseases, nutrient deficiencies, but also precisely detect and classify various environmental stressors with at least 98% accuracy. Strive for real-time capability while handling high-resolution drone imagery captured from multiple angles (elevations and orientations).", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"precision agriculture\",\n        \"description\": \"Develop a cutting-edge Mask R-CNN model for precision agriculture, capable of detecting diseases, nutrient deficiencies, and environmental stressors. The model should target at least 98% accuracy and be designed for real-time processing on high-resolution drone imagery captured from various angles.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Precision Agriculture Imagery\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"diseases\", \"nutrient deficiencies\", \"environmental stressors\"],\n            \"specification\": {\n                \"image_resolution\": \"high\",\n                \"various_angles\": \"yes\"\n            },\n            \"description\": \"A collection of high-resolution drone images with multiple angles and orientations for the agricultural use case.\",\n            \"preprocessing\": [\"data augmentation\"],\n            \"augmentation\": [\"random cropping\", \"horizontal flipping\"],\n            \"visualization\": [\"image segmentation\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Mask R-CNN\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"transfer_learning\": \"from diverse domain-specific repositories\"\n            },\n            \"description\": \"A state-of-the-art Mask R-CNN architecture with emphasis on advanced transfer learning, designed for object detection in precision agriculture scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporating domain-specific repositories to improve model adaptation to precision agriculture context.\",\n        \"Real-time performance is a key requirement due to high-resolution imagery and dynamic nature of drone captures.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"feature selection\"],\n        \"target_device\": \"cloud or edge device with high computational power\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-based inference\"],\n        \"demonstration\": \"Live demonstration of object detection and classification on sample drone images\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's instruction to build a high-performance Mask R-CNN model for precision agriculture with real-time capabilities. The performance metrics and accuracy target are clearly defined, while acknowledging the need for high-resolution image processing. The inclusion of domain-specific repositories and transfer learning shows the user's advanced knowledge. Some details like the exact transfer learning repositories, inference time constraints, and deployment endpoint are left unspecified for further clarification from the user.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series classification", "instruction": "Develop a novel deep learning architecture that combines Transformer-XL and Capsule Network principles for time-series classification of ancient astronomical signal from encoded celestial observations, while accounting for irregular sampling rates, temporal aliasing, and dynamic signal variability. The model should achieve sub-1% classification accuracy on a previously unseen dataset of ancient astronomical scrolls, with a focus on identifying different star constellations under variable weather conditions and marginal lighting, ensuring minimal deciphering errors for historians working in remote locations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"astronomy and history\",\n        \"description\": \"Design a deep learning architecture that combines Transformer-XL and Capsule Network principles for classifying ancient astronomical signals from encoded celestial observations. The model must handle irregular sampling rates, temporal aliasing, and dynamic signal variability, targeting sub-1% accuracy on a previously unseen dataset of ancient astronomical scrolls, focusing on star constellation identification under variable weather and lighting conditions with minimal deciphering errors.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"classification accuracy\",\n                \"value\": 0.01\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ancient Astronomical Scroll Dataset\",\n            \"modality\": [\"time series\", \"multimodal\"],\n            \"target_variables\": [\"star constellations\"],\n            \"specification\": {\n                \"sampling_rates\": \"irregular\",\n                \"aliasing\": true,\n                \"signal_variability\": true,\n                \"weather_conditions\": [\"variable\"],\n                \"lighting_conditions\": [\"marginal\"]\n            },\n            \"description\": \"A dataset containing encoded celestial observations from ancient astronomical scrolls, containing star constellations in varying weather and lighting conditions.\",\n            \"preprocessing\": [\n                \" Handling irregular sampling rates\",\n                \"Dealing with temporal aliasing\"\n            ],\n            \"augmentation\": [\n                \"Data augmentation for dynamic signal variability\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer-XL-Capsule Network Fusion\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"Transformer-XL\": {},\n                    \"Capsule Network\": {}\n                },\n                \"specifics for irregular sampling\": {},\n                \"aliasing mitigation\": {},\n                \"signal variability handling\": {}\n            },\n            \"description\": \"A novel deep learning model that combines Transformer-XL and Capsule Network mechanisms for accurate time-series classification of ancient astronomical signals.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Ancient signal deciphering techniques and their implications on model design\",\n        \"Best practices for handling variable data conditions in astronomical time-series analysis\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Custom feature extraction for constellation identification\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Optimized for sub-1% accuracy\"\n        ],\n        \"demonstration\": \"Demonstration of sub-1% classification accuracy on unseen data with low deciphering errors\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers the user's request to build a deep learning model that meets specific requirements for handling astronomical signal data. The performance metric and complexity constraints are clearly defined. The dataset is described in detail, taking into account the given challenges and variability. The model and knowledge fields are tailored to the astronomical context and signal challenges. The confidence score is high due to the comprehensive representation of the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a data-driven project manager, I require you to devise a sophisticated next-generation time-series forecasting system employing advanced Temporal Fusion Graph Convolutional Networks (TF-GCNs) fused with the Prophet algorithm. The project necessitates meticulous analysis of hourly solar flux, microscale UHI patterns, and CO2 flux data from a global network of metropolises, sampled every 15 minutes over a decade-long period. Key objectives are:\n\n1. Implement a hybrid architecture that combines deep learning, utilizing a TF-GCN for high-frequency dynamics and Prophet for seasonality, to refine short- and long-term forecasts within the volatile climate context.\n2. Deploy a real-time feature selection module that detects anomalies through a multi-modal fusion of satellite imagery and IoT sensor data, safeguarding against rare meteorological anomalies and infrastructure failures.\n3. Design a modular microservices architecture, enabling seamless integration with open-source satellite feeds and ground-level sensors, dynamically recalibrating forecasts as new real-time data streams arrive.\n4. Develop a high-fidelity dashboard with intuitive interfaces for non-technical urban planners, showcasing not only hourly temperature predictions but also energy consumption fluctuations and public space cooling demand, accompanied by dynamic explanations of contributing variables through storytelling techniques.\n5. Rigorously validate the ensemble model's performance through rigorous nested cross-validation, comparing its precision against state-of-the-art state-of-the-art methods like hierarchical reservoir computing and Explainable AI-enhanced ARIMA on a diverse, 20-year unseen dataset.\n6. Generate a comprehensive research report, emphasizing model explainability, probabilistic forecasting uncertainty, and addressing any nuances in variable dependencies while showcasing the adaptive nature of the system under changing environmental conditions. The ultimate goal is to deliver a pioneering, scalable, and actionable solution for sustainable urban heat management and energy planning.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate science and urban planning\",\n        \"description\": \"Design a next-generation time-series forecasting system using Temporal Fusion Graph Convolutional Networks (TF-GCNs) and Prophet algorithm, focusing on hourly solar flux, microscale UHI patterns, and CO2 flux data from global metropolises, with a 15-minute sampling over a decade. Key objectives include hybrid architecture, real-time feature selection, modular microservices, dashboard development, and rigorous performance validation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Model precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Global metropolises' hourly solar flux data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"solar flux\"],\n            \"specification\": {\n                \"sampling_rate\": \"15 minutes\",\n                \"duration\": \"decade\",\n                \"data_source\": \"global network of metropolises\"\n            },\n            \"description\": \"Hourly solar flux data from a global network of cities over a decade.\",\n            \"preprocessing\": [\"anomaly detection\", \"normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"time series plots\",\n                \"anomaly detection charts\"\n            ],\n            \"source\": \"user-link\"\n        },\n        {\n            \"name\": \"Microscale UHI patterns and CO2 flux data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"UHI patterns\", \"CO2 flux\"],\n            \"specification\": {\n                \"sampling_rate\": \"15 minutes\",\n                \"duration\": \"decade\",\n                \"data_source\": \"global network of metropolises\"\n            },\n            \"description\": \"Microscale UHI patterns and CO2 flux data for the same period as solar flux.\",\n            \"preprocessing\": [\"feature extraction\", \"resampling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Temporal Fusion GCN and Prophet (TF-GCN + Prophet)\",\n            \"family\": \"neural networks\",\n            \"type\": \"advanced time-series modeling\",\n            \"specification\": {\n                \"architecture\": \"TF-GCN for high-frequency, Prophet for seasonality\",\n                \"feature_combination_method\": \"fusion\"\n            },\n            \"description\": \"A combination of TF-GCN and Prophet for robust short- and long-term forecasting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Anomaly detection through multi-modal data fusion, real-time dynamic recalibration, and storytelling techniques for model explanations.\",\n        \"Uncertainty quantification and variable dependencies in probabilistic forecasting\",\n        \"Adaptive nature of the system under changing environmental conditions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"real-time anomaly detection\",\n            \"multi-modal fusion\"\n        ],\n        \"target_device\": \"cloud-based deployment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time\", \"streaming\"],\n        \"demonstration\": \"data-driven urban heat management dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON has been parsed based on the given instruction, which includes a high-level overview of a time-series forecasting project involving advanced techniques and real-world data. The user's expertise level is assumed to be high, given the complexity of the project requirements. The metrics and specifications indicate a need for thorough performance evaluation and scalability. Potential missing information includes the target_device, deployment_endpoint, and specific details for multi-modal fusion. However, these can be inferred or determined based on common best practices for the given project context.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a project manager, seek a cutting-edge transformer model, engineered for extreme efficiency - a next-generation M-BART variant. Mandate the utilization of the state-of-the-art Europarl v7.0 dataset, enriched with domain-specific fine-tuning for not only Spanish-English translations but also encompassing legal, financial, and technical terminologies. The model must exhibit superhuman accuracy, consistently achieving near-native fluency, while satisfying stringent real-time requirements. The target is to deliver sub-200ms response times in a voice-activated, latency-critical platform designed for seamless international business conversations, considering network efficiency and scalability for high-traffic scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"international business\",\n        \"description\": \"Seeking a cutting-edge transformer model optimized for efficiency, specifically an M-BART variant. The model should be trained on the Europarl v7.0 dataset, with domain-specific fine-tuning for Spanish-English translations and coverage of legal, financial, and technical domains. The target is to achieve superhuman accuracy and near-native fluency, meeting real-time requirements of sub-200ms response times in a voice-activated, latency-critical platform for seamless international business conversations. Network efficiency and scalability for high-traffic scenarios must be taken into account.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"fluency\",\n                \"value\": \"near-native\"\n            },\n            {\n                \"name\": \"response_time\",\n                \"value\": 0.2\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference_time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"network_efficiency\",\n                \"value\": null,\n                \"unit\": \"%\"\n            },\n            {\n                \"name\": \"scalability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Europarl v7.0\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translations in Spanish-English\"],\n            \"specification\": {\n                \"source_language\": \"Spanish\",\n                \"target_language\": \"English\",\n                \"domain_coverage\": [\"legal\", \"financial\", \"technical\"]\n            },\n            \"description\": \"A multilingual dataset enriched with domain-specific fine-tuning for Spanish-English translations in legal, financial, and technical domains.\",\n            \"preprocessing\": [\"domain adaptation\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Efficient M-BART Variant\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural machine translation\",\n            \"specification\": {\n                \"efficiency_techniques\": [\"real-time optimization\", \"latency reduction\"],\n                \"model_size\": \"optimized for efficiency\"\n            },\n            \"description\": \"A cutting-edge M-BART model designed for extreme efficiency in the context of machine translation and international business conversations.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"low-latency feature extraction\",\n            \"pruned architecture\"\n        ],\n        \"target_device\": \"voice-activated, latency-critical platform\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time inference\", \"optimized for voice processing\"],\n        \"demonstration\": \"Voice-activated translation demos showcasing sub-200ms response times\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response adheres to the instruction, focusing on seeking a specific model, its efficiency, dataset, and performance requirements for the domain of international business. Accuracy, near-native fluency, and real-time response times are targeted, along with network efficiency and scalability for high-traffic scenarios. The model and dataset details provide a clear understanding of the sought solution, while some specific values for performance and complexity metrics are left open due to their dependence on precise fine-tuning results.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "For a sophisticated Aeroacoustics Anomaly Detection project, enhance the existing time-series classification model by integrating a real-time fusion of multi-sensor data, adaptive STFT with multiple window sizes, and a deep fusion Hierarchy-Enriched Temporal Convolutional Network (HE-TCN). Incorporate a feature that differentiates between varying levels of subsonic and supersonic turbulence with sub-millisecond latency, and ensure robustness against extreme high-frequency fluctuations in wing vibrations and atmospheric conditions. Conduct rigorous evaluation using micro-F1 score, precision-at-threshold, and a novel turbulence discriminative index, highlighting the trade-off between performance and computational complexity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"aeroacoustics\",\n        \"description\": \"Enhance the existing model for Aeroacoustics Anomaly Detection by integrating multi-sensor data fusion, adaptive STFT with multiple window sizes, and a HE-TCN network. Model should differentiate subsonic and supersonic turbulence with sub-millisecond latency, and handle extreme high-frequency fluctuations in wing vibrations and atmospheric conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"micro-F1 score\"\n            },\n            {\n                \"name\": \"precision-at-threshold\"\n            },\n            {\n                \"name\": \"turbulence discriminative index\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"sub-millisecond\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"computational complexity\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Multi-sensor time-series data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"anomaly detection\", \"turbulence levels\"],\n            \"specification\": {\n                \"data_length\": \"variable\",\n                \"sampling_rate\": \"variable\"\n            },\n            \"description\": \"A collection of multi-sensor time-series data capturing wing vibrations, atmospheric conditions, and turbulence for anomaly detection and sub-classification tasks.\",\n            \"preprocessing\": [\n                \"multi-sensor data fusion\",\n                \"adaptive STFT with multiple window sizes\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Enhanced Time-Series Model\",\n            \"family\": \"HE-TCN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Hierarchy-Enriched Temporal Convolutional Network\",\n                \"integration\": \"real-time multi-sensor fusion\",\n                \"feature_extraction\": \"adaptive STFT with multiple window sizes\",\n                \"inference_latency\": \"sub-millisecond\"\n            },\n            \"description\": \"A deep learning model that fuses multi-sensor data, applies adaptive STFT for feature extraction, and employs a HE-TCN for Aeroacoustics anomaly detection with handling high-frequency fluctuations and varying turbulence levels.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Incorporate temporal context and hierarchical representations to capture varying frequency patterns in turbulence.\",\n        \"Trade-off should be highlighted between improving performance and reducing computational complexity.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"embedded systems for real-time applications\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized deep learning libraries\"],\n        \"demonstration\": \"High-performance, low-latency anomaly detection demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately reflects the user's request for enhancing a time-series classification model, integrating specific techniques, and addressing performance and complexity trade-offs. The user's high expertise level is assumed to contribute to their specific requirements for the project. Some fields, like model name and target device, might require more context to be filled in.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As the project lead for our avant-garde fashion tech venture, we aim to revolutionize the industry by developing an enhanced DeepFashion2-powered garment segmentation tool. mandate the integration of a novel EfficientNet-B7 architecture tailored for a state-of-the-art fully convolutional network (FCN), with a focus on achieving superior sub-pixel precision for even the most complex overlapping garments. In addition to meeting a rigorous 95%+ Intersection over Union (IoU) score, the tool must guarantee real-time, energy-efficient performance on portable devices operating under the latest power optimization regulations, while demonstrating sustainable computational resource usage.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"fashion tech\",\n        \"description\": \"Develop an enhanced DeepFashion2-powered garment segmentation tool using a novel EfficientNet-B7 architecture, tailored for an FCN. Focus on achieving superior sub-pixel precision and meet a 95%+ IoU score for complex overlapping garments. Ensure real-time, energy-efficient performance on portable devices under latest power optimization regulations, while demonstrating sustainable computational resource usage.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Intersection over Union (IoU)\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"real-time\"\n            },\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"relative to portable devices\"\n            },\n            {\n                \"name\": \"computational resource usage\",\n                \"value\": {\n                    \"sustainability\": \"guaranteed\"\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DeepFashion2 dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"specifics\": \"garment segmentation\"\n            },\n            \"description\": \"A dataset with images for garment segmentation, focusing on complex overlapping garments\",\n            \"preprocessing\": [\"EfficientNet-B7 adaptation\", \"sub-pixel refinement\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet-B7 (tailored for FCN)\",\n            \"family\": \"EfficientNets\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"fully convolutional network\",\n                \"focus\": \"sub-pixel precision\"\n            },\n            \"description\": \"A DeepFashion2-enhanced model using EfficientNet-B7 for garment segmentation with high precision and state-of-the-art performance.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"real-time compatibility\", \"power optimization\"],\n        \"target_device\": \"portable devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"energy-efficient methods\"],\n        \"demonstration\": \"detailed energy and performance analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a high-performance garment segmentation tool with specified requirements on architecture, IoU, and device performance. Performance metrics, energy efficiency, and computational resource usage are defined, reflecting the focus on real-time and sustainable operation. Although the actual values for metrics are not provided, the structure reflects the complexity and significance of the requested features. The lack of further details is addressed in the rationale.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As the PM for our high-end AI innovation lab, your mandate is to design a next-generation BERT fusion model, incorporating RoBERTa architecture. The model should excel in multi-step reasoning for SQuAD 2.0, pushing the frontier beyond 98% F1 score with rigorous attention to both precision and recall. In addition, optimize for distributed parallel execution, ensuring real-time responsiveness with an ultra-fast 0.5-second latency constraint, without compromising on interpretability and maintaining a model size no larger than 100 million parameters.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"AI research and development\",\n        \"description\": \"Design a next-generation BERT fusion model with the RoBERTa architecture, targeting multi-step reasoning for SQuAD 2.0. Strive for an F1 score above 98%, while ensuring rigorous attention to both precision and recall.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": 0.5,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": 100000000\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\"],\n            \"specification\": null,\n            \"description\": \"A dataset for evaluating multi-step reasoning in question answering, specifically focusing on SQuAD 2.0 benchmark.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT Fusion (RoBERTa-based)\",\n                       \"family\": \"BERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"BERT with RoBERTa improvements\",\n                \"interpretability\": \"yes\"\n            },\n            \"description\": \"A next-generation BERT model optimized for multi-step reasoning on SQuAD 2.0 with RoBERTa integration, aiming for high F1, precision, and recall.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"distributed parallel computing environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time execution\"],\n        \"demonstration\": \"Ultra-fast 0.5-second latency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's intent to design a state-of-the-art BERT fusion model with RoBERTa. It specifies the high-level requirements for SQuAD 2.0 multi-step reasoning, advanced performance metrics, distributed parallel execution, and interpretability constraints. Some details are left implicit, such as the exact interpretation of interpretability or the final deployment endpoint.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "For a complex text classification project, employ the recently released and expanded DECEPTIVE.AI dataset, demanding not only state-of-the-art performance in differentiating AI-generated (1) from authentic human-generated content (0), but also resilience to adversarial attacks and explainable reasoning. Implement a deep ensemble of advanced models, optimizing with bayesian hyperparameter optimization and generating model interpretability reports at each iteration.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"AI ethics and content analysis\",\n        \"description\": \"A complex text classification project is required to classify AI-generated content from human-generated content using the DECEPTIVE.AI dataset. The focus is on achieving state-of-the-art performance, adversarial attack resilience, and providing explainable reasoning.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Adversarial robustness\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"Interpretability\",\n                \"value\": \"required\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"DECEPTIVE.AI\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"authenticity (0 for human-generated, 1 for AI-generated)\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": null,\n                \"attributes\": \"recently released and expanded\"\n            },\n            \"description\": \"A dataset for text classification focusing on AI-generated content detection and resilience to adversarial attacks.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Deep Ensemble\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_components\": \"advanced models\",\n                \"hyperparameter_optimization\": \"Bayesian\",\n                \"interpretability\": \"required at each iteration\"\n            },\n            \"description\": \"Deep ensemble of state-of-the-art models optimized with Bayesian hyperparameter optimization for text classification, with explainability features.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The DECEPTIVE.AI dataset is a recent and expanded resource.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explainable reasoning and resilience to adversarial attacks\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a model, their expertise level, the specific natural language processing task (text classification), and the requirement for state-of-the-art performance and explainability. The DECEPTIVE.AI dataset is correctly identified and the user's expectations regarding adversarial robustness and interpretability are included in the performance and complexity metrics. The lack of specific metric values indicates that these details should be filled based on the user's requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "community detection", "instruction": "As the head of the advanced AI research team, we're tasked with developing a state-of-the-art community detection algorithm for an intricate global network, where users dynamically form and dissolve communities in milliseconds. The system must not only pinpoint subgroups with extreme precision, leveraging temporal dynamics and node centrality, but also interpret cultural nuances and subtle behavioral patterns to predict shifts in community structures. Furthermore, the algorithm must optimize for minimal energy consumption on edge devices while maintaining a high level of interpretability for non-technical stakeholders. Can you design an innovative algorithm that achieves these complex objectives, ensuring both computational efficiency and cultural sensitivity?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"social networks\",\n        \"description\": \"Develop a state-of-the-art community detection algorithm for an intricate global network with dynamic community formations and dissolution. The algorithm should have high precision, consider temporal dynamics and node centrality, interpret cultural nuances, and predict shifts in community structures. It must be computationally efficient for edge devices while maintaining interpretability for non-technical stakeholders.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time on edge devices\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Global dynamic network data\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"community assignments\", \"temporal dynamics\", \"node centrality\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensions\": null,\n                \"temporal resolution\": null\n            },\n            \"description\": \"A global network dataset with dynamic community behavior, including temporal data and node attributes like centrality.\",\n            \"preprocessing\": [\"temporal aggregation\", \"node feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"temporal community evolution visualization\",\n                \"node-centric data exploration\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced community detection algorithm (Temporally Enhanced, Centrality-Informed, and Culturally Sensitive, e.g., TEC-CS)\",\n            \"type\": \"neural networks or ensemble models\",\n            \"specification\": {\n                \"model architecture\": null,\n                \"interpretability techniques\": null\n            },\n            \"description\": \"A cutting-edge community detection algorithm designed for real-time network analysis, considering temporal, centrality, and cultural aspects.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cultural context and subtle behavioral patterns will require ethnographic data integration and cultural model adaptation.\",\n        \"Effective use of explainable AI techniques to cater to non-technical stakeholders.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Temporal feature extraction\",\n            \"Cultural feature extraction\"\n        ],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"custom edge computing platform\",\n        \"inference_engine\": [\"Energy-efficient inference\"],\n        \"demonstration\": \"Interpretable community detection dashboard\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the main aspects of the instruction, including the requirement for a cutting-edge community detection algorithm, its performance goals, and the need for efficiency and interpretability. The missing metric values are placeholders, as they require specific numbers that are not given in the instruction. The high expertise level indicates a thorough understanding of the problem and what's required.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for the machine learning team, design an advanced time series forecasting system that combines state-of-the-art deep ARIMA and LSTM models, requiring the integration of real-time anomaly detection mechanisms. Mandate the model to decipher intricate weekly and quarterly patterns with multi-level decomposition, and ensure model robustness through a hierarchical leave-one-month-out cross-validation process, taking into account potential seasonal fluctuations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"business forecasting\",\n        \"description\": \"Design an advanced time series forecasting system using state-of-the-art deep ARIMA and LSTM models, with real-time anomaly detection and multi-level decomposition for weekly and quarterly patterns. Apply a hierarchical leave-one-month-out cross-validation process, considering seasonal fluctuations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"forecast accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"anomaly detection precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \" Historical Time Series Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"target variable\"],\n            \"specification\": {\n                \"length\": null,\n                \"frequency\": \"weekly and quarterly\"\n            },\n            \"description\": \"Data with historical time series values to train and test the time series forecasting model.\",\n            \"preprocessing\": [\n                \"multi-level decomposition\",\n                \"seasonal adjustment\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Combined ARIMA and LSTM Model\",\n            \"family\": \"Deep Time Series\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": [\n                    \"ARIMA component\",\n                    \"LSTM component\"\n                ],\n                \"real-time anomaly detection\": true,\n                \"multi-level decomposition\": true\n            },\n            \"description\": \"An advanced time series forecasting system utilizing both ARIMA and LSTM models, with emphasis on detecting anomalies and capturing weekly and quarterly patterns.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Hierarchical leave-one-month-out cross-validation strategy\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"multi-level decomposition\",\n            \"seasonal pattern extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"real-time\"\n        ],\n        \"demonstration\": \"visualizations and performance metrics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build an advanced forecasting system, considering the expert level of a project manager. The problem description accurately reflects the requirements for multi-level decomposition, real-time anomaly detection, and seasonal fluctuations. Performance metrics for accuracy and anomaly detection precision are included. Complexity metrics for inference time and model size are included as well. Some fields may have null values due to the absence of exact values in the instruction, but overall, the information is structured to address the given requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As the project manager for a cutting-edge AI research team, your mission is to revolutionize the precision medicine domain by tackling the Oncomine gene expression dataset for simultaneous prediction of multiple cancer drug responses. Design a tabular regression model that not only boasts state-of-the-art adjusted R-squared and high predictive accuracy but also includes Shapley value explanation, automated feature selection, andemploys a novel ensemble approach to minimize prediction bias, while ensuring model's explainability and scalability over large-scale genomic data.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"precision medicine\",\n        \"description\": \"Design a tabular regression model for predicting multiple cancer drug responses on the Oncomine gene expression dataset, aiming for state-of-the-art adjusted R-squared, high accuracy, Shapley value explanation, automated feature selection, ensemble approach for minimizing prediction bias, and ensuring model explainability and scalability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"adjusted R-squared\",\n                \"value\": null\n            },\n            {\n                \"name\": \"predictive accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null\n            },\n            {\n                \"name\": \"scalability over large-scale genomic data\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Oncomine gene expression dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"drug responses\"],\n            \"specification\": {\n                \"rows\": null,\n                \"columns\": {\n                    \"genomic features\": null,\n                    \"gene expression\": null\n                },\n                \"data_format\": \"gene expression matrix\"\n            },\n            \"description\": \"A large-scale genomic data containing gene expression levels for multiple cancer drug response prediction.\",\n            \"preprocessing\": [\n                \"normalization\",\n                \"data cleaning\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"data exploration plots\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"ensemble_method\": \"novel\",\n                \"feature_selection\": \"automated\",\n                \"explanation_method\": \"Shapley value\"\n            },\n            \"description\": \"A tabular regression model using a novel ensemble approach, automated feature selection, and Shapley value explanation for multiple drug response prediction on the Oncomine dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Application domain requirements for precision medicine\",\n        \"Recent advancements in ensemble models for genomic data\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Shapley value-based feature importance calculation\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explainer-friendly visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's requirements to build a model for precision medicine, focusing on the Oncomine dataset, specific performance metrics like adjusted R-squared, and the novel ensemble approach. The model's features, such as automated feature selection and Shapley value explanation, align with the user's expectations. However, certain metric values and deployment details are not specified, indicating potential gaps in the information provided.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "As a seasoned time-series anomaly detection consultant, create a highly sophisticated deep learning ensemble model that combines multiple architectures (CNN, LSTM, and Transformer) for detecting rare market anomalies in cryptocurrencies. Require not only sub-second latency across Binance, Coinbase, Bitfinex, and additional exchanges, but also demonstrate resilience to data heterogeneity. Integrate explainability methods (LIME, SHAP, and counterfactual explanations), ensuring user-friendly, interactive visualizations for the security team on a real-time, mobile-ready dashboard. Design with strict consideration for GDPR-compliant data privacy and efficiency on resource-constrained edge devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"financial markets\",\n        \"description\": \"Create a deep learning ensemble model combining CNN, LSTM, and Transformer for detecting rare market anomalies in cryptocurrencies on platforms like Binance, Coinbase, and Bitfinex. Aim for sub-second latency and resilience to data heterogeneity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"latency\",\n                \"value\": \"sub-second\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"efficiency on edge devices\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cryptocurrency market data (Binance, Coinbase, Bitfinex)\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"market anomalies\"],\n            \"specification\": null,\n            \"description\": \"Time series data from multiple cryptocurrency exchanges containing historical prices for anomaly detection analysis.\",\n            \"preprocessing\": [\"Data normalization, handling data heterogeneity\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"LIME, SHAP, counterfactual explanations\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Learning Ensemble (CNN, LSTM, Transformer)\",\n            \"family\": \"ensemble models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"combines CNN, LSTM, and Transformer\"\n            },\n            \"description\": \"A highly sophisticated model for time-series anomaly detection in cryptocurrencies with explainability methods.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Mobile-ready, real-time\",\n            \"GDPR-compliant\"\n        ],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"GDPR-compliant and mobile-friendly\",\n        \"inference_engine\": [\"real-time\"],\n        \"demonstration\": \"Interactive visualizations for security team\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user has specified a clear requirement for a build task with an emphasis on high expertise. The detailed description of the problem includes multiple constraints and desired outcomes such as low latency, data privacy, and explainability. However, latency values and specific efficiency metrics on edge devices are not provided, which could be filled based on the user's expertise or discussed further.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "As the project manager for a satellite image segmentation project, enhance the land classification system by differentiating between five specific vegetation types (ropical, boreal, subtropical, savanna, and shrubland) and three nuanced urban growth patterns (suburban sprawl, gentrification, and revitalization). The team must develop an energy-efficient model, optimized for Raspberry Pi deployment, ensuring 95% precision, low false positives, and explainable AI. Conduct extensive research on a modular, interpretable neural network architecture, along with a real-time data preprocessing and visualization workflow, to meet these stringent criteria.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"remote sensing and satellite imagery\",\n        \"description\": \"Enhance a land classification system for satellite images, differentiating between tropical, boreal, subtropical, savanna, and shrubland vegetation types, and three urban growth patterns (suburban sprawl, gentrification, and revitalization).\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"false positives\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"satellite image dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\n                \"land types (tropical, boreal, subtropical, savanna, shrubland)\",\n                \"urban growth patterns (suburban sprawl, gentrification, revitalization)\"\n            ],\n            \"specification\": {\n                \"image_size\": {\n                    \"height\": null,\n                    \"width\": null\n                },\n                \"color_channels\": null,\n                \"data_format\": null\n            },\n            \"description\": \"A dataset with satellite images containing land cover information to differentiate vegetation and urban growth patterns.\",\n            \"preprocessing\": [\n                \"real-time preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"data visualization workflow\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"modular, interpretable neural network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"explainable AI\",\n                \"optimized_for\": \"Raspberry Pi\",\n                \"accuracy\": \"95%\",\n                \"energy_efficiency\": null\n            },\n            \"description\": \"An energy-efficient, modular, and interpretable neural network designed for differentiating between land types and urban growth patterns.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Research on modular and interpretable architectures for satellite image segmentation\",\n        \"Best practices for energy-efficient models on Raspberry Pi\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"real-time, energy-efficient feature extraction\"\n        ],\n        \"target_device\": \"Raspberry Pi\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for the Raspberry Pi environment\"],\n        \"demonstration\": {\n            \"explanation_required\": true\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The given instruction has been parsed to focus on model development, data preprocessing, and other requirements like deployability and explainability. The user's expertise level suggests a need for high-quality guidance. Model performance metrics, including precision and energy efficiency, are explicitly specified, along with the desired real-time processing. The omission of specific model metrics indicates room for inquiry if more details are needed.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Develop a cutting-edge visual comprehension model for a specialized educational knowledge repository, focusing on the TextVQA-Scholar edition. Require a deep fusion architecture that expertly bridges the gap between OCR-processed text and high-resolution images, differentiating between nuanced concepts, ensuring inter-disciplinary accuracy, and demonstrating robustness across various academic disciplines and eras.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"education\",\n        \"description\": \"Design a cutting-edge visual comprehension model for the TextVQA-Scholar edition, focusing on deep fusion between OCR-processed text and high-resolution images, differentiating nuanced concepts, and ensuring inter-disciplinary accuracy across various academic disciplines and time periods.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inter-disciplinary F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TextVQA-Scholar\",\n            \"modality\": [\"text\", \"image\"],\n            \"target_variables\": [\"target answer\"],\n            \"specification\": {\n                \"data size\": null,\n                \"image resolution\": null,\n                \"text complexity\": null\n            },\n            \"description\": \"A specialized educational knowledge repository dataset with OCR-processed text and high-resolution images for visual question answering tasks.\",\n            \"preprocessing\": [\n                \"OCR on text content\",\n                \"image resizing\"\n            ],\n            \"augmentation\": [\n                \"synthetic question generation\"\n            ],\n            \"visualization\": [\n                \"example case studies\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Deep Fusion Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"multimodal deep learning\",\n            \"specification\": {\n                \"architecture\": \"deep fusion\",\n                \"text and image model fusion method\": \"attention mechanism\"\n            },\n            \"description\": \"A high-performance visual comprehension model that integrates OCR text and image data using a deep fusion architecture, targeting nuanced differentiation and inter-disciplinary accuracy for TextVQA-Scholar.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"type\": \"recent research trends\",\n            \"value\": \"State-of-the-art methods in multimodal fusion, OCR advancements, and domain-specific accuracy\"\n        },\n        {\n            \"type\": \"best practices\",\n            \"value\": \"Adapting to different academic disciplines by fine-tuning on specialized data subsets\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain-specific feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"GPU-powered execution\"\n        ],\n        \"demonstration\": \"Case studies and A/B testing results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the build intent, high-level requirements for a specialized educational knowledge repository, deep fusion architecture, and the emphasis on differentiation, interdisciplinary accuracy, and robustness. Performance metrics are left undefined to reflect the need for reporting detailed accuracy measures. The model and dataset specifications are well-informed based on the requirements. However, missing details include the target_device and deployment specifics, which could be inferred from the user's expertise and the application context.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "The ambitious medical imaging initiative demands the design of an advanced V-Net++ architecture, specifically tailored for the intricate BraTS dataset with a focus on glioma segmentation. The project necessitates the implementation of an innovative, progressive optimization strategy involving Bayesian hyperparameter optimization, stacked ensemble techniques, and a rigorous performance benchmark of 0.95 Dice score. Furthermore, mandate the real-time fusion of Grad-CAM visualizations with layer-wise feature analysis, along with the creation of interactive heatmaps that adhere to radiologists' interpretive standards and the generation of comprehensive, interactive reports with a user-centric interface for seamless adoption in contemporary diagnostic routines, ensuring minimal latency and maximal clinical utility.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"medical imaging\",\n        \"description\": \"Design an advanced V-Net++ architecture tailored for the BraTS dataset to segment gliomas, with a focus on achieving a Dice score of at least 0.95. The project requires Bayesian hyperparameter optimization, stacked ensemble techniques, real-time Grad-CAM fusion, layer-wise feature analysis, and interactive heatmaps adhering to radiologist standards. Aim for minimal latency and a user-centric interface for clinical adoption.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Dice score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BraTS\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"glioma segmentation\"],\n            \"specification\": {\n                \"image_shape\": \"variable\",\n                \"modalities\": [\"MRI\"]\n            },\n            \"description\": \"A medical imaging dataset for glioma segmentation, requiring V-Net++ architecture and BraTS-specific adaptation.\",\n            \"preprocessing\": [\"BraTS-specific normalization\"],\n            \"augmentation\": [\"rigid and elastic deformations\"],\n            \"visualization\": [\"Grad-CAM fusion, layer-wise feature maps\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"V-Net++\",\n            \"family\": \"Medical Imaging\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"V-Net++\",\n                \"hyperparameters\": \"Bayesian optimization\",\n                \"ensemble\": \"stacked ensemble\"\n            },\n            \"description\": \"Advanced V-Net++ architecture for glioma segmentation, optimized using Bayesian hyperparameter tuning and stacked ensembles.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Grad-CAM visualizations, radiologist standards, interactive heatmaps, and user-centric interface for seamless adoption\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Bayesian optimization\", \"stacked ensemble\"],\n        \"target_device\": \"cloud or GPU for real-time performance\",\n        \"deployment_endpoint\": \"focus on radiologist-friendly user interface\",\n        \"inference_engine\": [\"real-time\", \"GPU accelerated\"],\n        \"demonstration\": \"interactive, comprehensive reports adhering to clinical standards\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the complexity of the project, including architectural requirements, optimization methods, and performance benchmarks. It accounts for the specific data preprocessing, visualization, and interactive aspects of the project. However, the response leaves room for improvement by specifying the exact values for latency and inference time, which may depend on the system setup.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "For the intricate feline morphological analysis project, develop a heterogeneous fusion model integrating Inceptionv5-Plus and ResNeXt-Enhanced, focusing on resolving inter-species genetic nuances, multi-scale feature extraction from fur patterns, and fine-tuning using a large-scale ImageNet-Plus dataset pre-trained on exotic and rare breeds. Implement adaptive RandAugment for data diversification, double-channel color standardization, and design a novel hybrid loss function combining Focal Loss and Class Weighted AUC for handling the extraordinary imbalance in a 50-breed collection with ultra-high resolution images.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"biological sciences\",\n        \"description\": \"Develop a heterogeneous fusion model integrating Inceptionv5-Plus and ResNeXt-Enhanced for feline morphological analysis, focusing on inter-species genetic nuances, multi-scale feature extraction from fur patterns, and handling an imbalance in a 50-breed dataset with ultra-high resolution images.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Class Weighted AUC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ImageNet-Plus (exotic and rare breeds)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"height\": null,\n                    \"width\": null,\n                    \"scale\": \"ultra-high\"\n                },\n                \"imbalance_ratio\": null\n            },\n            \"description\": \"A large-scale dataset pre-trained on exotic and rare feline breeds for fine-tuning.\",\n            \"preprocessing\": [\n                \"adaptive RandAugment\",\n                \"double-channel color standardization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Heterogeneous Fusion Model\",\n            \"family\": [\"Inceptionv5-Plus\", \"ResNeXt-Enhanced\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"heterogeneous fusion\",\n                \"pre-trained_model\": [\"ImageNet-Plus\"],\n                \"fine-tuning\": true\n            },\n            \"description\": \"A model combining Inceptionv5-Plus and ResNeXt-Enhanced for feline morphological analysis, incorporating novel techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Inter-species genetic nuances affect classification performance.\"\n        },\n        {\n            \"description\": \"Adaptive RandAugment improves data diversification.\"\n        },\n        {\n            \"description\": \"Hybrid loss function (Focal Loss and Class Weighted AUC) handles class imbalance.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Methodology for fine-tuning and deployment on ultra-high resolution images.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intention to build a model, incorporating Inceptionv5-Plus and ResNeXt-Enhanced. The problem area and task are well-defined for feline morphological analysis. Performance metrics like accuracy, F1 score, and the custom hybrid loss function are considered. User's high expertise is acknowledged. However, specific metric values and deployment details are yet to be provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "In light of the impending transition to renewable energy grids, design a high-dimensional, cascading autoencoder-ensemble with hybrid attention and meta-learning incorporated, specifically tailored for the next-generation smart grid. The model must conduct a 144-hour ultra-long-range forecast, achieving sub-millisecond accuracy (ideally below 1 microsecond) for both solar and wind power fluctuations, while seamlessly accommodating intermittent data and handling distributed energy resource variability. Furthermore, implement a dynamic anomaly detection system to identify outliers and adapt to sudden meteorological changes, ensuring minimal performance drift and real-time fault prediction with less than 1% false positive rate. Concoct an energy-efficient solution that can run on resource-constrained microcontrollers with less than 5 MB memory footprint, guaranteeing scalability and adaptability across diverse geographical regions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"smart grid, renewable energy\",\n        \"description\": \"Design a high-dimensional, cascading autoencoder-ensemble with hybrid attention and meta-learning for next-gen smart grid, targeting a 144-hour ultra-long-range forecast, achieving sub-millisecond accuracy for solar and wind power fluctuations. The model must handle intermittent data and diverse energy resources. A dynamic anomaly detection system is required with a low false positive rate and real-time fault prediction. The solution must be energy-efficient, suitable for microcontrollers with less than 5 MB memory.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.001\n            },\n            {\n                \"name\": \"Forecast Error\",\n                \"value\": \"sub-millisecond\"\n            },\n            {\n                \"name\": \"False Positive Rate\",\n                \"value\": 0.01\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": \"less than 1 microsecond\"\n            },\n            {\n                \"name\": \"Memory Footprint\",\n                \"value\": 5,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"Energy Efficiency\",\n                \"value\": \"optimized for resource-constrained microcontrollers\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"solar power\", \"wind power\"],\n            \"specification\": {\n                \"size\": \"144-hour\",\n                \"granularity\": \"ultra-long range\"\n            },\n            \"description\": \"Data capturing solar and wind power fluctuations for the smart grid with considerations for intermittent and distributed energy resources.\",\n            \"preprocessing\": [\"intermittent data handling\", \"data normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"anomaly detection patterns\"],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Cascading Autoencoder-Ensemble with Hybrid Attention and Meta-Learning\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_details\": \"high-dimensional, cascading, autoencoder ensemble, hybrid attention, and meta-learning\"\n            },\n            \"description\": \"A deep learning model for long-term renewable energy forecasting and dynamic anomaly detection\"\n        }\n    ],\n    \"knowledge\": [\n        \"Smart grid requirements, intermittent and distributed energy resources, and the need for real-time adaptability.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"intermittency\", \"data imputation\"],\n        \"target_device\": \"microcontrollers\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for low-resource environments\"],\n        \"demonstration\": \"144-hour forecasting and anomaly detection in real-time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's request for a complex machine learning project in the renewable energy domain, with specific requirements for accuracy, efficiency, and adaptability. The model name, ensemble, and attention mechanisms are accurately captured, as well as the performance and complexity constraints. The user's expertise level is assumed to be high based on the high-level requirements. However, some details like the dataset name and model specifications are assumed to be inferred based on the given problem description, and the confidence score reflects the completeness of the given information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "As a project manager for the machine learning team, devise a novel link prediction model for Citeseer that mandates the incorporation of feature importance analysis, guarantees interpretable results with a minimum precision of 95%, and necessitates the development of an adaptable framework to handle Big Data, accommodating terabytes of expanding scholarly citations while maintaining real-time efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"scholarly research\",\n        \"description\": \"Design a novel link prediction model for Citeseer that includes feature importance analysis, ensures interpretable results with a minimum precision of 95%, and requires an adaptable framework for Big Data, supporting terabytes of expanding citations with real-time efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Real-time efficiency\",\n                \"value\": null,\n                \"unit\": \"terabytes/s\"\n            },\n            {\n                \"name\": \"Inference time\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Citeseer\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"citation links\"],\n            \"specification\": {\n                \"size\": \"Big Data\",\n                \"source\": \"scholarly citations\"\n            },\n            \"description\": \"A large-scale scholarly citation dataset with the need for a scalable and interpretable link prediction model.\",\n            \"preprocessing\": [\"feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"interpretable\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"feature_importance\": true,\n                \"explainability\": true\n            },\n            \"description\": \"A link prediction model on Citeseer with focus on feature importance, interpretability, and Big Data handling.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection for interpretability\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"efficient distributed computing framework\"],\n        \"demonstration\": \"model explainability and performance on live data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's requirements for a link prediction model on Citeseer, specifying interpretability, high precision, Big Data handling, and real-time efficiency. The user's expertise level is assumed to be high, indicating a more complex and sophisticated task. The performance metric of precision is set to a minimum of 95% as specified. The adaptability to Big Data is addressed through the need for a scalable and efficient framework. The dataset and service sections are adapted to meet the model requirements and data handling challenges.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "node classification", "instruction": "For a cutting-edge machine learning project, design a GraphSAGE enhancement, focusing on a tridecathlon-inspired Hierarchical Dual-Resolution Attention-based Stellar Interaction Detector (HD-DSAGNN). This intricate model should consist of five specialized modules for parsing multi-faceted astrophysical signals, including protostellar clusters, glitches in gravitational waves, and variable pulsars. Aim to achieve a state-of-the-art AUC of 99% in discerning obscure astronomical phenomena, accounting for temporal cosmic variations and seamless integration of telescope arrays. Mandate the implementation of resilient fault detection and self-healing mechanisms in a high-availability system that can handle exabyte-scale data flow from distributed observatories without compromising data integrity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Design a GraphSAGE enhancement for a Hierarchical Dual-Resolution Attention-based Stellar Interaction Detector (HD-DSAGNN), targeting tridecathlon-inspired tasks. The model should tackle protostellar clusters, glitches in gravitational waves, and variable pulsars, aiming for a state-of-the-art AUC of 99% while accounting for temporal cosmic variations. The model must include fault detection and self-healing mechanisms in a high-availability system for handling exabyte-scale data from distributed observatories.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"AUC\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"TB\"\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"minutes\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Tridecathlon dataset\",\n            \"modality\": [\n                \"graph\"\n            ],\n            \"target_variables\": [\n                \"astronomical phenomena\"\n            ],\n            \"specification\": {\n                \"nodes\": {\n                    \"types\": [\n                        \"protostellar clusters\",\n                        \"gravitational wave glitches\",\n                        \"variable pulsars\"\n                    ]\n                },\n                \"temporal_data\": true,\n                \"exabyte_scale\": true\n            },\n            \"description\": \"Dataset containing multi-faceted astrophysical signals for the HD-DSAGNN model, including temporal data from distributed observatories.\",\n            \"preprocessing\": [\n                \"temporal feature extraction\",\n                \"exabyte-scale data normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"data flow and interaction visualizations\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HD-DSAGNN enhancement\",\n            \"family\": \"GraphSAGE\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architectural_modules\": \"5 specialized for parsing multi-faceted signals\",\n                \"attention Mechanisms\": \"Hierarchical Dual-Resolution\",\n                \"self-healing mechanisms\": true\n            },\n            \"description\": \"An advanced GraphSAGE model for hierarchical and attention-based interaction detection in astrophysical signals\"\n        }\n    ],\n    \"knowledge\": [\n        \"High temporal variations and cosmic dynamics in celestial events\",\n        \"Exabyte-scale data handling and fault tolerance\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"resilient fault detection\",\n            \"self-healing mechanisms\"\n        ],\n        \"target_device\": \"high-performance computing cluster\",\n        \"deployment_endpoint\": \"distributed observatory network\",\n        \"inference_engine\": [\n            \"optimized for exabyte-scale data\"\n        ],\n        \"demonstration\": \"demonstration of multi-telescope array integration and signal processing\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's intent to design a GraphSAGE enhancement, with a focus on the high-end requirements. It details the area, task, metrics, datasets, and models specified by the user. The instructions on feature engineering, target device, and demonstration are clear. The AI's confidence is high due to the comprehensive and structured nature of the request, although some metric values are left unspecified for optimization.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the lead of a high-performance ML team, design an intricate time-series forecasting system for an intricate solar energy initiative. Mandate modeling hourly data with fine-grained seasonality patterns, long-term trends, and unexpected anomalies, while deploying a real-time adaptive model that integrates live weather forecasts and predictive maintenance strategies for equipment, targeting a 95% accuracy in 7-day energy output predictions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"solar energy\",\n        \"description\": \"Design a sophisticated time-series forecasting system for an intricate solar energy initiative, handling hourly data with fine-grained seasonality, long-term trends, and anomaly detection. The system must integrate live weather forecasts and predictive maintenance for equipment, aiming for 95% accuracy in 7-day energy output predictions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.95,\n                \"unit\": \"%\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Real-time adaptability\",\n                \"value\": 1,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Solar energy hourly data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"hourly energy production\"],\n            \"specification\": {\n                \"time_frequency\": \"hourly\",\n                \"seasonality\": \"fine-grained\",\n                \"data_length\": null\n            },\n            \"description\": \"Contains detailed hourly data of solar energy production, including historical, live weather conditions, and equipment health data.\",\n            \"preprocessing\": [\n                \"Data normalization\",\n                \"Time series decomposition\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Hourly and seasonal patterns\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced time-series model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"Integrating RNNs, LSTMs, and attention mechanisms\",\n                \"input_features\": \"Hourly data, weather forecasts, equipment health indicators\"\n            },\n            \"description\": \"A deep learning model that addresses time-series complexities, such as seasonality, trends, and anomaly detection, while being adaptive with live data.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Integration of real-time weather data for improved forecasting accuracy\"\n        },\n        {\n            \"description\": \"Adaptive model that considers predictive maintenance for equipment health\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Live data integration\",\n            \"Integration with IoT devices\"\n        ],\n        \"target_device\": \"Real-time, edge or cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Distributed systems for scalability\"\n        ],\n        \"demonstration\": \"Real-time dashboard for 7-day energy output predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON structure adheres to the user's request for a complex time-series forecasting system with high accuracy. It accounts for fine-grained seasonality, long-term trends, anomaly detection, live integration, and predictive maintenance. The user's expertise indicates a need for a sophisticated solution. Some details are open-ended, as the user did not specify specific models or deployment details. This response captures the essence of the problem, performance requirements, and technologies involved.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Develop an advanced image recognition system for the real estate analytics platform, which not only discerns diverse property features (like energy-efficient installations, shared workspaces, and seasonal outdoor features) but also deciphers subtle context clues, weather impacts on aesthetics, and adaptive recognition based on market trends. Mandate high precision, real-time learning, and automated error correction, ensuring effortless user experience and dynamic property valuation for a competitive market edge.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"real estate analytics\",\n        \"description\": \"Develop an advanced image recognition system for property features, context clues, weather impact on aesthetics, and adaptive recognition based on market trends. Target high precision, real-time learning, and automated error correction for seamless user experience and dynamic property valuation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"milliseconds\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"real estate property images\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"property features\", \"context clues\", \"weather impact\", \"market trends\"],\n            \"specification\": {\n                \"image_dimensions\": {\"width\": null, \"height\": null},\n                \"training_images_count\": null,\n                \"test_images_count\": null\n            },\n            \"description\": \"A dataset containing images of properties with various features, contextual clues, weather impact, and adaptive market trends for training and testing.\",\n            \"preprocessing\": [\n                \"data augmentation\",\n                \"image resizing\"\n            ],\n            \"augmentation\": [\n                \"rotation\",\n                \"flipping\",\n                \"cropping\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Property Recognition Model\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"deep convolutional neural networks\",\n                \"layers\": \"dual-attention layers and recurrent layers\",\n                \"accuracy_goal\": \"state-of-the-art performance\"\n            },\n            \"description\": \"A highly-advanced AI model that combines deep learning for image recognition, with real-time adaptive learning, error correction, and precise feature extraction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider incorporating transfer learning from similar domains or time-lagged datasets for faster convergence and better generalization.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"autoencoding for dimensionality reduction\", \"feature extraction from metadata\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"real-time property valuation API\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"GPU acceleration\"],\n        \"demonstration\": \"live feed of image analysis with immediate property valuation updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON closely represents the user's instruction, specifying an advanced image recognition system for real estate analytics with requirements for high precision, real-time learning, and adaptive recognition. Specific performance metrics (precision) and time complexity (in milliseconds) are included. Details about the model and dataset preprocessing, augmentation, and deployment are incorporated. However, some values (like image dimensions and accuracy rate) are left as null for precise determination based on additional data or user clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "Craft a sophisticated financial phishing SMS detection system using the comprehensive Credit Card Fraudulence Detection dataset. Design an innovative model that merges a multi-layered, depth-wise hierarchical Convolutional Neural Network (CNN) with an advanced Transformer-based Attention mechanism. Strive for exceptional accuracy, targeting precision beyond 99%, while adhering to stringent real-time constraints of under 2 milliseconds. Additionally, optimize the model for seamless, ultra-responsive performance on diverse mobile devices with minimal computational overhead.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Craft a financial phishing SMS detection system using the Credit Card Fraudulence Detection dataset. Design an innovative model combining a multi-layered depth-wise hierarchical Convolutional Neural Network (CNN) with an advanced Transformer-based Attention mechanism.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 2.0,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"computational overhead\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Credit Card Fraudulence Detection dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"phishing detection\"],\n            \"specification\": {\n                \"rows_count\": null,\n                \"columns_count\": null,\n                \"text_length\": null,\n                \"category_distribution\": null\n            },\n            \"description\": \"A dataset for detecting financial phishing SMS messages.\",\n            \"preprocessing\": [],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Depth-wise Hierarchical CNN + Transformer Attention\",\n                       \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"CNN_layers\": {\n                        \"depth\": null,\n                        \"kernel_sizes\": [],\n                        \"strides\": []\n                    },\n                    \"Transformer_layers\": {\n                        \"attention_heads\": null,\n                        \"hidden_units\": null\n                    }\n                },\n                \"train_accuracy\": null,\n                \"test_accuracy\": null\n            },\n            \"description\": \"A complex model fusing a multi-layered depth-wise CNN with a Transformer for advanced phishing detection.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"optimized for real-time performance\"\n        ],\n        \"demonstration\": \"mobile responsiveness and ultra-fast response\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the essence of the financial phishing SMS detection system, detailing a specific model architecture, performance goals, and real-time constraints. The user's high expertise level suggests an understanding of the requirements for a complex task. However, exact values for performance metrics, model specifications, and preprocessing steps are missing due to the lack of detailed information in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a seasoned project manager, you are directing the cutting-edge AI research for a high-impact time-series forecasting model in an ultra-competitive energy sector. The Electricity dataset, now divided into specialized subsets showcasing temporal non-linearities and fine-grained seasonality, necessitates the design of a model capable of processing lengthy historical sequences (96 time steps with 321 diverse attributes, INPUT_SEQ_LEN=96, INPUT_DIM=321) for not only instant predictions but also 96-step-ahead forecasting (PRED_SEQ_LEN=96, PRED_DIM=321) with unprecedented accuracy. Strive for state-of-the-art performance with optimized variations of mean absolute percentage error (MAPE) and symmetric mean absolute percentage error (SMAPE), integrating explainable AI and scalable infrastructure for real-time grid optimization. Demand a phased, incremental prototype demo with progressive validation, accompanied by a thorough resource allocation plan, before the system's commercial launch.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy sector\",\n        \"description\": \"Develop a cutting-edge AI research project for time-series forecasting, focusing on the Electricity dataset. The model should handle non-linearities, fine-grained seasonality, lengthy historical sequences, and 96-step-ahead forecasting with 321 attributes. Target state-of-the-art performance using MAPE and SMAPE optimization, explainability, and scalable infrastructure for real-time grid optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MAPE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"SMAPE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Electricity dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"electricity data\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 96,\n                \"INPUT_DIM\": 321,\n                \"PRED_SEQ_LEN\": 96,\n                \"PRED_DIM\": 321\n            },\n            \"description\": \"A time-series dataset with fine-grained seasonality, divided into specialized subsets for electricity forecasting, containing 96 time steps and 321 attributes.\",\n            \"preprocessing\": [\"time series decomposition\", \"resampling\"],\n            \"augmentation\": [],\n            \"visualization\": [\"time series patterns\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art time-series forecasting\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": \"high\",\n                \"scalability\": \"real-time\"\n            },\n            \"description\": \"A deep learning model for time-series forecasting with explainable AI and optimized for the Electricity dataset's requirements.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Project will involve phased prototype demo with progressive validation\",\n        \"Resource allocation plan for infrastructure and personnel\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"sequence encoding\"],\n        \"target_device\": \"ultra-low-latency\",\n        \"deployment_endpoint\": \"smart energy grid infrastructure\",\n        \"inference_engine\": [\"GPU-based\", \"real-time processing\"],\n        \"demonstration\": \"phased prototype followed by live demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON representation reflects the user's detailed instruction, indicating the high level of expertise required and the specific task within the time-series analysis area. Performance metrics, dataset requirements, and model development expectations align with the given directions. However, some metric values are left unspecified for optimization, as the user demands state-of-the-art performance and scalability.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager is tasked with upgrading the current machine translation platform, targeting the specialized WMT19 Legalstra collection. Mandate the use of state-of-the-art transformer combinations, such as Hugging Face's M2M100 and T6, tailored for legal terminology. Demand a multi-step validation process incorporating case-sensitive detokenized BLEU, striving for a minimum score of 48 while preserving precise term usage and seamless readability in the translated Russian-English legal documents. Additionally, incorporate fine-grained evaluation of coherence, idiom preservation, and legal jargon accuracy to ensure a holistic high-quality translation output.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"legal translation\",\n        \"description\": \"Upgrade the machine translation platform, focusing on the WMT19 Legalstra collection. Use state-of-the-art transformer models like M2M100 and T6 from Hugging Face, specialized in legal terminology.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Case-sensitive detokenized BLEU\",\n                \"value\": 48.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Memory consumption\",\n                \"value\": null\n            }\n        ],\n        \"specific_metrics\": {\n            \"coherence\": {\n                \"description\": \"Fine-grained evaluation of document flow and consistency\",\n                \"value\": null\n            },\n            \"idiom_preservation\": {\n                \"description\": \"Preservation of natural language expressions\",\n                \"value\": null\n            },\n            \"legal_jargon_accuracy\": {\n                \"description\": \"Accuracy in the usage of legal terminology\",\n                \"value\": null\n            }\n        }\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WMT19 Legalstra\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated Russian-English text\"],\n            \"specification\": {\n                \"language_pair\": [\"Russian\", \"English\"],\n                \"domain\": \"legal\"\n            },\n            \"description\": \"A specialized legal translation dataset for the WMT19 collection\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hugging Face's M2M100 and T6\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"transformer\", \"specialized for legal terms\"]\n            },\n            \"description\": \"State-of-the-art models for legal domain machine translation using M2M100 and T6\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Legal terminology adaptation\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Demonstrations of multi-step validation process and fine-grained evaluation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's intent to build a high-quality machine translation platform using state-of-the-art transformers and specific requirements like case-sensitive BLEU, coherence, idiom preservation, and legal jargon accuracy. The expertise level indicates a clear understanding of the technical aspects. Details about datasets and performance metrics provide a solid basis for the project. However, specific values for inference time and memory consumption, as well as the fine-grained metrics, are not provided, so the confidence score is slightly lower.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for the advanced machine learning team, you are mandated to create a novel time-series classification system for deciphering intricate gesture dynamics using the UWave Gesture Library. The dataset consists of high-resolution time series data from 8 wearable sensors (INPUT_SEQ_LEN = 945 with 16 distinct channels, INPUT_DIM expanded to 16), with rigorous data segmentation: 75% for training (including transfer learning warm-up), 10% for validation (employing adaptive learning rates), and 15% for unseen test sequences. The task requires not just designing an interpretable LSTM-attention hybrid model, but also optimizing for real-time performance, achieving a minimum accuracy of 97% and ensuring latency constraints below 20 milliseconds. Remember to address class imbalance and propose a method for quantifying model uncertainty in the report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"gesture recognition\",\n        \"description\": \"Create a novel time-series classification system using the UWave Gesture Library for high-resolution gesture dynamics data from 8 wearable sensors. The system must employ an interpretable LSTM-attention hybrid model, optimize for real-time performance, and achieve minimum accuracy of 97% with latency constraints below 20 milliseconds. Address class imbalance and propose a method for quantifying model uncertainty in the report.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"Latency\",\n                \"value\": 20,\n                \"unit\": \"milliseconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"Milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"UWave Gesture Library\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"Gesture Classification\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 945,\n                \"INPUT_DIM\": 16,\n                \"data_split\": {\n                    \"train\": 75,\n                    \"validation\": 10,\n                    \"test\": 15\n                }\n            },\n            \"description\": \"High-resolution time series data from 8 wearable sensors (16 channels) for gesture recognition.\",\n            \"preprocessing\": [\"transfer learning warm-up\", \"addressing class imbalance\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"LSTM-Attention Hybrid Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"interpretable\",\n            \"specification\": {\n                \"interpretability_features\": \"LSTM-attention architecture\"\n            },\n            \"description\": \"A hybrid model that combines LSTM for sequence learning and attention mechanism for interpretability in gesture recognition.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Address class imbalance strategy\",\n        \"Quantifying model uncertainty method\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"LSTM pre-processing\", \"Attention mechanism\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Include latency-sensitive performance demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers the high-level requirements specified in the instruction, including the task of building a novel time-series classification system, using LSTM-attention hybrid model, targeting real-time performance, addressing class imbalance, and quantifying model uncertainty. The JSON schema has been adhered to, with specific details and performance metrics tailored to the project's needs.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As the project manager for our upscale luxury retail network, you must lead the deployment of a highly specialized AI-based inventory control system. This system necessitates the development of a cutting-edge Mask R-CNN++ model, integrating advanced object recognition algorithms tailored for SKU-110K exclusive, luxury items with distinct visual features, including intricate packaging. The model must exhibit extraordinary accuracy, maintaining an error rate below 0.5% while differentiating precisely between high-end fashion, gourmet food, and high-tech gadgets in cluttered, luxury boutiques. Alongside the model's creation, prepare a meticulously detailed performance report showcasing the model's harmonic mean of precision and recall (H-MAP), its adaptive strategies to fluctuations in luxury trends, and a rigorous side-by-side comparison with premium industry competitors, incorporating live product demonstrations in diverse store conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"luxury retail\",\n        \"description\": \"Deploy a highly specialized AI-based inventory control system with a Mask R-CNN++ model for SKU-110K exclusive luxury items in cluttered boutiques, focusing on high-end fashion, gourmet food, and high-tech gadgets with an error rate below 0.5% and a performance report on harmonic mean of precision and recall (H-MAP), adaptive strategies to luxury trends, and competitor comparison through live demonstrations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"error rate\",\n                \"value\": 0.005\n            },\n            {\n                \"name\": \"H-MAP\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SKU-110K luxury items\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"luxury items categories\"],\n            \"specification\": {\n                \"items_count\": 110000,\n                \"visual_features\": [\"distinctive packaging\", \"intricate designs\"]\n            },\n            \"description\": \"A dataset of luxury items with high-end fashion, gourmet food, and high-tech gadgets for training and testing the Mask R-CNN++ model.\",\n            \"preprocessing\": [\n                \"removal of clutter\",\n                \"normalization for luxury boutique conditions\"\n            ],\n            \"augmentation\": [\n                \"adaptive augmentation for luxury trends\",\n                \"fine-tuning for SKU-specific features\"\n            ],\n            \"visualization\": [\n                \"item categorization visualizations\",\n                \"performance metric demonstrations\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Mask R-CNN++\",\n            \"family\": \"Advanced object recognition\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"accuracy\": \"error rate < 0.5%\",\n                \"adaptability\": \"luxury trend fluctuations\",\n                \"detection precision\": \"high-end fashion, gourmet food, and high-tech gadgets\"\n            },\n            \"description\": \"A specialized AI model for SKU-110K exclusive luxury items with advanced object recognition and clutter handling.\"\n        }\n    ],\n    \"knowledge\": [\n        \"dynamic luxury trends\",\n        \"adapting to competitive landscape\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"customer behavior analysis\",\n            \"item-specific feature extraction\"\n        ],\n        \"target_device\": \"high-end store environments\",\n        \"deployment_endpoint\": \"luxury retail network\",\n        \"inference_engine\": [\n            \"real-time object detection engine\",\n            \"demonstration software\"\n        ],\n        \"demonstration\": \"live product recognition in diverse store conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON structure is based on the user's instruction, which is a project management task involving the development and deployment of an advanced AI inventory control system. The user's high expertise level is considered in the response. The model, performance metrics, and complexity requirements are detailed to address the specific luxury item categories and accuracy goals. The request for competitor comparison and live demonstrations adds to the breadth of the response.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for a cutting-edge AI team, your mandate is to develop a highly specialized text classifier for the arXiv repository, focusing on esoteric subfields of Physics, Math, and Computer Science. Mandate the creation of a state-of-the-art HAN+BERT fusion model, with strict requirements of achieving not just 90% but 95% micro- and macro-averaged F1 scores. In addition, ensure the system's scalability for efficient real-time learning, able to adapt to the ever-evolving dataset with sub-second latency for new paper submissions, while maintaining robustness and performance optimization in a dynamically changing academic landscape.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"scientific research\",\n        \"description\": \"Develop a highly specialized text classifier for esoteric subfields of Physics, Math, and Computer Science from the arXiv repository. The target is to build a HAN+BERT fusion model targeting 95% micro- and macro-averaged F1 scores.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"micro-avg F1 score\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"macro-avg F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time learning latency\",\n                \"value\": \"sub-second\",\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"scalability for dynamic dataset\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv repository (Physics, Math, and Computer Science)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Paper Category\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": \"\",\n                    \"test\": \"\"\n                },\n                \"features\": \"Pre-processed text data\"\n            },\n            \"description\": \"A repository of scientific papers with text data focusing on esoteric subfields of Physics, Math, and Computer Science.\",\n            \"preprocessing\": [\"text normalization\", \"tokenization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HAN+BERT Fusion Model\",\n            \"family\": \"neural networks\",\n            \"type\": \"transformer-based\",\n            \"specification\": {\n                \"parameters\": {\n                    \"HAN Layers\": \"\",\n                    \"BERT Layers\": \"\"\n                },\n                \"computational_resources\": {\n                    \"FLOPs\": \"\"\n                }\n            },\n            \"description\": \"A state-of-the-art fusion model combining Hierarchical Attention Networks (HAN) and BERT for text classification in esoteric subfields of Physics, Math, and Computer Science.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Adaptation to dynamic dataset evolution and robustness against concept drift are key concerns.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"dynamic feature extraction\", \"contextual embeddings\"],\n        \"target_device\": \"cloud-based with GPU support\",\n        \"deployment_endpoint\": {\n            \"real-time learning\": true,\n            \"latency\": \"sub-second\"\n        },\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"A live demo showcasing real-time classification and adaptability\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the user's instructions, focusing on creating a specialized text classifier, details of the HAN+BERT model, performance metrics, and scalability requirements. The high expertise level is assumed for the project manager. Although some specific values are not provided (like the exact FLOPs or layer details), the overall structure and emphasis on state-of-the-art techniques, performance, and scalability are captured.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "sequential recommendation", "instruction": "Create a real-time, personalized hotel finder for a global tourism platform, leveraging the massive 'Expaned Trivago v5.0' dataset. Mandate a GAT-based model with temporal encoding and spatiotemporal attention, optimizing for scalability, low latency, and dynamic adjustments based on micro-trends and users' real-time location, ensuring a smooth mobile experience even in peak season demands.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"global tourism\",\n        \"description\": \"Create a real-time, personalized hotel finder using the 'Expanded Trivago v5.0' dataset, leveraging a GAT-based model with temporal encoding and spatiotemporal attention. Optimize for scalability, low latency, and dynamic adjustments based on micro-trends and users' real-time location.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Scalability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Expanded Trivago v5.0\",\n            \"modality\": [\"multimodal\", \"time series\"],\n            \"target_variables\": [\"hotel recommendations\"],\n            \"specification\": {\n                \"dataset_size\": \"massive\",\n                \"data_format\": \"structured or unstructured\",\n                \"temporal_features\": true,\n                \"spatial_features\": true\n            },\n            \"description\": \"A large-scale dataset containing user behavior, hotel information, and geographical data for personalized hotel recommendations.\",\n            \"preprocessing\": [\n                \"Temporal feature extraction\",\n                \"Spatial data normalization\"\n            ],\n            \"augmentation\": [\"GAT-based data augmentation\"],\n            \"visualization\": [\n                \"Spatio-temporal visualization of user behavior patterns\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GAT-based recommendation model\",\n            \"family\": \"Graph Attention Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"GAT with temporal encoding and spatiotemporal attention\",\n                \"model_complexity\": {\n                    \"heads\": null,\n                    \"hidden_units\": null\n                }\n            },\n            \"description\": \"A real-time sequential recommendation model using GAT, temporal encoding, and spatiotemporal attention for dynamic hotel suggestions.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"micro-trends\": \"Analyzing real-time data for changing user preferences and market trends\",\n            \"mobile_optimization\": \"Design for smooth performance on mobile devices\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Temporal feature engineering\",\n            \"User profiling\"\n        ],\n        \"target_device\": \"mobile, IoT\",\n        \"deployment_endpoint\": \"global tourism platform API\",\n        \"inference_engine\": [\n            \"Lightweight inference algorithms for real-time response\"\n        ],\n        \"demonstration\": \"Real-time demo showcasing personalized hotel suggestions based on user location and micro-trends\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure reflects the user's requirements for a recommendation system, leveraging the Trivago dataset, GAT-based model with temporal and spatiotemporal features. Expertise level indicates a deep understanding of the problem. Performance metrics and complexity constraints capture the optimization for scalability and low latency. Specifics on preprocessing, data augmentation, and deployment are taken into account. The missing values for metrics reflect the need for model performance details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The challenging assignment for the ML team, now tasked with developing a quantum-enhanced QA prototype using a novel hybrid of MiniQ-Bert and Raspberry-Light, is to tackle the niche domain of Sustainable Agriculture Query Repository. The model must not only outperform existing systems with an astonishing precision of at least 98%, but also demonstrate quantum speedup by achieving sub-100 picosecond response times on antique Raspberry Pi single-board computers, all while preserving energy efficiency and minimizing quantum decoherence effects. Conduct intricate performance diagnostics, propose innovative error mitigation techniques, and present a thorough comparative analysis of the quantum-classical divide in decision-making for this specialized domain.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"sustainable agriculture\",\n        \"description\": \"Develop a quantum-enhanced QA prototype using a MiniQ-Bert and Raspberry-Light hybrid for the Sustainable Agriculture Query Repository. The model must have a precision of at least 98%, demonstrate quantum speedup with sub-100 picosecond response times on Raspberry Pi, and prioritize energy efficiency and quantum decoherence mitigation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Response time\",\n                \"value\": 100,\n                \"unit\": \"picoseconds\"\n            },\n            {\n                \"name\": \"Energy efficiency\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Quantum decoherence mitigation\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sustainable Agriculture Query Repository\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions\", \"answers\"],\n            \"specification\": null,\n            \"description\": \"A dataset containing agricultural queries and their corresponding answers for the quantum-enhanced QA model.\",\n            \"preprocessing\": [\n                \"Domain specific tokenization\",\n                \"Quantum-enhanced data preprocessing\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Quantum vs classical performance comparisons\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"MiniQ-Bert + Raspberry-Light Hybrid\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Quantum-enhanced QA model\",\n                \"quantum_elements\": [\"MiniQ-Bert\", \"Raspberry-Light\"]\n            },\n            \"description\": \"A quantum-enhanced QA model for the Sustainable Agriculture domain with high precision and sub-100 ps response time on Raspberry Pi.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Quantum-classical divide research in decision-making for sustainable agriculture\",\n        \"Innovative error mitigation techniques for quantum NLP\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Quantum-aware feature extraction\",\n            \"Energy-efficient feature pruning\"\n        ],\n        \"target_device\": \"Raspberry Pi\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Quantum accelerated\"],\n        \"demonstration\": \"Comparative analysis of quantum vs classical decision-making in sustainable agriculture\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately reflects the user's instruction, specifying a high-level ML task of building a quantum-enhanced QA model, the target domain, performance goals, and the requirement for energy efficiency and quantum decoherence mitigation. Details about data preprocessing, model specifics, and service requirements are also included based on the constraints. Some metric values are left as null to indicate the need to optimize them to meet the target.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Design a novel fusion of vector-quantized variational recurrent autoencoder (VQ-RAE) and a deep transformer-attention mechanism within a multi-modal Gated Time Series Network (MTSNet) for the exceptionally challenging task of forecasting precise sub-second changes in environmental acoustic fingerprints. The project must not only anticipate acoustic fluctuations in a busy international airport for the next 72 hours, but also disentangle and present the latent patterns of aircraft take-offs, landings, and background noise, necessitating ultra-high-resolution temporal forecasting and an intricate understanding of the dynamic interplay of diverse sound events.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"aviation & environmental monitoring\",\n        \"description\": \"Design a fusion of VQ-RAE and deep transformer-attention in MTSNet for precise sub-second environmental acoustic fingerprint forecasting in a busy international airport. The project aims to predict acoustic fluctuations over the next 72 hours, while disentangling patterns of aircraft events and background noise with ultra-high-resolution temporal forecasting.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean absolute error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mean squared error\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSE for sub-second forecasting\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint during training\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            },\n            {\n                \"name\": \"computational efficiency under ultra-high-resolution\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"机场环境 acoustic data\",\n            \"modality\": [\"audio\", \"time series\"],\n            \"target_variables\": [\"acoustic fluctuations\"],\n            \"specification\": {\n                \"length\": null,\n                \"sampling_rate\": null,\n                \"number_of_channels\": null,\n                \"frequency_range\": null\n            },\n            \"description\": \"A multi-modal dataset capturing sub-second environmental sound events at a busy international airport.\",\n            \"preprocessing\": [\n                \"vector quantization\",\n                \"normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"time-series analysis\", \"event clustering\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"VQ-RAE + MTSNet (Fusion)\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"vector-quantized VAE with transformer-attention and MTSNet\",\n                \"time_steps_took_for_72_hours\": null,\n                \"latency_per_forecast\": null,\n                \"number_of_layers\": null,\n                \"hidden_units\": null\n            },\n            \"description\": \"A deep learning model that combines VQ-RAE and a transformer-attention mechanism within MTSNet for precise sub-second environmental acoustic forecasting.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Challenges in ultra-high-resolution and dynamic event disentanglement.\",\n        \"Impact of aircraft take-offs, landings, and background noise on acoustic fluctuations.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"embedding extraction for diverse sound events\"],\n        \"target_device\": \"cloud-based for efficient computation\",\n        \"deployment_endpoint\": \"real-time forecasting API for the airport\",\n        \"inference_engine\": [\"TensorFlow serving or PyTorch serving\"],\n        \"demonstration\": \"visual dashboard with live forecasts and event patterns\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's request for a complex, multi-modal forecasting model with VQ-RAE, transformer-attention, and MTSNet. The performance metrics cover the expected goals for precision, while computational efficiency is highlighted. The dataset and preprocessing requirements are in line with the task's need for high-resolution and event disentanglement. However, more specific metric values or architectural details could be missing.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "The project manager is tasked with directing the AI team to develop a sophisticated, real-time time-series anomaly detection system. Utilizing a cutting-edge ensemble of tailored lightweight explainable transformers, they must achieve exceptional performance, with precision above 75% on the exclusive Hexagon ML/UCR archives, while ensuring minimal computational footprint and transparent anomaly explanations for critical industrial monitoring applications.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"industrial monitoring\",\n        \"description\": \"The project manager needs an AI team to develop a sophisticated, real-time time-series anomaly detection system using a cutting-edge ensemble of lightweight explainable transformers. The system must have precision above 75% on the Hexagon ML/UCR archives while prioritizing minimal computational footprint and transparent anomaly explanations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.75\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Hexagon ML/UCR archives\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"anomalies\"],\n            \"specification\": {\n                \"description\": \"Exclusive time-series data for anomaly detection\"\n            },\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Ensemble of Lightweight Explainable Transformers\",\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"description\": \"Cutting-edge, lightweight with explainability for transparency\"\n            }\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"tailored feature extraction\"],\n        \"target_device\": \"real-time, lightweight\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"interpretable models\"],\n        \"demonstration\": \"transparent anomaly explanations for critical applications\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response captures the essence of the user's request for a time-series anomaly detection project with high precision, explainable transformers, and real-time capabilities. Performance metrics, complexity constraints, and dataset preprocessing are clearly defined. However, specific model names and values for metrics such as inference time and memory size are not provided since these may require additional research or are not directly specified in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Redesign the machine translation project for the multidisciplinary TEDx Talks dataset, focusing on Iberian Portuguese variants in coastal regions. Demand the exclusive use of the Hugging Face's latest M6 transformer, integrating IBM's UniMorph for nuanced tokenization, and necessitate the codebase to be optimized for nanodevices with less than 100MB ROM and limited operational capacity of 8-hour translation sessions without external power supplementation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"multidisciplinary research\",\n        \"description\": \"Redesign the machine translation project for the TEDx Talks dataset, specifically targeting Iberian Portuguese in coastal regions. Use Hugging Face's latest M6 transformer and integrate IBM's UniMorph for tokenization. Ensure code optimization for nanodevices with limited ROM (<100MB) and an operational capacity of 8-hour translation sessions without external power.\",\n        \"performance_metrics\": [],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory size (ROM)\",\n                \"value\": 100,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"operational capacity\",\n                \"value\": 8,\n                \"unit\": \"hours\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"TEDx Talks (Iberian Portuguese coastal)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"tokenizer\": \"IBM UniMorph\"\n            },\n            \"description\": \"A dataset of TEDx Talks in Iberian Portuguese, focusing on coastal regions, for machine translation.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hugging Face M6 Transformer\",\n            \"family\": \"Transformer models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"framework\": \"Hugging Face\",\n                \"model\": \"M6\",\n                \"tokenizer\": \"IBM UniMorph\"\n            },\n            \"description\": \"A machine translation model based on Hugging Face's latest M6 transformer, optimized for Iberian Portuguese coastal data.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"nanodevice\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed based on the user's requirements for a machine translation project targeting the TEDx Talks dataset in Iberian Portuguese coastal regions. Hugging Face's M6 transformer and IBM's UniMorph are specified. The model's complexity has been adapted for nanodevices with limited ROM and operational capacity. The performance metrics are not explicitly defined as the user focuses on resource constraints.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "For the avant-garde machine learning venture, devise a sophisticated QA prototype harnessing HFT-BERT++ (an enhanced HFT-BERT with domain-specific improvements), exclusively fine-tuned on the esoteric 'Elucidation of Esoteric Enigmas and Unseen Quandaries' (EEQE) dataset. The dataset contains cryptic questions that intertwine historical minutiae, esoteric wisdom, and linguistic nuances from five lesser-known ancient civilizations. The system must achieve an F1 score above 97% for intricate comprehension, guarantee sub-100 millisecond response times for a frictionless user experience, and exhibit seamless cross-lingual functionality, retaining accuracy even when decoding queries in Coptic, Old Norse, Linear B, Etruscan, and Ogham scripts simultaneously.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"knowledge extraction\",\n        \"description\": \"Design a sophisticated QA prototype using HFT-BERT++ with domain-specific enhancements, fine-tuned exclusively on the 'Elucidation of Esoteric Enigmas and Unseen Quandaries' (EEQE) dataset. The dataset contains cryptic questions intertwining historical, esoteric, and linguistic elements from five lesser-known ancient civilizations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.97,\n                \"description\": \"for intricate comprehension\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": 100,\n                \"unit\": \"milliseconds\",\n                \"description\": \"guarantee sub-100 ms for a frictionless user experience\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Elucidation of Esoteric Enigmas and Unseen Quandaries (EEQE)\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": null,\n            \"description\": \"Cryptic questions with historical, esoteric, and linguistic nuances from 5 lesser-known ancient civilizations.\",\n            \"preprocessing\": [\n                \"domain-specific data cleaning\",\n                \"character encoding for diverse scripts\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HFT-BERT++\",\n            \"family\": \"transformer-based QA model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"fine-tuning details\": \"Esoteric improvements, exclusive on EEQE dataset\",\n                \"cross-lingual ability\": [\"Coptic\", \"Old Norse\", \"Linear B\", \"Etruscan\", \"Ogham\"],\n                \"accuracy\": \"aim for 97% F1 score\"\n            },\n            \"description\": \"A sophisticated QA model enhanced with HFT-BERT++ and tailored for the EEQE dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"ancient civilizations\",\n        \"historical minutiae\",\n        \"esoteric wisdom\",\n        \"linguistic nuances\",\n        \"cross-lingual functionality\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"question understanding\",\n            \"contextual relevance\"\n        ],\n        \"target_device\": \"GPU (for fast inference)\",\n        \"deployment_endpoint\": \"cloud-based platform\",\n        \"inference_engine\": [\"GPU-accelerated inference\"],\n        \"demonstration\": \"responsive and seamless user interface\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been created based on the user's requirements, detailing a build task for a question-answering prototype. The F1 score, response time, and cross-lingual capabilities are specified. However, some aspects like dataset size or network architecture details are assumed due to the nature of the instructions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "The cutting-edge challenge faced by the ML team is to develop a highly specialized question-answering bot for niche luxury fashion e-commerce, integrating the exclusive LuxuriousFashionsQA dataset. Implement an advanced few-shot capable ALBERT model, enhancing not only product knowledge, fashion trends, and personalized recommendations, but also deciphering subtle brand nuances and handling complex fashion queries with artistic precision. Mandate that the system not only responds within a fraction of a second but also dynamically learns from user interactions to refine its knowledge, ensuring a sophisticated, aspirational user experience at all times.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"luxury fashion e-commerce\",\n        \"description\": \"The task is to develop a highly specialized question-answering bot for niche luxury fashion e-commerce, integrating the LuxuriousFashionsQA dataset. The bot should use an advanced ALBERT model with few-shot capabilities, focusing on product knowledge, fashion trends, personalized recommendations, brand nuances, and complex fashion queries with artistic precision.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": \"fraction of a second\"\n            },\n            {\n                \"name\": \"dynamic learning\",\n                \"value\": \"yes\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"LuxuriousFashionsQA dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Answers to fashion-related questions\"],\n            \"specification\": {\n                \"size\": \"customized for few-shot learning\",\n                \"format\": \"structured QA pairs\"\n            },\n            \"description\": \"A dataset tailored for luxury fashion questions, providing comprehensive product and trend information.\",\n            \"preprocessing\": [\"few-shot formatting\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ALBERT model with few-shot capability\",\n            \"family\": \"neural networks\",\n            \"type\": \"transformers\",\n            \"specification\": {\n                \"architecture\": \"ALBERT\",\n                \"few-shot learning\": true,\n                \"product knowledge, fashion trends, and brand nuances integration\": true\n            },\n            \"description\": \"An advanced ALBERT model designed for question answering with enhanced product knowledge and nuanced fashion understanding.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Niche luxury fashion expertise and focus on artistic precision\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"few-shot adaptation of product and fashion knowledge\"\n        ],\n        \"target_device\": \"high-performance server\",\n        \"deployment_endpoint\": \"luxury fashion e-commerce platform\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"User interactions and continuous refinement\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intention for building a question-answering bot, integrating a few-shot ALBERT model with specific requirements for luxury fashion. The metrics align with the user's mandate for fast response time and dynamic knowledge improvement. The missing values for complexity metrics are understood as a goal for optimization, and the dataset details are interpreted based on the provided LuxuriousFashionsQA dataset context. The user's high expertise level indicates their ability to communicate these complex requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Design a distributed, fault-tolerant node classification model using the complex NELL-2.0 dataset, employing a novel hybrid Hypergraph Neural Network (HGNN) fusion approach. Focus on defending against adversarial attacks, managing high-dimensional, multi-aspect relationships, and optimizing computational efficiency for massive graphs. Strive for a minimum precision of 95% and conduct a rigorous nested cross-validation, comparing various edge weighting strategies and regularization techniques. Document comprehensive performance dynamics and potential scalability challenges in a detailed, interactive report with visualizations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graphs\",\n        \"description\": \"Design a distributed, fault-tolerant node classification model using the NELL-2.0 dataset with a novel hybrid Hypergraph Neural Network (HGNN) fusion approach. Focus on adversarial defense, managing multi-aspect relationships, and optimizing computational efficiency for large graphs.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ],\n        \"validation_method\": {\n            \"type\": \"nested cross-validation\",\n            \"details\": \"comparing various edge weighting strategies and regularization techniques\"\n        },\n        \"additional_details\": \"防御 adversarial attacks and extensive performance dynamics analysis, addressing scalability challenges\"\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL-2.0\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node classes\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimensionality\": null,\n                \"high_dim_relations\": true\n            },\n            \"description\": \"A complex knowledge graph dataset with high-dimensional, multi-aspect relationships.\",\n            \"preprocessing\": [\"adversarial attack defense\"],\n            \"augmentation\": [],\n            \"visualization\": [\"performance dynamics\", \"scalability\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid Hypergraph Neural Network (HGNN)\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"novelty\": \"hybrid fusion approach\"\n            },\n            \"description\": \"A distributed and fault-tolerant model designed for NELL-2.0, focusing on HGNN fusion, adversarial defense, and computational efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Large graphs and real-world knowledge graphs like NELL-2.0 call for specialized models to manage multi-aspect relationships.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Edge weighting optimization\",\n            \"Regularization technique exploration\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"interactive performance report and visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's requirements for a distributed, fault-tolerant model with a focus on specific performance metrics, defense against adversarial attacks, and managing complex relationships. The validation method and additional details address the user's interest in comparing edge weighting strategies and regularization techniques. However, some metric values are left unspecified, leaving room for optimization within the constraints of large-scale graph processing and user-specified computational efficiency.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "The mission for the upcoming machine learning project revolves around developing a groundbreaking image classifier for esoteric and niche bird species classification using the challenging CUB-200-2011 dataset. The team should employ cutting-edge neural architecture, combining EfficientNet-B7 with adversarial training, to handle camouflage, ethological motion, and intricate feather details. The system must achieve a remarkable F1 score of 97% for both endangered and extremely rare species, while also demonstrating improved interpretability through Grad-CAM heatmaps for each class. In addition, provide a comprehensive analysis of False Positives and False Negatives, highlighting the most challenging misclassifications for further refining the model.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"zoology\",\n        \"description\": \"Develop a groundbreaking image classifier for esoteric and niche bird species using CUB-200-2011 dataset. Employ EfficientNet-B7 with adversarial training to address camouflage, ethological motion, and intricate feather details. Target F1 score of at least 97% for endangered and rare species, with improved interpretability via Grad-CAM heatmaps.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.97,\n                \"groups\": [\n                    {\"species\": [\"endangered\", \"rare\"], \"target\": \"both\"}\n                ]\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CUB-200-2011\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"bird species\"],\n            \"specification\": {\n                \"size\": \"custom\",\n                \"challenging_features\": [\"camouflage\", \"ethological motion\", \"intricate feather details\"]\n            },\n            \"description\": \"A challenging dataset for bird species classification, including camouflage, ethological motion, and fine feather details.\",\n            \"preprocessing\": [\n                \"adversarial training\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\"Grad-CAM heatmaps\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet-B7 + Adversarial Training\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"model_architecture\": \"EfficientNet-B7\",\n                \"training_strategy\": \"adversarial\"\n            },\n            \"description\": \"A state-of-the-art model using EfficientNet-B7 architecture with adversarial training for esoteric bird species classification.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"title\": \"False Positives and Negatives Analysis\",\n            \"description\": \"An in-depth analysis of misclassifications, focusing on identifying the most challenging cases for model refinement.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"CAM-based interpretability\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"False Positives and Negatives visualized through Grad-CAM heatmaps\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's instruction, including the dataset, task, performance requirements, and the use of cutting-edge techniques. However, as with previous examples, specific computational complexity values and deployment details are left open for optimization based on the user's requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "As a seasoned project manager overseeing an innovative machine learning project, our team must commence with the CiteSeer graph dataset, meticulously extract and enrich node attributes via feature engineering. Utilize state-of-the-art GraphSAGE architecture with attention mechanisms, optimizing for both micro-averaged F1 score and macro-averaged metrics. Establish quarterly milestones, focusing on incremental improvements in precision and recall for distinct subdomains within computer science theory and information retrieval, while maintaining a balance between computational efficiency and model complexity.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Start a machine learning project with CiteSeer graph dataset, focusing on node attribute extraction and feature engineering. Employ GraphSAGE with attention mechanisms for classification. Set quarterly milestones targeting incremental improvements in precision and recall for computer science theory and information retrieval subdomains, prioritizing computational efficiency and model complexity.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Micro-averaged F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Macro-averaged F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"CiteSeer graph dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node labels\"],\n            \"specification\": {\n                \"nodes\": {\"attribute_count\": null},\n                \"edges\": {\"count\": null}\n            },\n            \"description\": \"A dataset for node classification in the context of CiteSeer, focusing on computer science theory and information retrieval.\",\n            \"preprocessing\": [\"attribute extraction\", \"feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GraphSAGE with Attention\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"GraphSAGE\",\n                \"attention_mechanism\": \"Yes\"\n            },\n            \"description\": \"A state-of-the-art model for node classification using GraphSAGE with attention mechanisms.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"node attribute extraction\", \"feature enrichment\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Quarterly progress updates\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON response captures the key elements of the user's instruction, detailing the project's focus, datasets, model architecture, and performance and complexity targets. The user's experience level is reflected in the high expertise level, and quarterly milestones are mentioned to accommodate incremental improvements. However, specific metric values or constraints are not provided, leaving room for optimization.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "Utilize the 'SubmarineAcousticSignals' dataset, demanding multi-level analysis. Implement a sequential process: first, preprocess the data by applying Discrete Wavelet Transform, then compare various anomaly detection methods (e.g., DWT+Isolation Forest, HMM+PCA), and finally evaluate performance with Receiver Operating Characteristic (ROC) curves. Emphasize the need for robust discrimination of distinct acoustic anomalies amidst the complex underwater environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"underwater acoustic monitoring\",\n        \"description\": \"Utilize the 'SubmarineAcousticSignals' dataset for multi-level analysis, starting with Discrete Wavelet Transform preprocessing, followed by comparing anomaly detection methods like DWT+Isolation Forest and HMM+PCA. Focus on robust discrimination of anomalies amidst the complex underwater environment with ROC curve evaluation.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"ROC AUC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"runtime for preprocessing\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"runtime for anomaly detection\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SubmarineAcousticSignals\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"acoustic anomalies\"],\n            \"specification\": {\n                \"data_format\": \"raw\",\n                \"sample_rate\": null,\n                \"dimensions\": null,\n                \"size\": null\n            },\n            \"description\": \"A dataset capturing acoustic signals from the underwater environment for anomaly detection, emphasizing robust discrimination amidst complexity.\",\n            \"preprocessing\": [\n                \"Discrete Wavelet Transform\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": [\n                \"DWT + Isolation Forest\",\n                \"HMM + PCA\"\n            ],\n            \"type\": \"ensemble models\",\n            \"specification\": {\n                \"anomaly_detection_methodologies\": [\n                    {\n                        \"method\": \"Isolation Forest\",\n                        \"implementation\": \"\"\n                    },\n                    {\n                        \"method\": \"Hidden Markov Model\",\n                        \"implementation\": \"\"\n                    },\n                    {\n                        \"method\": \"PCA\",\n                        \"implementation\": \"\"\n                    }\n                ]\n            },\n            \"description\": \"An ensemble method combining Discrete Wavelet Transform with Isolation Forest and/or HMM with PCA for anomaly detection in SubmarineAcousticSignals.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The complex underwater environment may introduce various noise sources and interference that could affect the performance of anomaly detection models.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"adapt to underwater environment characteristics\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"visualize ROC curves for each anomaly detection method\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's request for a multi-level analysis using the provided dataset and the specified anomaly detection techniques. It acknowledges the need for robust discrimination in complex underwater conditions and includes performance evaluation with ROC curves. The confidence score is medium due to the lack of specific model implementation details and performance targets.\",\n        \"score\": 0.7\n    }\n}\n"}
{"task": "node classification", "instruction": "As a project manager for a cutting-edge machine learning team, we've been tasked with utilizing the unique arXiv dataset from high-energy physics for a niche node classification problem. The challenge lies in developing a model that can not only differentiate between theoretical papers and experimental reports but also decipher subcategories within theoretical physics, such as quantum gravity and string theory. Our objective is to create a deep learning model with state-of-the-art algorithms like Graph Neural Networks, and we must optimize for precision and recall in the presence of sparse and complex citation networks. Can you outline the data preprocessing steps, propose a suitable architecture, and estimate the potential F1 score we could achieve with this novel approach?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"high-energy physics\",\n        \"description\": \"Develop a machine learning model using Graph Neural Networks for classifying arXiv dataset into theoretical papers, experimental reports, and subcategories within theoretical physics (e.g., quantum gravity and string theory). Optimize for precision and recall, considering sparse and complex citation networks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv high-energy physics dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"paper type\", \"subcategories\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"size\": null,\n                    \"dimensions\": null\n                },\n                \"edges\": {\n                    \"size\": null,\n                    \"types\": [\"citation\"]\n                }\n            },\n            \"description\": \"A dataset containing theoretical and experimental papers in high-energy physics, with subcategories in theoretical physics and complex citation network structure.\",\n            \"preprocessing\": [\n                \"Clean and standardize citation data\",\n                \"Handle missing values and duplicates\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Analyze citation network topology\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Graph Neural Network (GNN)\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art GNN variant (e.g., Graph Convolutional Networks, Graph Attention Networks)\",\n                \"number_of_layers\": null,\n                \"hidden_units\": null,\n                \"optimizer\": \"AdaGrad, RMSProp, or Adam with proper learning rate\"\n            },\n            \"description\": \"A deep learning model specifically designed for node classification in high-energy physics, leveraging Graph Neural Networks to classify papers by type and subcategory.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Sparse data can benefit from node sampling techniques during model training\",\n        \"Complex citation networks might require graph regularization or attention mechanisms\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature engineering for node embeddings\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Online inference with GPU acceleration\"\n        ],\n        \"demonstration\": \"Present F1 score on validation set and a detailed explanation of the model's performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been structured based on the user's detailed requirements. It clearly mentions the arXiv dataset, the need for node classification, the types of papers to classify, and the focus on precision and recall. Specifics about data preprocessing, model architecture, and GNN variants are proposed. The user's high expertise level is also reflected in the confidence score. However, the potential F1 score and some metric values are placeholders, as they would need to be estimated or actualized through model training.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "Investigate the 'HistoricalAtmosphericComposition' dataset that spans decades, integrating multiple factors like temperature, precipitation, and volcanic activity. Apply state-of-the-art time-series ensemble algorithms to detect subtle anomalies in Atmospheric CO2 levels, ensuring accurate distinction between long-term trends, seasonal variations, and potential human-induced disruptions for comprehensive environmental sustainability assessment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"environmental science\",\n        \"description\": \"Investigate the 'HistoricalAtmosphericComposition' dataset containing decades of data on temperature, precipitation, and volcanic activity. Apply state-of-the-art time-series ensemble algorithms for detecting anomalies in Atmospheric CO2 levels, differentiating long-term trends, seasonal variations, and human-induced disruptions for an extensive sustainability assessment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HistoricalAtmosphericComposition\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"Atmospheric CO2 levels\"],\n            \"specification\": {\n                \"start_year\": null,\n                \"end_year\": null,\n                \"time_resolution\": null,\n                \"variables_count\": null\n            },\n            \"description\": \"Dataset with time-series data on temperature, precipitation, and volcanic activity spanning decades to analyze CO2 levels.\",\n            \"preprocessing\": [\n                \"Data cleaning and normalization\",\n                \"Handling missing values\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Seasonality and trend analysis\",\n                \"Anomaly detection plots\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"ensemble time-series algorithms\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"ensemble_method\": null,\n                \"model_components\": []\n            },\n            \"description\": \"A state-of-the-art ensemble model for detecting anomalies in Atmospheric CO2 levels with focus on trends and seasonal variations\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using advanced imputation techniques for handling missing data\",\n        \"Recent papers suggest using stacked or blended models for ensemble approaches\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Explain detected anomalies and their potential causes\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction includes a clear task, dataset, and a focus on time-series analysis. The user's expertise level is assumed high, suggesting a deeper understanding of the task. The performance metrics are left open-ended to allow for the specification of desired accuracy levels. Complexities in terms of inference time and memory usage are noted for potential optimization. Some missing information, like model details and deployment specifics, can be completed based on the user's expertise.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for the machine learning development team, your objective is to tackle a specialized time-series classification challenge on the Heartbeat dataset. The dataset, now segmented into train, validation, and test sets, consists of complex physiological signals with a high-dimensional input (INPUT_SEQ_LEN=405, INPUT_DIM=61). Your team must design and implement an advanced model that can perform multi-step reasoning to extract patterns, handle irregularities, and predict binary labels (0, 1) with exceptional accuracy. Emphasize the need for not only training but also optimizing for model robustness and generalization across diverse sub-sequences.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Tackle a specialized time-series classification challenge on the Heartbeat dataset, with complex physiological signals and a high-dimensional input (405 timesteps, 61 dimensions). The model should perform multi-step reasoning, handle irregularities, and predict binary labels (0, 1) with high accuracy. Focus on model robustness and generalization across diverse sub-sequences.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Model size\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Robustness to subsequence diversity\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Heartbeat dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"binary labels\"],\n            \"specification\": {\n                \"INPUT_SEQ_LEN\": 405,\n                \"INPUT_DIM\": 61\n            },\n            \"description\": \"A time-series dataset consisting of complex physiological signals segmented into train, validation, and test sets.\",\n            \"preprocessing\": [\"segmentation\", \"resampling\"],\n            \"augmentation\": [\"time warping\", \"dropout\"],\n            \"visualization\": [\"spectrogram representation\", \"time-series overlays\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced time-series model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"for multi-step reasoning, e.g., RNNs, LSTMs, or Transformers\"\n            },\n            \"description\": \"Design and implement an advanced model capable of multi-step reasoning on complex time-series data with high-dimensional inputs for Heartbeat dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider Transformers for capturing long-range dependencies and handling irregularities\",\n        \"Investigate ensemble methods for robustness\",\n        \"Employ regularization techniques to improve generalization\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"extracting relevant features from high-dimensional data\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-based inference for speed\"],\n        \"demonstration\": \"Demonstrate performance on test set and cross-validation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response incorporates the user's role, emphasizing advanced model design and optimization for a complex time-series classification task. It specifies the desired domain, performance metrics, and specifies aspects of the dataset, model, and knowledge to be applied. Some metrics are left undefined for a comprehensive yet open-ended task, and emphasizes the user's high expertise level.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "The impending task demands the creation of a cutting-edge image recognition system, designed to discern not only standard astronomical objects but also elusive and rare celestial phenomena in deep space, as depicted in the High Energy Observatories (HEO) dataset. To tackle the challenges of detecting faint cosmic events and the complex interstellar patterns, I propose employing a state-of-the-art Mixture-of-Experts (MoE) Transformer architecture within ViT, harnessing its parallel processing capabilities and adaptability. The model's performance must surpass current benchmarks in precision, while demonstrating resilience to noise and the ability to recognize elusive phenomena that have only been observed once, ensuring groundbreaking discoveries for astrophysics researchers.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Create a state-of-the-art image recognition system for detecting standard and rare celestial objects in the High Energy Observatories (HEO) dataset. Use Mixture-of-Experts (MoE) Transformer within ViT architecture to handle faint cosmic events, complex interstellar patterns, and noise with a focus on precision and recognition of elusive, rare phenomena.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High Energy Observatories (HEO) dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"astronomical objects, rare celestial phenomena\"],\n            \"specification\": {\n                \"image_size\": null,\n                \"number_of_classes\": null,\n                \"data_split\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                }\n            },\n            \"description\": \"Dataset containing deep space images for detecting standard and rare celestial objects for the advanced image recognition system.\",\n            \"preprocessing\": [\"data augmentation for noise reduction\"],\n            \"augmentation\": [\"image cropping, rotation, noise addition\"],\n            \"visualization\": [\"sample images and performance heatmap\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"MoE-Transformer based on ViT\",\n            \"family\": \"Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Mixture-of-Experts (MoE) Transformer\",\n                \"base_model\": \"ViT\",\n                \"number_of_layers\": null,\n                \"parallelism\": true,\n                \"adaptability\": true\n            },\n            \"description\": \"A state-of-the-art model using Mixture-of-Experts in a ViT architecture designed for image classification in astrophysics, with a focus on precision and rare event detection.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Tackling faint cosmic events and complex interstellar patterns requires careful noise reduction techniques and attention to subtle details.\"\n        },\n        {\n            \"text\": \"Model should be resilient to detection of rare phenomena only observed once.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"transfer learning on HEO dataset\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"Online visualizations and performance comparison with benchmarks\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's advanced level of expertise and their specific requirements for the image recognition system. The problem, performance, and complexity metrics are defined, while some details, like model's exact performance and model's detailed architecture, are yet to be filled in as they can be derived based on the high-level description. The missing values in model specifications and complexity metrics allow for some optimization.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As a project manager for a high-precision media campaign cost prediction project, your data science team must develop a robust regression model using the Media Campaign Cost Dataset. This dataset, with its extensive numerical features, is divided into non-overlapping train, validation, and test sets. The challenge now is to minimize the RMSLE by at least 15% compared to previous models, while ensuring interpretability and feature importance analysis is incorporated. Remember to document the model's explainability and validate the results on all subsets.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"marketing or advertising\",\n        \"description\": \"Develop a robust regression model using the Media Campaign Cost Dataset for high-precision media campaign cost prediction. The dataset is divided into train, validation, and test sets. Target is to minimize RMSLE by at least 15% compared to previous models and include interpretability and feature importance analysis.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSLE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model explainability\",\n                \"value\": \"incorporated\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Media Campaign Cost Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"Campaign Cost\"],\n            \"specification\": {\n                \"split\": \"non-overlapping train, validation, and test sets\"\n            },\n            \"description\": \"A dataset with numerical features for predicting media campaign costs. The dataset is divided for model training, validation, and testing.\",\n            \"preprocessing\": [\"required data cleaning and normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"interpretable regression models\",\n            \"type\": \"statistical models\",\n            \"specification\": {\n                \"target_rmsle_improvement\": \"15%\",\n                \"interpretability\": \"incorporated\"\n            },\n            \"description\": \"A regression model for high-precision media campaign cost prediction with emphasis on explainability and a 15% reduction in RMSLE compared to previous models.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Focus on interpretability techniques such as feature importance analysis and model-agnostic explanations.\"\n        },\n        {\n            \"description\": \"Document the choice of model and reasoning for its selection to ensure transparency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"interpretable feature selection\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"include results on train, validation, and test sets\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON represents a clear intention to build a model for a specific marketing campaign cost prediction task. The performance metrics, complexity requirements, and dataset details align with the user's goal of model improvement and interpretability. Some elements like model name, family, and exact improvement are hypothetical as they are not specified in the instruction, but the structure reflects the general direction and intent of the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a seasoned AI project manager, I've been tasked with developing a comprehensive plant species recognition system using the diverse 'Flora Genesis Dataset', which contains over 10,000 high-resolution images. Our team must employ a state-of-the-art convolutional neural network (CNN), such as ResNet50 with transfer learning, while optimizing for both accuracy (aiming for above 95% top-5 classification) and real-time inference speed (targeting at least 30fps). Can you outline a detailed project roadmap, including data preprocessing, model architecture, and potential challenges, along with expected performance on various devices?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"botany\",\n        \"description\": \"Develop a plant species recognition system using the Flora Genesis Dataset with over 10,000 high-resolution images. Employ ResNet50 with transfer learning for state-of-the-art performance, targeting accuracy above 95% top-5 classification and real-time inference at 30fps.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"top-5 classification accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"inference time (FPS)\",\n                \"value\": 30\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time (device-dependent)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"model size (memory footprint)\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Flora Genesis Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"plant species\"],\n            \"specification\": {\n                \"size\": 10000,\n                \"image_resolution\": \"high\",\n                \"image_format\": null\n            },\n            \"description\": \"A large dataset with over 10,000 high-resolution images for plant species recognition.\",\n            \"preprocessing\": [\"data augmentation\", \"image resizing\"],\n            \"augmentation\": [\"random cropping\", \"horizontal flipping\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ResNet50 with transfer learning\",\n            \"family\": \"CNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"ResNet50\",\n                \"transfer_learning\": true,\n                \"parameters\": null\n            },\n            \"description\": \"A state-of-the-art CNN using ResNet50 for plant species recognition with transfer learning capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilize transfer learning to leverage pre-trained weights on large datasets for better performance.\",\n        \"Fine-tune the last few layers of ResNet50 to adapt to the specific Flora Genesis Dataset.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": [\"mobile, cloud, PC\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow, Caffe2\"],\n        \"demonstration\": \"Performance testing and visualization on various devices\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's expert-level expertise, task of developing a plant recognition system, the specific dataset, model, performance targets, and the need for real-time inference. Data preprocessing, augmentation, and model architecture are included based on best practices for image classification. However, specific device-dependent performance and model size details are left to be filled based on further testing and optimization.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "text classification", "instruction": "As a project manager for the machine learning team, your task is to develop a state-of-the-art sentiment analysis model for the IMDB dataset. You must achieve an advanced F1-score of at least 95% while considering unigrams, bigrams, and word embeddings, ensuring robustness against sarcasm and context shifts in 19th-century movie reviews.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"sentiment analysis\",\n        \"description\": \"Develop a state-of-the-art sentiment analysis model using the IMDB dataset, targeting an advanced F1-score of at least 95%. Incorporate unigrams, bigrams, and word embeddings, with a focus on robustness against sarcasm and context shifts in 19th-century movie reviews.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"IMDB movie reviews\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment\"],\n            \"specification\": {\n                \"data_split\": {\n                    \"train\": \"90%\",\n                    \"test\": \"10%\",\n                    \"time_period\": \"19th century\"\n                },\n                \"data_augmentation\": [\"unigrams\", \"bigrams\"]\n            },\n            \"description\": \"A dataset containing movie reviews for sentiment analysis, specifically focusing on 19th-century reviews.\",\n            \"preprocessing\": [\"handling sarcasm\", \"contextual analysis\"],\n            \"augmentation\": [\"unigram extraction\", \"bigram extraction\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"incorporates Transformers or LSTM with word embeddings\",\n                \"dropout_rate\": null,\n                \"embedding_size\": null\n            },\n            \"description\": \"A high-performing sentiment analysis model using unigrams, bigrams, and word embeddings for robustness against sarcasm and context shifts.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction for unigrams and bigrams\", \"word embedding preprocessing\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"performance on unseen 19th-century movie reviews\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the task as a high-expertise 'build' project, with performance and complexity requirements aligning with the user's instruction. It includes details on the IMDB dataset, features, and model requirements. It also acknowledges the need for data preprocessing, feature engineering, and robustness against sarcasm and context shifts. The confidence score is set to high, assuming the specific metric value(s) are not provided in the instruction.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "image segmentation", "instruction": "As the head of a prestigious historical archive, we seek an advanced image segmentation algorithm that can not only decipher complex layouts of medieval illustrations but also distinguish between multi-layered text and images, accounting for artistic variations and fade patterns, to enhance the digital accessibility of our extensive manuscript collection.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"historical archives\",\n        \"description\": \"Develop an advanced image segmentation algorithm to decipher complex layouts of medieval illustrations, distinguish between multi-layered text and images, and account for artistic variations and fade patterns to improve digital accessibility of a manuscript collection.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Medieval Manuscript Collection\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_size\": null,\n                \"color_depth\": null,\n                \"image_complexity\": {\n                    \"artistic variations\": true,\n                    \"fade patterns\": true\n                }\n            },\n            \"description\": \"A dataset containing medieval illustrations and multi-layered text images with diverse artistic styles and potential fade patterns.\",\n            \"preprocessing\": [\n                \"color normalization\",\n                \"image resizing\"\n            ],\n            \"augmentation\": [\n                \"artistic style transfer\",\n                \"contrast adjustment\"\n            ],\n            \"visualization\": [\n                \"example segmentation masks\",\n                \"artifact detection heatmaps\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Image Segmentation\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A cutting-edge deep learning model designed specifically for complex image segmentation tasks on medieval illustrations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Account for historical context and artistic styles in the feature extraction process\",\n        \"Consider edge detection techniques to handle multi-layered elements\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"histogram of oriented gradients (HOG)\",\n            \"visual attention-inspired feature aggregation\"\n        ],\n        \"target_device\": \"high-performance server or cloud\",\n        \"deployment_endpoint\": \"customized web-based access platform\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"Interactive segmentation results and visual explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the user's instruction, targeting an advanced image segmentation algorithm for historical archive application. Performance metrics are not explicitly specified but anticipated to be high given the advanced nature of the task. The provided description covers the key aspects of the problem, emphasizing the complexity and importance of preserving historical context. Specific model names and feature engineering steps are not provided, as they are open to suggestion or optimization.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "tabular regression", "instruction": "You're faced with a multivariate regression challenge using the Concrete Compressive Strength dataset. The dataset consists of intricate numerical features split into train, validation, and test subsets. Your objective is to build a highly accurate predictive model, optimizing for root mean squared error (RMSE), and detail the step-by-step feature engineering and model selection process.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"engineering\",\n        \"description\": \"You're faced with a multivariate regression problem using the Concrete Compressive Strength dataset. The task includes building a highly accurate model, optimizing for root mean squared error (RMSE), and requiring a step-by-step explanation of feature engineering and model selection processes, with data split into train, validation, and test subsets.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Concrete Compressive Strength dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"Concrete compressive strength\"],\n            \"specification\": {\n                \"train_size\": null,\n                \"validation_size\": null,\n                \"test_size\": null\n            },\n            \"description\": \"Dataset with numerical features for concrete properties, split into train, validation, and test subsets.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": null,\n                \"hyperparameters\": {\n                    \"optimizer\": null,\n                    \"learning_rate\": null,\n                    \"regularization\": null\n                }\n            },\n            \"description\": \"Highly accurate predictive model for multivariate regression using feature engineering and model selection techniques\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"handling missing values\",\n            \"feature scaling\",\n            \"categorical encoding\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"TensorFlow\",\n            \"PyTorch\"\n        ],\n        \"demonstration\": \"Step-by-step process for feature engineering, model selection, and performance evaluation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a highly accurate model for a multivariate regression task with specific performance metric (RMSE) and a focus on feature engineering and model selection. The structure is based on the details provided, including the data split and source, with room for missing specifics like dataset sizes, model architecture, and target device. The confidence score could be increased with more details or if the user's expertise level aligns with the complexity of the task.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For an avant-garde underwater image segmentation project using the Esoteric Seabed Odyssey (ESO) dataset, devise a hybrid model integrating a novel FusionNet architecture with a Generative Adversarial Network (GAN) adaptation. The objective is to distinguish not only coral reefs and marine life, but also rare, bioluminescent organisms and intricate underwater sediment patterns under extreme tidal conditions, such as zero-visibility or strong ocean currents. The model must maintain high-resolution segmentation while maintaining real-time performance, outperforming existing state-of-the-art algorithms in accuracy, precision, and F1 score by at least 20%. Additionally, the model should include a unique interpretability component to visually highlight the reasoning behind its segmentation decisions in the user interface.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"underwater exploration\",\n        \"description\": \"Design a hybrid model using a novel FusionNet architecture and a GAN adaptation for underwater image segmentation, targeting ESO dataset. The model should identify coral reefs, marine life, bioluminescent organisms, and sediment patterns under extreme tidal conditions with high-resolution and real-time performance. Performance should outperform existing algorithms by at least 20% in accuracy, precision, and F1 score.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.2\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Esoteric Seabed Odyssey (ESO) dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"segmentation masks for coral reefs, marine life, bioluminescent organisms, and sediment patterns\"],\n            \"specification\": {\n                \"image_resolution\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"dataset_size\": null,\n                \"data_distribution\": null\n            },\n            \"description\": \"A dataset for underwater image segmentation containing images captured under extreme tidal conditions.\",\n            \"preprocessing\": [\n                \"Zero-visibility and strong current data augmentation\"\n            ],\n            \"augmentation\": [\"GAN adaptation\"],\n            \"visualization\": [\n                \"Interpretability component\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"FusionNet with GAN adaptation\",\n            \"family\": \"Hybrid models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"FusionNet\",\n                \"GAN_integration\": true,\n                \"number_of_parameters\": null,\n                \"FLOPs\": null,\n                \"train_speed\": null,\n                \"inference_time_with_interpretability\": null\n            },\n            \"description\": \"A state-of-the-art model that combines FusionNet and a GAN for high-resolution underwater image segmentation with interpretability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The project requires an understanding of underwater imaging and handling challenging conditions like zero-visibility and strong currents.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"embedded devices for real-time performance\",\n        \"deployment_endpoint\": \"potentially cloud-based or IoT platform\",\n        \"inference_engine\": [\"optimized for latency-sensitive tasks\"],\n        \"demonstration\": \"User interface with visual explanations of segmentation decisions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essential details from the user's instruction, including the hybrid model, specific dataset (ESO), performance requirements, and interpretability feature. Expertise level is assumed to be high due to the project's complexity. However, some missing values (like specific network parameters or dataset specification) indicate room for further clarification in the user's instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project manager needs a sophisticated machine translation system enhancing Swahili-English translations, utilizing the Masakhane corpus. Mandate the integration of advanced Transformer and RNN architectures, and impose a 15% increase in BLEU benchmark, targeting state-of-the-art performance with limited-resource language support.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"cross-lingual communication\",\n        \"description\": \"The project requires a sophisticated machine translation system for Swahili-English translations, using the Masakhane corpus. Advanced Transformer and RNN architectures must be integrated, with a target of achieving a 15% improvement in BLEU score compared to the current state of the art, while accommodating limited-resource language support.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 1.15\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"resource efficiency\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Masakhane corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Swahili sentences\", \"English translations\"],\n            \"specification\": {\n                \"language pairs\": [\"Swahili-English\"],\n                \"corpus size\": null,\n                \"multilingual support\": \"limited\"\n            },\n            \"description\": \"A corpus focusing on Swahili-English translations for improving machine translation performance.\",\n            \"preprocessing\": [\n                \"data cleaning\",\n                \"tokenizer adaptation\"\n            ],\n            \"augmentation\": [\n                \"back-translation\"\n            ],\n            \"visualization\": [\n                \"alignment visualization\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": [\"Transformer\", \"RNN\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": {\n                    \"layers\": [\"Transformer blocks\", \"RNN layers\"],\n                    \"attention mechanism\": \"transformer\"\n                },\n                \"resource usage\": {\n                    \"parameters\": null,\n                    \"computation\": null\n                }\n            },\n            \"description\": \"State-of-the-art machine translation model using advanced Transformer and RNN architectures for Swahili-English translations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Limited-resource language support strategies\",\n        \"Transformer-RNN integration techniques\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual embeddings\",\n            \"cross-lingual knowledge transfer\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"efficient transformer inference\"\n        ],\n        \"demonstration\": \"public leaderboard performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's requirements for a machine translation system, incorporating advanced architectures, a targeted increase in BLEU, and support for limited-resource languages. The Masakhane corpus and integration of Transformer and RNN models are appropriately detailed, as are the performance and complexity metrics. However, the exact specifications and models are not explicitly defined in this structure, as the user has not provided those details.\",\n        \"score\": 0.85\n    }\n}"}
{"task": "image classification", "instruction": "As a machine learning project manager, you've been tasked with classifying a diverse set of feline images into their respective subspecies. Develop a comprehensive strategy, including a suggested deep learning architecture and pre-processing pipeline, to build an ensemble model that can handle complex features and subtle variations in cat breeds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"animal taxonomy\",\n        \"description\": \"Classify diverse feline images into their respective subspecies with a focus on complex features and subtle breed variations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"macro F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"subspecies\"],\n            \"specification\": {\n                \"size\": null,\n                \"depth\": null,\n                \"resolution\": null,\n                \"number of samples\": null\n            },\n            \"description\": \"A diverse collection of feline images for training and evaluation\",\n            \"preprocessing\": [\n                \"data augmentation\",\n                \"image resizing\"\n            ],\n            \"augmentation\": [\n                \"rotation\",\n                \"flipping\",\n                \"color jittering\"\n            ],\n            \"visualization\": [\n                \"class activation maps\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Ensemble Model\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": [\"convolutional layers\", \"attention mechanisms\", \"transfer learning\"],\n                \"ensemble_size\": null,\n                \"backbone models\": null\n            },\n            \"description\": \"A deep learning ensemble model using convolutional neural networks, with emphasis on handling complex features and breed variations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning from existing cat breeds datasets like ImageNet could be beneficial.\",\n        \"Use of a large ensemble to compensate for class imbalance and improve overall performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"extracting texture and color features\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"efficient inference libraries (e.g., TensorFlow or PyTorch)\"\n        ],\n        \"demonstration\": \"Confidence maps and breed confusion matrices for user evaluation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction suggests a clear task of developing a deep learning ensemble model for image classification, with emphasis on complex features and breed variations. Performance metrics are not explicitly stated, but a high-expertise user implies the need for advanced techniques. Specific numbers are not provided, allowing for optimization during the process. Model architecture and preprocessing steps align with the computer vision and image classification area.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "Develop a specialized node classification model for the NELL-995 dataset, employing a state-of-the-art Hyperedge Convolutional Graph Neural Network (HCGNN) architecture. Ensure the model handles entity disambiguation and noisy triplets effectively, with a target accuracy above 90%. Perform an extensive hyperparameter sensitivity analysis and generate a comprehensive report on performance variations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"knowledge graphs\",\n        \"description\": \"Develop a specialized node classification model for the NELL-995 dataset using Hyperedge Convolutional Graph Neural Network (HCGNN) to handle entity disambiguation and noisy triplets. Aim for a target accuracy of over 90%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"NELL-995\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"node class\"],\n            \"specification\": null,\n            \"description\": \"A knowledge graph dataset containing noisy triplets and entity disambiguation challenges.\",\n            \"preprocessing\": [\"entity disambiguation\", \"triplet cleaning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HCGNN\",\n            \"family\": \"Hyperedge Convolutional Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"state-of-the-art HCGNN\"\n            },\n            \"description\": \"A node classification model employing HCGNN to tackle entity disambiguation and noisy triplets in NELL-995 dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"NELL-995 is a dataset derived from NELL, which may contain errors, and handling these challenges is crucial.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"performance variations due to hyperparameter tuning and sensitivity analysis\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response represents the user's intent to build a node classification model using the HCGNN architecture for the NELL-995 dataset. The model's performance is aimed at 90% accuracy, and the need for entity disambiguation and noisy triplet handling is addressed. The user's expertise level suggests a more advanced understanding of the task. Hyperparameter sensitivity analysis and performance reports are part of the model development requirements. The provided details are comprehensive and cover the key aspects of the user's instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "For a cutting-edge HR analytics platform, develop a sophisticated link prediction model that not only evaluates historical team success ratings but also considers real-time project compatibility, individual skill diversity, and evolving job roles, ensuring optimal group dynamics for high-stake, cross-functional initiatives.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"human resources analytics\",\n        \"description\": \"Develop a link prediction model for an HR analytics platform, taking into account historical team success ratings, real-time project compatibility, individual skill diversity, and evolving job roles to optimize group dynamics for high-stakes, cross-functional initiatives.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory Size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"HR graph data (team relationships, skill data, project data)\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"link prediction scores\"],\n            \"specification\": {\n                \"nodes\": {\n                    \"success ratings\": {},\n                    \"project compatibility\": {},\n                    \"skill diversity\": {},\n                    \"job roles\": {}\n                },\n                \"edges\": {\n                    \"team success\": {},\n                    \"project collaboration\": {}\n                }\n            },\n            \"description\": \"A graph dataset including historical team data, project data, skill data, and job role evolution for link prediction.\",\n            \"preprocessing\": [\n                \"Normalize data\",\n                \"Handle missing values\"\n            ],\n            \"augmentation\": [\n                \"Graph Sampling\"\n            ],\n            \"visualization\": [\n                \"Node feature distribution\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Sophisticated Link Prediction Model\",\n            \"family\": \"Graph Neural Networks (GNN)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"model_layers\": {},\n                    \"graph_attention_layers\": {}\n                },\n                \"hyperparameters\": {\n                    \"learning_rate\": null,\n                    \"batch_size\": null\n                }\n            },\n            \"description\": \"A graph-based model with attention mechanisms for link prediction in HR analytics, considering historical team dynamics and real-time factors.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider state-of-the-art GNN algorithms like GraphSAGE, GAT, or Temporal GNNs for evolving job roles.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Custom features from skill diversity and job role evolution\",\n            \"Real-time updates on project compatibility\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Online inference with optimized batching\"],\n        \"demonstration\": \"Demonstrate AUC-ROC and accuracy on a test set\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's request for a complex link prediction model in an HR analytics context, addressing various factors and performance metrics. The level of expertise suggests a technical understanding of the requirements. Some specific model details are left flexible to accommodate various advanced GNN options. The missing values in complexity metrics highlight the need for an optimized model without strict constraints.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a senior project manager for an advanced imaging project, your team needs to develop a high-resolution galaxy and star classification system using a challenging custom dataset sourced from obscure NASA archives. In addition to choosing a cutting-edge model (preferably with state-of-the-art transfer learning), ensure interpretability and address the complexities of classifying celestial objects with varying textures under low-light conditions, while maintaining minimal false positives.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"astronomy\",\n        \"description\": \"Develop a high-resolution galaxy and star classification system using a custom dataset sourced from obscure NASA archives, incorporating state-of-the-art transfer learning techniques. Emphasize interpretability and address challenges of classifying celestial objects with varying textures under low-light conditions, with a focus on minimizing false positives.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"false positive rate\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"model explainability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"custom NASA archive dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"galaxies\", \"stars\"],\n            \"specification\": {\n                \"size\": null,\n                \"image_size\": null,\n                \"low_light_conditions\": true\n            },\n            \"description\": \"A challenging dataset containing high-resolution images of galaxies and stars sourced from obscure NASA archives, with varying textures and low-light conditions.\",\n            \"preprocessing\": [\n                \"image augmentation (low light, varying texture)\"\n            ],\n            \"augmentation\": [\"random brightness/contrast\", \"noise addition\"],\n            \"visualization\": [\"image sample gallery\", \"feature map analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art transfer learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": null,\n                \"transfer_learning_framework\": null,\n                \"interpretability_tools\": true\n            },\n            \"description\": \"A cutting-edge deep learning model with transfer learning capabilities, designed for high-resolution galaxy and star classification in challenging conditions.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Low-light data handling techniques for astronomy applications\",\n        \"Interpretability methods for complex classification models\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction from transfer learning models\",\n            \"light curve analysis\"\n        ],\n        \"target_device\": \"cloud-based or specialized astronomical processing hardware\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"GPU-accelerated\", \"optimized for astronomical datasets\"],\n        \"demonstration\": \"predictions on unseen images with interpretability layer\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON has been parsed to reflect a build task for a complex imaging project focused on galaxy and star classification. The user's high expertise level justifies the requirement for cutting-edge transfer learning. The task addresses challenges such as interpretability, low-light classification, and false positive reduction. Performance metrics are not specific to avoid misinterpretation but can be filled in with actual values during project execution. The dataset, model, and knowledge sections incorporate the key details based on the user's description and the nature of the problem domain.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "machine translation", "instruction": "For an upcoming exclusive project, we aim to revolutionize machine translation by tackling the under-explored domain of ancient Sanskrit and its lesser-known dialects paired with a lesser-resourced African language, such as Tswana. The team must design a novel transformer architecture that not only accurately renders complex philosophical concepts with 99% precision but also successfully captures the nuances of oral storytelling traditions found in these ancient tongues. Additionally, the model must exhibit adaptability, seamlessly translating between these esoteric languages and everyday English, maintaining a user-friendly interface for non-experts.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics and philosophy\",\n        \"description\": \"Design a novel transformer architecture for translating ancient Sanskrit (including its lesser-known dialects) and Tswana into English, focusing on accuracy for complex philosophical concepts (99% precision), capturing oral storytelling nuances, and maintaining a user-friendly interface.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.99\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"adaptability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ancient Sanskrit and Tswana\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"languages\": [\"Sanskrit (ancient and dialects)\", \"Tswana\", \"English\"],\n                \"domain_content\": [\"philosophical concepts\", \"oral storytelling traditions\"]\n            },\n            \"description\": \"A multilingual dataset containing complex philosophical texts and oral storytelling content in Sanskrit and Tswana, paired with English translations.\",\n            \"preprocessing\": [\n                \"Normalize scripts for input to the transformer model\",\n                \"Annotate philosophical concepts for accuracy evaluation\"\n            ],\n            \"augmentation\": [\n                \"Synthesize sentences to diversify input and capture context variations\"\n            ],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Ancient Language Translator\",\n            \"family\": \"Transformer-based\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"custom transformer designed for ancient languages and storytelling\",\n                \"target languages\": [\"English\", \"everyday language\"]\n            },\n            \"description\": \"A highly specialized transformer model for translating ancient Sanskrit and Tswana into English, emphasizing precision and storytelling nuances.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Cultural context and linguistic intricacies play a crucial role in accurate translation.\",\n        \"Collaboration with linguists and philosophy experts may be required.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Sentence segmentation\", \"Contextual embeddings\"],\n        \"target_device\": \"hybrid (cloud for training, mobile for users)\",\n        \"deployment_endpoint\": \"user-friendly API\",\n        \"inference_engine\": [\"GPU-accelerated inference\"],\n        \"demonstration\": \"Interactive demos showcasing translation examples and accuracy.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided information covers the user's request to build a transformer model for the specified task and domain. It details accuracy requirements, adaptations, and interfaces. The high expertise level suggests the user is familiar with the technicalities. Data preprocessing and model specificity are appropriately addressed. Missing elements can be filled based on further collaboration or clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "Create an advanced image classifier using the Kaggle Early Retinopathy Detection Challenge dataset, which demands the deployment of a custom VGG19 architecture tailored for multi-classification of five distinct retinopathy stages (0-4). The model must surpass a minimum accuracy of 92% while maintaining a precision of >95% to reduce false negatives. Focus on optimizing the model for deployment on resource-constrained devices in ophthalmology clinics, ensuring real-time performance with minimal latency for critical diagnosis.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"medical diagnosis\",\n        \"description\": \"Create an advanced image classifier for the Kaggle Early Retinopathy Detection Challenge dataset, using a custom VGG19 architecture tailored for multi-class classification of five distinct retinopathy stages (0-4). The model should achieve a minimum accuracy of 92% and a precision >95% to minimize false negatives.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.92\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"real-time\",\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Kaggle Early Retinopathy Detection Challenge dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"retinopathy stage\"],\n            \"specification\": {\n                \"dataset_size\": \"training and test split provided by the challenge\",\n                \"image_shape\": \"Adapting to the VGG19 architecture\"\n            },\n            \"description\": \"A dataset for early retinopathy detection, consisting of medical images.\",\n            \"preprocessing\": [\n                \"Resizing to VGG19 input size\",\n                \"Data augmentation (likely required for performance)\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom VGG19 for Retinopathy Classification\",\n            \"family\": \"VGG19\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Modified for multi-classification with 5 stages\",\n                \"optimizer\": \"Adapting to resource-constrained devices\",\n                \"accuracy_target\": 0.92,\n                \"precision_target\": 0.95\n            },\n            \"description\": \"A custom VGG19 designed for real-time performance with reduced latency in ophthalmology clinics.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"retinopathy_impact\": \"Critical for timely and accurate diagnosis in ophthalmology\",\n            \"device_challenges\": \"Addressing resource constraints to ensure real-time deployment\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Adaptive feature selection for resource efficiency\"],\n        \"target_device\": \"resource-constrained devices (e.g., low-end smartphones, embedded devices in clinics)\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Lightweight deep learning frameworks optimized for on-device inference\"],\n        \"demonstration\": \"Demonstration of real-time performance and accuracy in low-resource environments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects the user's instruction for a custom VGG19 architecture tailored for the Early Retinopathy Detection Challenge. Performance metrics and precision requirements have been met, while the focus on resource-constrained devices and real-time latency aligns with the user's goal. The user's high level of expertise suggests their understanding of the necessary adjustments.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "node classification", "instruction": "The project manager is tasked with developing a sophisticated node classification model using a state-of-the-art Hierarchical Graph Neural Network (HGNN) on the highly imbalanced and temporally evolving Coauthorship Science dataset. The model must not only distinguish authors by research domains but also consider the impact of collaboration network evolution over time and the integration of missing metadata for enhanced predictive accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Develop a sophisticated node classification model using a Hierarchical Graph Neural Network (HGNN) for the Coauthorship Science dataset. The model must handle imbalanced data and temporal evolution of the collaboration network, as well as integrate missing metadata for improved predictive accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 Score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coauthorship Science dataset\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [\"author domains\"],\n            \"specification\": {\n                \"imbalance_ratio\": null,\n                \"temporal_dimensions\": null\n            },\n            \"description\": \"A highly imbalanced and temporally evolving dataset that captures collaboration among authors in science research, with the need for handling missing metadata.\",\n            \"preprocessing\": [\"data balancing, temporal feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hierarchical Graph Neural Network (HGNN)\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"time-aware\": true\n            },\n            \"description\": \"A sophisticated model for node classification in a dynamic collaboration network that incorporates HGNN and handles imbalanced data with missing metadata.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art models for imbalanced data\",\n        \"Techniques for handling missing metadata in GNNs\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"node embeddings, temporal fusion\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"on-prem, cloud-based\"],\n        \"demonstration\": \"showcase model performance on specific slices of the data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The user's instruction is well-defined, indicating a build intent with a high-level requirement. The project covers advanced techniques, making the expertise level high. Performance metrics are not specified but expected to be optimized for accuracy and efficiency in dealing with an imbalanced and dynamic dataset. The provided JSON structure reflects the key aspects of the model and dataset based on the user's requirements.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "In the context of a specialized biodiversity research initiative, design a state-of-the-art Vision Transformer (ViT-16) for classifying high-resolution Landsat 8 satellite imagery into precise land cover types, such as tropical rainforest, coral reef, arid desert, and urban sprawl. The model should exhibit exceptional generalization ability, surpassing 96% multi-class F1-score on a challenging, region-diverse test set. Emphasize on efficient deployment on AWS Graviton and provide in-depth analysis of inference speed, memory footprint, and environmental impact for a sustainable monitoring system.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"biodiversity research\",\n        \"description\": \"Design a state-of-the-art Vision Transformer (ViT-16) for classifying high-resolution Landsat 8 satellite imagery into land cover types like tropical rainforest, coral reef, arid desert, and urban sprawl, aiming for a multi-class F1-score of at least 96% on a diverse test set.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"multi-class F1-score\",\n                \"value\": 0.96\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference speed\",\n                \"value\": null,\n                \"unit\": \"milliseconds/image\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"environmental impact\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Landsat 8 satellite imagery\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"land cover types\"],\n            \"specification\": null,\n            \"description\": \"High-resolution satellite images for land cover classification.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ViT-16\",\n            \"family\": \"Vision Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A specialized deep learning model for land cover classification in biodiversity research.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Leverage AWS Graviton for efficient deployment\"\n        },\n        {\n            \"text\": \"Focus on sustainability by analyzing environmental impact\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"AWS Graviton\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"AWS Inference APIs\"],\n        \"demonstration\": \"Explain how to create a monitoring system with performance metrics and sustainability indicators\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the main objectives of the user's instruction, including the design of a ViT-16 model, target performance, and emphasis on efficiency and environmental impact. The provided expertise level indicates a deep understanding of the requirements. Some fields (e.g., memory footprint, inference speed, and precise values for metrics) are left unspecified for optimization as per the user's focus.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Develop a scalable and high-precision energy consumption forecasting system using an advanced, parallelized LSTM model. The dataset spans multiple years and includes various meteorological factors. The model must handle real-time updates with minimal lag, ensuring sub-100 millisecond prediction intervals for the next 72 hours while demonstrating exceptional accuracy and interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy management\",\n        \"description\": \"Develop a scalable, high-accuracy energy consumption forecasting system using a parallelized LSTM model. The dataset includes multi-year data and meteorological factors for real-time forecasting with sub-100ms intervals for the next 72 hours, prioritizing interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Error\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": {\"unit\": \"milliseconds\", \"value\": \"<100\"}\n            },\n            {\n                \"name\": \"Prediction Lag\",\n                \"value\": {\"unit\": \"milliseconds\", \"value\": \"<100\"}\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Energy consumption with meteorological data\",\n            \"modality\": [\"time series\", \"tabular\"],\n            \"target_variables\": [\"energy consumption\"],\n            \"specification\": null,\n            \"description\": \"Dataset containing multi-year energy consumption data and meteorological factors for training the parallelized LSTM model.\",\n            \"preprocessing\": [\"data cleaning\", \"feature engineering\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Parallelized LSTM\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": \"parallelized\",\n                \"time-series model\": true\n            },\n            \"description\": \"Advanced LSTM model designed for scalable and interpretable energy consumption forecasting with real-time updates.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"name\": \"Parallel Computing Techniques for LSTM\",\n            \"description\": \"Utilize distributed computing to enhance model performance and scalability.\"\n        },\n        {\n            \"name\": \"Model Explainability\",\n            \"description\": \"Include methods for interpretable AI in the forecasting system.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time updates\"],\n        \"target_device\": \"suitable for real-time environments\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"Distributed computing framework\"],\n        \"demonstration\": \"Model performance on recent data samples and interpretability features.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's request for a scalable and high-accuracy energy consumption forecasting system, using a parallelized LSTM model. Meteorological data is incorporated, and the system is designed for real-time updates with minimal lag. The performance metrics, including accuracy, precision, and MAE, are not directly provided but can be inferred to be a priority. Specific deployment details are left open for clarification based on the user's expertise level.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "In light of our ambitious expansion plan, we aim to develop a highly specialized BERT-based Q&A system, using RoBERTa's enhanced version, to excel in complex SQuAD 2.0 inquiries with an advanced precision target of 95%. Although latency of 1.5 seconds is acceptable for peak performance, the model must demonstrate state-of-the-art efficiency while handling multi-layered and domain-specific questions from diverse sources.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"business/industry expansion\",\n        \"description\": \"Develop a highly specialized BERT-based Q&A system, using RoBERTa's enhanced version, to excel in complex SQuAD 2.0 inquiries with a precision target of 95%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": 1.5,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SQuAD 2.0\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"questions, answers\"],\n            \"specification\": {\n                \"type\": \"large-scale\",\n                \"complexity\": \"multi-layered and domain-specific\"\n            },\n            \"description\": \"A dataset for question answering targeting complex inquiries from diverse sources.\",\n            \"preprocessing\": [\"BERT fine-tuning\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"BERT (RoBERTa enhanced)\",\n            \"family\": \"Transformer-based models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"efficiency\": \"state-of-the-art\"\n            },\n            \"description\": \"An advanced BERT model fine-tuned for complex question answering tasks with high precision and low latency.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Domain-specific expertise and latency constraints\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"BERT training and fine-tuning\"],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"multi-source Q&A API\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"Interactive and custom report on model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent for model development, the expert level, and the requirements for a complex Q&A system. It covers the NLP area (BERT-based) and the downstream task (SQuAD 2.0). Performance metrics with a precision target and acceptable latency are included, along with state-of-the-art efficiency. The dataset description, preprocessing, and source align with the given instruction. Specific details such as the model's fine-tuning and inference optimization, as well as the demonstration, are included based on the information provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "Develop a state-of-the-art real-time object detection model, specifically a YOLOv5 variant, for the \"Adaptive Market Analytics Dataset\", which necessitates accurate recognition of 100 distinct product categories, maintaining a precision and recall above 95% to ensure minimal false positives and negatives. The model should be optimized for edge computing, delivering inventory updates within 1 second per high-resolution shelf image while ensuring robustness against diverse lighting and occlusion conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"market analysis\",\n        \"description\": \"The goal is to develop a state-of-the-art real-time object detection model, specifically a YOLOv5 variant, for the Adaptive Market Analytics Dataset. The model must accurately identify 100 distinct product categories with precision and recall above 95% to minimize false positives and negatives. It should be optimized for edge computing, achieving inventory updates within 1 second for high-resolution shelf images and be robust against diverse lighting and occlusion conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time per image\",\n                \"value\": 1.0,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Adaptive Market Analytics Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"100 distinct product categories\"],\n            \"specification\": {\n                \"image_resolution\": \"high\"\n            },\n            \"description\": \"A dataset with high-resolution shelf images containing diverse products in various lighting and occlusion conditions.\",\n            \"preprocessing\": [\"lighting normalization\", \"image resizing\"],\n            \"augmentation\": [\"random occlusion\"],\n            \"visualization\": [\"image samples with annotations\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"State-of-the-art YOLOv5 variant with at least 100 object detection classes\",\n                \"optimization\": \"optimized for edge computing\"\n            },\n            \"description\": \"A YOLOv5 model optimized for real-time object detection, especially in the context of market analytics with high precision and recall for product categories.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Edge computing friendly design for low-latency and resource-constrained environments.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"lightweight model architecture\",\n            \"quantization\"\n        ],\n        \"target_device\": \"edge device\",\n        \"deployment_endpoint\": \"A dedicated edge computing platform\",\n        \"inference_engine\": [\"OpenVINO\", \"TensorFlow Lite\"],\n        \"demonstration\": \"Generated model inference results on representative images with high-resolution shelf scenes.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers the main requirements from the user's instruction, including the YOLOv5 model development, performance metrics, and optimization for edge computing. However, it assumes the model's architecture details and assumes the user has prior knowledge of edge-friendly lightweight optimizations. The choice of inference engines and missing values in complexity metrics (like memory footprint) are left for potential user clarification.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "The project's goal is to design a high-performance smart city system, necessitating the implementation of advanced M2Det-based object detection. We require the model to analyze diverse 'PKLot' dataset scenarios, demonstrating multi-level feature fusion for enhanced context recognition. The system must demonstrate robustness in challenging lighting and weather conditions, maintain a stringent 95% detection accuracy, and ensure real-time operation with responses within 150 milliseconds, while also optimizing for energy efficiency. Provide detailed monthly progress reports and conduct extensive on-field testing in various locations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city\",\n        \"description\": \"Design a high-performance smart city system using M2Det-based object detection, targeting multi-level feature fusion for enhanced context recognition in PkLot dataset scenarios, with a focus on robustness, 95% detection accuracy, real-time operation (150 ms response time), and energy efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Detection accuracy\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"Inference time\",\n                \"value\": 150\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Energy efficiency\",\n                \"value\": null,\n                \"unit\": \"Percentage\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PkLot dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"data_distribution\": \"diverse scenarios\",\n                \"challenging_conditions\": \"challenging lighting and weather conditions\"\n            },\n            \"description\": \"A dataset specifically for M2Det-based object detection in smart city scenarios for PkLot analysis.\",\n            \"preprocessing\": [\"multi-level feature fusion\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"M2Det\",\n            \"family\": \"neural networks\",\n            \"type\": \"object detection\",\n            \"specification\": {\n                \"architecture\": \"advanced for M2Det\",\n                \"feature_fusion\": \"multi-level\"\n            },\n            \"description\": \"Advanced M2Det model with multi-level feature fusion for smart city object detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Contextual understanding through feature fusion, on-field testing in diverse locations, and real-world performance\",\n        \"Monthly progress reports for system development and performance tracking\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"on-field testing\"],\n        \"target_device\": [\"embedded systems for real-time operation\"],\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for energy efficiency\"],\n        \"demonstration\": \"extensive, highlighting system's performance under various conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON format captures the user's intent to build a model, their expertise level, and the specifics for a smart city system using M2Det for object detection with performance requirements and challenges. Datasets, models, feature engineering, and system deployment details have been appropriately interpreted. The confidence score is high because the information is generally complete, with the remaining open-ended points (like model accuracy, specific deployment, and metric units) reflecting common scenarios where more details may be available during implementation.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "Incorporate a real-time anomaly detection component with a focus on detecting sudden shifts in the data streams, ensuring scalability for large datasets with at least 1 million observations. Additionally, prioritize efficiency by optimizing for speed-accuracy tradeoff with lightweight transformers, maintaining an accuracy benchmark of 75% or higher.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"real-time monitoring\",\n        \"description\": \"Include a real-time anomaly detection component that focuses on detecting sudden shifts in data streams, ensuring scalability for large datasets with at least 1 million observations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": {\n                    \"min_value\": 0.75,\n                    \"max_value\": 1.0\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": {\n                    \"unit\": \"milliseconds\",\n                    \"optimization_target\": \"low\"\n                }\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": {\n                    \"unit\": \"MB\",\n                    \"optimization_target\": \"minimal\"\n                }\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Large real-time data streams\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"sudden shifts\"],\n            \"specification\": {\n                \"minimum_observations\": 1000000,\n                \"data_structure\": \"streaming\"\n            },\n            \"description\": \"A large, scalable dataset containing real-time data streams with the focus on anomaly detection for sudden shifts.\",\n            \"preprocessing\": [\n                \"streaming data handling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Real-time Anomaly Detection with Lightweight Transformers\",\n            \"family\": \"Transformers\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"small\",\n                \"architecture\": \"lightweight\",\n                \"speed_accuracy_tradeoff\": true\n            },\n            \"description\": \"A lightweight transformer-based model for real-time anomaly detection, optimized for large datasets with a focus on speed-accuracy tradeoff and maintaining accuracy at 75% or higher.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Utilize online learning algorithms to adapt to streaming data and update model efficiently.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"incremental feature extraction\"],\n        \"target_device\": \"edge computing\",\n        \"deployment_endpoint\": \"stream processing service\",\n        \"inference_engine\": [\"real-time inference\"],\n        \"demonstration\": \"Live data stream demonstrating anomaly detection and updates in near real-time.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the user's instruction, which calls for a scalable real-time anomaly detection system with a focus on large datasets and efficiency. The performance metrics address the accuracy requirement, and complexity metrics emphasize inference time and memory optimization for speed. The dataset, model, and service sections are detailed according to the needs of real-time anomaly detection. Light-weight transformers, streaming data handling, online learning, and edge deployment are all mentioned as key components. The confidence score is high because the response covers the primary requirements and reflects the user's high level of expertise.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "time-series anomaly detection", "instruction": "\"Investigate the development of a real-time, deep learning-based anomaly detection system for 'AviationMaintenancePrognostics', using long short-term memory (LSTM) networks. The task is to predict potential machinery failures in aircraft engines based on complex sensor data, demanding exceptional precision to prevent costly downtime and ensure flight safety. Additionally, the system should adapt to varying environmental conditions and incorporate a proactive maintenance strategy, while maintaining a low false alarm rate to minimize crew workload and passenger disruption.\"", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"aviation maintenance\",\n        \"description\": \"Investigate the development of a real-time, deep learning-based anomaly detection system for aviation maintenance prognostics using LSTM networks. The aim is to predict machinery failures in aircraft engines based on complex sensor data with high precision, adaptability to varying environmental conditions, and a proactive maintenance strategy while minimizing false alarms.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"False Alarm Rate (FAR)\",\n                \"value\": 0.05\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"Memory Footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AviationMaintenancePrognostics Sensor Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"machinery failures\", \"environmental conditions\"],\n            \"specification\": null,\n            \"description\": \"Sensor data capturing complex variables from aircraft engines to predict machinery failures under various environmental conditions.\",\n            \"preprocessing\": [\n                \"Data normalization\",\n                \"Feature scaling\"\n            ],\n            \"augmentation\": [\n                \"Time series data augmentation\"\n            ],\n            \"visualization\": [\n                \"Anomaly detection results and performance visualization\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Aviation LSTM Anomaly Detection System\",\n            \"family\": \"LSTM\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A real-time deep learning model based on LSTM networks designed for aviation maintenance anomaly detection and machinery failure prediction.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Environmental data fusion and consideration of different operating conditions\",\n        \"Utilizing transfer learning to adapt to varying conditions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            {\n                \"name\": \"Windowing and Feature Extraction\"\n            }\n        ],\n        \"target_device\": \"embedded devices for on-board systems\",\n        \"deployment_endpoint\": \"Airline maintenance operations center\",\n        \"inference_engine\": [\n            \"Custom optimized inference algorithms\"\n        ],\n        \"demonstration\": \"Live demonstration during flight operations and maintenance routine\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the user's request for a real-time anomaly detection system using LSTM for aviation maintenance, emphasizing precision, adaptability, and minimizing false alarms. Performance metrics are set with specific requirements (e.g., low false alarm rate), and preprocessing, augmentation, and deployment strategies are included to address the complex nature of the data and task. The system's intent to be built aligns well with the provided instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "link prediction", "instruction": "Can the machine learning team develop a sophisticated link prediction algorithm, specifically tailored for the Citeseer dataset, with limited computational resources, maintaining an accuracy of at least 90% while optimizing for efficiency?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"link prediction\",\n        \"application_domain\": \"academic research\",\n        \"description\": \"Develop a sophisticated link prediction algorithm tailored for the Citeseer dataset, considering computational limitations and aiming for an accuracy of at least 90% while optimizing for efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Citeseer\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"link prediction\"],\n            \"specification\": null,\n            \"description\": \"A dataset specifically used for link prediction, part of the academic citation network Citeseer.\",\n            \"preprocessing\": [\n                \"reduce noise\",\n                \"normalize feature vectors\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"efficient link prediction algorithm\",\n            \"type\": \"graph neural networks\",\n            \"specification\": {\n                \"parameters\": {\n                    \"model architecture\": \"lightweight\",\n                    \"optimization techniques\": [\"compression\", \"pruning\"]\n                }\n            },\n            \"description\": \"A sophisticated yet resource-efficient graph neural network model for link prediction in the Citeseer dataset.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using node embedding techniques and sparse matrix representations to handle large graph data.\",\n        \"Explore efficient algorithms for training and inference, such as mini-batch training and approximate inference.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"dimensionality reduction\",\n            \"feature extraction\"\n        ],\n        \"target_device\": \"optimized for edge or low-resource devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"lightweight inference libraries\"],\n        \"demonstration\": \"Demonstrate real-time performance and resource usage\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's request for a link prediction model with a focus on computational efficiency, maintaining accuracy above 90%. The dataset is specified as Citeseer, and preprocessing and model specifications are tailored to these constraints. However, due to the lack of details on exact algorithms and model architectures, the model description has been left vague. The confidence score is high given the information provided and the clear objectives.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "In light of the expanding scope of our diverse portfolio, develop a cutting-edge tabular regression model for a cross-domain analysis. Can we not only forecast the combined budget and duration of upcoming creative endeavors, but also predict the potential return on investment and customer satisfaction scores, all while considering external market factors and project risk metrics derived from historical data?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"cross-domain, creative endeavors\",\n        \"description\": \"Develop a cutting-edge tabular regression model to forecast combined budget and duration, predict potential return on investment, and customer satisfaction scores for upcoming projects, considering external market factors and historical risk metrics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Combined budget prediction accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Duration forecasting accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"ROI prediction\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Customer satisfaction prediction\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Combined project data (historical)\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\n                \"Budget\",\n                \"Duration\",\n                \"Return on Investment\",\n                \"Customer Satisfaction\",\n                \"Market Factors\",\n                \"Risk Metrics\"\n            ],\n            \"specification\": null,\n            \"description\": \"A dataset containing historical data for diverse creative endeavors, including budget, duration, and various performance indicators to inform the model development.\",\n            \"preprocessing\": [\n                \"Data cleaning\",\n                \"Feature scaling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Feature importance\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced tabular regression model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"optimizer\": \"customizable\",\n                \"hyperparameters\": \"optimized for multi-target regression\"\n            },\n            \"description\": \"A cutting-edge model built for complex, multi-target regression with consideration of external factors and risk management.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Expanding portfolio scope and understanding of cross-domain correlations can enhance model performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Time series analysis for market factors\",\n            \"Feature selection for project risk metrics\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"TensorFlow Serving\", \"Docker\"],\n        \"demonstration\": \"Interactive dashboard for visualization of results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response reflects a clear understanding of the user's request for a high-expertise model, focusing on the cross-domain analysis and multiple targets. The performance metrics capture the requested tasks, and the model specifications cater to advanced regression. The dataset includes preprocessing and augmentation steps suggested by the complex nature of the problem. The confidence score is high, as the information provided should support a successful model development process based on the given instructions.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image segmentation", "instruction": "As a project manager for the advanced urban scene analysis team, we need to integrate image segmentation on the Cityscapes dataset into an autonomous vehicle project. mandate real-time performance with precise car detection, low-latency decision-making, and optimization for edge computing hardware.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"autonomous vehicles & urban scene analysis\",\n        \"description\": \"Integrate image segmentation, specifically on the Cityscapes dataset, into an autonomous vehicle project focusing on real-time performance, precise car detection, low-latency decision-making, and optimization for edge computing hardware.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy (car detection)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"car segmentation masks\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"number_of_classes\": null,\n                \"scene_types\": [\"urban scenes\"]\n            },\n            \"description\": \"A high-quality dataset for semantic understanding in urban street scenes, specifically designed for autonomous vehicles.\",\n            \"preprocessing\": [\n                \"real-time augmentation\",\n                \"sizing for edge hardware\"\n            ],\n            \"augmentation\": [\n                \"online data augmentation\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Real-time Image Segmentation\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"MobileNet-based or efficient for edge computing\",\n                \"faster_inference\": true\n            },\n            \"description\": \"A real-time, optimized model for image segmentation, designed for car detection and low-latency decision-making in an autonomous vehicle project, targeting edge computing hardware.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"transfer learning\",\n            \"domain adaptation\"\n        ],\n        \"target_device\": \"edge computing hardware\",\n        \"deployment_endpoint\": \"on-device or custom edge server\",\n        \"inference_engine\": [\"OpenVINO\", \"TensorRT\"],\n        \"demonstration\": \"live demo with latency measurements\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent, problem requirements, and dataset specifics from the given instruction. Performance metrics are set to be real-time, accuracy for car detection, and latency. Complexity metrics focus on inference time and memory footprint, aligned with edge computing. The Cityscapes dataset is selected and the model family, preprocessing, and inference optimizations are specified. However, some metric values are left unspecified due to the absence of exact targets, such as the desired real-time performance rate or specific accuracy threshold.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "tabular classification", "instruction": "As a financial institution aiming for precision, we must develop a predictive model for large-scale loan default risk assessment. Instruct the ML team to design an algorithm using historical data, intricate credit scoring factors, and real-time monitoring signals, ensuring the system can handle complex applicant profiles and dynamically adapt to market trends.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"finance\",\n        \"description\": \"Develop a predictive model for large-scale loan default risk assessment using historical financial data, intricate credit scoring factors, and real-time monitoring signals. The model must handle complex applicant profiles and adapt dynamically to market trends.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"GB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Historical loan data\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"default risk\"],\n            \"specification\": {\n                \"rows\": null,\n                \"columns\": null,\n                \"data_types\": null,\n                \"timestamp_range\": null\n            },\n            \"description\": \"Data containing applicant profiles, financial history, and credit scoring factors.\",\n            \"preprocessing\": [\"handling missing values\", \"feature scaling\"],\n            \"augmentation\": [],\n            \"visualization\": [\"feature importance\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"algorithm\": \"deep learning\",\n                \"architecture\": \"custom\",\n                \"hyperparameters\": {}\n            },\n            \"description\": \"A model for loan default risk assessment with support for intricate credit factors and real-time adaptation.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"name\": \"Dynamic threshold adjustment\",\n            \"description\": \"Implement a system that can adjust prediction thresholds based on market trends.\"\n        },\n        {\n            \"name\": \"Feature selection\",\n            \"description\": \"Select credit scoring factors that have the most significant impact on default risk.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"custom risk assessment features\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU accelerated\"],\n        \"demonstration\": \"explainable AI results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the user's request for a complex loan default risk assessment model with a focus on precision and handling of intricate data. The model is specified as a neural network, and the user's high expertise level is taken into account. Some fields, such as specific model names and detailed performance metrics, are left undefined for further collaboration with the ML team. The provided information should be sufficient to initiate the project.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a multi-platform e-commerce platform, you're tasked with developing an automated system for real-time translation of product descriptions between Korean and English, catering to diverse user bases. You must evaluate advanced neural machine translation models like Transformer-XL and M-BART, considering factors like sentence context preservation, domain adaptation, and efficiency. Gather a large, high-quality parallel dataset and optimize for speed without compromising readability or cultural nuances.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Develop an automated system for real-time translation of product descriptions between Korean and English on a multi-platform e-commerce platform, focusing on preserving sentence context, domain adaptation, and optimizing for efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"sentence context preservation\",\n                \"value\": null\n            },\n            {\n                \"name\": \"domain adaptation accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"efficiency (inference time)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"parallel Korean-English product descriptions\",\n            \"modality\": [\"text\", \"text\"],\n            \"target_variables\": [\"translated text (Ko-En, En-Ko)\"],\n            \"specification\": {\n                \"size\": null,\n                \"linguistic diversity\": null\n            },\n            \"description\": \"A large, high-quality parallel dataset for training and evaluating real-time translation systems between Korean and English.\",\n            \"preprocessing\": [\"data cleaning, filtering, and balancing\"],\n            \"augmentation\": [\"contextual sentence generation\"],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": [\"Transformer-XL\", \"M-BART\"],\n            \"family\": [\"neural machine translation models\", \"transformer-based\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"contextual_relevance\": null,\n                \"domain_adaptation_techniques\": null,\n                \"parameters\": null\n            },\n            \"description\": \"Advanced neural machine translation models evaluating Transformer-XL and M-BART for context preservation, domain adaptation, and efficiency.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific embeddings\", \"context-aware sentence embeddings\"],\n        \"target_device\": \"multi-platform, supporting real-time translation\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for GPU/TPU\"],\n        \"demonstration\": \"example translations showcasing context preservation and cultural nuances\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response is based on the provided instruction, with a high expertise level for the user. The problem section covers the task of real-time translation, focusing on specific models, performance metrics, and efficiency. The dataset is described as large and parallel, with requirements for preprocessing, augmentation, and preconditions. The requested models are Transformer-XL and M-BART, and the service section includes optimizing for real-time performance across a multi-platform e-commerce context while preserving nuances. Some metric values are unspecified as they require specific numerical data not present in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "The wildlife conservation project demands a real-time, energy-efficient object detection system, utilizing the tailored EfficientDet model trained on the iWildCam alpha version with sub-species classification. It must excel in low-light scenarios and perform well on resource-constrained, solar-powered surveillance drones in remote African savannas to monitor endangered species with minimal latency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"wildlife conservation\",\n        \"description\": \"Develop a real-time, energy-efficient object detection system using the tailored EfficientDet model trained on the iWildCam alpha version. The system should have sub-species classification capabilities and be optimized for low-light scenarios, specifically designed for resource-constrained, solar-powered surveillance drones in remote African savannas to monitor endangered species with minimal latency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mAP@0.5\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency (FLOPs per image)\",\n                \"value\": null,\n                \"unit\": \"floating point operations per image\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"iWildCam alpha version\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"sub-species labels\"],\n            \"specification\": {\n                \"data format\": \"images with annotations\",\n                \"image resolution\": null,\n                \"number of classes\": null\n            },\n            \"description\": \"A dataset for object detection, specifically for wildlife, containing sub-species annotations trained on the EfficientDet model.\",\n            \"preprocessing\": [\"low-light image enhancement\"],\n            \"augmentation\": [\"solar lighting conditions\"],\n            \"visualization\": [\"infrared and thermal heatmaps\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientDet\",\n            \"family\": \"custom EfficientDet\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"tailored for low-light and resource constraints\",\n                \"efficient layers\": null,\n                \"specific FLOPs\": null\n            },\n            \"description\": \"An object detection model optimized for real-time performance, targeting sub-species classification in low-light environments and designed for constrained drone deployments.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Solar-powered drone technology and low-light image processing techniques.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"lighting and noise reduction algorithms\",\n            \"drone flight planning for optimal surveillance coverage\"\n        ],\n        \"target_device\": \"resource-constrained, solar-powered surveillance drone\",\n        \"deployment_endpoint\": \"remote African savanna monitoring network\",\n        \"inference_engine\": [\"optimized for real-time inference on embedded devices\"],\n        \"demonstration\": \"live video feed and annotated results from the drone\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided JSON captures the user's requirements for a real-time, energy-efficient object detection system. The area, downstream task, and application domains are identified, along with the target metrics and specific adjustments for low-light conditions and resource constraints. The model's custom EfficientDet adaptation is clear, as are the required source and target dataset details. The service section addresses deployment and drone operation aspects related to the conservation project. The high-level customization based on the instruction shows a high confidence score.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "Our mission is to develop a highly specialized text classifier for a niche collection of esoteric academic journals, focusing on deciphering intricate sub-disciplines within quantum gravity theory and topological quantum computing. We propose employing a state-of-the-art, Explainable Quantum-Inspired Attention Network (XQI-AN) architecture, which demands not only an impressive 92% Macro-F1 score for fine-grained categorization but also exhibits self-adaptation and error-driven pruning mechanisms to maintain a precision above 95% over time, all while operating seamlessly on limited computational resources. The classifier should also present a clear interpretability report, elucidating the model's decision-making process for expert validation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"esoteric academic journals\",\n        \"description\": \"Develop a highly specialized text classifier for quantum gravity theory and topological quantum computing, using the XQI-AN architecture. Target is 92% Macro-F1 score and at least 95% precision with self-adaptation and error-driven pruning. The classifier must operate on limited resources and provide interpretability reports.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Macro-F1 score\",\n                \"value\": 0.92\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Niche quantum gravity & topological quantum computing articles\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"category (quantum gravity theory, topological quantum computing)\"],\n            \"specification\": {\n                \"data_source\": \"academic journals\",\n                \"domain_relevance\": \"esoteric\",\n                \"data_dim\": {\n                    \"vocabulary_size\": null,\n                    \"word_length\": null,\n                    \"sentence_length\": null\n                }\n            },\n            \"description\": \"A dataset consisting of esoteric academic journal articles focused on quantum gravity theory and topological quantum computing.\",\n            \"preprocessing\": [\"text cleaning, tokenization, and embeddings\"],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"model interpretability and decision-making process\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"XQI-AN\",\n            \"family\": \"Quantum-Inspired Attention Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"self-adaptation\": \"enabled\",\n                \"error-driven pruning\": \"enabled\",\n                \"resource_consumption\": \"limited\"\n            },\n            \"description\": \"A state-of-the-art text classifier using the Explainable Quantum-Inspired Attention Network (XQI-AN) architecture.\"\n        }\n    ],\n    \"knowledge\": [\n        \"interpretability report for expert validation\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"handling esoteric language and domain-specific terms\"],\n        \"target_device\": \"limited computational resources\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"clear decision-making process illustrations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is constructed based on the user's detailed requirements for a specialized text classifier for quantum gravity and topological quantum computing. The user's high expertise is reflected in the selection of advanced architecture and requirements. Performance metrics such as macro-F1 score and precision are included, along with specific self-adaptation and pruning features. The dataset is designed to cater to esoteric content, while maintaining interpretability. Missing information, like the exact computational resource constraints, can be determined by further discussion with the user.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "text classification", "instruction": "You are now tackling a high-level text classification project, utilizing the diverse and unstructured WebMD Review dataset. Your objective is to discern the nuanced sentiment (positive with 1 and negative with 0) through multiple-step reasoning, considering aspects like context, sarcasm, and subtext. Ensure robust performance by fine-tuning a state-of-the-art language model and employing advanced feature extraction techniques for enhanced accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"A text classification project on the WebMD Review dataset aims to discern the nuanced sentiment (positive or negative) with a focus on context, sarcasm, and subtext through multiple-step reasoning. The goal is to achieve robust performance by fine-tuning a state-of-the-art language model and using advanced feature extraction techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Memory consumption\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"WebMD Review Dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"sentiment (1 for positive, 0 for negative)\"],\n            \"specification\": null,\n            \"description\": \"A large-scale, diverse and unstructured dataset from WebMD containing medical reviews for text sentiment analysis, considering sarcasm and subtext.\",\n            \"preprocessing\": [\n                \"Text cleaning, tokenization, and normalization\"\n            ],\n            \"augmentation\": [\n                \"Synthetic data generation via back-translation or other methods\"\n            ],\n            \"visualization\": [\n                \"BERT attention visualization\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Pre-trained language model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model\": {\n                    \"name\": \"BERT\",\n                    \"fine-tuned\": true\n                },\n                \"pretraining\": \"Large-scale\",\n                \"contextual_embedding\": true\n            },\n            \"description\": \"A fine-tuned state-of-the-art language model for high-level text classification, capable of handling nuances in context, sarcasm, and subtext.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Consider incorporating techniques like sentiment lexicons, contextual embeddings, and multimodal inputs to improve model understanding.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Sentiment lexicons\",\n            \"Contextual embeddings\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Sample review analysis and explanations of model decisions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately represents the user's instruction for a high-level text classification project with the WebMD dataset, emphasizing the need for fine-tuning, advanced feature extraction, and handling context and sarcasm. The performance metrics are left unspecified for optimality, and the specified preprocessing, augmentation, and feature engineering steps correspond to the requirements. Expertise level is set to 'high' due to the complexity of handling nuances in sentiment analysis. However, model details such as the specific language model type and performance scores are missing, as they would require further information.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Can you develop an advanced time-series forecasting model, specifically tailored for our transportation service, that predicts daily peak hours for different routes? The model should incorporate historical ridership data, weather patterns, and special events to enhance accuracy, providing real-time insights for effective staff deployment and operational efficiency optimization.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"transportation service\",\n        \"description\": \"Develop an advanced time-series forecasting model to predict daily peak hours for different routes, incorporating historical ridership data, weather patterns, and special events for improved accuracy and operational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RMSE\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Transportation Service Data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"daily peak hours\", \"ridership\", \"weather patterns\", \"special events\"],\n            \"specification\": {\n                \"length\": null,\n                \"sampling_rate\": null,\n                \"dimensions\": null\n            },\n            \"description\": \"A dataset containing historical ridership, weather data, and event information, used to train and predict peak hours for various routes.\",\n            \"preprocessing\": [\n                \"Data cleaning\",\n                \"Normalization\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Anomaly detection\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced Time-Series Model (e.g., LSTM, Prophet, ARIMA)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": null,\n                \"hyperparameters\": null\n            },\n            \"description\": \"An advanced machine learning model designed for time-series forecasting, specifically tailored for predicting daily peak hours in our transportation service context.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature extraction from multiple modalities\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Real-time data integration\"\n        ],\n        \"demonstration\": \"Daily dashboard for route-wise peak hour predictions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response parses the user's request for a time-series forecasting model that incorporates various data sources for peak hour predictions, performance metric expectations, and some complexity metrics. Expertise level is assumed to be high given the request for advanced modeling techniques. However, specific model structure, hyperparameters, inference time values, and deployment details are not provided, as those would require further clarification from the user.\",\n        \"score\": 0.9\n    }\n}\n"}
{"task": "image classification", "instruction": "As a project manager for an upcoming mobile application, we aim to develop a real-time, user-friendly dog breed recognition system using the Stanford Dogs Dataset. The client desires high accuracy, low latency, and a discussion on sophisticated deep learning architectures like convolutional neural networks (CNN) vs transfer learning methods (VGG, ResNet), considering the app's resource constraints and practical deployment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"medium\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"mobile application\",\n        \"description\": \"Develop a real-time, user-friendly dog breed recognition system using the Stanford Dogs Dataset. Focus on high accuracy, low latency, and a comparison between CNNs and transfer learning methods like VGG and ResNet, considering app's resource constraints and practical deployment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Stanford Dogs Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"dog breed\"],\n            \"specification\": null,\n            \"description\": \"A dataset for real-time dog breed recognition, containing images of dogs for classification.\",\n            \"preprocessing\": [\"resize, normalization\"],\n            \"augmentation\": [\"random crops, flips\"],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": [\"convolutional neural networks (CNN)\", \"transfer learning (VGG, ResNet)\"],\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A real-time dog breed recognition system using either CNNs or transfer learning architectures, like VGG or ResNet, tailored for mobile app deployment with resource constraints.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Consider the impact of model complexity on accuracy and latency, as well as the trade-offs between training from scratch and utilizing pre-trained models.\"\n        },\n        {\n            \"content\": \"Transfer learning can potentially save computational resources while maintaining good accuracy if the pre-trained models have similar domain knowledge.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"mobile\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"on-device\", \"cloud-based\"],\n        \"demonstration\": \"Demonstrate accuracy and latency measures on a sample of the app's data flow\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure has been accurately parsed to reflect the user's request for a dog breed recognition system using the Stanford Dogs Dataset. It includes specific requirements on accuracy, latency, and a comparison between CNNs and transfer learning methods. The resource constraints and deployment considerations have been addressed. However, some metric values are left unspecified to cater to potential optimization or discussion with the client.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "As a project manager for a specialized machine learning team, design a node classification model using the TGN architecture for the exclusive task of differentiating influential students in a high school's evolving emotional connectivity network. The team should handle temporal data with adaptive graph convolutions, showcasing nuanced shifts in social dynamics over monthly intervals, while optimizing computational efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"education\",\n        \"description\": \"Design a node classification model using the TGN architecture for detecting influential students in a high school's emotional connectivity network, considering temporal data with adaptive graph convolutions and monthly social dynamics shifts, while prioritizing computational efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"High school emotional connectivity network\",\n            \"modality\": [\"graph\", \"time series\"],\n            \"target_variables\": [\"influential student\"],\n            \"specification\": {\n                \"time_steps\": \"monthly intervals\",\n                \"temporal_dimension\": null,\n                \"node_features\": null\n            },\n            \"description\": \"A graph dataset containing time-varying emotional connectivity data, representing student relationships as nodes and monthly changes as edges.\",\n            \"preprocessing\": [\n                \"Temporal data normalization\",\n                \"Graph representation learning\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"Dynamic network analysis\"\n            ],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"TGN Model for High School Influential Student Detection\",\n            \"family\": \"Temporal Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"adaptation_layers\": \"adaptive graph convolutions\",\n                \"architecture\": \"TGN\"\n            },\n            \"description\": \"A node classification model using TGN, tailored for influential student detection in a high school's emotionally evolving connectivity network with monthly temporal resolution.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Temporal feature extraction\",\n            \"Graph feature extraction\"\n        ],\n        \"target_device\": \"optimized for high-performance on cloud platforms\",\n        \"deployment_endpoint\": \"yet to be determined\",\n        \"inference_engine\": [\n            \"GPU-based execution for efficiency\"\n        ],\n        \"demonstration\": \"Visualizations of model performance and inferred influential students\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the main elements of the user's instruction, such as the model, problem area, and performance metrics. The model details, TGN architecture, and temporal data handling are included. The requirements for computational efficiency are acknowledged through the attention to inference time and memory usage, with some flexibility in metric values. The dataset description includes preprocessing and source, though additional specifics could be beneficial.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular classification", "instruction": "You've been assigned a precision oncology dataset, comprising genetic markers and lifestyle factors of 10,000 patients across five distinct subtypes of cancer. Your task is to design a novel algorithm for tabular classification, focusing on rare subtype combinations. Apply advanced imputation techniques, create condition-specific embeddings, and optimize using a stacked ensemble model. Report not only the macro F1 Score but also per-class precision and recall on the strictly confidential held-out eval set.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular classification\",\n        \"application_domain\": \"medical oncology\",\n        \"description\": \"Design a novel algorithm for tabular classification on a precision oncology dataset with 10,000 patients across 5 cancer subtypes, focusing on rare subtype combinations. Apply advanced imputation techniques, condition-specific embeddings, and use a stacked ensemble model. Report macro F1 Score, per-class precision, and recall.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Macro F1 Score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Per-class Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Per-class Recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": []\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Precision Oncology Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"cancer subtype\"],\n            \"specification\": {\n                \"size\": {\n                    \"total_samples\": 10000,\n                    \"unseen_ratio\": \"confidential\"\n                },\n                \"diversity\": \"5 distinct cancer subtypes with rare combinations\"\n            },\n            \"description\": \"A dataset with genetic markers and lifestyle factors of 10,000 patients across various cancer subtypes, focusing on rare combinations.\",\n            \"preprocessing\": [\n                \"advanced imputation techniques\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Stacked Ensemble\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"techniques\": [\"condition-specific embeddings\"]\n            },\n            \"description\": \"A novel tabular classification model using advanced imputation, condition-specific embeddings, and a stacked ensemble architecture.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"report and analysis on the held-out eval set\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured according to the given instruction, detailing a machine learning task for tabular classification in oncology. Performance metrics, including macro F1 Score and per-class precision and recall, are included. Advanced data preprocessing and a specific model architecture are described. Since the dataset size and the nature of the held-out set are confidential, they are marked as 'confidential'. The user's high expertise suggests a deeper understanding of the domain, and the lack of model type implies the need for clarification if a more specific type is required.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image segmentation", "instruction": "In a cutting-edge research facility focusing on precision dermatology, the team has developed an advanced deep learning system that captures not only the basic skin conditions but also discerns rare and subtle variations like atypical Melanocytic Nevi, rosacea subtype identification, and psoriasis subtypes. The challenge now lies in creating a highly sophisticated image segmentation model that can handle high-resolution, ultra-high-frequency dermoscopic images with extreme contrast and texture variation. The system should not only classify but also localize these minute features, enabling real-time analysis during live consultations. Can you design and implement this innovative solution, considering factors like computational efficiency and interpretability for clinicians?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"precision dermatology\",\n        \"description\": \"Develop a highly sophisticated image segmentation model for a deep learning system that captures advanced skin conditions like atypical Melanocytic Nevi, rosacea subtype identification, and psoriasis subtypes in high-resolution dermoscopic images. The model should handle extreme contrast and texture variation, classify and localize minute features, and support real-time analysis during live consultations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"computational efficiency\",\n                \"value\": null,\n                \"unit\": \"inference time\"\n            },\n            {\n                \"name\": \"interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"precision dermatology dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"skin conditions\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": null,\n                    \"height\": null,\n                    \"depth\": null\n                },\n                \"image_frequency\": \"ultra-high\",\n                \"contrast\": \"extreme\",\n                \"texture_variation\": null\n            },\n            \"description\": \"A dataset of high-resolution, dermoscopic images with a wide range of skin conditions for developing the segmentation model.\",\n            \"preprocessing\": [\"image resizing\", \"normalization\"],\n            \"augmentation\": [\n                \"data augmentation for texture and contrast variations\"\n            ],\n            \"visualization\": [\"example segmentation masks\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Dermoscopic Image Segmentation Model\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"explainable AI-friendly\",\n                \"parameters\": null\n            },\n            \"description\": \"A deep learning model designed for image segmentation in dermatology, with focus on interpretability for clinicians.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"The dataset is in high-resolution and contains rare skin variations, making it a challenge to handle extreme contrast and texture.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction from high-frequency data\"],\n        \"target_device\": \"embedded for real-time analysis\",\n        \"deployment_endpoint\": \"facility-specific cloud platform\",\n        \"inference_engine\": [\"GPU-optimized inference\"],\n        \"demonstration\": \"live consultations with model interpretation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a complex model within the precision dermatology domain. The user has high expertise, which influences the inclusion of interpretability requirements. Specific performance metrics are not provided, indicating the need for further clarification on acceptable thresholds. Some details, such as model architecture and exact performance values, are left unspecified as the user's instruction is focused on the overall concept.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "In the realm of the arXiv repository, a unique challenge awaits a machine learning team to design an algorithm that employs Hyperbolic Graph Convolutional Networks (HGNN) for author affiliation prediction. The model must discern researchers' subfields within condensed matter physics with utmost precision, targeting a mean average precision of 0.90, while optimizing for scalability in handling an immense number of papers and minimizing the reliance on node degree information.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"scientific research\",\n        \"description\": \"Design an algorithm using Hyperbolic Graph Convolutional Networks (HGNN) for author affiliation prediction in the arXiv repository, specifically focused on condensed matter physics. Aim for a mean average precision of 0.90 while ensuring scalability and minimizing dependence on node degree information.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean average precision\",\n                \"value\": 0.90\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"dependence on node degree\",\n                \"value\": null,\n                \"unit\": \"reduced reliance\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv author affiliation data\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"author subfield in condensed matter physics\"],\n            \"specification\": {\n                \"size\": \"immense number of papers\",\n                \"degree-based information\": \"minimal\"\n            },\n            \"description\": \"A graph dataset representing papers with authors, including their affiliations in condensed matter physics for HGNN-based affiliation prediction.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hyperbolic Graph Convolutional Network (HGNN)\",\n            \"family\": \"hyperbolic GNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"node degree influence\": \"minimized\"\n            },\n            \"description\": \"A machine learning model utilizing Hyperbolic Graph Convolutional Networks to accurately predict author affiliations in condensed matter physics with a target mean average precision of 0.90.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response incorporates the user's intent for model development, specifying the problem domain (graph machine learning), task (node classification), and performance target (mean average precision). The dataset and model properties align with the arXiv repository and the requirements for HGNN. The description of the dataset and model specifies the scalability and minimal reliance on node degree information. However, exact complexity metric values are not given due to the open-endedness of scalability and the emphasis on minimizing node degree dependency.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "community detection", "instruction": "As a bioinformatics novice, I need to analyze a large-scale PPI network for identifying intricate biological communities using deep learning. Can you guide me in designing a scalable, interpretable, and efficient algorithm suitable for novices with limited computational resources, while ensuring high accuracy?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"low\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"community detection\",\n        \"application_domain\": \"bioinformatics\",\n        \"description\": \"Analyze a large-scale PPI (protein-protein interaction) network for identifying intricate biological communities using deep learning. Require a scalable, interpretable, and efficient algorithm for bioinformatics novices with limited computational resources, ensuring high accuracy.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"hours\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory usage\",\n                \"unit\": \"GB\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Large-scale PPI network\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"community labels\"],\n            \"specification\": {\n                \"nodes_count\": null,\n                \"edges_count\": null,\n                \"average_degree\": null\n            },\n            \"description\": \"A large PPI network dataset for protein interaction analysis.\",\n            \"preprocessing\": [\n                \"network pruning\",\n                \"sampling\"\n            ],\n            \"augmentation\": [\n                \"synthetic data generation\"\n            ],\n            \"visualization\": [\n                \"network visual representation\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Scalable Community Detection Model\",\n            \"family\": \"Deep Graph Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"interpretable deep learning model\",\n                \"flops\": \"limited for resource constraints\",\n                \"explainability techniques\": \"used\"\n            },\n            \"description\": \"A user-friendly deep learning model for community detection in large-scale PPI networks, optimized for scalability, interpretability, and efficiency on low-resource environments.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Use lightweight models, distributed computing, and efficient data processing techniques\",\n        \"Choose appropriate feature representations to reduce computational demands\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature selection\",\n            \"graph feature extraction\"\n        ],\n        \"target_device\": \"low-end computing devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"approximate computing\", \"lightweight libraries\"],\n        \"demonstration\": \"interactive visualization of results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the user's need for an analysis on a large-scale PPI network with minimal computational requirements. The user's novice expertise level suggests a focus on scalability, interpretability, and efficiency. The JSON includes the relevant areas of graph machine learning and bioinformatics. Performance metrics like accuracy and computational complexity considerations are mentioned. The model and data preprocessing steps are tailored to the user's needs and limited resources.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a cutting-edge machine learning project manager, your team has been tasked with a unique and intricate time-series forecasting challenge in the realm of renewable energy. You've been handed the \"Sustainable Energy Grid Forecasting\" dataset, which contains detailed temporal data from an extensive global network of wind, solar, and hydroelectric power plants. The dataset is divided into four distinct sections: historical training data, calibration validation, efficiency benchmarking, and a confidential sector reserved for rigorous stress testing.\n\nThe input sequences for your model consist of not only past power generation patterns but also real-time weather data, energy demand predictions, and regional grid stability indicators over a variable sequence length (MIN_SEQ_LEN=100, MAX_SEQ_LEN=200) with a total of 1000 features per entry (INPUT_DIM=1000). Your model must predict the future power output over the next 24 hours, with variable lengths ranging from 48 to 96 hours (PRED_SEQ_LEN=48-96, OUTPUT_DIM=1000). Performance will be evaluated using a weighted combination of mean squared error (MSE), mean absolute error (MAE), and a novel metric called 'Renewable Energy Coherence Score' (RECS), which measures the model's ability to capture the intermittent nature of renewable energy.\n\nIn addition to statistical accuracy, your team is expected to create a predictive dashboard that displays real-time forecasting results, alongside valuable insights on potential energy generation outliers and grid management recommendations. The ultimate goal is to develop a model that not only forecasts energy production but also contributes to the overall optimization and resilience of the sustainable energy grid.\n\nYour mission, as the project manager, is to strategize the allocation of resources, define milestones, and ensure the successful integration of this innovative time-series forecasting solution into the market.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"renewable energy\",\n        \"description\": \"Develop a time-series forecasting model for Sustainable Energy Grid Forecasting dataset. The model should predict future power output over 24 hours using historical training data, real-time weather data, energy demand predictions, and grid stability indicators. Evaluation includes MSE, MAE, and a custom 'Renewable Energy Coherence Score'. Additionally, design a predictive dashboard with insights on outliers and grid management.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"RECS\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Sustainable Energy Grid Forecasting\",\n            \"modality\": [\n                \"time series\",\n                \"multimodal\"\n            ],\n            \"target_variables\": [\"future power output\"],\n            \"specification\": {\n                \"MIN_SEQ_LEN\": 100,\n                \"MAX_SEQ_LEN\": 200,\n                \"INPUT_DIM\": 1000,\n                \"PRED_SEQ_LEN\": { \"min\": 48, \"max\": 96 },\n                \"OUTPUT_DIM\": 1000\n            },\n            \"description\": \"A dataset with temporal data from wind, solar, and hydroelectric power plants, including real-time weather data, energy demand predictions, and grid stability indicators.\",\n            \"preprocessing\": [\n                \"sequence normalization\",\n                \"feature engineering\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"time-series plot\",\n                \"feature importance\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"recurrent neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"sequence_length\": \"variable\",\n                \"num_features\": 1000,\n                \"prediction_window\": \"48-96 hours\"\n            },\n            \"description\": \"A time-series forecasting model that incorporates various input modalities for renewable energy grid prediction and optimization.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Data preprocessing should handle the multi-feature and variable sequence length nature of the problem.\",\n        \"Consider using LSTM or Transformer-based architectures for handling time-series and multimodal data.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"seasonality analysis\",\n            \"cross-correlation analysis\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"API integration with grid management system\",\n        \"inference_engine\": [\n            \"TensorFlow serving\",\n            \"real-time data streaming\"\n        ],\n        \"demonstration\": \"Live dashboard showcasing real-time forecasts and recommendations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response captures the core aspects of the user's instruction, focusing on model development, data requirements, and dashboard creation. The user's high expertise level indicates a deep understanding of the challenge and the expectations. The metrics, performance, and resource allocation are specified based on the context. The provided metrics require further clarification (e.g., exact target values), but the overall structure is valid.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "object detection", "instruction": "As a project manager for the machine learning team, we must enhance the car model detection system by not only selecting an optimal dataset between COCO and Pascal VOC for our task, but also ensuring compatibility with diverse lighting conditions and occlusions. Additionally, mandate real-time processing speed while maintaining at least 95% precision on low-resolution surveillance footage, preferably using a lightweight model architecture.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"autonomous systems or surveillance\",\n        \"description\": \"Enhance the car model detection system by selecting a suitable dataset (COCO or Pascal VOC) for car object detection, ensuring compatibility with diverse lighting conditions and occlusions. Require real-time processing speed on low-resolution surveillance footage with a minimum precision of 95% and a lightweight model architecture.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"real-time processing speed\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"COCO or Pascal VOC (user needs to choose)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"compatibility\": \"diverse lighting conditions, occlusions\",\n                \"image_resolution\": \"low-resolution\",\n                \"dataset_size\": null\n            },\n            \"description\": \"A dataset for car object detection that should cater to specific requirements of lighting, occlusions, and low-resolution footage.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"lightweight architecture\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": \"lightweight\",\n                \"real_time_inference\": true\n            },\n            \"description\": \"A lightweight neural network model designed for real-time car model detection on low-resolution surveillance footage.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"real-time processing environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for low-resource devices\"],\n        \"demonstration\": \"visual examples demonstrating enhanced detection performance in diverse conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The provided instruction captures the project manager's intention to build a car model detection system with emphasis on data selection (COCO or Pascal VOC), performance under specific constraints, and model lightweight requirements. The performance metrics and complexity metrics align with the real-time processing speed and precision requirement. Some fields, like specific model name, are left for user discretion or further details.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "As a project manager for the advanced medical Q&A system, mandate the development team to design a model specifically tailored for highly specific and rare medical conditions using the comprehensive BioASQ dataset. Implement a dual-encoder architecture with fine-tuned BioBERT, ensuring not only factual accuracy but also the generation of evidence-based answers linked to indexed scientific papers, while maintaining a stringent 95% precision benchmark.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"medical\",\n        \"description\": \"Develop a model for an advanced medical Q&A system, focused on highly specific and rare medical conditions using the BioASQ dataset. The model should employ a dual-encoder architecture with fine-tuned BioBERT, ensuring factual accuracy, evidence-based answers, and a precision benchmark of at least 95%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\" \n            },\n            {\n                \"name\": \"Memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"BioASQ dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"size\": \"comprehensive\",\n                \"domain\": \"medical\"\n            },\n            \"description\": \"A large-scale dataset for medical question answering, containing questions and indexed scientific papers.\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Dual-encoder architecture\",\n            \"family\": \"BioBERT\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Fine-tuned for medical domain\",\n                \"encoder_type\": \"dual-encoder\"\n            },\n            \"description\": \"A high-performance model using a dual-encoder architecture with fine-tuned BioBERT for medical Q&A, targeting specific and rare conditions.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"Required to generate evidence-based, fact-checked answers linked to scientific papers\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent for a model build with a high-level of expertise. The problem area and downstream task align with the medical NLP domain, and specific details are provided for the dataset (BioASQ), model (BioBERT dual-encoder), and performance metrics (95% precision). However, some open-ended complexity metrics are left unspecified, as the user has not given concrete guidelines. Overall, the response is comprehensive and well-suited to the given project management role.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "As a seasoned project manager for a cutting-edge machine learning team, you've been tasked with overseeing the development of a state-of-the-art Visual Question Answering (VQA) model. Your team will be working on the Celestial Maps dataset, which showcases ancient constellations and their associated myths through meticulously restored astronomical illustrations. The challenge here is not only to interpret visual information but also to incorporate astrological knowledge for accurate and detailed answers to complex queries about historical astronomy, mythology, and celestial events. The model must achieve a top-tier accuracy of 75% on a rigorous test set that tests for context preservation and nuanced understanding of the illustrations. Additionally, the project should present a user-friendly interface that seamlessly integrates this knowledge for non-expert users to explore and learn about the cosmos.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"astronomy, education\",\n        \"description\": \"The task is to develop a state-of-the-art Visual Question Answering (VQA) model on the Celestial Maps dataset that incorporates astrological knowledge for questions related to historical astronomy, mythology, and celestial events. The model should achieve at least 75% accuracy on a context-preserving and nuanced test set, and include a user-friendly interface for non-experts.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": 0.75\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Celestial Maps dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [],\n            \"specification\": {\n                \"image_type\": \"astronomical illustrations\",\n                \"data_format\": \"masks and labels for constellations and mythology\"\n            },\n            \"description\": \"A dataset containing ancient constellations and associated myths through restored astronomical illustrations for VQA.\",\n            \"preprocessing\": [\n                \"image restoration and normalization\",\n                \"annotation for constellations and myths\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"visual interpretability for model predictions\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art VQA model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"incorporating astrological knowledge\"\n            },\n            \"description\": \"Advanced VQA model designed for understanding astronomical illustrations with astrological context\"\n        }\n    ],\n    \"knowledge\": [\n        \"The model needs to incorporate astrological background for accurate responses to complex astronomical questions.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"contextual information extraction from illustrations\",\n            \"embedding astrological knowledge into model\"\n        ],\n        \"target_device\": \"cloud or desktop\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU-optimized\"],\n        \"demonstration\": \"A web-based interface for interactive VQA on celestial maps\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure covers the user's intent to build a high-end VQA model with the specific dataset, performance objectives, and interface requirements. The model's interdisciplinary nature, accuracy requirement, and user-friendliness are reflected. However, without specific model name or architecture details, there's a margin of uncertainty, and inference time and memory footprint should be targeted for optimization.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image segmentation", "instruction": "For the upgraded PlantVillage project, develop a multi-step deep learning pipeline using an advanced attention-guided DenseNet architecture. Enhance the model with feature hierarchy and cross-attention fusion, necessitating detailed analysis of both global and local context. Incorporate grad-cam visualizations for interpretability, as well as a user-friendly interactive dashboard to present complex disease patterns in an intuitive manner, ensuring non-experts can easily comprehend plant health conditions from the segmentation results.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image segmentation\",\n        \"application_domain\": \"agriculture\",\n        \"description\": \"Develop a multi-step deep learning pipeline using an advanced attention-guided DenseNet architecture for the upgraded PlantVillage project. The model should incorporate feature hierarchy and cross-attention fusion for comprehensive analysis of global and local context. Include grad-CAM visualizations for interpretability, and design an interactive dashboard for non-experts to easily interpret plant health conditions from segmentation results.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"model interpretability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"PlantVillage (upgraded)\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"plant health conditions\"],\n            \"specification\": {\n                \"image_size\": \"variable (to fit DenseNet architecture)\",\n                \"number_of_classes\": null\n            },\n            \"description\": \"Dataset for plant disease detection, consisting of images and segmentation ground truths for enhancing model performance.\",\n            \"preprocessing\": [\n                \"image normalization\",\n                \"data augmentation\"\n            ],\n            \"augmentation\": [\n                \"rotation\",\n                \"translation\",\n                \"scale\"\n            ],\n            \"visualization\": [\n                \"grad-CAM\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Attention-guided DenseNet\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_depth\": null,\n                \"attention_layers\": true,\n                \"feature_hierarchy\": true,\n                \"cross_attention_layers\": true\n            },\n            \"description\": \"A deep learning model with attention-guided DenseNet architecture, designed for advanced image segmentation in the PlantVillage project, incorporating feature hierarchy and cross-attention fusion.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Advanced architectural techniques for plant disease detection and interpretability in non-experts.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"gradient-weighted feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"user-friendly dashboard\"],\n        \"demonstration\": \"Interactive segmentation results visualization and explanations.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The given JSON response captures the user's intent to build a complex model for the upgraded PlantVillage project. The instruction details are translated into a high-expertise level and are tailored to the computer vision area. The model specifics and requirements for interpretability, performance, and complexity are well-defined. Some missing values are placeholders, such as exact model depths, which would likely depend on further consultation. The interactive dashboard is a key aspect for fulfilling the non-expert user requirement.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "sequential recommendation", "instruction": "The task is to enhance the Bilibili Danmu dataset analysis for a real-time, personalized sequential recommendation system. The objective is to develop a state-of-the-art Bert4Rec model that integrates deep contextual understanding of user comments, handles multilingual inputs, and optimizes for diversity in recommendations while maintaining high accuracy on a dynamically evolving user base.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"recommendation systems\",\n        \"downstream_task\": \"sequential recommendation\",\n        \"application_domain\": \"online video platform (Bilibili)\",\n        \"description\": \"Enhance the Bilibili Danmu dataset analysis for a real-time, personalized sequential recommendation system. Aim to build a state-of-the-art Bert4Rec model that integrates deep contextual understanding of user comments, supports multilingual inputs, and optimizes for diversity while maintaining high accuracy with a dynamic user base.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Diversity metric\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Bilibili Danmu dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"user comments\"],\n            \"specification\": {\n                \"dynamic evolution\": \"yes\",\n                \"multilingual\": \"yes\"\n            },\n            \"description\": \"A dynamic dataset from Bilibili's live stream comments for personalized recommendation, incorporating real-time user inputs and multilingual context.\",\n            \"preprocessing\": [\"cleaning, tokenization\"],\n            \"augmentation\": [\"contextual word embeddings\"],\n            \"visualization\": [\"user comment distributions\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Bert4Rec\",\n            \"family\": \"neural networks\",\n            \"type\": \"sequence-based recommendation\",\n            \"specification\": {\n                \"architecture\": \"BERT-based, personalized, deep contextual understanding\",\n                \"language models\": [\"Multilingual BERT\"],\n                \"diversification techniques\": \"e.g., incorporating diversity losses\"\n            },\n            \"description\": \"A state-of-the-art model for real-time sequential recommendation that fuses user comments' deep context with BERT, handles multilingual inputs, and optimizes for diversity while preserving high accuracy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"User base is dynamic, requiring model adaptability\",\n        \"Multilingual user comments, need to handle non-Chinese languages\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"context-aware user profiles\", \"real-time comment embeddings\"],\n        \"target_device\": \"cloud-based\",\n        \"deployment_endpoint\": \"potentially API-based\",\n        \"inference_engine\": [\"API serving technology\"],\n        \"demonstration\": \"Live streaming recommendations showcasing personalized and diverse comments\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's intent, high expertise level, and specifies the problem area, task, and requirements for the Bilibili Danmu dataset analysis. The model choice (Bert4Rec) and its key features align with the user's objective. Performance and complexity metrics are included with no concrete values, allowing for flexibility. Missing values will be filled according to the user's request for state-of-the-art models and adaptability to a dynamic user base.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As the maritime safety project unfolds, we require a DeepLabv3+ model capable of real-time object detection for various vessel types, their dimensions, and headings. It must be fine-tuned using the comprehensive SeaShips dataset, targeting 95% accuracy even in challenging weather conditions. The model's efficiency is crucial for cloud-based GPU deployment, with an added constraint of optimizing for minimal latency during inference.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"marine safety\",\n        \"description\": \"Develop a DeepLabv3+ model for real-time object detection of various vessel types with dimensions and headings, targeting 95% accuracy in challenging weather conditions. The model must be fine-tuned using the SeaShips dataset and optimized for cloud-based GPU deployment, with minimal latency constraints.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SeShips dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vessel types\", \"dimensions\", \"headings\"],\n            \"specification\": {\n                \"size\": \"comprehensive\",\n                \"weather_conditions\": \"challenging\"\n            },\n            \"description\": \"A comprehensive dataset for fine-tuning the DeepLabv3+ model for real-time maritime object detection.\",\n            \"preprocessing\": [\"augmentation for diverse weather conditions\"],\n            \"augmentation\": [\"data augmentation for various weather conditions\"],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"DeepLabv3+\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"deployment\": \"cloud-based GPU\",\n                \"optimizer\": \"GPU-optimized\"\n            },\n            \"description\": \"Fine-tuned DeepLabv3+ model for real-time object detection in maritime scenarios with high accuracy and efficiency constraints.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"The model must handle challenging weather conditions.\"\n        },\n        {\n            \"description\": \"Cloud-based GPU deployment and low latency are critical requirements.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"real-time adaptation for weather changes\"],\n        \"target_device\": \"cloud-based GPU\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for cloud GPUs\"],\n        \"demonstration\": \"real-time detection video or simulation results\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the key aspects of the requested machine learning project, including the model requirements (DeepLabv3+, fine-tuning, performance, and deployment), dataset (SeaShips), performance metrics (accuracy, inference time, and latency), and additional constraints (weather conditions and cloud-based GPU). The user's high expertise level is reflected in the response, and missing details can be obtained through clarification or assumed based on the high-level instruction.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a project manager for a cutting-edge machine learning team, we have been tasked with developing an advanced time-series classification algorithm. The dataset at hand, the highly intricate Ethanol Concentration dataset, consists of intricate, realistic time series data with numerous temporal dependencies (INPUT_SEQ_LEN=1751, INPUT_DIM=3, including complex patterns). The challenge lies in forecasting labels accurately within a multi-class classification problem with labels ranging from 0 to 3. Crucially, the model must exhibit exceptional robustness across train, validation, and test sets, while demonstrating high precision and recall. The model's performance must surpass benchmark standards and efficiently handle irregularities in the data. Your assignment is to design and implement this state-of-the-art solution, ensuring efficient resource allocation and a thorough understanding of feature interactions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"process monitoring, industrial engineering\",\n        \"description\": \"Develop an advanced time-series classification algorithm for the Ethanol Concentration dataset, focusing on forecasting labels with irregularities (INPUT_SEQ_LEN=1751, INPUT_DIM=3), multi-class classification (labels 0 to 3), and robustness. Target high precision and recall while outperforming benchmarks in terms of model performance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Ethanol Concentration\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"labels\"],\n            \"specification\": {\n                \"sequence_length\": 1751,\n                \"input_dimensions\": 3,\n                \"complex_patterns\": true\n            },\n            \"description\": \"A complex time-series dataset with irregularities, including temporal dependencies, for the advanced time-series classification task.\",\n            \"preprocessing\": [\"imputation, detrending, normalization\"],\n            \"augmentation\": [\"time warping, random noise\"],\n            \"visualization\": [\"temporal pattern analysis\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Time-Series Classifier\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art model designed for time-series classification\",\n                \"robustness\": \"high across different sets\",\n                \"feature_interactions\": true\n            },\n            \"description\": \"A deep learning model specifically tailored for the given time-series classification problem, emphasizing robustness, precision, and recall.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Understand and incorporate feature interactions to enhance model performance\",\n        \"Robust data imputation and handling techniques for irregularities\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature selection, feature extraction\"],\n        \"target_device\": \"cloud-based (GPU server)\",\n        \"deployment_endpoint\": \"API for real-time predictions\",\n        \"inference_engine\": [\"TensorFlow serving, Keras-TensorFlow\"],\n        \"demonstration\": \"Demonstrate performance across different metrics and data splits\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is based on the user's requirements to develop an advanced time-series classification model for the Ethanol Concentration dataset. The high level of expertise is assumed, and performance expectations are set to surpass benchmarks. The instruction provides enough detail for preprocessing, model architecture, and final deployment. However, specific performance metric values and memory constraints are not provided, which may need further clarification.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "node classification", "instruction": "For the upcoming project on Amazon's massive e-commerce graph, design a state-of-the-art GraphSAGE model with FastGCN enhancement for node classification. Focus on 90% accuracy while ensuring efficiency in processing millions of nodes and real-time recommendations, considering edge attributes and time-varying connections.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Design a state-of-the-art GraphSAGE model with FastGCN enhancement for node classification on Amazon's massive e-commerce graph. Focus on achieving 90% accuracy and ensuring efficiency in processing millions of nodes and real-time recommendations, considering edge attributes and time-varying connections.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"real-time\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Amazon e-commerce graph\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"product categories\"],\n            \"specification\": {\n                \"node_count\": \"millions\",\n                \"edge_attributes\": true,\n                \"time_series\": true\n            },\n            \"description\": \"A large-scale graph with millions of nodes representing products and their connections in Amazon's e-commerce platform, considering edge attributes and time-varying relationships.\",\n            \"preprocessing\": [\"edge attribute extraction\", \"time-varying connection handling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"GraphSAGE with FastGCN\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"state-of-the-art\",\n                \"enhancements\": [\"FastGCN\"],\n                \"node_classification_performance\": \"accuracy\",\n                \"target_value\": \"90%\"\n            },\n            \"description\": \"A GraphSAGE model enhanced with FastGCN for node classification in Amazon's e-commerce graph, aiming for 90% accuracy and efficiency for large-scale and real-time scenarios.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Edge attribute importance and time-varying connections play a crucial role in model performance.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"real-time recommendation service\",\n        \"inference_engine\": [\"Apache Spark\", \"GPU-accelerated\"],\n        \"demonstration\": \"A demonstrative system showcasing real-time recommendations on a sample of the massive graph.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON accurately captures the user's intention to build a complex machine learning model for the specified problem. The problem description covers the task, performance targets, and the unique aspects of the dataset (edge attributes and time-varying connections). The user's high expertise level suggests their understanding of the model's requirements. Some areas like feature engineering and deployment details are left open-ended as the user did not provide explicit specifics.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "multimodal emotion recognition", "instruction": "As a machine learning project manager, can you guide me on implementing state-of-the-art emotion recognition using the IEMOCAP dataset? Compare multiple deep learning architectures (Transformer, CNN, RNN) with ensemble methods, considering fine-grained emotions, cross-validation, and real-time performance, while adhering to dataset ethics and ensuring interpretability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"multimodal emotion recognition\",\n        \"application_domain\": \"human-computer interaction\",\n        \"description\": \"Implement a state-of-the-art emotion recognition system using the IEMOCAP dataset, comparing multiple deep learning architectures (Transformer, CNN, RNN) with ensemble methods, focusing on fine-grained emotions, cross-validation, real-time performance, and dataset ethics while ensuring interpretability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"real-time performance\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"IEMOCAP\",\n            \"modality\": [\"audio\", \"video\"],\n            \"target_variables\": [\"emotions\"],\n            \"specification\": {\n                \"number_of_samples\": null,\n                \"data_dimensions\": {\n                    \"audio\": null,\n                    \"video\": null\n                }\n            },\n            \"description\": \"Dataset for emotion recognition, particularly focusing on fine-grained emotions, captured through audio and video modalities.\",\n            \"preprocessing\": [\n                \"feature extraction\",\n                \"alignment\"\n            ],\n            \"augmentation\": [\"data augmentation for multimodal data\"],\n            \"visualization\": [\"visually inspect emotional patterns\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Transformer, CNN, RNN\",\n            \"family\": \"Deep Learning\",\n            \"type\": [\"neural networks\"],\n            \"specification\": {\n                \"architecture_details\": {\n                    \"Transformer\": {},\n                    \"CNN\": {},\n                    \"RNN\": {}\n                }\n            },\n            \"description\": \"State-of-the-art deep learning architectures, including Transformers, CNNs, and RNNs, for emotion recognition in an ensemble model.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Ethics guidelines for handling IEMOCAP dataset\",\n        \"Best practices for interpretability in deep learning models\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"adapt to multimodal data\",\n            \"feature fusion\"\n        ],\n        \"target_device\": \"real-time systems (mobile, cloud)\",\n        \"deployment_endpoint\": \"API for real-time emotion recognition\",\n        \"inference_engine\": [\n            \"TensorFlow.js\",\n            \"MobileDNN\"\n        ],\n        \"demonstration\": \"create a user interface for model predictions and explanations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been structured based on the user's needs, focusing on a machine learning project for emotion recognition using IEMOCAP. The user's expertise level suggests they require a high-level response. Performance metrics like accuracy, F1 score, and real-time performance are requested. Ensemble methods, fine-grained emotions, cross-validation, and interpretability are considered. The dataset, models, and service requirements are adapted accordingly. However, specific metric values and technical details are yet to be filled based on the available information.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Design a sophisticated video question-answering framework utilizing the extensive YouTube-8M dataset, incorporating state-of-the-art 3D ConvNets and Enhanced Multi-Modal Transformer architectures. Require the system to demonstrate multi-level understanding across diverse themes, analyze complex spatiotemporal dynamics, and consistently provide precise and contextually relevant responses with minimal latency. Additionally, assess its capacity to adapt to new topics and handle unforeseen visual elements in real-time.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"video content analysis\",\n        \"description\": \"Design a video question-answering framework using the YouTube-8M dataset, integrating 3D ConvNets and Enhanced Multi-Modal Transformer architectures. The system must exhibit multi-level understanding across diverse themes, analyze complex spatiotemporal dynamics, and provide precise, contextually relevant responses with low latency. It must also demonstrate adaptability to new topics and handle unseen visual elements in real-time.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response latency\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"spatiotemporal analysis\",\n                \"value\": null,\n                \"unit\": \"frame-level or video-level\"\n            },\n            {\n                \"name\": \"adaptability score\",\n                \"value\": null,\n                \"unit\": \"percentage of new topics\"\n            },\n            {\n                \"name\": \"real-time handling\",\n                \"value\": null,\n                \"unit\": \"measurement of real-time capability\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube-8M\",\n            \"modality\": [\"video\"],\n            \"target_variables\": [\"questions and ground truth answers\"],\n            \"specification\": null,\n            \"description\": \"A large-scale video dataset for training a video question-answering framework, containing diverse visual content and thematic information.\",\n            \"preprocessing\": [\"3D ConvNet feature extraction\", \"Enhanced Multi-Modal Transformer preprocessing\"],\n            \"augmentation\": [\"temporal and spatial data augmentation\"],\n            \"visualization\": [\"model performance on diverse themes\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"3D ConvNets\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A state-of-the-art video question-answering model that combines 3D ConvNets for spatial understanding and Enhanced Multi-Modal Transformers for temporal and multi-modal context.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Focus on improving the model's ability to understand complex spatiotemporal dynamics and its capacity to handle unseen visual elements by applying advanced architectures and techniques.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"3D feature fusion, context modeling\"],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"demonstrate multi-level understanding on sample video questions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response incorporates the user's instruction by designing a video QA framework using the YouTube-8M dataset, focusing on advanced architectures and performance requirements. The structure reflects the complexities mentioned, such as spatiotemporal understanding, real-time handling, and adaptability. While specific values are not included, the focus is on how to build the system based on the requested features.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for our cutting-edge smart city initiative, request a YOLO-based object detection model specialized in vehicle, pedestrian, and traffic sign recognition. mandate a multi-stage training process using the Cityscapes dataset, ensuring optimized real-time performance of 30 FPS on diverse hardware, while maintaining state-of-the-art accuracy above 85% and considering trade-offs between speed and precision. Additionally, include a thorough evaluation of model's scalability and robustness across varying weather and lighting conditions.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"smart city\",\n        \"description\": \"Request a YOLO-based object detection model for vehicle, pedestrian, and traffic sign recognition in a smart city project, emphasizing multi-stage training on Cityscapes dataset, with real-time performance of 30 FPS, accuracy of at least 85%, and a focus on speed-precision trade-off. Evaluate scalability and robustness across different weather and lighting conditions.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean average precision (mAP)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.85\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"30 FPS\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Cityscapes\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"vehicle\", \"pedestrian\", \"traffic sign\"],\n            \"specification\": {\n                \"image_size\": null,\n                \"annotation_format\": null,\n                \"train_size\": null\n            },\n            \"description\": \"A dataset for multi-stage training with vehicle, pedestrian, and traffic sign annotations for diverse city scenes.\",\n            \"preprocessing\": [\n                \"multi-stage training\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLO-based object detection model\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_speed\": \"optimized for real-time (30 FPS)\",\n                \"model_architecture\": \"YOLO for multi-object detection\"\n            },\n            \"description\": \"A state-of-the-art YOLO model designed for real-time object detection in smart city environment with vehicle, pedestrian, and traffic sign recognition\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consideration of trade-offs between speed and precision.\",\n        \"Evaluation under varying weather and lighting conditions\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"multi-stage training\"\n        ],\n        \"target_device\": \"diverse hardware\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for real-time performance\"],\n        \"demonstration\": \"Model evaluation across varying conditions\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the essence of the user's instruction, detailing the YOLO-based object detection model, training process, performance requirements, and additional evaluation criteria. Given the user's high expertise level, certain details are left unspecified, such as exact metric values or model architecture specifications, allowing for flexible responses while addressing the main points in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "The project requires a specialized machine translation system specifically tailored for the e-commerce sector, translating between Chinese and English with a focus on the Taobao dataset. Utilize the robust U-Net architecture with enhanced domain adaptation for retail and fashion. Target not only a minimum BLEU score of 37 but strive for a gradual improvement of 1.5 points after each iteration. Emphasize system scalability to integrate seamlessly with diverse e-commerce platforms, ensuring real-time, context-aware translation of product descriptions that maintain both product accuracy and stylistic fluency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"e-commerce\",\n        \"description\": \"Design a specialized machine translation system for the Taobao dataset, translating between Chinese and English with a focus on retail and fashion. Employ the U-Net architecture with domain adaptation techniques for the e-commerce context. Aim for a minimum BLEU score of 37 and strive for a gradual increase of 1.5 points per iteration. Ensure system scalability for real-time, context-aware translation of product descriptions while maintaining product accuracy and stylistic fluency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"BLEU score\",\n                \"value\": 37\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"system scalability\",\n                \"value\": \"high\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Taobao dataset\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"Chinese to English translation\"],\n            \"specification\": {\n                \"language\": {\n                    \"source\": \"Chinese\",\n                    \"target\": \"English\"\n                },\n                \"domain\": \"retail and fashion\"\n            },\n            \"description\": \"A dataset focusing on Chinese and English product descriptions for the e-commerce domain, particularly targeting the Taobao platform.\",\n            \"preprocessing\": [\"domain adaptation for retail and fashion\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"U-Net with Enhanced Domain Adaptation\",\n            \"family\": \"neural networks\",\n            \"type\": \"sequence-to-sequence\",\n            \"specification\": {\n                \"architecture\": {\n                    \"type\": \"U-Net\"\n                },\n                \"domain adaptation\": \"retail and fashion\"\n            },\n            \"description\": \"A U-Net based machine translation model tailored for e-commerce, focusing on Chinese to English translation with domain adaptation for retail and fashion.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Real-time, context-aware translation is a priority, and maintaining product accuracy and stylistic fluency in product descriptions is key.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain-specific feature extraction\"\n        ],\n        \"target_device\": \"cloud\",\n        \"deployment_endpoint\": \"a scalable and real-time translation service\",\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"example product description translations showcasing accuracy and fluency\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's intent to build a specialized machine translation system, the requirements for the specific architecture, dataset, and performance metrics. The requested domain adaptation and performance improvement increment were captured. However, there are some uncertain values like inference time, which can be filled with actual or desired figures based on the system's design. Given the complexity and specificity of the task, the user's high expertise level suggests a more detailed understanding, warranting the high confidence score.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a project manager for the advanced renewable energy analytics team, you've been tasked with forecasting solar power output for an upcoming international sustainability conference. Your team needs to develop a sophisticated time-series model that considers multiple weather patterns, historical trends, and seasonal variations, predicting energy generation for the next two weeks with a 95% accuracy. Can you propose and implement an advanced algorithm that fulfills these requirements?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"sustainability and renewable energy\",\n        \"description\": \"As a project manager, the team needs to forecast solar power output for an international conference, considering multiple weather patterns, historical trends, and seasonal variations with a target accuracy of 95% over the next two weeks.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Solar power generation history\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"Power generation\"],\n            \"specification\": {\n                \"length\": \"2 years of historical data\",\n                \"data_points_per_week\": \"Enough for capturing trends and seasonality\"\n            },\n            \"description\": \"Historical data of solar power generation that includes weather data, historical patterns, and seasonal variations.\",\n            \"preprocessing\": [\n                \"Data cleaning\",\n                \"Feature scaling\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [\n                \"A time-series plot showing historical trends\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced time-series forecasting model (e.g., LSTM, Prophet, or ARIMA+XGBoost ensemble)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"complexity\": \"Advanced\",\n                \"model_structure\": \"Considering multiple inputs and capturing long-term dependencies\"\n            },\n            \"description\": \"A sophisticated model that combines multiple weather data inputs, historical power generation, and seasonal patterns to achieve high accuracy forecasts.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider integrating real-time weather data for accurate predictions.\",\n        \"Explore techniques for handling non-stationarity and temporal dependencies.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature extraction from weather data\",\n            \"Interpolation of missing data points\"\n        ],\n        \"target_device\": \"Cloud-based for efficient computation on large data\",\n        \"deployment_endpoint\": \"A hosted API for real-time predictions\",\n        \"inference_engine\": [\"TensorFlow serving\", \"Apache Arrow for data exchange\"],\n        \"demonstration\": \"Web application with interactive visualizations and explanations of forecasts\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response includes all the relevant details based on the instruction. It captures the user's high expertise level, specifies the time-series analysis problem, defines performance metrics, and explains the need for multiple weather patterns and historical data. It also accounts for data preprocessing, advanced model selection, and integration of knowledge for improved accuracy. Target device, deployment, and inference details are included to address the deployment aspect of the task. Some uncertainty exists due to the lack of specific model parameters, which might be determined after further research or consultation with the user.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "object detection", "instruction": "As a project manager for the machine learning team, your objective is to develop an object detection model that not only accurately identifies individual fruits in a diverse set of images with varying lighting and background conditions, but also estimates the fruit's ripeness level and counts the total number of fruits per image, ensuring real-time processing for a mobile app implementation.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"mobile app, food industry\",\n        \"description\": \"Develop an object detection model to accurately identify individual fruits, estimate ripeness level, and count fruit counts in images accounting for varying lighting and background conditions, targeting real-time processing for mobile app use.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"mAP (mean Average Precision)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score for ripeness level prediction\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": \"real-time processing\",\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Diverse Fruits dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"fruit class\", \"ripeness level\"],\n            \"specification\": {\n                \"image resolution\": null,\n                \"variety of lighting and backgrounds\": \"yes\"\n            },\n            \"description\": \"A collection of images containing various fruits in different lighting and background conditions.\",\n            \"preprocessing\": [\"image normalization, resizing\"],\n            \"augmentation\": [\"brightness, contrast, and color jitter\"],\n            \"visualization\": [\n                \"example image samples, feature maps for each step of the detection process\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"state-of-the-art object detection\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"e.g., YOLOv5, RetinaNet, or EfficientDet\",\n                \"real-time capability\": true\n            },\n            \"description\": \"A model that efficiently detects fruits, estimates ripeness, and counts while handling varying image conditions for mobile app use.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\"image preprocessing techniques, feature extraction for ripeness estimation\"],\n        \"target_device\": \"mobile device\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimizations for real-time processing on mobile devices\"],\n        \"demonstration\": \"processing time, accuracy, and model performance on test data\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's objective for developing an object detection model for mobile app use. It includes details on accuracy, performance metrics, real-time processing requirements, and dataset characteristics. The missing values are placeholders for specific numbers that would be determined in a more detailed consultation or data analysis phase.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "visual question answering", "instruction": "Create an advanced Visual Question and Answering (VQA) prototype leveraging the large-scale YouTube-8M dataset, integrating advanced 3D ResNeXt-101 and Hierarchical Cross-Modal Transformer. Focus on the system's ability to discern subtle visual cues, decode fine-grained spatiotemporal relationships, demonstrate exceptional genre-specific knowledge in art history, and demonstrate robustness to unconventional visual challenges while ensuring near-zero latency and dynamic topic adaptability in real-life scenarios.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"multimodal learning\",\n        \"downstream_task\": \"visual question answering\",\n        \"application_domain\": \"art history and multimedia\",\n        \"description\": \"Create an advanced Visual Question and Answering (VQA) prototype using the YouTube-8M dataset. The system should integrate 3D ResNeXt-101 and Hierarchical Cross-Modal Transformer, focusing on subtle visual cues, fine-grained spatiotemporal relationships, genre-specific knowledge, and robustness to unconventional visual challenges with near-zero latency and dynamic topic adaptability.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy on subtlety detection\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1 score for spatiotemporal relationships\",\n                \"value\": null\n            },\n            {\n                \"name\": \"genre-specific knowledge score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"robustness against unconventional challenges\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": \"near-zero\"\n            },\n            {\n                \"name\": \"topic adaptability score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory size\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"YouTube-8M\",\n            \"modality\": [\"video\", \"text\"],\n            \"target_variables\": [\"question-answer pairs\"],\n            \"specification\": null,\n            \"description\": \"A large-scale dataset for visual question answering, integrating both video and text modalities.\",\n            \"preprocessing\": [\n                \"3D ResNeXt-101 feature extraction\",\n                \"Hierarchical Cross-Modal Transformer preprocessing\"\n            ],\n            \"augmentation\": [\n                \"dynamic topic adaptation\"\n            ],\n            \"visualization\": [\n                \"model behavior analysis on subtlety detection\"\n            ],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"3D ResNeXt-101 and Hierarchical Cross-Modal Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": null,\n            \"description\": \"A VQA model leveraging advanced 3D ResNeXt-101 architecture and Hierarchical Cross-Modal Transformer for understanding visual and textual data.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"content\": \"Focus on discerning subtle visual cues and genre-specific art history knowledge.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"3D ResNeXt-101 feature extraction\",\n            \"Hierarchical Cross-Modal Transformer fusion\"\n        ],\n        \"target_device\": \"high-performance GPU\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"GPU acceleration\"],\n        \"demonstration\": \"real-life scenarios with varying topics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's intent to build a VQA prototype, emphasizes the high level of expertise required, and includes specific details based on the instruction. However, some metric values, like performance on subtlety and genre-specific knowledge, and inference time are left unspecified for optimisation purposes as the user didn't provide concrete targets. Overall, the response reflects the instruction's requirements while leaving room for further customization.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a forward-thinking ML project director, your team has been handed the \"Advanced Renewable Integration Dynamics\" challenge, requiring enhanced time-series forecasting for global renewables. The \"Eco-Energy Grid Adaptive Insights\" dataset now encompasses minute-level historical data, extreme weather events, and local socio-economic factors (newly added: INTERVAL_DATA) over an expanded sequence length (MIN_SEQ_LEN=200, MAX_SEQ_LEN=400) with 2000 features per entry (INPUT_DIM=2000). Your model must predict not only standard 48-96 hour ahead power generation but also respond to sudden shifts in energy mix and intermittent energy sources (EXTREMITY_SCORE).\n\nTo improve upon the established metrics, you'll design a custom evaluation metric, 'Regenerative Intensity Coherence Index' (RICI), and incorporate grid flexibility metrics. The project necessitates the development of a real-time dashboard showing adaptive forecasts, predictive maintenance signals, and actionable recommendations for grid operators, accounting for blackout risks and carbon neutrality targets.\n\nFurthermore, strategize for energy storage integration, regional policy implications, and scalable deployment strategies. Ensure the solution meets the stringent requirements for both short-term accuracy and long-term sustainability, aiming to transform the renewable energy landscape and set a new benchmark for grid resilience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy and utilities\",\n        \"description\": \"Enhance time-series forecasting for global renewable energy integration with minute-level historical data, extreme weather events, and socio-economic factors. The model should predict 48-96 hour ahead power generation and respond to energy mix shifts, incorporating RICI and grid flexibility metrics. Create a real-time dashboard for adaptive forecasts, predictive maintenance, and grid operator recommendations accounting for blackout risks and carbon neutrality targets. Include energy storage integration, policy implications, and scalable deployment strategies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"RICI (Regenerative Intensity Coherence Index)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Short-term accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Long-term sustainability\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Real-time response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Eco-Energy Grid Adaptive Insights\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"Power generation\"],\n            \"specification\": {\n                \"MIN_SEQ_LEN\": 200,\n                \"MAX_SEQ_LEN\": 400,\n                \"INPUT_DIM\": 2000,\n                \"EXTREMITY_SCORE\": true,\n                \"INTERVAL_DATA\": true\n            },\n            \"description\": \"Dataset with historical minutely data, extreme weather, socio-economic factors, and 2000 features per entry with sequence lengths of 200-400.\",\n            \"preprocessing\": [\"Data normalization, outlier detection\"],\n            \"augmentation\": [\"Temporal shift, time-series resampling\"],\n            \"visualization\": [\"Sequence patterns, feature importance\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"Advanced time-series forecasting model\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_architecture\": \"Custom deep learning with RNN or LSTM, Transformer-based\",\n                \"parameters\": {\n                    \"residual connections\": true,\n                    \"attention mechanisms\": true\n                }\n            },\n            \"description\": \"A deep learning model for real-time renewable energy forecasting with RICI metric, flexibility, and grid resilience.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Energy storage integration strategies, regional policy analysis\",\n        \"Scalable infrastructure design for grid resilience\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"Integration of new data sources\"],\n        \"target_device\": \"real-time (cloud or edge)\",\n        \"deployment_endpoint\": \"A scalable and secure cloud infrastructure\",\n        \"inference_engine\": [\"Customized inference API for real-time updates\"],\n        \"demonstration\": \"Virtual or interactive dashboard showcasing predictive analytics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON structure captures the user's intent for a complex renewable energy project with multiple requirements. It accounts for model architecture, evaluation metric, and additional features like energy storage. The performance metrics and complexity constraints are placeholders for specific values, reflecting the need for extensive planning and evaluation. The 'expertise' field is marked as high due to the demand for in-depth strategies and real-time aspects. The missing values indicate details to be filled based on more specific instructions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "A sophisticated task requires the machine learning team to develop an arXiv-oriented algorithm using Hyperbolic Graph Convolutional Networks (HGNN), aiming for not just 0.90 mean average precision in identifying authors' subfields within condensed matter physics, but also achieving state-of-the-art performance across various subdomains. The project must handle a massive, dynamically growing dataset while maintaining Explainable AI principles and eschewing heavy dependence on node degree calculations for enhanced interpretability and efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"academic research, specifically condensed matter physics\",\n        \"description\": \"Develop an arXiv-oriented algorithm utilizing Hyperbolic Graph Convolutional Networks (HGNN) for identifying authors' subfields within condensed matter physics. Aim for mean average precision of at least 0.90, and strive for state-of-the-art performance across multiple subdomains. The project should handle large, dynamically growing datasets and prioritize Explainable AI principles while minimizing dependency on node degree calculations for interpretability and efficiency.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"mean average precision\",\n                \"value\": 0.9\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"memory scalability\",\n                \"value\": null,\n                \"unit\": \"on large datasets\"\n            },\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"on large and dynamically growing data\"\n            },\n            {\n                \"name\": \"dependence on node degree calculations\",\n                \"value\": 0,\n                \"unit\": \"low or no dependence\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv condensed matter physics dataset\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"author subfields\"],\n            \"specification\": {\n                \"size\": \"massive and dynamically growing\",\n                \"dimensionality\": null\n            },\n            \"description\": \"A large and constantly expanding dataset of author connections and publications in condensed matter physics for training and evaluating the HGNN model.\",\n            \"preprocessing\": [\n                \"handle data growth\",\n                \"normalize timestamp for dynamic analysis\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HGNN (Hyperbolic Graph Convolutional Networks)\",\n            \"family\": \"graph neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"explainability\": \"high\",\n                \"dependence_on_node_degree\": \"minimal\"\n            },\n            \"description\": \"An advanced algorithm for author subfield classification using HGNN in the context of condensed matter physics, with a focus on Explainable AI and efficient node feature processing.\"\n        }\n    ],\n    \"knowledge\": [\n        \"State-of-the-art performance comparison against existing algorithms in subdomains\",\n        \"Explainability techniques for HGNN\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"interpretable feature selection\", \"graph feature extraction\"],\n        \"target_device\": \"cloud or high-performance computing\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for large-scale graph processing\"],\n        \"demonstration\": \"model interpretability through visualizations of feature importance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the sophisticated nature of the task with high-level requirements for performance, explainability, and scalability. The model name,HGNN, and the family of neural networks are clear indicators of the user's intention. While details such as exact performance benchmarks and specific feature engineering methods are not specified, they can be inferred from the user's instructions, reflecting a potential need for consulting or clarification.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "image classification", "instruction": "As the lead project manager for an upcoming international mobile app, we need to design a sophisticated AI-driven pet classification system. The application should instantly recognize various breeds, including rare and indigenous canines, with sub-second responsiveness, while maintaining a sleek interface. The development team must compare and contrast the performance of cutting-edge deep learning techniques, such as EfficientNet and MnasNet, alongside domain-specific fine-tuning using pre-trained models like InceptionV3 and MobileNetV2. The project should balance accuracy, computational efficiency, and power consumption to ensure optimal deployment on diverse hardware platforms with limited resources, taking into account scalability for future updates and security measures.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"mobile app, international\",\n        \"description\": \"Design a sophisticated AI-driven pet classification system for an international mobile app, recognizing rare and indigenous canines instantly with sub-second responsiveness. Compare and contrast performance of deep learning techniques like EfficientNet, MnasNet, InceptionV3, and MobileNetV2, while ensuring computational efficiency, power consumption for diverse hardware, and scalability for future updates. Incorporate security measures.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"average response time\",\n                \"value\": {\n                    \"min\": \"sub-second\",\n                    \"max\": \"sub-second\"\n                }\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"power consumption\",\n                \"value\": null,\n                \"unit\": \"Watts\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Various pet breeds dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"breed\"],\n            \"specification\": {\n                \"data_format\": \"RGB\",\n                \"size\": \"Variable (accounting for rare breeds)\"\n            },\n            \"description\": \"A comprehensive dataset containing images of various breeds of dogs, including rare and indigenous ones.\",\n            \"preprocessing\": [\"resize to common input dimensions\"],\n            \"augmentation\": [\n                \"random cropping\",\n                \"color jittering\"\n            ],\n            \"visualization\": [\"example image batches\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"EfficientNet vs. MnasNet vs. InceptionV3 vs. MobileNetV2\",\n            \"family\": [\"deep learning architectures\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_size\": {\n                    \"EfficientNet\": \"compact\",\n                    \"MnasNet\": \"efficient\",\n                    \"InceptionV3\": \"complex\",\n                    \"MobileNetV2\": \"lightweight\"\n                },\n                \"resource_consumption\": \"compare and contrast\"\n            },\n            \"description\": \"A comparative analysis of the提到的 deep learning models for pet classification, with a focus on efficiency.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"data_security\": \"follow industry-standard security measures for data protection\"\n        },\n        {\n            \"future_updates\": \"employ modular design for easy scalability\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"domain-specific fine-tuning\"],\n        \"target_device\": \"mobile devices with limited resources\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [\"CPU, GPU, and potentially dedicated AI accelerators\"],\n        \"demonstration\": \"user-friendly interface showcasing real-time breed classification\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's intent to build a complex AI-driven system. The provided expert level of expertise suggests a focus on detailed requirements. Performance metrics are given as general targets, indicating that accuracy and response times should be optimized. The dataset, model, and knowledge sections cover the specified aspects, including accuracy, efficiency, and security concerns. The service requirements account for the mobile app deployment and future updates.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "text classification", "instruction": "Our challenge is to design an advanced text classification system specifically tailored for a rare array of esoteric academic periodicals, diving deep into esoteric aspects of high-energy physics, particularly in string theory and quantum topology. We aim to leverage the sophisticated XQI-AN++ architecture, which necessitates not only a superior performance of 95%+ F1-score on a granular taxonomy of quantum gravity concepts and topological quantum computation algorithms, but also demonstrates self-evolution with dynamic precision over 97% even in resource-constrained environments. The model must integrate a cutting-edge explainability module, generating real-time, human-understandable explanations for rigorous expert evaluation and validation, while preserving model transparency and accountability.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"text classification\",\n        \"application_domain\": \"esoteric academic periodicals\",\n        \"description\": \"Design an advanced text classification system for rare esoteric academic journals focused on high-energy physics (string theory and quantum topology). Use XQI-AN++ architecture, aiming for a F1-score of 95%+ for quantum gravity concepts and topological quantum computation algorithms, with self-evolution and dynamic precision >97% in resource-constrained environments. Require a cutting-edge explainability module for real-time, human-understandable explanations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.95\n            },\n            {\n                \"name\": \"Dynamic precision\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"unit\": \"ms\",\n                \"value\": null\n            },\n            {\n                \"name\": \"model explainability\",\n                \"unit\": \"%\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Esoteric High-Energy Physics Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"quantum gravity concepts\", \"topological quantum computation algorithms\"],\n            \"specification\": {\n                \"size\": null,\n                \"dimension\": null,\n                \"length\": null\n            },\n            \"description\": \"A specialized corpus containing esoteric articles from high-energy physics journals (string theory and quantum topology).\",\n            \"preprocessing\": [],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"infer-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"XQI-AN++\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"XQI-AN++\",\n                \"resource_constraints\": null\n            },\n            \"description\": \"Advanced text classification model using XQI-AN++ adapted for esoteric academic periodicals with explainability and dynamic precision features.\"\n        }\n    ],\n    \"knowledge\": [\n        \"The target domain is highly specialized, requiring expertise in string theory and quantum topology.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"explainability enhancement\"\n        ],\n        \"target_device\": \"resource-constrained environments\",\n        \"deployment_endpoint\": null,\n        \"inference_engine\": [],\n        \"demonstration\": \"rigorous expert evaluation and validation\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been parsed to capture the detailed requirements, including a specialized text classification system, target metrics, architecture, and explainability needs. The user's high expertise level indicates they expect in-depth knowledge and attention to esoteric details. However, specific model architecture and feature details are not explicitly provided, requiring a deeper understanding of the XQI-AN++ and the niche corpus to fill these gaps.\",\n        \"score\": 0.85\n    }\n}\n"}
{"task": "machine translation", "instruction": "As a pioneer in global e-commerce, your challenge is to design a sophisticated, scalable real-time translation system that bridges Korean and English for an expanding user base. Assess not just Transformer-XL and M-BART, but also novel models integrating BERT-Transformer and Cross-lingual ALBERT, emphasizing context-aware translation, domain-specific enhancements in fashion, tech, and lifestyle, alongside models' adaptability to regional dialects and colloquialisms. Conduct an extensive evaluation, focusing on algorithmic efficiency, response latency, and preserving cultural subtleties, all while optimizing for a seamless, high-accuracy user experience through extensive testing on a diverse, manually curated parallel corpus.", "resposne": "{\n  \"user\": {\n    \"intent\": \"build\",\n    \"expertise\": \"high\"\n  },\n  \"problem\": {\n    \"area\": \"natural language processing\",\n    \"downstream_task\": \"machine translation\",\n    \"application_domain\": \"e-commerce\",\n    \"description\": \"Design a real-time translation system for Korean to English, integrating Transformer-XL, M-BART, BERT-Transformer, and Cross-lingual ALBERT, with a focus on context-awareness, domain-specific enhancements (fashion, tech, and lifestyle), and adaptability to regional dialects. Evaluate the system on algorithmic efficiency, response latency, and cultural subtlety preservation, targeting a seamless, high-accuracy user experience.\",\n    \"performance_metrics\": [\n      {\n        \"name\": \"algorithmic efficiency\",\n        \"value\": null\n      },\n      {\n        \"name\": \"response latency\",\n        \"value\": null\n      },\n      {\n        \"name\": \"cultural subtlety preservation\",\n        \"value\": null\n      }\n    ],\n    \"complexity_metrics\": [\n      {\n        \"name\": \"inference time\",\n        \"value\": null,\n        \"unit\": \"milliseconds\"\n      },\n      {\n        \"name\": \"memory usage\",\n        \"value\": null,\n        \"unit\": \"MB\"\n      }\n    ]\n  },\n  \"dataset\": [\n    {\n      \"name\": \"parallel corpus for Korean and English\",\n      \"modality\": [\"text\"],\n      \"target_variables\": [\"translated text\"],\n      \"specification\": {\n        \"size\": null,\n        \"contextual_length\": null,\n        \"domain_diversity\": \"Fashion, Tech, Lifestyle\"\n      },\n      \"description\": \"A manually curated dataset for evaluating translation models in a variety of contexts and domains.\",\n      \"preprocessing\": [\"context-aware data augmentation\"],\n      \"augmentation\": [\"domain-specific text synthesis\"],\n      \"visualization\": [\"model performance heatmaps\"],\n      \"source\": \"user-link\"\n    }\n  ],\n  \"model\": [\n    {\n      \"name\": \"Transformer-XL\",\n      \"family\": \"Transformer-based models\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"contextual_length\": null,\n        \"parameters\": null\n      },\n      \"description\": \"A model incorporating long-context dependency for translation between Korean and English.\"\n    },\n    {\n      \"name\": \"M-BART\",\n      \"family\": \"Transformer sequence-to-sequence models\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"model_size\": null,\n        \"architecture\": \"Encoder-Decoder architecture\"\n      },\n      \"description\": \"A pre-trained model for multilingual text generation, including translation.\"\n    },\n    {\n      \"name\": \"BERT-Transformer\",\n      \"family\": \"BERT and Transformers\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"adapter_layers\": null,\n        \"contextualization\": true\n      },\n      \"description\": \"A hybrid model using BERT for context understanding and Transformer for translation.\"\n    },\n    {\n      \"name\": \"Cross-lingual ALBERT\",\n      \"family\": \"Cross-lingual transformers\",\n      \"type\": \"neural networks\",\n      \"specification\": {\n        \"domain_adaptation\": true,\n        \"specialization\": \"Korean-English\"\n      },\n      \"description\": \"A model fine-tuned for context-aware cross-lingual adaptation, focusing on regional dialects.\"\n    }\n  ],\n  \"service\": {\n    \"feature_engineering\": [\"contextual embeddings\", \"domain-specific tuning\"],\n    \"target_device\": \"cloud-based\",\n    \"deployment_endpoint\": null,\n    \"inference_engine\": [\"GPU parallel processing\"],\n    \"demonstration\": \"user-friendly UI for real-time translation\"\n  },\n    \"confidence\": {\n      \"rationale\": \"The JSON reflects the user's instruction by specifying a complex project to design a real-time translation system with multiple models, evaluations on algorithm efficiency, response time, and cultural nuances. The application domain, user expertise, and the focus on regional dialects are captured. However, certain details like model specifications, latency, and efficiency values, are left unspecified, assuming they require further exploration or are to be determined by expert analysis.\",\n      \"score\": 0.85\n    }\n  }\n"}
{"task": "time-series anomaly detection", "instruction": "\"Design and engineer a cutting-edge, scalable real-time anomaly detection system for 'AviationMaintenancePrognostics' with state-of-the-art deep LSTM networks. The system must handle multivariate, high-dimensional sensor data from diverse aircraft models, accounting for temporal dependencies, seasonality, and extreme weather events. Implement a reinforcement learning algorithm to optimize maintenance schedules proactively, ensuring predictive accuracy within stringent safety margins (less than 1% false positives), and reducing the total cost of ownership while minimizing passenger inconvenience due to unexpected inspections.\"", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"aviation\",\n        \"description\": \"Design a cutting-edge real-time anomaly detection system for AviationMaintenancePrognostics using deep LSTM networks. It should handle multivariate, high-dimensional sensor data from various aircraft models, account for temporal dependencies, seasonality, and extreme weather events.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"false positive rate\",\n                \"value\": 0.01\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"scalability\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"AviationMaintenancePrognostics\",\n            \"modality\": [\"time series\", \"multimodal\"],\n            \"target_variables\": [\"sensor data\", \"weather data\"],\n            \"specification\": {\n                \"data dimensions\": \"high\",\n                \"time-series length\": \"varied\"\n            },\n            \"description\": \"Multivariate data from diverse aircraft models with temporal dependencies, seasonality, and extreme weather events.\",\n            \"preprocessing\": [\n                \"data normalization\",\n                \"handling missing values\",\n                \"resampling\"\n            ],\n            \"augmentation\": [\"synthetic data generation\"],\n            \"visualization\": [\"anomaly patterns\"],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"deep LSTM network\",\n            \"family\": \"neural networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"LSTM\",\n                \"state-of-the-art\": true,\n                \"sensor input dimension\": \"high\",\n                \"temporal output\": true\n            },\n            \"description\": \"A cutting-edge real-time anomaly detection system using deep LSTM networks for aviation maintenance prognostics.\"\n        }\n    ],\n    \"knowledge\": [\n        \"use of reinforcement learning for proactive maintenance optimization\",\n        \"accounting for complex dependencies and seasonality\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"feature extraction from sensor and weather data\"\n        ],\n        \"target_device\": \"cloud for scalability\",\n        \"deployment_endpoint\": {\n            \"service\": \"real-time streaming\"\n        },\n        \"inference_engine\": [\"GPU-accelerated\"],\n        \"demonstration\": \"live demonstration of real-time detection and optimization\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's request for a complex anomaly detection system, utilizing LSTM networks for time-series analysis in the aviation context, with emphasis on scalability, real-time operation, and maintenance optimization. Specific performance and complexity requirements are captured, as well as the inclusion of reinforcement learning and account for various data aspects. Some metric values are left open for optimization, as indicated in the user's instructions.\",\n        \"score\": 0.98\n    }\n}\n"}
{"task": "object detection", "instruction": "Forge a highly advanced YOLOv5-based real-time object detection system tailored for the \"Adaptive Market Analytics Dataset\". This project mandate requires not only the identification of an expanded set of 200 unique product categories with precision and recall exceeding 97%, but also introduces additional challenges such as handling high-resolution images with dynamic resolutions up to 4K, maintaining sub-millisecond response times (i.e., under 1ms) for edge device deployment, and demonstrating robustness in low-light scenarios and complex multi-object occlusions. The system must also incorporate a novel adaptive learning mechanism to continually refine category boundaries with minimal human supervision.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"object detection\",\n        \"application_domain\": \"market analytics\",\n        \"description\": \"Forge a real-time YOLOv5-based object detection system for the Adaptive Market Analytics Dataset, targeting 200 unique product categories with precision and recall over 97%.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"precision\",\n                \"value\": 0.97\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 0.001,\n                \"unit\": \"ms\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Adaptive Market Analytics Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"product categories\"],\n            \"specification\": {\n                \"max_resolution\": \"4K\",\n                \"dynamic_resolutions\": true\n            },\n            \"description\": \"A dataset containing high-resolution images for object detection with varying complexities in low-light and multi-object occlusions.\",\n            \"preprocessing\": [\n                \"image resizing and augmentation for high-resolution images\"\n            ],\n            \"augmentation\": [\n                \"low-light simulation\",\n                \"occlusion synthesis\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"YOLOv5\",\n            \"family\": \"YOLO\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Adaptive YOLOv5\",\n                \"category_count\": 200,\n                \"accuracy_threshold\": null\n            },\n            \"description\": \"A YOLOv5-based object detection system specifically designed for the Adaptive Market Analytics Dataset with novel adaptive learning and edge deployment capabilities.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"text\": \"Adaptive learning and minimal human supervision to refine category boundaries\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"adaptation for edge devices\"],\n        \"target_device\": \"edge devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized YOLOv5 inference\"],\n        \"demonstration\": \"robustness in low-light and multi-object scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The parsed JSON is structured based on the user's specified requirements for the object detection project, including the YOLOv5 model, performance metrics, handling high-resolution images, response time, and the need for adaptability and low-light robustness. The user's high expertise level implies a comprehensive understanding of the project requirements. Some details could be further specified, but overall, the JSON provides a solid foundation for the machine learning project.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "Create a fault-tolerant, scalable deep learning platform that integrates a state-of-the-art, distributed LSTM model for accurate and multi-year energy consumption forecasts. The system must seamlessly incorporate real-time data from diverse meteorological sources, guaranteeing sub-100ms response time for next week's predictions, while maintaining >98% precision, interpretability, and resilience to anomalies in a distributed computing environment.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"energy management\",\n        \"description\": \"Create a fault-tolerant, scalable deep learning platform for energy consumption forecasts using distributed LSTM models. The system should integrate real-time meteorological data, aim for sub-100ms response time for next week's predictions, and maintain >98% precision, interpretability, and anomaly resilience in a distributed environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Response time\",\n                \"value\": 100,\n                \"unit\": \"ms\",\n                \"condition\": \"next week's predictions\"\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": 0.98\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"ms\"\n            },\n            {\n                \"name\": \"Resilience to anomalies\",\n                \"value\": \"guaranteed\"\n            },\n            {\n                \"name\": \"Scalability\",\n                \"value\": \"high\"\n            },\n            {\n                \"name\": \"Interpretability\",\n                \"value\": \"essential\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Real-time meteorological data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"energy consumption\", \"weather data\"],\n            \"specification\": {\n                \"data_source\": \"diverse\",\n                \"integration_frequency\": \"real-time\"\n            },\n            \"description\": \"Multi-year, real-time data from multiple meteorological sources required for accurate energy consumption forecasts.\",\n            \"preprocessing\": [\"anomaly detection\", \"data normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"data correlation analysis\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Distributed LSTM model\",\n            \"family\": \"Deep Learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model_structure\": \"state-of-the-art\",\n                \"distributed\": true\n            },\n            \"description\": \"A fault-tolerant, distributed LSTM model for energy consumption forecasting with focus on long-term and accurate predictions.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"resampling for multi-year data\", \"seasonality handling\"],\n        \"target_device\": \"distributed computing environment\",\n        \"deployment_endpoint\": \"unspecified\",\n        \"inference_engine\": [\"optimized distributed inference\"],\n        \"demonstration\": \"real-time anomaly detection and prediction demonstration\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been created based on the given instruction, including the user's intent to build a deep learning platform, the need for scalability, LSTM model, real-time data integration, and various performance and complexity metrics. The model, dataset, and preprocessing steps are designed according to the requirements and the target domain. Some details might be further elaborated or need clarification, such as deployment endpoint and specific feature engineering techniques.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "For a comprehensive research project on the intersection of conservation and AI, develop an advanced, scalable Vision Transformer (ViT-16+) tailored for classifying intricate details in high-resolution Landsat 8 satellite imagery. The model should not only distinguish among complex land cover categories like rare ecosystems, sensitive coastal zones, semi-arid expanses, and rapidly urbanizing landscapes, but also achieve a minimum F1-score of 97% on a meticulously curated, global-scale dataset with regional variations. Prioritize model efficiency for deployment on AWS Graviton, and conduct an exhaustive evaluation of inference performance, memory optimization, and the carbon footprint analysis, ensuring the system's efficacy and sustainability for long-term ecological monitoring.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"conservation and environmental monitoring\",\n        \"description\": \"Develop an advanced, scalable Vision Transformer (ViT-16+) for classifying intricate details in high-resolution Landsat 8 satellite imagery, targeting rare ecosystems, sensitive coastal zones, semi-arid expanses, and rapidly urbanizing landscapes.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1-score\",\n                \"value\": 0.97\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"milliseconds\"\n            },\n            {\n                \"name\": \"memory optimization\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"carbon footprint analysis\",\n                \"value\": null,\n                \"unit\": \"kgCO2e\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Global-scale Landsat 8 satellite imagery\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"land cover categories\"],\n            \"specification\": {\n                \"resolution\": {\n                    \"width\": null,\n                    \"height\": null\n                },\n                \"size\": {\n                    \"total_images\": null,\n                    \"image_variations\": \"regionally diversified\"\n                }\n            },\n            \"description\": \"A comprehensive, curated dataset capturing intricate land cover details with regional variations.\",\n            \"preprocessing\": [\"georectification\", \"image normalization\"],\n            \"augmentation\": [],\n            \"visualization\": [\"spectral analysis\", \"heatmaps\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"ViT-16+ for land cover classification\",\n            \"family\": \"Vision Transformer\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": {\n                    \"num_layers\": 16,\n                    \"patch_size\": 16\n                },\n                \"params\": {\n                    \"efficiency\": \"optimized for AWS Graviton\"\n                }\n            },\n            \"description\": \"Advanced transformer model tailored for Landsat 8 imagery classification in diverse ecological contexts.\"\n        }\n    ],\n    \"knowledge\": [\n        \"AWS Graviton architecture and compatibility\",\n        \"Energy consumption and carbon footprint calculations for AWS\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"feature extraction\", \"dimensionality reduction\"],\n        \"target_device\": \"AWS Graviton\",\n        \"deployment_endpoint\": \"AWS platform or Graviton-specific service\",\n        \"inference_engine\": [\"AWS Deep Learning Inference\"],\n        \"demonstration\": \"long-term ecological monitoring and interpretative visualizations\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers the user's requirements for building a specialized ViT model, the target dataset, and performance evaluations while accounting for the user's expertise level. The user's request for efficiency on AWS Graviton and carbon footprint analysis are incorporated, providing a well-rounded approach to the project. However, the exact values for complexity metrics are not provided as they would require further information to determine.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "node classification", "instruction": "The project manager is now mandated to design an advanced node classification system employing a hybrid HGNN architecture, which addresses the complexities of the dynamically evolving Coauthorship Science dataset. They must handle data scarcity due to imbalances, incorporate temporal dependencies using multi-scale temporal convolutions, and ingeniously integrate sparse metadata from different sources to enhance not only domain distinction but also trace the nuanced influence of cooperation on research output over long periods. Additionally, the model's robustness should be evaluated using Receiver Operating Characteristic (ROC) curves and F1-score for fine-grained authorship attribution.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"scientific collaboration\",\n        \"description\": \"Design an advanced node classification system using a hybrid HGNN architecture for the Coauthorship Science dataset. The system must handle data scarcity, incorporate temporal dependencies with multi-scale temporal convolutions, and integrate sparse metadata to enhance authorship distinction and trace long-term cooperation effects. Model evaluation should include ROC curves and F1-score for fine-grained authorship attribution.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"ROC curves\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Coauthorship Science\",\n            \"modality\": [\"graph\"],\n            \"target_variables\": [\"authorship\", \"temporal information\", \"metadata\"],\n            \"specification\": {\n                \"imbalance\": true,\n                \"sparse_data\": true\n            },\n            \"description\": \"A dynamically evolving graph dataset representing scientific collaborations, incorporating temporal dependencies and sparse metadata for node classification.\",\n            \"preprocessing\": [\n                \"Handling data imbalance\",\n                \"Incorporating sparse metadata\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"HybridHGNN\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"hybrid HGNN\",\n                \"temporal_convolutions\": \"multi-scale\"\n            },\n            \"description\": \"An advanced graph neural network with a hybrid architecture for node classification in the Coauthorship Science dataset, designed to handle evolving complexities and multi-source metadata integration.\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            \"temporal feature engineering\",\n            \"metadata feature extraction\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"fine-grained authorship attribution\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the requirements and complexities of the project. The user's high expertise level is reflected in the provided instruction. The model and dataset details are aligned with the hybrid HGNN approach, multi-scale temporal convolutions, and metadata integration. The performance metrics, such as ROC curves and F1-score, are specified, and the handling of data scarcity and sparse data sources is noted. The user-defined evaluation method of model robustness and fine-grained attribution is captured. Overall, the response provides a comprehensive representation of the user's requirements.\",\n        \"score\": 0.95\n    }\n}"}
{"task": "image classification", "instruction": "As a project manager for an advanced AI lab, devise a multifaceted feline image classification initiative. Specify requirements for a cutting-edge deep learning framework incorporating transfer learning, attention mechanisms, and fine-grained feature extraction. Also, detail a robust data preprocessing pipeline addressing illumination, occlusions, and resolution inconsistencies, ensuring the ensemble model can distinguish among rare and cryptic subspecies with exceptional accuracy.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"artificial intelligence research\",\n        \"description\": \"Design a multifaceted feline image classification project using a cutting-edge deep learning framework with transfer learning, attention mechanisms, and fine-grained feature extraction for exceptional accuracy, especially for rare and cryptic subspecies. Requirements include a robust data preprocessing pipeline addressing illumination, occlusions, and resolution inconsistencies.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Top-1 Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Fine-grained Accuracy\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference Time\",\n                \"value\": null,\n                \"unit\": \"seconds per image\"\n            },\n            {\n                \"name\": \"Memory Footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Feline Image Dataset\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"class labels\"],\n            \"specification\": {\n                \"image_size\": {\n                    \"width\": null,\n                    \"height\": null,\n                    \"aspect_ratio\": null\n                },\n                \"divisions\": {\n                    \"train\": null,\n                    \"validation\": null,\n                    \"test\": null\n                }\n            },\n            \"description\": \"A large-scale dataset containing diverse feline images with various illumination, occlusions, and resolution variations.\",\n            \"preprocessing\": [\n                \"Image resizing and normalization\",\n                \"Illumination and color correction\",\n                \"Occlusion removal or handling\",\n                \"Resolving resolution inconsistencies\"\n            ],\n            \"augmentation\": [\n                \"Data augmentation techniques (e.g., rotation, flipping, cropping)\"\n            ],\n            \"visualization\": [\n                \"Sample images from each class for validation and monitoring\"\n            ],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Feline Classification Model\",\n            \"family\": \"Deep Convolutional Neural Network\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"Incorporates transfer learning (e.g., from ImageNet)\",\n                \"Attention Mechanisms\": true,\n                \"Fine-Grained Feature Extraction Layers\": \"Specific layers for subspecies identification\",\n                \"Parameters\": {\n                    \"Number of Layers\": null,\n                    \"Depth\": null,\n                    \"Complexity Ratio\": null\n                }\n            },\n            \"description\": \"A highly advanced deep learning model tailored for feline image classification with specialized techniques.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Transfer learning speeds up convergence and reduces overfitting.\",\n        \"Attention mechanisms improve classification by focusing on informative image regions.\",\n        \"Fine-grained feature extraction is crucial for differentiating rare and cryptic subspecies.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Feature extraction from ResNet or VGG backbone\",\n            \"Fine-tuning with domain-specific data\"\n        ],\n        \"target_device\": \"Cloud-based GPU environments for efficient computations\",\n        \"deployment_endpoint\": \"Deployment on a centralized platform for easy access\",\n        \"inference_engine\": [\n            \"TensorFlow Serving\",\n            \"PyTorch INference\"\n        ],\n        \"demonstration\": \"Interactive dashboard showcasing classification results and model performance\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's high-level instructions, specifying a complex AI project with advanced techniques. The key aspects, such as the deep learning framework, data preprocessing, and requirements for accuracy in distinguishing rare subspecies, are included. However, exact metric values and model specifications are left unspecified to account for the project manager's need to make nuanced decisions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "As a seasoned project manager for the AI Time-Series Analytics Division, your team's mission is to develop a cutting-edge deep learning model for the 'ESC-50 heartbeat sound dataset', divided into train, validation, and unseen sub-sequences. Target the complex R-peak annotated signals with a tailored convolutional neural network (CNN) incorporating recurrent neural network (RNN) layers, specifically LSTMs, for temporal sequence learning. The model must demonstrate superior performance in identifying rare arrhythmias (Atrial Fibrillation) while exhibiting robustness against noise and phase variations. Additionally, focus on optimizing for inter-subject variability and achieve a minimum F1 score of 0.95 on unseen test data, ensuring the model's ability to generalize across diverse patient populations.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"healthcare\",\n        \"description\": \"Develop a deep learning model using a tailored CNN with RNN layers, specifically LSTMs, for the ESC-50 heartbeat sound dataset. The model should target complex R-peak annotated signals, focusing on identifying rare arrhythmias like Atrial Fibrillation. It must exhibit robustness against noise and phase variations, and optimize for inter-subject variability. The goal is to achieve at least an F1 score of 0.95 on unseen test data for generalization across diverse patient populations.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": 0.95\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"model robustness\",\n                \"value\": null\n            },\n            {\n                \"name\": \"inter-subject variability\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"ESC-50 heartbeat sound dataset\",\n            \"modality\": [\"audio\", \"time series\"],\n            \"target_variables\": [\"R-peak annotated signals\"],\n            \"specification\": {\n                \"split\": [\"train\", \"validation\", \"unseen\"],\n                \"sequence_length\": null,\n                \"sample_rate\": null\n            },\n            \"description\": \"A time-series dataset containing heartbeat sounds with R-peak annotations, including the 'Atrial Fibrillation' category.\",\n            \"preprocessing\": [\n                \"noise reduction\",\n                \"phase normalization\"\n            ],\n            \"augmentation\": [\n                \"temporal shift\",\n                \"additive white Gaussian noise\"\n            ],\n            \"visualization\": [\n                \"Signal spectrograms\"\n            ],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"CNN-RNN (LSTM)\",\n            \"family\": \"deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"CNN with LSTM layers\",\n                \"input_shape\": null,\n                \"output_shape\": null,\n                \"training_details\": \"focusing on temporal sequence learning\"\n            },\n            \"description\": \"A deep learning model designed to classify heartbeat sounds with R-peak annotations, particularly targeting Atrial Fibrillation with robustness to noise and phase variations.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Inter-subject variability: handling diverse patient populations\",\n        \"Rare arrhythmia identification: optimizing for Atrial Fibrillation detection\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"extracting relevant time-domain and frequency-domain features\"],\n        \"target_device\": \"cloud for efficient GPU usage\",\n        \"deployment_endpoint\": \"AI-powered heartbeat analysis platform\",\n        \"inference_engine\": [\"TensorFlow or PyTorch\"],\n        \"demonstration\": \"Interactive demonstration for healthcare professionals\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately represents the user's instruction, detailing the target dataset, task, performance requirements, and model specifics. The user's high expertise level is captured, and there are specified areas for optimizing model robustness, inter-subject variability, and F1 score. The JSON omits specific metric values as they were not included in the instruction.\",\n        \"score\": 0.98\n    }\n}"}
{"task": "node classification", "instruction": "As a seasoned project manager for an elite AI research team, we have been commissioned to delve into a multifaceted node classification problem using the intricate arXiv dataset from frontier high-energy physics. The task now necessitates not only discerning theoretical papers from experimental reports but also discerning and classifying subgenres within theoretical physics, such as advanced topics like Loop Quantum Gravity and M-theory. To conquer this challenge, we need to design a highly sophisticated Graph Neural Network architecture, which integrates feature engineering techniques for sparse and intricately woven citation graphs. Moreover, we must prioritize balanced accuracy, optimizing for both precision and recall, and anticipate the potential improvement in F1 score, taking into account the model's adaptability and robustness in a dynamically evolving field. Can you detail the comprehensive data preprocessing pipeline, propose a customized network blueprint, and forecast a realistic F1 score range that can be achieved with this groundbreaking strategy?", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"graph machine learning\",\n        \"downstream_task\": \"node classification\",\n        \"application_domain\": \"high-energy physics\",\n        \"description\": \"Design a Graph Neural Network for the arXiv dataset, differentiating between theoretical papers, experimental reports, and classifying subgenres within theoretical physics like Loop Quantum Gravity and M-theory. Focus on feature engineering for sparse citation graphs, balancing precision and recall with balanced accuracy, and target F1 score optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"F1 score\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"arXiv dataset (high-energy physics)\",\n            \"modality\": [\"graph\", \"text\"],\n            \"target_variables\": [\"paper type\", \"subgenre\"],\n            \"specification\": {\n                \"graph_size\": \"large\",\n                \"edge_density\": \"sparse\",\n                \"textual_data_length\": \"variable\"\n            },\n            \"description\": \"A dataset containing high-energy physics papers with intricate citation graphs and text data.\",\n            \"preprocessing\": [\n                \"cleaning and standardization\",\n                \"node and edge feature extraction from text\",\n                \"graph normalization\"\n            ],\n            \"augmentation\": [\n                \"graph sampling techniques for data imbalance\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Custom GNN\",\n            \"family\": \"Graph Neural Networks\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"combining Graph Convolution Networks (GCN) and attention mechanisms\",\n                \"sparse connectivity handling\": \"efficient graph algorithms for sparse graphs\"\n            },\n            \"description\": \"A graph neural network designed to handle sparse, intricately woven citation graphs, integrating feature engineering for subgenre classification.\"\n        }\n    ],\n    \"knowledge\": [\n        \"In dynamic high-energy physics, models should be adaptable and robust to new findings.\",\n        \"Possible data imbalance between theoretical and experimental papers\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"Node and edge embeddings\",\n            \"Graph-level features\"\n        ],\n        \"target_device\": \"high-performance server\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"Apache Spark GraphX\", \"DGL\"],\n        \"demonstration\": \"Produce visual and performance-based results for incremental improvements\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's detailed request, including expert-level expertise, the complex arXiv dataset, GNN architecture, feature engineering, and performance metric expectations. The absence of specific values or thresholds for F1 score and response time acknowledges the need for forecasting and optimized adaptability. However, additional details may be required, like specific nodes in the dataset, server deployment requirements, or demonstration methods.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As the director of a leading ML team, devise a highly sophisticated time-series forecasting framework for a complex solar energy project. In addition to meticulously handling hourly data with extreme granularity for seasonality, long-term trends, and unforeseen anomalies, mandate the implementation of an advanced model that seamlessly fuses live meteorological forecasts, predictive maintenance algorithms, and dynamic updating, aiming for a minimum error rate of 5% in 7-day energy yield predictions, while ensuring real-time adaptability and system resilience.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"solar energy\",\n        \"description\": \"Design a sophisticated time-series forecasting framework for a solar energy project, handling hourly data with extreme granularity for seasonality, long-term trends, and anomaly detection, incorporating live meteorological forecasts, predictive maintenance algorithms, and dynamic updating. Aim for a minimum 7-day energy yield prediction error rate of 5% while ensuring real-time adaptability and system resilience.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"error rate\",\n                \"value\": 0.05,\n                \"unit\": \"%\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"real-time adaptability\",\n                \"value\": null\n            },\n            {\n                \"name\": \"system resilience\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Solar Energy Project hourly data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"energy yield\"],\n            \"specification\": {\n                \"granularity\": \"hourly\"\n            },\n            \"description\": \"Hourly data with comprehensive seasonality, long-term trends, and anomalies for the solar energy project.\",\n            \"preprocessing\": [\n                \"handling seasonality\",\n                \"extracting long-term trends\"\n            ],\n            \"augmentation\": [\n                \"incorporating meteorological forecasts\"\n            ],\n            \"visualization\": [],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Advanced Time-Series Forecasting Framework\",\n            \"family\": \"fusion of ML models\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"components\": [\n                    {\n                        \"name\": \"Meteo Integration\",\n                        \"description\": \"Advanced live meteorological forecasts\"\n                    },\n                    {\n                        \"name\": \"Predictive Maintenance\",\n                        \"description\": \"Dynamic update with predictive maintenance algorithms\"\n                    }\n                ],\n                \"framework\": \"Real-time adaptable and resilient\"\n            },\n            \"description\": \"A deep learning model that combines meteorological forecasts, predictive maintenance, and dynamic updating for time-series forecasting in solar energy.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Live meteorological data source integration\",\n        \"Best practices for real-time anomaly detection\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"integration of meteorological data\",\n            \"dynamic forecasting adjustments\"\n        ],\n        \"target_device\": \"cloud or IoT infrastructure\",\n        \"deployment_endpoint\": \"specific solar energy management system\",\n        \"inference_engine\": [\n            \"real-time computation and adaptability\"\n        ],\n        \"demonstration\": \"prediction visualizations and system resilience scenarios\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The response covers the given complex task by specifying the high-level intent, the user's expertise, the relevant area, the specific performance metric (error rate), and the requirements for model fusion, dynamic updates, and system adaptability. The datasets, model, and additional knowledge points are derived directly from the user's instruction. Missing values, such as complexity metrics, are kept null to indicate that specific optimizations are targeted. The overall response is comprehensive and tailored to the user's expectations.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "For the ambitious feline taxonomic exploration project, design an innovative deep learning architecture, FusionVet-4, that harmoniously integrates Inceptionv5-Plus and ResNeXt-Enhanced-Advanced. This model should decipher subtle genetic variations among Felidae subfamilies, perform multi-resolution spectroscopic analysis on fur patterns' subatomic textures, and refine its understanding through transfer learning using a massive and exclusive ImageNet-PlusExt dataset. Implement an advanced augmentation strategy, HyperDiversityGAN, to enrich data with rare breed occlusions and employ a hybrid loss function, EntropyBoostedLoss, that integrates Focal Loss, Class Weighted AUC, and an evolutionary component to handle the extraordinary class imbalance in a prestigious collection of ultra-high definition, high-resolution cat images spanning 100 diverse breeds.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"zoology or wildlife\",\n        \"description\": \"Design an innovative deep learning architecture, FusionVet-4, integrating Inceptionv5-Plus and ResNeXt-Enhanced-Advanced for taxonomic exploration in Felidae subfamilies. Model should perform multi-resolution spectroscopic analysis and use transfer learning with ImageNet-PlusExt. Aim for handling rare breed data augmentation through HyperDiversityGAN and balance class imbalance with EntropyBoostedLoss (Focal Loss, Class Weighted AUC, and evolutionary component). Dataset: high-resolution images of 100 diverse cat breeds with extraordinary class imbalance.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"exclusive ImageNet-PlusExt\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"genetic variations, fur patterns\", \"cat breeds\"],\n            \"specification\": {\n                \"image_size\": { \"height\": null, \"width\": null },\n                \"number_of_classes\": 100\n            },\n            \"description\": \"A massive dataset with high-resolution images, high-definition and 100 diverse breeds, for spectroscopic analysis and cat taxonomic research.\",\n            \"preprocessing\": [\"HyperDiversityGAN augmentation\"],\n            \"augmentation\": [\"rare breed occlusions\"],\n            \"visualization\": [\"spectroscopic analysis of fur patterns\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"FusionVet-4\",\n            \"family\": [\"Inceptionv5-Plus\", \"ResNeXt-Enhanced-Advanced\"],\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture_details\": \"Integrating two models and multi-resolution analysis\",\n                \"transfer_learning_dataset\": \"ImageNet-PlusExt\"\n            },\n            \"description\": \"A deep learning model for feline taxonomy, focusing on subtle genetic variations and multi-resolution spectroscopic analysis.\"\n        }\n    ],\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the core aspects of the user's instruction. It details the deep learning project's objective, dataset requirements, and desired model characteristics. Performance metrics are placeholders for actual performance values, and complexity metrics are similarly left open for quantitative targets. The model requirements, like the integration of Inceptionv5-Plus and ResNeXt-Enhanced-Advanced, are incorporated. However, for a full confidence score of 1.0, more specific details would be needed, such as exact model architecture dimensions, specific implementation details, and more concrete performance targets.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "image classification", "instruction": "As a visionary AI strategist, you are tasked with leading a groundbreaking project for a unique conservation organization. Your challenge is to develop a state-of-the-art image classification model that can discern not only the 100 obscure sub-species of critically endangered felids but also detect their distinct behavioral traits from a colossal dataset characterized by complex lighting conditions and surreal backgrounds. Mandate the fusion of a cutting-edge, rare architecture called \"HoloCatNet,\" which integrates transfer learning from neuron-optimized celestial feline cells, with a bio-inspired data augmentation method that seamlessly blends live action sequences with rare GAN-generated environments, ensuring exceptional accuracy on minimal annotations while preserving the authenticity of the feline dynamics.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"computer vision\",\n        \"downstream_task\": \"image classification\",\n        \"application_domain\": \"conservation science\",\n        \"description\": \"Develop a state-of-the-art image classification model for discerning 100 obscure sub-species of critically endangered felids, while also identifying their unique behavioral traits. The dataset features complex lighting conditions and surreal backgrounds. The model must utilize HoloCatNet, a cutting-edge architecture with transfer learning from neuron-optimized celestial feline cells, and incorporate bio-inspired data augmentation blending live action sequences with rare GAN-generated environments. The model should achieve high accuracy with minimal annotations and maintain authenticity of feline dynamics.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"specificity to obscure sub-species\",\n                \"value\": null\n            },\n            {\n                \"name\": \"recall for behavioral traits\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"memory usage\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"\",\n            \"modality\": [\"image\"],\n            \"target_variables\": [\"sub-species\", \"behavioral traits\"],\n            \"specification\": {\n                \"complexity\": {\n                    \"lighting conditions\": \"complex\",\n                    \"backgrounds\": \"surreal\"\n                },\n                \"size\": \"colossal\"\n            },\n            \"description\": \"Dataset with live action sequences and rare GAN-generated environments for 100 obscure felid sub-species, featuring diverse lighting conditions and surreal backgrounds.\",\n            \"preprocessing\": [\n                \"HoloCatNet transfer learning\",\n                \"bio-inspired data augmentation\"\n            ],\n            \"augmentation\": [\"GAN-generated environments\"],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"HoloCatNet\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"rare\",\n                \"transfer learning\": \"neuron-optimized celestial feline cells\"\n            },\n            \"description\": \"A cutting-edge model combining HoloCatNet architecture and transfer learning for high accuracy on minimal annotations.\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"description\": \"Use of HoloCatNet and bio-inspired data augmentation techniques to capture authenticity of feline dynamics.\"\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": \"preserving authenticity of feline dynamics\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response captures the user's high-level directive to build a sophisticated classification model while addressing the specific requirements for the dataset, including complexity, augmentation, and performance metrics. Some details remain ambiguous, such as the exact model name and performance values, but the structure is valid and comprehensive given the information provided.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "machine translation", "instruction": "Design and enhance a machine translation system utilizing a hybrid model of M6 Transformer and Transformer-XL, which seamlessly integrates an adaptive context-aware mechanism with Long Short-Term Memory (LSTM) units. The objective is to achieve a highly specialized bilingually weighted METEOR score of 45 in translation tasks for under-studied linguistic pairs, particularly focusing on obscure technical jargon and regional vernacular from the recently compiled Qantum Corpus. This system must maintain native-like fluency while operating within an ultra-realtime constraint of 50 milliseconds, optimizing for a custom energy-efficient hybrid architecture specifically tailored for a scalable, high-traffic platform catering to simultaneous live conversations in multiple industries, while ensuring near-zero errors during peak loads on resource-constrained devices.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"machine translation\",\n        \"application_domain\": \"linguistics and industry-specific communication\",\n        \"description\": \"Design an enhanced machine translation system using a hybrid M6 Transformer and Transformer-XL model with adaptive context-aware mechanism and LSTM units. Focus on achieving a specialized bilingually weighted METEOR score of 45, particularly for under-studied linguistic pairs with obscure technical jargon and regional vernacular from the Quantum Corpus. The system must maintain native-like fluency and operate within 50 milliseconds while optimizing for an energy-efficient, scalable architecture suitable for live conversations across multiple industries on resource-constrained devices.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"bilingually weighted METEOR score\",\n                \"value\": 45.0\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": 50.0,\n                \"unit\": \"milliseconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Quantum Corpus\",\n            \"modality\": [\"text\"],\n            \"target_variables\": [\"translated text\"],\n            \"specification\": {\n                \"coverage\": \"under-studied linguistic pairs with technical jargon and regional vernacular\"\n            },\n            \"description\": \"A dataset containing source and target text pairs, specifically compiled to address obscure technical terms and regional variations in the Quantum field.\",\n            \"preprocessing\": [\"customized preprocessing for quantum jargon and regional dialect\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Hybrid M6 + Transformer-XL with Adaptive Context-aware Mechanism and LSTM\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"custom energy-efficient hybrid architecture\",\n                \"application\": \"high-traffic, ultra-realtime for live conversations\",\n                \"device compatibility\": \"resource-constrained\"\n            },\n            \"description\": \"A specialized model combining the strengths of M6 Transformer and Transformer-XL with LSTM units for context-aware translation, targeting energy efficiency and scalability.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Quantum Corpus challenges in handling linguistic nuances and specialized vocabulary\",\n        \"Necessity of optimizing for energy-efficient and resource-constrained devices\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"customized for quantum jargon and regional dialect\"],\n        \"target_device\": \"ultra-realtime and resource-constrained devices\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"optimized for energy efficiency\"],\n        \"demonstration\": \"live translation during peak loads with near-zero errors\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response is structured based on the given instruction. It highlights the need for a specialized machine translation system, specific model architecture, performance requirements, and dataset characteristics. The user's high expertise level is reflected in the deep technical requirements. Some fields like deployment endpoint and specific details on energy optimization are less defined due to the nature of the problem description.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series anomaly detection", "instruction": "Design a robust, real-time time-series anomaly detection model for website traffic that specifically isolates hourly fluctuations by country, considering granular user behavior patterns such as clickstreams, session durations, and dwell times. The system must also incorporate advanced statistical techniques to distinguish transient spikes from periodic trends, ensuring accurate alerts for potential security threats and offering in-depth diagnostic reports to inform targeted optimization strategies.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series anomaly detection\",\n        \"application_domain\": \"web security and optimization\",\n        \"description\": \"Design a robust time-series model for real-time website traffic anomaly detection, isolating hourly fluctuations by country. The model must incorporate granular user behavior patterns like clickstreams, session durations, and dwell times, using advanced statistical techniques to differentiate transient spikes from periodic trends. It aims to identify potential security threats and provide diagnostic reports for targeted optimization.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Precision\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Recall\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Website traffic data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"country\", \"hourly traffic\", \"clickstreams\", \"session durations\", \"dwell times\"],\n            \"specification\": {\n                \"data_type\": [\"numerical\", \"categorical\"],\n                \"time_interval\": \"hourly\",\n                \"country_scope\": \"global\"\n            },\n            \"description\": \"Time series data including country-wise hourly traffic, user behavior patterns, and other granular data.\",\n            \"preprocessing\": [\"cleaning\", \"normalization\", \"feature extraction\"],\n            \"augmentation\": [\"seasonality adjustments\", \"smoothing techniques\"],\n            \"visualization\": [\"hourly trends per country\", \"user behavior patterns\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Anomaly Detection Model (Time Series)\",\n            \"family\": \"Neural Networks (LSTM, RNN) or Statistical Techniques (ARIMA, Seasonal ARIMA)\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"model architecture\": [\"hourly embeddings\", \"country embeddings\"],\n                \"forecast window\": \"1 hour\",\n                \"feature importance\": [\"clickstreams\", \"session durations\", \"dwell times\"],\n                \"anomaly detection algorithm\": \"Deep learning-based or statistical\"\n            },\n            \"description\": \"A real-time model using deep learning or statistical methods, designed for hourly country-specific website traffic anomalies detection.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Gaussian Mixture Models for clustering\",\n        \"Rolling window analysis for real-time monitoring\",\n        \"False Positive reduction techniques\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"user-based feature interactions\", \"dynamic thresholding\"],\n        \"target_device\": \"cloud for real-time processing\",\n        \"deployment_endpoint\": \"API for alerts and diagnostic reports\",\n        \"inference_engine\": [\"TensorFlow\", \"Apache Spark MLlib\"],\n        \"demonstration\": \"Interactive dashboard with live alerts and detailed reports\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON reflects the user's request for a real-time, country-specific website traffic anomaly detection model. The user's high expertise level suggests they understand the technical aspects. Performance metrics, model specifications, and service requirements are detailed to address the requirements of anomaly detection, statistical techniques, and reporting. Some metrics are left unspecified for optimization.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series classification", "instruction": "Elaborate on the time-series classification project for a cutting-edge industrial system, where data from smart sensors must undergo energy-efficient feature extraction, handle non-linear seasonality patterns, and justify model selections through feature importance analysis. Design a state-of-the-art model with exceptional interpretability and minimal latency (<10ms), ensuring it can operate in a dynamically evolving production environment with high throughput and efficiency.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series classification\",\n        \"application_domain\": \"industrial system\",\n        \"description\": \"Propose a time-series classification project for an advanced industrial system, involving smart sensors data. Focus on energy-efficient feature extraction, non-linear seasonality handling, and model selection with feature importance analysis. The model must be state-of-the-art, highly interpretable, and have minimal latency (<10ms) for high throughput and efficiency in a dynamically evolving production environment.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"F1-score\",\n                \"value\": null\n            },\n            {\n                \"name\": \"latency\",\n                \"value\": 0.01,\n                \"unit\": \"seconds\"\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"energy efficiency\",\n                \"value\": null,\n                \"unit\": \"FLOPs/Watt\"\n            },\n            {\n                \"name\": \"memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            },\n            {\n                \"name\": \"throughput\",\n                \"value\": null,\n                \"unit\": \"samples/second\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"smart sensors data\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"class label\"],\n            \"specification\": {\n                \"length\": null,\n                \"sampling_rate\": null,\n                \"dimensions\": null\n            },\n            \"description\": \"Data collected from advanced industrial sensors that are dynamic and diverse.\",\n            \"preprocessing\": [\"energy-efficient feature extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"non-linear seasonality analysis\"],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"State-of-the-art time-series classifier\",\n            \"family\": \"neural networks\",\n            \"type\": \"deep learning\",\n            \"specification\": {\n                \"interpretability\": \"high\",\n                \"latency_threshold\": 0.001,\n                \"complexity\": null\n            },\n            \"description\": \"A model that leverages state-of-the-art techniques for energy-efficient feature extraction, advanced in handling non-linear seasonality, and is designed with interpretability features.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Consider using architectures like Transformer or attention-based models to address non-linear seasonality.\",\n        \"Employ feature selection methods like LASSO or SHAP for interpretability and efficient feature importance analysis\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"efficient feature extraction\", \"handling dynamic patterns\"],\n        \"target_device\": \"low-latency industrial edge devices\",\n        \"deployment_endpoint\": \"real-time prediction service\",\n        \"inference_engine\": [\"optimized for industrial environment\"],\n        \"demonstration\": \"visualize model performance and interpretability over time\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response has been structured according to the user's requirements, including details on the industrial system, focus on efficiency, and interpretability. Performance metrics such as accuracy, F1-score, and latency are considered. Energy efficiency and throughput are included in complexity metrics. The dataset and model families are chosen based on the task of time-series classification. Specific details about feature extraction methods and model architecture hint at a well-rounded solution. However, some metric values are marked as null as they are not directly provided in the instruction.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "time-series forecasting", "instruction": "As a highly experienced project manager for an elite AI research and development team, your mission is to engineer a groundbreaking, adaptive time-series forecasting engine for the 'Atmospheric Phenomena Prognosticator (APP)'. This expansive dataset, boasting over a century of global climate data with complex, irregular sampling frequencies, has been meticulously stratified into five segments: a comprehensive historical training set (85% for deep model comprehension), a multifaceted validation suite for advanced hyperparameter optimization (10%), a dynamic early warning buffer (5%), a confidential extreme event investigation zone (5%), and a sensitive real-time adaptation partition (5%). Each input sequence reflects intricate long-term dynamics (500-step time windows, INPUT_SEQ_LEN), a multitude of interrelated dimensions (30 variables, INPUT_DIM), and nuanced indicators of impending catastrophes.\r\n\r\nYour team's paramount objective is to devise and execute a sophisticated deep learning architecture that not only minimizes Mean Squared Error (MSE) and Mean Absolute Error (MAE), but also introduces innovative metrics such as the Climate Disaster Identification Index (CDII) and the Temporal Resilience Score (TRS) to gauge the model's prowess in both predicting and mitigating rare climate hazards like Category 5 hurricanes, vortex tornados, and extreme temperature swings. Moreover, the model must demonstrate exceptional resistance to dynamic weather shifts (concept drift), calendar-based patterns (seasonality), and withstand real-world operational challenges, ensuring precise and reliable forecasts even in unprecedented meteorological conditions. In addition to technical excellence, provide a detailed plan for continuous monitoring, re-calibration, and benchmarking against state-of-the-art models in the field.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"time-series analysis\",\n        \"downstream_task\": \"time-series forecasting\",\n        \"application_domain\": \"climate science and AI research\",\n        \"description\": \"Engineer a groundbreaking, adaptive time-series forecasting engine for the Atmospheric Phenomena Prognosticator (APP) using a global climate dataset with over a century of data, irregular sampling frequencies, and stratified into multiple segments. Focus on deep learning architecture, minimizing MSE, MAE, introducing CDII and TRS metrics, and addressing dynamic weather shifts, seasonality, and operational challenges.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"MSE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"MAE\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Climate Disaster Identification Index (CDII)\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Temporal Resilience Score (TRS)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"dynamic weather shift resistance\",\n                \"value\": null\n            },\n            {\n                \"name\": \"seasonality handling\",\n                \"value\": null\n            },\n            {\n                \"name\": \"resistance to calendar-based patterns\",\n                \"value\": null\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Atmospheric Phenomena Prognosticator (APP) dataset\",\n            \"modality\": [\"time series\"],\n            \"target_variables\": [\"Global climate data\"],\n            \"specification\": {\n                \"segmentation\": [\n                    {\n                        \"name\": \"Historical training set\",\n                        \"percentage\": 85\n                    },\n                    {\n                        \"name\": \"Validation suite\",\n                        \"percentage\": 10\n                    },\n                    {\n                        \"name\": \"Early warning buffer\",\n                        \"percentage\": 5\n                    },\n                    {\n                        \"name\": \"Extreme event investigation\",\n                        \"percentage\": 5\n                    },\n                    {\n                        \"name\": \"Real-time adaptation\",\n                        \"percentage\": 5\n                    }\n                ],\n                \"time_window\": \"500-step time windows (INPUT_SEQ_LEN)\",\n                \"dimensions\": \"30 variables (INPUT_DIM)\",\n                \"complexity\": \"irregular sampling frequencies and meteorological hazards\"\n            },\n            \"description\": \"A dataset containing over a century of global climate data with varying sampling frequencies, stratified into segments for different stages of development and testing.\",\n            \"preprocessing\": [\n                \"Stratification into specified segments\"\n            ],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-upload\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"Adaptive Time-Series Forecasting Engine\",\n            \"family\": \"Deep learning\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"architecture\": \"custom, possibly incorporating transformers or recurrent neural networks\",\n                \"抵抗动态天气变化\": \"强调\",\n                \"处理季节性\": \"强调\",\n                \"应对业务挑战\": \"强调\"\n            },\n            \"description\": \"A deep learning model specifically designed for Atmospheric Phenomena Prognosticator's complex data, addressing performance metrics and addressing rare climate hazards\"\n        }\n    ],\n    \"knowledge\": [],\n    \"service\": {\n        \"feature_engineering\": [\n            {\n                \"name\": \"handling long-term dynamics\"\n            },\n            {\n                \"name\": \"incorporating multi-dimensional information\"\n            }\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [],\n        \"demonstration\": {\n            \"methodology\": \"continuous monitoring, re-calibration, and benchmarking\"\n        }\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response accurately captures the user's detailed requirements, highlighting the extensive nature of the project, performance metrics, and resistance to various challenges. The complexity metrics are present as points of focus. The user's high expertise level is reflected in the model architecture description. The only potential missing information could be specific details about the deep learning architecture or exact operational specifics, but these can be inferred based on the given instructions.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "tabular regression", "instruction": "As the project director of a groundbreaking AI research team, your mandate is to enhance a top-tier tabular regression project by tackling a challenging dataset - the expanded Mohs Hardness Dataset. This includes domain-specific variables, complex non-linear interactions, and diverse stratified subsets (80% train, 10% validation, and 10% test). The focus is to engineer intricate features, develop a novel regularization strategy that exploits multi-level interactions, and optimize for Precision@k to reduce the Mean Absolute Deviation (MAD) while maintaining high robustness. Your contributions must be clearly documented in a comprehensive, reproducible framework with step-by-step experimental design, including hyperparameter tuning using Bayesian optimization and feature selection techniques. Additionally, prepare a detailed comparison with state-of-the-art methods and justify the choice of evaluation metrics in the report.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"tabular data analysis\",\n        \"downstream_task\": \"tabular regression\",\n        \"application_domain\": \"material science\",\n        \"description\": \"Enhance a tabular regression project on the expanded Mohs Hardness Dataset, focusing on domain-specific variables, complex non-linear interactions, and stratified subsets (80% train, 10% validation, 10% test). Develop a novel regularization strategy, optimize for Precision@k to minimize Mean Absolute Deviation (MAD) with robustness, and include feature engineering, hyperparameter tuning with Bayesian optimization, and feature selection techniques.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Precision@k\",\n                \"value\": null\n            },\n            {\n                \"name\": \"Mean Absolute Deviation (MAD)\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"inference time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"Expanded Mohs Hardness Dataset\",\n            \"modality\": [\"tabular\"],\n            \"target_variables\": [\"hardness\"],\n            \"specification\": {\n                \"size\": {\n                    \"train\": 0.8,\n                    \"validation\": 0.1,\n                    \"test\": 0.1\n                },\n                \"dimension\": null,\n                \"length\": null\n            },\n            \"description\": \"A dataset for tabular regression with domain-specific variables and stratified subsets for train, validation, and test.\",\n            \"preprocessing\": [\"domain-specific feature engineering\", \"stratified sampling\"],\n            \"augmentation\": [],\n            \"visualization\": [],\n            \"source\": \"user-link\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"\",\n            \"type\": \"neural networks\",\n            \"specification\": {\n                \"parameters\": {\n                    \"multi-level interactions\": true,\n                    \"regularization\": \"novel\"\n                },\n                \"complexity\": {\n                    \"FLOPs\": null,\n                    \"memory footprint\": null\n                }\n            },\n            \"description\": \"An AI model designed for tabular regression on the expanded Mohs Hardness Dataset with feature engineering, novel regularization, and optimization for Precision@k and MAD.\"\n        }\n    ],\n    \"knowledge\": [\n        \"Diverse feature combinations, Bayesian optimization for efficient hyperparameter tuning, and feature selection for model simplicity.\",\n        \"Comparison with state-of-the-art methods using Precision@k and MAD for evaluation, considering model performance, robustness, and efficiency.\"\n    ],\n    \"service\": {\n        \"feature_engineering\": [\n            \"domain-specific feature engineering\",\n            \"multi-level interaction exploitation\"\n        ],\n        \"target_device\": \"\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\n            \"Bayesian optimization for hyperparameter tuning\"\n        ],\n        \"demonstration\": \"Comprehensive, reproducible framework with step-by-step experimental design and report.\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON response covers the user's instruction, with emphasis on enhancing a top-tier tabular regression project. It includes a clear problem description, requirements for dataset and model, and details on feature engineering, regularization, optimization methods, and evaluation metrics. High expertise level is assumed for handling complexity and state-of-the-art comparison. Some metric values are left unspecified to allow for optimization during the project.\",\n        \"score\": 0.95\n    }\n}\n"}
{"task": "question answering", "instruction": "For the AI-Med initiative, design a cutting-edge Dermatology Q&A system that not only employs the 'SkinDisorders-Comorbidity' dataset but also incorporates real-time patient feedback and comparison with competitor algorithms, while maintaining explainability, citing multiple PubMed articles per response, and adhering to strict cross-validation protocols.", "resposne": "{\n    \"user\": {\n        \"intent\": \"build\",\n        \"expertise\": \"high\"\n    },\n    \"problem\": {\n        \"area\": \"natural language processing\",\n        \"downstream_task\": \"question answering\",\n        \"application_domain\": \"healthcare, dermatology\",\n        \"description\": \"Design a Dermatology Q&A system using the 'SkinDisorders-Comorbidity' dataset, integrating real-time patient feedback and competitor algorithm comparison, with explainability and the ability to cite PubMed articles for each response.\",\n        \"performance_metrics\": [\n            {\n                \"name\": \"Accuracy\",\n                \"value\": null\n            },\n            {\n                \"name\": \"AUC-ROC\",\n                \"value\": null\n            }\n        ],\n        \"complexity_metrics\": [\n            {\n                \"name\": \"Response time\",\n                \"value\": null,\n                \"unit\": \"seconds\"\n            },\n            {\n                \"name\": \"Memory footprint\",\n                \"value\": null,\n                \"unit\": \"MB\"\n            }\n        ]\n    },\n    \"dataset\": [\n        {\n            \"name\": \"SkinDisorders-Comorbidity\",\n            \"modality\": [\"text\", \"tabular\"],\n            \"target_variables\": [\"disease entities\", \"comorbidity information\"],\n            \"specification\": {\n                \"dataset_size\": null,\n                \"dimensionality\": null,\n                \"length\": null\n            },\n            \"description\": \"A dataset for dermatology containing disease entities and comorbidity information for Q&A development.\",\n            \"preprocessing\": [\"PubMed article retrieval\", \"structured data extraction\"],\n            \"augmentation\": [],\n            \"visualization\": [\"model interpretability\"],\n            \"source\": \"direct-search\"\n        }\n    ],\n    \"model\": [\n        {\n            \"name\": \"\",\n            \"family\": \"neural networks or deep learning\",\n            \"type\": \"natural language processing\",\n            \"specification\": {\n                \"architecture\": \"explainable AI with interpretability layers\",\n                \"citation_approach\": \"incorporating PubMed articles for every response\"\n            },\n            \"description\": \"A Dermatology Q&A system leveraging cutting-edge techniques with explainability and PubMed citations\"\n        }\n    ],\n    \"knowledge\": [\n        {\n            \"PubMed_citations\": {\n                \"specific_citations\": [\n                    \"PubMed ID 1\",\n                    \"PubMed ID 2\",\n                    \"PubMed ID 3\"\n                ]\n            }\n        }\n    ],\n    \"service\": {\n        \"feature_engineering\": [\"patient feedback integration\", \"algorithm comparison mechanism\"],\n        \"target_device\": \"cross-validation compliant environment\",\n        \"deployment_endpoint\": \"\",\n        \"inference_engine\": [\"real-time processing\"],\n        \"demonstration\": \"explanatory and transparent Q&A responses\"\n    },\n    \"confidence\": {\n        \"rationale\": \"The JSON captures the core aspects of the user's requirements, including real-time patient feedback, competitor algorithm comparison, explainability, PubMed citations, and cross-validation. However, some specifics like dataset size, model architecture, and exact performance metrics are not provided as they would typically require more detailed specifications or benchmarks.\",\n        \"score\": 0.85\n    }\n}\n"}
