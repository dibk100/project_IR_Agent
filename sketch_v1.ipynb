{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e99872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 (no detections), 1.6ms\n",
      "Speed: 1.1ms preprocess, 1.6ms inference, 0.2ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Cable Block Diagram 자동 구조화 프로토타입\n",
    "# ========================================\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import json\n",
    "\n",
    "# ========================================\n",
    "# 1️⃣ PDF → 이미지 변환 (pdf2image 사용 가능)\n",
    "# ========================================\n",
    "# 여기서는 이미 이미지 파일로 변환했다고 가정\n",
    "image_path = \"./data/test_image.JPG\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# ========================================\n",
    "# 2️⃣ Text Detection (YOLOv8)\n",
    "# ========================================\n",
    "# pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "text_model = YOLO(\"yolov8n.pt\")  # pre-trained text detection\n",
    "text_results = text_model.predict(image)\n",
    "\n",
    "text_bboxes = []\n",
    "for r in text_results[0].boxes.xyxy:  # xyxy format\n",
    "    x1, y1, x2, y2 = r.tolist()\n",
    "    text_bboxes.append([int(x1), int(y1), int(x2), int(y2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "848e4b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-seg.pt to 'yolov8n-seg.pt': 100% ━━━━━━━━━━━━ 6.7MB 88.9MB/s 0.1s\n",
      "\n",
      "0: 448x640 2 refrigerators, 1 clock, 4.4ms\n",
      "Speed: 1.0ms preprocess, 4.4ms inference, 161.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Detected text bboxes: [[145, 31, 588, 756], [0, 30, 659, 752], [236, 91, 313, 160]]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# 1️⃣ 이미지 읽기\n",
    "image_path = \"./data/test_image.JPG\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# 2️⃣ Text Detection용 YOLO 모델 로드\n",
    "# yolov8n-text.pt : text detection용 pre-trained 모델\n",
    "text_model = YOLO(\"yolov8n-seg.pt\")\n",
    "\n",
    "# 3️⃣ Inference\n",
    "# conf 낮춰서 작은 글자도 잡히게 설정\n",
    "text_results = text_model.predict(source=image, conf=0.1, imgsz=640)\n",
    "\n",
    "# 4️⃣ Bounding box 추출\n",
    "text_bboxes = []\n",
    "for r in text_results[0].boxes.xyxy:  # xyxy format\n",
    "    x1, y1, x2, y2 = r.tolist()\n",
    "    text_bboxes.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "\n",
    "print(\"Detected text bboxes:\", text_bboxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdfc329e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[145, 31, 588, 756], [0, 30, 659, 752], [236, 91, 313, 160]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e71d2e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 3️⃣ OCR (TrOCR)\n",
    "# ========================================\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "ocr_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "ocr_texts = []\n",
    "for bbox in text_bboxes:\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    crop = image[y1:y2, x1:x2]\n",
    "    # TrOCR expects PIL image\n",
    "    from PIL import Image\n",
    "    crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    pixel_values = processor(images=crop_pil, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = ocr_model.generate(pixel_values)\n",
    "    text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    ocr_texts.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ea8db39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 640x416 1 refrigerator, 27.2ms\n",
      "Speed: 0.8ms preprocess, 27.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x576 (no detections), 11.9ms\n",
      "Speed: 1.1ms preprocess, 11.9ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 608x640 (no detections), 11.4ms\n",
      "Speed: 0.6ms preprocess, 11.4ms inference, 0.1ms postprocess per image at shape (1, 3, 608, 640)\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 4️⃣ Structure Detection (Boxes, Lines, Symbols)\n",
    "# ========================================\n",
    "# YOLO 모델 fine-tuned on diagram structures\n",
    "structure_model = YOLO(\"yolov8n-seg.pt\")  # BOX_SOLID, BOX_DASH, LINE, SYMBOL\n",
    "\n",
    "structure_results = []\n",
    "for bbox in text_bboxes:\n",
    "    # 주변 padding 영역\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    pad = 20\n",
    "    crop = image[max(0,y1-pad):y2+pad, max(0,x1-pad):x2+pad]\n",
    "    \n",
    "    results = structure_model.predict(crop)\n",
    "    for r in results[0].boxes.data:\n",
    "        x_s, y_s, x_e, y_e, conf, cls = r.tolist()\n",
    "        structure_results.append({\n",
    "            \"bbox\": [int(x_s)+x1-pad, int(y_s)+y1-pad, int(x_e)+x1-pad, int(y_e)+y1-pad],\n",
    "            \"class\": int(cls),\n",
    "            \"confidence\": float(conf)\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ee30652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5️⃣ Embedding Generation (Text + Structure)\n",
    "# ========================================\n",
    "# 여기는 placeholder, 실제로는 Transformer encoder 또는 CNN+Geom vector\n",
    "def generate_embedding(text, structures):\n",
    "    \"\"\"\n",
    "    text: string\n",
    "    structures: list of dict with bbox/class\n",
    "    return: np.array latent vector\n",
    "    \"\"\"\n",
    "    # 예: 단순히 길이 + bbox 통합 벡터\n",
    "    text_vec = np.array([len(text)])\n",
    "    struct_vec = np.array([len(structures)])\n",
    "    return np.concatenate([text_vec, struct_vec])\n",
    "\n",
    "embeddings = []\n",
    "for text, bbox in zip(ocr_texts, text_bboxes):\n",
    "    # text 주변 구조물 필터링\n",
    "    related_structs = [s for s in structure_results \n",
    "                       if (s['bbox'][0] >= bbox[0]-20 and s['bbox'][2] <= bbox[2]+20)]\n",
    "    embeddings.append(generate_embedding(text, related_structs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a44d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6️⃣ Embedding Inversion → Spec List\n",
    "# ========================================\n",
    "# placeholder: 실제 Transformer decoder 학습 필요\n",
    "spec_list = []\n",
    "for text, bbox, embedding in zip(ocr_texts, text_bboxes, embeddings):\n",
    "    spec_list.append({\n",
    "        \"text\": text,\n",
    "        \"bbox\": bbox,\n",
    "        \"related_structures\": [s for s in structure_results \n",
    "                               if (s['bbox'][0] >= bbox[0]-20 and s['bbox'][2] <= bbox[2]+20)],\n",
    "        \"embedding\": embedding.tolist()\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cadb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 구조화 JSON 출력 완료!\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 7️⃣ JSON 출력\n",
    "# ========================================\n",
    "output = {\n",
    "    \"nodes\": spec_list\n",
    "}\n",
    "\n",
    "with open(\"../data/cable_diagram_structured.json\", \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(\"✅ 구조화 JSON 출력 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
